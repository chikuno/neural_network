Greeting is an act of communication in which human beings intentionally make their presence known to each other, to show attention to, and to suggest a type of relationship (usually cordial) or social status (formal or informal) between individuals or groups of people coming in contact with each other. Greetings are sometimes used just prior to a conversation or to greet in passing, such as on a sidewalk or trail. While greeting customs are highly culture- and situation-specific and may change within a culture depending on social status and relationship, they exist in all known human cultures. Greetings can be expressed both audibly and physically, and often involve a combination of the two. This topic excludes military and ceremonial salutes but includes rituals other than gestures. A greeting, or salutation, can also be expressed in written communications, such as letters and emails. Some epochs and cultures have had very elaborate greeting rituals, e.g. greeting a sovereign. Conversely, secret societies have often furtive or arcane greeting gestures and rituals, such as a secret handshake, which allows members to recognize each other. In some languages and cultures, the word or gesture is used as both greeting and farewell. Greeting gestures A greeting can consist of an exchange of formal expression, kisses, handshakes, hugs, and various gestures. The form of greeting is determined by social etiquette, as well as by the relationship of the people. The formal greeting may involve a verbal acknowledgment and sometimes a handshake, but beyond that, facial expression, gestures, body language, and eye contact can all signal what type of greeting is expected. Gestures are the most obvious signal, for instance, greeting someone with open arms is generally a sign that a hug is expected. However, crossing arms can be interpreted as a sign of hostility. The facial expression, body language, and eye contact reflect emotions and interest level. A frown, slouching and lowered eye contact suggests disinterest, while smiling and an exuberant attitude is a sign of welcome. Many different gestures are used throughout the world as simple greetings. In Western cultures, the handshake is very common, though it has numerous subtle variations in the strength of grip, the vigour of the shake, the dominant position of one hand over the other, and whether or not the left hand is used. Historically, when men normally wore hats out of doors, male greetings to people they knew, and sometimes those they did not, involved touching, raising slightly ("tipping"), or removing their hat in a variety of gestures. This basic gesture remained normal in very many situations from the Middle Ages until men typically ceased wearing hats in the mid-20th century. Hat-raising began with an element of recognition of superiority, where only the socially inferior party might perform it, but gradually lost this element; King Louis XIV of France made a point of at least touching his hat to all women he encountered. However, the gesture was never used by women, for whom their head-covering included considerations of modesty. When a man was not wearing a hat he might touch his hair to the side of the front of his head to replicate a hat-tipping gesture. This was typically performed by lower classmen to social superiors, such as peasants to the land-owner, and is known as "tugging the forelock", which still sometimes occurs as a metaphor for submissive behaviour. The Arabic term salaam (literally "peace", from the spoken greeting that accompanies the gesture), refers to the practice of placing the right palm on the heart, before and after a handshake. In Moroccan society, same-sex people do not greet each other the same as do opposite sex. While same-sex people (men or women) will shake hands, kiss on the cheek and even hug multiple times, a man and woman greeting each other in public will not go further than a handshake. This is due to Moroccan culture being conservative. Verbal greetings in Morocco can go from a basic salaam, to asking about life details to make sure the other person is doing well. In the kingdom of Morocco, the greeting should always be made with the right hand, as the left hand is traditionally considered unclean. The most common Chinese greeting, Gongshou, features the right fist placed in the palm of the left hand and both shaken back and forth two or three times, it may be accompanied by a head nod or bow. The gesture may be used on meeting and parting, and when congratulating, thanking, or apologizing. In India, it is common to see the Namaste greeting (or "Sat Sri Akal" for Sikhs) where the palms of the hands are pressed together and held near the heart with the head gently bowed. Among Christians in certain parts of the world such as Poland, the greeting phrase "Praise the Lord" has had common usage, especially in the pre-World War II era. Adab, meaning respect and politeness, is a hand gesture used as a secular greeting in South Asia, especially of Urdu-speaking communities of Uttar Pradesh, Hyderabad, and Bengal in India, as well as among the Muhajir people of Pakistan. The gesture involves raising the right hand towards the face with palm inwards such that it is in front of the eyes and the fingertips are almost touching the forehead, as the upper torso is bent forward. It is typical for the person to say "adab arz hai", or just "adab". It is often answered with the same or the word "Tasleem" is said as an answer or sometimes it is answered with a facial gesture of acceptance. In Indonesia, a nation with a huge variety of cultures and religions, many greetings are expressed, from the formalized greeting of the highly stratified and hierarchical Javanese to the more egalitarian and practical greetings of outer islands. Javanese, Batak and other ethnicities currently or formerly involved in the armed forces will salute a government-employed superior, and follow with a deep bow from the waist or short nod of the head and a passing, loose handshake. Hand position is highly important; the superior's hand must be higher than the inferior's. Muslim men will clasp both hands, palms together at the chest and utter the correct Islamic slametan (greeting) phrase, which may be followed by cheek-to-cheek contact, a quick hug or loose handshake. Pious Muslim women rotate their hands from a vertical to the perpendicular prayer-like position in order to barely touch the fingertips of the male greeter and may opt-out of the cheek-to-cheek contact. If the male is an Abdi Dalem royal servant, courtier or particularly "peko-peko" (taken directly from Japanese to mean obsequious) or even a highly formal individual, he will retreat backwards with head downcast, the left arm crossed against the chest and the right arm hanging down, never showing his side or back to his superior. His head must always be lower than that of his superior. Younger Muslim males and females will clasp their elder's or superior's outstretched hand to the forehead as a sign of respect and obeisance. If a manual worker or a person with obviously dirty hands salute or greets an elder or superior, he will show deference to his superior and avoid contact by bowing, touching the right forehead in a very quick salute or a distant "slamet" gesture. The traditional Javanese Sungkem involves clasping the palms of both hands together, aligning the thumbs with the nose, turning the head downwards and bowing deeply, bending from the knees. In a royal presence, the one performing sungkem would kneel at the base of the throne. A gesture called a wai is used in Thailand, where the hands are placed together palm to palm, approximately at nose level, while bowing. The wai is similar in form to the gesture referred to by the Japanese term gassho by Buddhists. In Thailand, the men and women would usually press two palms together and bow a little while saying "Sawadee ka" (female speaker) or "Sawadee krap" (male speaker). Kisses In Europe, the formal style of upper-class greeting used by a man to a woman in the Early Modern Period was to hold the woman's presented hand (usually the right) with his right hand and kiss it while bowing. In cases of a low degree of intimacy, the hand is held but not kissed. The ultra-formal style, with the man's right knee on the floor, is now only used in marriage proposals, as a romantic gesture. Cheek kissing is common in Europe, parts of Canada (Quebec) and Latin America and has become a standard greeting mainly in Southern Europe but also in some Central European countries. While cheek kissing is a common greeting in many cultures, each country has a unique way of kissing. In Russia, Poland, Slovenia, Serbia, Macedonia, Montenegro, the Netherlands, Iran and Egypt it is customary to "kiss three times, on alternate cheeks". Italians, Spanish, Hungarian, Romanians, Bosnia-and-Herzegovinans usually kiss twice in a greeting and in Mexico and Belgium only one kiss is necessary. In the Galapagos women kiss on the right cheek only and in Oman, it is not unusual for men to kiss one another on the nose after a handshake. French culture accepts a number of ways to greet depending on the region. Two kisses are most common throughout all of France but in Provence three kisses are given and in Nantes four are exchanged. However, in Finistère at the western tip of Brittany and Deux-Sèvres in the Poitou-Charentes region, one kiss is preferred. Other greeting gestures Adab Bowing Cheek kissing Elbow bump Eskimo kissing Fist bump, in which two individuals touch fists Fist-and-palm Hand-kissing Handshake Hat raising or tipping High-five Hug Kowtow Mano (gesture) Namaste Pranāma Pressing noses Salute Sampeah Tehniyat Waving, the gesture of moving one's hand back and forth Wai Vanakkam Spoken greeting A spoken greeting or verbal greeting is a customary or ritualised word or phrase used to introduce oneself or to greet someone. Greeting habits are highly culture- and situation-specific and may change within a culture depending on social status. As with gestures, some languages and cultures use the same word as both greeting and farewell. Examples of colexified greetings are "Good day" in English, "Drud" in Persian, "Sat Shri Akaal" in Punjabi, "As-salamu alaykum" in Arabic, "Aloha" in Hawaiian, "Shalom" in Hebrew, "Namaste" in Hindi, "Ayubowan" in Sri Lanka, "Sawatdi" in Thai and "Ciao" in Italian. In English, some common verbal greetings are: "Hello", "hi", and "hey" — General verbal greetings. The latter two are less formal. According to the Oxford English Dictionary, the first citation of "hey" is found as early as 1225, and is defined as "a call to attract attention . . . an exclamation to express exultation . . . or surprise." The English language's other monosyllabic greeting, "Hi", is actually much newer, having become popular in the 1920s. Many languages use the word as a greeting, though a variety of spellings exist, including "hei" and "hej". "Good morning", "good afternoon", "good evening" — More formal verbal greetings used at the appropriate time of day. The similar "good night" and "good day" are more commonly used as phrases of parting rather than greeting, although in Australian English "G'day" is a very common greeting. "What's up?", "How's it going?" and "What's happening?" — informal greetings used frequently "How do you do?" Has two usages, depending on the country. For example in Ireland it should be treated as a salutation, whereas in England it should be treated as a question that requires an answer. "Howdy" — Informal greeting. Derived from "how do you do," it is common in the rural regions of the United States. Voicemail greetings Voicemail greetings are pre-recorded messages that are automatically played to callers when the answering machine or voicemail system answers the call. Some systems allow for different greetings to be played to different callers. Musical greetings In rural Burundi, familiar women greet each other in a complex interlocking vocal rhythm called akazehe, regardless of the meeting's contextual occasion or time. See also == References ==

Etiquette (/ˈɛtikɛt, -kɪt/) can be defined as a set of norms of personal behavior in polite society, usually occurring in the form of an ethical code of the expected and accepted social behaviors that accord with the conventions and norms observed and practiced by a society, a social class, or a social group. In modern English usage, the French word étiquette (label and tag) dates from the year 1750 and also originates from the French word for "ticket," possibly symbolizing a person’s entry into society through proper behavior. There are many important historical figures that have helped to shape the meaning of the term as well as provide varying perspectives. History In the third millennium BCE, the Ancient Egyptian vizier Ptahhotep wrote The Maxims of Ptahhotep (2375–2350 BCE), a didactic book of precepts extolling civil virtues such as truthfulness, self-control, and kindness towards other people. Recurrent thematic motifs in the maxims include learning by listening to other people, being mindful of the imperfection of human knowledge, that avoiding open conflict whenever possible should not be considered weakness, and that the pursuit of justice should be foremost. Yet, in human affairs, the command of a god ultimately prevails in all matters. Some of Ptahhotep's maxims indicate a person's correct behaviours in the presence of great personages (political, military, religious), and instructions on how to choose the right master and how to serve him. Other maxims teach the correct way to be a leader through openness and kindness, that greed is the base of all evil and should be guarded against, and that generosity towards family and friends is praiseworthy. Confucius (551–479 BCE) was a Chinese intellectual and philosopher whose works emphasized personal and governmental morality, correctness of social relationships, the pursuit of justice in personal dealings, and sincerity in all personal relations. Baldassare Castiglione (1478–1529 CE), count of Casatico, was an Italian courtier and diplomat, soldier, and author of The Book of the Courtier (1528), an exemplar courtesy book dealing with questions of the etiquette and morality of the courtier during the Italian Renaissance. Louis XIV (1638–1715), King of France, used a codified etiquette to tame the French nobility and assert his supremacy as the absolute monarch of France. In consequence, the ceremonious royal court favourably impressed foreign dignitaries whom the king received at the seat of French government, the Palace of Versailles, to the south-west of Paris. Benjamin Franklin (1706–1790), an American inventor and Founding Father, contributed to the American understanding of etiquette through his emphasis on practical morality and social harmony. In his autobiography published in 1791, Franklin outlined a personal system of self-improvement centered around thirteen virtues, including sincerity, humility, and temperance. He viewed etiquette as a means of fostering effective communication, avoiding unnecessary conflict, and promoting cooperation in both personal and public life. Franklin distrusted ostentatious formality and believed manners should serve a purpose rooted in usefulness, sincerity, and democratic ideals. George Washington (1732–1799), the first President of the United States and commander of the Continental Army during the American Revolutionary War, was heavily influenced in his youth by a set of social maxims titled Rules of Civility & Decent Behaviour in Company and Conversation. Adapted from an earlier French text, these 110 rules emphasized humility, respect for others, restraint, and the importance of maintaining decorum in public life. Though Washington did not write the rules himself, copying them by hand served as early moral training and significantly shaped his public persona. The maxims promoted the idea that civil behavior was a reflection of personal virtue and that etiquette could serve as a tool for cultivating leadership and moral character. Despite George Washington’s strong public support for education, many of his contemporaries criticized his intellect, labeling him as poorly educated and lacking eloquence. Figures like Aaron Burr, Thomas Jefferson, and John Adams described him as unrefined, grammatically weak, and intellectually limited. Due to Washington’s personal sensitivity to the level of his academic exposure, these critiques only increased the motivation to copy the 110 rules. Although there may not be any evidence of George Washington verbally passing on the maxims, his actions and character served as a physical example of these beliefs. Politeness In the 18th century, during the Age of Enlightenment, the adoption of etiquette was a self-conscious process for acquiring the conventions of politeness and the normative behaviours (charm, manners, demeanour) which symbolically identified the person as a genteel member of the upper class. To identify with the social élite, the upwardly mobile middle class and the bourgeoisie adopted the behaviours and the artistic preferences of the upper class. To that end, socially ambitious people of the middle classes occupied themselves with learning, knowing, and practising the rules of social etiquette, such as the arts of elegant dress and gracious conversation, when to show emotion, and courtesy with and towards women. In the early 18th century, Anthony Ashley-Cooper, 3rd Earl of Shaftesbury, wrote influential essays that defined politeness as the art of being pleasing in company; and discussed the function and nature of politeness in the social discourse of a commercial society: 'Politeness' may be defined as dext'rous management of our words and actions, whereby we make other people have better opinion of us and themselves. Periodicals, such as The Spectator, a daily publication founded in 1711 by Joseph Addison and Richard Steele, regularly advised their readers on the etiquette required of a gentleman, a man of good and courteous conduct; their stated editorial goal was "to enliven morality with wit, and to temper wit with morality… to bring philosophy out of the closets and libraries, schools and colleges, to dwell in clubs and assemblies, at tea-tables and coffeehouses"; to which end, the editors published articles written by educated authors, which provided topics for civil conversation, and advice on the requisite manners for carrying a polite conversation, and for managing social interactions. Conceptually allied to etiquette is the notion of civility (social interaction characterised by sober and reasoned debate) which for socially ambitious men and women also became an important personal quality to possess for social advancement. In the event, gentlemen's clubs, such as Harrington's Rota Club, published an in-house etiquette that codified the civility expected of the members. Besides The Spectator, other periodicals sought to infuse politeness into English coffeehouse conversation, the editors of The Tatler were explicit that their purpose was the reformation of English manners and morals; to those ends, etiquette was presented as the virtue of morality and a code of behaviour. In the mid-18th century, the first, modern English usage of etiquette (the conventional rules of personal behaviour in polite society) was by Philip Stanhope, 4th Earl of Chesterfield, in the book Letters to His Son on the Art of Becoming a Man of the World and a Gentleman (1774), a correspondence of more than 400 letters written from 1737 until the death of his son, in 1768; most of the letters were instructive, concerning varied subjects that a worldly gentleman should know. The letters were first published in 1774, by Eugenia Stanhope, the widow of the diplomat Philip Stanhope, Chesterfield's bastard son. Throughout the correspondence, Chesterfield endeavoured to decouple the matter of social manners from conventional morality, with perceptive observations that pragmatically argue to Philip that mastery of etiquette was an important means for social advancement, for a man such as he. Chesterfield's elegant, literary style of writing epitomised the emotional restraint characteristic of polite social intercourse in 18th-century society: I would heartily wish that you may often be seen to smile, but never heard to laugh while you live. Frequent and loud laughter is the characteristic of folly and ill-manners; it is the manner in which the mob express their silly joy at silly things; and they call it being merry. In my mind there is nothing so illiberal, and so ill-bred, as audible laughter. I am neither of a melancholy nor a cynical disposition, and am as willing and as apt to be pleased as anybody; but I am sure that since I have had the full use of my reason nobody has ever heard me laugh. In the 19th century, Victorian era (1837–1901) etiquette developed into a complicated system of codified behaviours, which governed the range of manners in society—from the proper language, style, and method for writing letters, to correctly using cutlery at table, and to the minute regulation of social relations and personal interactions between men and women and among the social classes. In the 21st century, specifically in the early 2020s as digital communication became more readily available and used in everyday life, the notion of digital etiquette, or netiquette, evolved into a flexible, socially negotiated code of conduct guiding behavior in online spaces. Unlike traditional etiquette, which often revolved around visible symbols of status and formal conduct, digital etiquette today is platform-dependent, highly situational, and subtly influenced by unspoken social norms. For instance, a video call may press for visible presence (“camera on”) and active engagement such as contributing ideas or giving visual cues of attention, while sending an email might demand carefully crafted language, formal greetings, and rapid response times to signal competence and respect. An essential aspect of today’s netiquette is the management of presence and attention. The expectation to be responsive has become a symbol of respect, while behaviors such as multitasking during meetings or disabling cameras may be interpreted as disrespect or disengagement. Manners Sociological perspectives In a society, manners are described as either good manners or as bad manners to indicate whether a person's behaviour is acceptable to the cultural group. As such, manners enable ultrasociality and are integral to the functioning of the social norms and conventions that are informally enforced through self-regulation. The perspectives of sociology indicate that manners are a means for people to display their social status, and a means of demarcating, observing, and maintaining the boundaries of social identity and of social class. In The Civilizing Process (1939), sociologist Norbert Elias said that manners arose as a product of group living, and persist as a way of maintaining social order. Manners proliferated during the Renaissance in response to the development of the 'absolute state'—the progression from small-group living to large-group living characterised by the centralized power of the State. The rituals and manners associated with the royal court of England during that period were closely bound to a person's social status. Manners demonstrate a person's position within a social network, and a person's manners are a means of negotiation from that social position. From the perspective of public health, in The Healthy Citizen (1995), Alana R. Petersen and Deborah Lupton said that manners assisted the diminishment of the social boundaries that existed between the public sphere and the private sphere of a person's life, and so gave rise to "a highly reflective self, a self who monitors his or her behavior with due regard for others with whom he or she interacts, socially"; and that "the public behavior of individuals came to signify their social standing; a means of presenting the self and of evaluating others, and thus the control of the outward self was vital." Sociologist Pierre Bourdieu applied the concept of habitus to define the societal functions of manners. The habitus is the set of mental attitudes, personal habits, and skills that a person possesses—his or her dispositions of character that are neither self-determined, nor pre-determined by the external environment, but which are produced and reproduced by social interactions—and are "inculcated through experience and explicit teaching", yet tend to function at the subconscious level. Manners are likely to be a central part of the dispositions that guide a person's ability to decide upon socially-compliant behaviours. Anthropologic perspective In Purity and Danger: An Analysis of Concepts of Pollution and Taboo (2003) the anthropologist Mary Douglas said that manners, social behaviors, and group rituals enable the local cosmology to remain ordered and free from those things that may pollute or defile the integrity of the culture. Ideas of pollution, defilement, and disgust are attached to the margins of socially acceptable behaviour in order to curtail unacceptable behaviour, and so maintain "the assumptions by which experience is controlled" within the culture. Evolutionary perspectives In studying the expression of emotion by humans and animals, naturalist Charles Darwin noted the universality of facial expressions of disgust and shame among infants and blind people, and concluded that the emotional responses of shame and disgust are innate behaviours. Public health specialist Valerie Curtis said that the development of facial responses was concomitant with the development of manners, which are behaviours with an evolutionary role in preventing the transmission of diseases, thus, people who practise personal hygiene and politeness will most benefit from membership in their social group, and so stand the best chance of biological survival, by way of opportunities for reproduction. From the study of the evolutionary bases of prejudice, social psychologists Catherine Cottrell and Steven Neuberg said that human behavioural responses to 'otherness' might enable the preservation of manners and social norms. The feeling of "foreignness"—which people experience in their first social interaction with someone from another culture—might partly serve an evolutionary function: 'Group living surrounds one with individuals [who are] able to physically harm fellow group members, to spread contagious disease, or to "free ride" on their efforts'; therefore, a commitment to sociality is a risk: 'If threats, such as these, are left unchecked, the costs of sociality will quickly exceed its benefits. Thus, to maximize the returns on group "living", individual group members should be attuned to others' features or behaviors.' Therefore, people who possess the social traits common to the cultural group are to be trusted, and people without the common social traits are to be distrusted as 'others', and thus treated with suspicion or excluded from the group. That pressure of social exclusivity, born from the shift towards communal living, excluded uncooperative people and persons with poor personal hygiene. The threat of social exclusion led people to avoid personal behaviours that might embarrass the group or that might provoke revulsion among the group. To demonstrate the transmission of social conformity, anthropologists Joseph Henrich and Robert Boyd developed a behavioural model in which manners are a means of mitigating social differences, curbing undesirable personal behaviours, and fostering co-operation within the social group. Natural selection favoured the acquisition of genetically transmitted mechanisms for learning, thereby increasing a person's chances for acquiring locally adaptive behaviours: "Humans possess a reliably developing neural encoding that compels them both to punish individuals who violate group norms (common beliefs or practices) and [to] punish individuals who do not punish norm-violators." Categories Social manners are in three categories: (i) manners of hygiene, (ii) manners of courtesy, and (iii) manners of cultural norm. Each category accounts for an aspect of the functional role that manners play in a society. The categories of manners are based upon the social outcome of behaviour, rather than upon the personal motivation of the behaviour. As a means of social management, the rules of etiquette encompass most aspects of human social interaction; thus, a rule of etiquette reflects an underlying ethical code and a person's fashion and social status. Manners of hygiene concern avoiding the transmission of disease, and usually are taught by the parent to the child by way of parental discipline, positive behavioural enforcement of body-fluid continence (toilet training), and the avoidance of and removal of disease vectors that risk the health of children. Society expects that by adulthood the manners for personal hygiene have become a second-nature behaviour, violations of which shall provoke physical and moral disgust. Hygiene etiquette during the COVID-19 pandemic included social distancing and warnings against public spitting. Manners of courtesy concern self-control and good-faith behaviour, by which a person gives priority to the interests of another person, and priority to the interests of a socio-cultural group, in order to be a trusted member of that group. Courtesy manners maximize the benefits of group-living by regulating the nature of social interactions; however, the performance of courtesy manners occasionally interferes with the avoidance of communicable disease. Generally, parents teach courtesy manners in the same way they teach hygiene manners, but the child also learns manners directly (by observing the behaviour of other people in their social interactions) and by imagined social interactions (through the executive functions of the brain). A child usually learns courtesy manners at an older age than when he or she was toilet trained (taught hygiene manners), because learning the manners of courtesy requires that the child be self-aware and conscious of social position, which then facilitate understanding that violations (accidental or deliberate) of social courtesy will provoke peer disapproval within the social group. Manners of cultural norms concern the social rules by which a person establishes his or her identity and membership in a given socio-cultural group. In abiding the manners of cultural norm, a person demarcates socio-cultural identity and establishes social boundaries, which then identify whom to trust and whom to distrust as 'the other'. Cultural norm manners are learnt through the enculturation with and the routinisation of 'the familiar', and through social exposure to the 'cultural otherness' of people identified as foreign to the group. Transgressions and flouting of the manners of cultural norm usually result in the social alienation of the transgressor. The nature of culture-norm manners allows a high level of intra-group variability, but the manners usually are common to the people who identify with the given socio-cultural group. Courtesy books 16th century The Book of the Courtier (1528), by Baldassare Castiglione, identified the manners and the morals required by socially ambitious men and women for success in a royal court of the Italian Renaissance (14th–17th c.); as an etiquette text, The Courtier was an influential courtesy book in 16th-century Europe. On Civility in Children (1530), by Erasmus of Rotterdam, instructs boys in the means of becoming a young man; how to walk and talk, speak and act in the company of adults. The practical advice for acquiring adult self-awareness includes explanations of the symbolic meanings—for adults—of a boy's body language when he is fidgeting and yawning, scratching and bickering. On completing Erasmus's curriculum of etiquette, the boy has learnt that civility is the point of good manners: the adult ability to 'readily ignore the faults of others, but avoid falling short, yourself,' in being civilised. 20th century Etiquette in Society, in Business, in Politics, and at Home (1922), by Emily Post documents the "trivialities" of desirable conduct in daily life, and provided pragmatic approaches to the practice of good manners—the social conduct expected and appropriate for the events of life, such as a baptism, a wedding, and a funeral. As didactic texts, books of etiquette (the conventional rules of personal behaviour in polite society) usually feature explanatory titles, such as The Ladies' Book of Etiquette, and Manual of Politeness: A Complete Hand Book for the Use of the Lady in Polite Society (1860), by Florence Hartley; Amy Vanderbilt's Complete Book of Etiquette (1957); Miss Manners' Guide to Excruciatingly Correct Behavior (1979), by Judith Martin; and Peas & Queues: The Minefield of Modern Manners (2013), by Sandi Toksvig. Such books present ranges of civility, socially acceptable behaviours for their respective times. Each author cautions the reader that to be a well-mannered person they must practise good manners in their public and private lives. The How Rude! comic-book series addresses and discusses adolescent perspectives and questions of etiquette, social manners, and civility. Business In commerce, the purpose of etiquette is to facilitate the social relations necessary for realising business transactions; in particular, social interactions among workers, and between labour and management. Business etiquette varies by culture, such as the Chinese and Australian approaches to conflict resolution. The Chinese business philosophy is based upon guanxi (personal connections), whereby person-to-person negotiation resolves difficult matters, whereas Australian business philosophy relies upon attorneys-at-law to resolve business conflicts through legal mediation; thus, adjusting to the etiquette and professional ethics of another culture is an element of culture shock for businesspeople. In 2011, etiquette trainers formed the Institute of Image Training and Testing International (IITTI) a non-profit organisation to train personnel departments in measuring and developing and teaching social skills to employees, by way of education in the rules of personal and business etiquette, in order to produce business workers who possess standardised manners for successfully conducting business with people from other cultures. In the retail branch of commerce, the saying "the customer is always right" summarises the profit-orientation of good manners, between the buyer and the seller of goods and services: There are always two sides to the case, of course, and it is a credit to good manners that there is scarcely ever any friction in stores and shops of the first class. Salesmen and women are usually persons who are both patient and polite, and their customers are most often ladies in fact as well as "by courtesy." Between those before and those behind the counters, there has sprung up in many instances a relationship of mutual goodwill and friendliness. It is, in fact, only the woman who is afraid that someone may encroach upon her exceedingly insecure dignity, who shows neither courtesy nor consideration to any except those whom she considers it to her advantage to please. See also References == Further reading ==

Philosophy ('love of wisdom' in Ancient Greek) is a systematic study of general and fundamental questions concerning topics like existence, reason, knowledge, value, beauty, mind, and language. It is a rational and critical inquiry that reflects on its methods and assumptions. Historically, many of the individual sciences, such as physics and psychology, formed part of philosophy. However, they are considered separate academic disciplines in the modern sense of the term. Influential traditions in the history of philosophy include Western, Arabic–Persian, Indian, and Chinese philosophy. Western philosophy originated in Ancient Greece and covers a wide area of philosophical subfields. A central topic in Arabic–Persian philosophy is the relation between reason and revelation. Indian philosophy combines the spiritual problem of how to reach enlightenment with the exploration of the nature of reality and the ways of arriving at knowledge. Chinese philosophy focuses principally on practical issues about right social conduct, government, and self-cultivation. Major branches of philosophy are epistemology, ethics, logic, and metaphysics. Epistemology studies what knowledge is and how to acquire it. Ethics investigates moral principles and what constitutes right conduct. Logic is the study of correct reasoning and explores how good arguments can be distinguished from bad ones. Metaphysics examines the most general features of reality, existence, objects, and properties. Other subfields are aesthetics, philosophy of language, philosophy of mind, philosophy of religion, philosophy of science, philosophy of mathematics, philosophy of history, and political philosophy. Within each branch, there are competing schools of philosophy that promote different principles, theories, or methods. Philosophers use a great variety of methods to arrive at philosophical knowledge. They include conceptual analysis, reliance on common sense and intuitions, use of thought experiments, analysis of ordinary language, description of experience, and critical questioning. Philosophy is related to many other fields, such as the natural and social sciences, mathematics, business, law, and journalism. It provides an interdisciplinary perspective and studies the scope and fundamental concepts of these fields. It also investigates their methods and ethical implications. Etymology The word philosophy comes from the Ancient Greek words φίλος (philos) 'love' and σοφία (sophia) 'wisdom'. Some sources say that the term was coined by the pre-Socratic philosopher Pythagoras, but this is not certain. The word entered the English language primarily from Old French and Anglo-Norman starting around 1175 CE. The French philosophie is itself a borrowing from the Latin philosophia. The term philosophy acquired the meanings of "advanced study of the speculative subjects (logic, ethics, physics, and metaphysics)", "deep wisdom consisting of love of truth and virtuous living", "profound learning as transmitted by the ancient writers", and "the study of the fundamental nature of knowledge, reality, and existence, and the basic limits of human understanding". Before the modern age, the term philosophy was used in a wide sense. It included most forms of rational inquiry, such as the individual sciences, as its subdisciplines. For instance, natural philosophy was a major branch of philosophy. This branch of philosophy encompassed a wide range of fields, including disciplines like physics, chemistry, and biology. An example of this usage is the 1687 book Philosophiæ Naturalis Principia Mathematica by Isaac Newton. This book referred to natural philosophy in its title, but it is today considered a book of physics. The meaning of philosophy changed toward the end of the modern period when it acquired the more narrow meaning common today. In this new sense, the term is mainly associated with disciplines like metaphysics, epistemology, and ethics. Among other topics, it covers the rational study of reality, knowledge, and values. It is distinguished from other disciplines of rational inquiry such as the empirical sciences and mathematics. Conceptions of philosophy General conception The practice of philosophy is characterized by several general features: it is a form of rational inquiry, it aims to be systematic, and it tends to critically reflect on its own methods and presuppositions. It requires attentively thinking long and carefully about the provocative, vexing, and enduring problems central to the human condition. The philosophical pursuit of wisdom involves asking general and fundamental questions. It often does not result in straightforward answers but may help a person to better understand the topic, examine their life, dispel confusion, and overcome prejudices and self-deceptive ideas associated with common sense. For example, Socrates stated that "the unexamined life is not worth living" to highlight the role of philosophical inquiry in understanding one's own existence. And according to Bertrand Russell, "the man who has no tincture of philosophy goes through life imprisoned in the prejudices derived from common sense, from the habitual beliefs of his age or his nation, and from convictions which have grown up in his mind without the cooperation or consent of his deliberate reason." Academic definitions Attempts to provide more precise definitions of philosophy are controversial and are studied in metaphilosophy. Some approaches argue that there is a set of essential features shared by all parts of philosophy. Others see only weaker family resemblances or contend that it is merely an empty blanket term. Precise definitions are often only accepted by theorists belonging to a certain philosophical movement and are revisionistic according to Søren Overgaard et al. in that many presumed parts of philosophy would not deserve the title "philosophy" if they were true. Some definitions characterize philosophy in relation to its method, like pure reasoning. Others focus on its topic, for example, as the study of the biggest patterns of the world as a whole or as the attempt to answer the big questions. Such an approach is pursued by Immanuel Kant, who holds that the task of philosophy is united by four questions: "What can I know?"; "What should I do?"; "What may I hope?"; and "What is the human being?" Both approaches have the problem that they are usually either too wide, by including non-philosophical disciplines, or too narrow, by excluding some philosophical sub-disciplines. Many definitions of philosophy emphasize its intimate relation to science. In this sense, philosophy is sometimes understood as a proper science in its own right. According to some naturalistic philosophers, such as W. V. O. Quine, philosophy is an empirical yet abstract science that is concerned with wide-ranging empirical patterns instead of particular observations. Science-based definitions usually face the problem of explaining why philosophy in its long history has not progressed to the same extent or in the same way as the sciences. This problem is avoided by seeing philosophy as an immature or provisional science whose subdisciplines cease to be philosophy once they have fully developed. In this sense, philosophy is sometimes described as "the midwife of the sciences". Other definitions focus on the contrast between science and philosophy. A common theme among many such conceptions is that philosophy is concerned with meaning, understanding, or the clarification of language. According to one view, philosophy is conceptual analysis, which involves finding the necessary and sufficient conditions for the application of concepts. Another definition characterizes philosophy as thinking about thinking to emphasize its self-critical, reflective nature. A further approach presents philosophy as a linguistic therapy. According to Ludwig Wittgenstein, for instance, philosophy aims at dispelling misunderstandings to which humans are susceptible due to the confusing structure of ordinary language. Phenomenologists, such as Edmund Husserl, characterize philosophy as a "rigorous science" investigating essences. They practice a radical suspension of theoretical assumptions about reality to get back to the "things themselves", that is, as originally given in experience. They contend that this base-level of experience provides the foundation for higher-order theoretical knowledge, and that one needs to understand the former to understand the latter. An early approach found in ancient Greek and Roman philosophy is that philosophy is the spiritual practice of developing one's rational capacities. This practice is an expression of the philosopher's love of wisdom and has the aim of improving one's well-being by leading a reflective life. For example, the Stoics saw philosophy as an exercise to train the mind and thereby achieve eudaimonia and flourish in life. History As a discipline, the history of philosophy aims to provide a systematic and chronological exposition of philosophical concepts and doctrines. Some theorists see it as a part of intellectual history, but it also investigates questions not covered by intellectual history such as whether the theories of past philosophers are true and have remained philosophically relevant. The history of philosophy is primarily concerned with theories based on rational inquiry and argumentation; some historians understand it in a looser sense that includes myths, religious teachings, and proverbial lore. Influential traditions in the history of philosophy include Western, Arabic–Persian, Indian, and Chinese philosophy. Other philosophical traditions are Japanese philosophy, Latin American philosophy, and African philosophy. Western Western philosophy originated in Ancient Greece in the 6th century BCE with the pre-Socratics. They attempted to provide rational explanations of the cosmos as a whole. The philosophy following them was shaped by Socrates (469–399 BCE), Plato (427–347 BCE), and Aristotle (384–322 BCE). They expanded the range of topics to questions like how people should act, how to arrive at knowledge, and what the nature of reality and mind is. The later part of the ancient period was marked by the emergence of philosophical movements, for example, Epicureanism, Stoicism, Skepticism, and Neoplatonism. The medieval period started in the 5th century CE. Its focus was on religious topics and many thinkers used ancient philosophy to explain and further elaborate Christian doctrines. The Renaissance period started in the 14th century and saw a renewed interest in schools of ancient philosophy, in particular Platonism. Humanism also emerged in this period. The modern period started in the 17th century. One of its central concerns was how philosophical and scientific knowledge are created. Specific importance was given to the role of reason and sensory experience. Many of these innovations were used in the Enlightenment movement to challenge traditional authorities. Several attempts to develop comprehensive systems of philosophy were made in the 19th century, for instance, by German idealism and Marxism. Influential developments in 20th-century philosophy were the emergence and application of formal logic, the focus on the role of language as well as pragmatism, and movements in continental philosophy like phenomenology, existentialism, and post-structuralism. The 20th century saw a rapid expansion of academic philosophy in terms of the number of philosophical publications and philosophers working at academic institutions. There was also a noticeable growth in the number of female philosophers, but they still remained underrepresented. Arabic–Persian Arabic–Persian philosophy arose in the early 9th century CE as a response to discussions in the Islamic theological tradition. Its classical period lasted until the 12th century CE and was strongly influenced by ancient Greek philosophers. It employed their ideas to elaborate and interpret the teachings of the Quran. Al-Kindi (801–873 CE) is usually regarded as the first philosopher of this tradition. He translated and interpreted many works of Aristotle and Neoplatonists in his attempt to show that there is a harmony between reason and faith. Avicenna (980–1037 CE) also followed this goal and developed a comprehensive philosophical system to provide a rational understanding of reality encompassing science, religion, and mysticism. Al-Ghazali (1058–1111 CE) was a strong critic of the idea that reason can arrive at a true understanding of reality and God. He formulated a detailed critique of philosophy and tried to assign philosophy a more limited place besides the teachings of the Quran and mystical insight. Following Al-Ghazali and the end of the classical period, the influence of philosophical inquiry waned. Mulla Sadra (1571–1636 CE) is often regarded as one of the most influential philosophers of the subsequent period. The increasing influence of Western thought and institutions in the 19th and 20th centuries gave rise to the intellectual movement of Islamic modernism, which aims to understand the relation between traditional Islamic beliefs and modernity. Indian One of the distinguishing features of Indian philosophy is that it integrates the exploration of the nature of reality, the ways of arriving at knowledge, and the spiritual question of how to reach enlightenment. It started around 900 BCE when the Vedas were written. They are the foundational scriptures of Hinduism and contemplate issues concerning the relation between the self and ultimate reality as well as the question of how souls are reborn based on their past actions. This period also saw the emergence of non-Vedic teachings, like Buddhism and Jainism. Buddhism was founded by Gautama Siddhartha (563–483 BCE), who challenged the Vedic idea of a permanent self and proposed a path to liberate oneself from suffering. Jainism was founded by Mahavira (599–527 BCE), who emphasized non-violence as well as respect toward all forms of life. The subsequent classical period started roughly 200 BCE and was characterized by the emergence of the six orthodox schools of Hinduism: Nyāyá, Vaiśeṣika, Sāṃkhya, Yoga, Mīmāṃsā, and Vedanta. The school of Advaita Vedanta developed later in this period. It was systematized by Adi Shankara (c. 700–750 CE), who held that everything is one and that the impression of a universe consisting of many distinct entities is an illusion. A slightly different perspective was defended by Ramanuja (1017–1137 CE), who founded the school of Vishishtadvaita Vedanta and argued that individual entities are real as aspects or parts of the underlying unity. He also helped to popularize the Bhakti movement, which taught devotion toward the divine as a spiritual path and lasted until the 17th to 18th centuries CE. The modern period began roughly 1800 CE and was shaped by encounters with Western thought. Philosophers tried to formulate comprehensive systems to harmonize diverse philosophical and religious teachings. For example, Swami Vivekananda (1863–1902 CE) used the teachings of Advaita Vedanta to argue that all the different religions are valid paths toward the one divine. Chinese Chinese philosophy is particularly interested in practical questions associated with right social conduct, government, and self-cultivation. Many schools of thought emerged in the 6th century BCE in competing attempts to resolve the political turbulence of that period. The most prominent among them were Confucianism and Daoism. Confucianism was founded by Confucius (551–479 BCE). It focused on different forms of moral virtues and explored how they lead to harmony in society. Daoism was founded by Laozi (6th century BCE) and examined how humans can live in harmony with nature by following the Dao or the natural order of the universe. Other influential early schools of thought were Mohism, which developed an early form of altruistic consequentialism, and Legalism, which emphasized the importance of a strong state and strict laws. Buddhism was introduced to China in the 1st century CE and diversified into new forms of Buddhism. Starting in the 3rd century CE, the school of Xuanxue emerged. It interpreted earlier Daoist works with a specific emphasis on metaphysical explanations. Neo-Confucianism developed in the 11th century CE. It systematized previous Confucian teachings and sought a metaphysical foundation of ethics. The modern period in Chinese philosophy began in the early 20th century and was shaped by the influence of and reactions to Western philosophy. The emergence of Chinese Marxism—which focused on class struggle, socialism, and communism—resulted in a significant transformation of the political landscape. Another development was the emergence of New Confucianism, which aims to modernize and rethink Confucian teachings to explore their compatibility with democratic ideals and modern science. Other traditions Traditional Japanese philosophy assimilated and synthesized ideas from different traditions, including the indigenous Shinto religion and Chinese and Indian thought in the forms of Confucianism and Buddhism, both of which entered Japan in the 6th and 7th centuries. Its practice is characterized by active interaction with reality rather than disengaged examination. Neo-Confucianism became an influential school of thought in the 16th century and the following Edo period and prompted a greater focus on language and the natural world. The Kyoto School emerged in the 20th century and integrated Eastern spirituality with Western philosophy in its exploration of concepts like absolute nothingness (zettai-mu), place (basho), and the self. Latin American philosophy in the pre-colonial period was practiced by indigenous civilizations and explored questions concerning the nature of reality and the role of humans. It has similarities to indigenous North American philosophy, which covered themes such as the interconnectedness of all things. Latin American philosophy during the colonial period, starting around 1550, was dominated by religious philosophy in the form of scholasticism. Influential topics in the post-colonial period were positivism, the philosophy of liberation, and the exploration of identity and culture. Early African philosophy was primarily conducted and transmitted orally. It focused on community, morality, and ancestral ideas, encompassing folklore, wise sayings, religious ideas, and philosophical concepts like Ubuntu. Systematic African philosophy emerged at the beginning of the 20th century. It discusses topics such as ethnophilosophy, négritude, pan-Africanism, Marxism, postcolonialism, the role of cultural identity, relativism, African epistemology, and the critique of Eurocentrism. Core branches Philosophical questions can be grouped into several branches. These groupings allow philosophers to focus on a set of similar topics and interact with other thinkers who are interested in the same questions. Epistemology, ethics, logic, and metaphysics are sometimes listed as the main branches. There are many other subfields besides them and the different divisions are neither exhaustive nor mutually exclusive. For example, political philosophy, ethics, and aesthetics are sometimes linked under the general heading of value theory as they investigate normative or evaluative aspects. Furthermore, philosophical inquiry sometimes overlaps with other disciplines in the natural and social sciences, religion, and mathematics. Epistemology Epistemology is the branch of philosophy that studies knowledge. It is also known as theory of knowledge and aims to understand what knowledge is, how it arises, what its limits are, and what value it has. It further examines the nature of truth, belief, justification, and rationality. Some of the questions addressed by epistemologists include "By what method(s) can one acquire knowledge?"; "How is truth established?"; and "Can we prove causal relations?" Epistemology is primarily interested in declarative knowledge or knowledge of facts, like knowing that Princess Diana died in 1997. But it also investigates practical knowledge, such as knowing how to ride a bicycle, and knowledge by acquaintance, for example, knowing a celebrity personally. One area in epistemology is the analysis of knowledge. It assumes that declarative knowledge is a combination of different parts and attempts to identify what those parts are. An influential theory in this area claims that knowledge has three components: it is a belief that is justified and true. This theory is controversial and the difficulties associated with it are known as the Gettier problem. Alternative views state that knowledge requires additional components, like the absence of luck; different components, like the manifestation of cognitive virtues instead of justification; or they deny that knowledge can be analyzed in terms of other phenomena. Another area in epistemology asks how people acquire knowledge. Often-discussed sources of knowledge are perception, introspection, memory, inference, and testimony. According to empiricists, all knowledge is based on some form of experience. Rationalists reject this view and hold that some forms of knowledge, like innate knowledge, are not acquired through experience. The regress problem is a common issue in relation to the sources of knowledge and the justification they offer. It is based on the idea that beliefs require some kind of reason or evidence to be justified. The problem is that the source of justification may itself be in need of another source of justification. This leads to an infinite regress or circular reasoning. Foundationalists avoid this conclusion by arguing that some sources can provide justification without requiring justification themselves. Another solution is presented by coherentists, who state that a belief is justified if it coheres with other beliefs of the person. Many discussions in epistemology touch on the topic of philosophical skepticism, which raises doubts about some or all claims to knowledge. These doubts are often based on the idea that knowledge requires absolute certainty and that humans are unable to acquire it. Ethics Ethics, also known as moral philosophy, studies what constitutes right conduct. It is also concerned with the moral evaluation of character traits and institutions. It explores what the standards of morality are and how to live a good life. Philosophical ethics addresses such basic questions as "Are moral obligations relative?"; "Which has priority: well-being or obligation?"; and "What gives life meaning?" The main branches of ethics are meta-ethics, normative ethics, and applied ethics. Meta-ethics asks abstract questions about the nature and sources of morality. It analyzes the meaning of ethical concepts, like right action and obligation. It also investigates whether ethical theories can be true in an absolute sense and how to acquire knowledge of them. Normative ethics encompasses general theories of how to distinguish between right and wrong conduct. It helps guide moral decisions by examining what moral obligations and rights people have. Applied ethics studies the consequences of the general theories developed by normative ethics in specific situations, for example, in the workplace or for medical treatments. Within contemporary normative ethics, consequentialism, deontology, and virtue ethics are influential schools of thought. Consequentialists judge actions based on their consequences. One such view is utilitarianism, which argues that actions should increase overall happiness while minimizing suffering. Deontologists judge actions based on whether they follow moral duties, such as abstaining from lying or killing. According to them, what matters is that actions are in tune with those duties and not what consequences they have. Virtue theorists judge actions based on how the moral character of the agent is expressed. According to this view, actions should conform to what an ideally virtuous agent would do by manifesting virtues like generosity and honesty. Logic Logic is the study of correct reasoning. It aims to understand how to distinguish good from bad arguments. It is usually divided into formal and informal logic. Formal logic uses artificial languages with a precise symbolic representation to investigate arguments. In its search for exact criteria, it examines the structure of arguments to determine whether they are correct or incorrect. Informal logic uses non-formal criteria and standards to assess the correctness of arguments. It relies on additional factors such as content and context. Logic examines a variety of arguments. Deductive arguments are mainly studied by formal logic. An argument is deductively valid if the truth of its premises ensures the truth of its conclusion. Deductively valid arguments follow a rule of inference, like modus ponens, which has the following logical form: "p; if p then q; therefore q". An example is the argument "today is Sunday; if today is Sunday then I don't have to go to work today; therefore I don't have to go to work today". The premises of non-deductive arguments also support their conclusion, although this support does not guarantee that the conclusion is true. One form is inductive reasoning. It starts from a set of individual cases and uses generalization to arrive at a universal law governing all cases. An example is the inference that "all ravens are black" based on observations of many individual black ravens. Another form is abductive reasoning. It starts from an observation and concludes that the best explanation of this observation must be true. This happens, for example, when a doctor diagnoses a disease based on the observed symptoms. Logic also investigates incorrect forms of reasoning. They are called fallacies and are divided into formal and informal fallacies based on whether the source of the error lies only in the form of the argument or also in its content and context. Metaphysics Metaphysics is the study of the most general features of reality, such as existence, objects and their properties, wholes and their parts, space and time, events, and causation. There are disagreements about the precise definition of the term and its meaning has changed throughout the ages. Metaphysicians attempt to answer basic questions including "Why is there something rather than nothing?"; "Of what does reality ultimately consist?"; and "Are humans free?" Metaphysics is sometimes divided into general metaphysics and specific or special metaphysics. General metaphysics investigates being as such. It examines the features that all entities have in common. Specific metaphysics is interested in different kinds of being, the features they have, and how they differ from one another. An important area in metaphysics is ontology. Some theorists identify it with general metaphysics. Ontology investigates concepts like being, becoming, and reality. It studies the categories of being and asks what exists on the most fundamental level. Another subfield of metaphysics is philosophical cosmology. It is interested in the essence of the world as a whole. It asks questions including whether the universe has a beginning and an end and whether it was created by something else. A key topic in metaphysics concerns the question of whether reality only consists of physical things like matter and energy. Alternative suggestions are that mental entities (such as souls and experiences) and abstract entities (such as numbers) exist apart from physical things. Another topic in metaphysics concerns the problem of identity. One question is how much an entity can change while still remaining the same entity. According to one view, entities have essential and accidental features. They can change their accidental features but they cease to be the same entity if they lose an essential feature. A central distinction in metaphysics is between particulars and universals. Universals, like the color red, can exist at different locations at the same time. This is not the case for particulars including individual persons or specific objects. Other metaphysical questions are whether the past fully determines the present and what implications this would have for the existence of free will. Other major branches There are many other subfields of philosophy besides its core branches. Some of the most prominent are aesthetics, philosophy of language, philosophy of mind, philosophy of religion, philosophy of science, and political philosophy. Aesthetics in the philosophical sense is the field that studies the nature and appreciation of beauty and other aesthetic properties, like the sublime. Although it is often treated together with the philosophy of art, aesthetics is a broader category that encompasses other aspects of experience, such as natural beauty. In a more general sense, aesthetics is "critical reflection on art, culture, and nature". A key question in aesthetics is whether beauty is an objective feature of entities or a subjective aspect of experience. Aesthetic philosophers also investigate the nature of aesthetic experiences and judgments. Further topics include the essence of works of art and the processes involved in creating them. The philosophy of language studies the nature and function of language. It examines the concepts of meaning, reference, and truth. It aims to answer questions such as how words are related to things and how language affects human thought and understanding. It is closely related to the disciplines of logic and linguistics. The philosophy of language rose to particular prominence in the early 20th century in analytic philosophy due to the works of Frege and Russell. One of its central topics is to understand how sentences get their meaning. There are two broad theoretical camps: those emphasizing the formal truth conditions of sentences and those investigating circumstances that determine when it is suitable to use a sentence, the latter of which is associated with speech act theory. The philosophy of mind studies the nature of mental phenomena and how they are related to the physical world. It aims to understand different types of conscious and unconscious mental states, like beliefs, desires, intentions, feelings, sensations, and free will. An influential intuition in the philosophy of mind is that there is a distinction between the inner experience of objects and their existence in the external world. The mind-body problem is the problem of explaining how these two types of thing—mind and matter—are related. The main traditional responses are materialism, which assumes that matter is more fundamental; idealism, which assumes that mind is more fundamental; and dualism, which assumes that mind and matter are distinct types of entities. In contemporary philosophy, another common view is functionalism, which understands mental states in terms of the functional or causal roles they play. The mind-body problem is closely related to the hard problem of consciousness, which asks how the physical brain can produce qualitatively subjective experiences. The philosophy of religion investigates the basic concepts, assumptions, and arguments associated with religion. It critically reflects on what religion is, how to define the divine, and whether one or more gods exist. It also includes the discussion of worldviews that reject religious doctrines. Further questions addressed by the philosophy of religion are: "How are we to interpret religious language, if not literally?"; "Is divine omniscience compatible with free will?"; and, "Are the great variety of world religions in some way compatible in spite of their apparently contradictory theological claims?" It includes topics from nearly all branches of philosophy. It differs from theology since theological debates typically take place within one religious tradition, whereas debates in the philosophy of religion transcend any particular set of theological assumptions. The philosophy of science examines the fundamental concepts, assumptions, and problems associated with science. It reflects on what science is and how to distinguish it from pseudoscience. It investigates the methods employed by scientists, how their application can result in knowledge, and on what assumptions they are based. It also studies the purpose and implications of science. Some of its questions are "What counts as an adequate explanation?"; "Is a scientific law anything more than a description of a regularity?"; and "Can some special sciences be explained entirely in the terms of a more general science?" It is a vast field that is commonly divided into the philosophy of the natural sciences and the philosophy of the social sciences, with further subdivisions for each of the individual sciences under these headings. How these branches are related to one another is also a question in the philosophy of science. Many of its philosophical issues overlap with the fields of metaphysics or epistemology. Political philosophy is the philosophical inquiry into the fundamental principles and ideas governing political systems and societies. It examines the basic concepts, assumptions, and arguments in the field of politics. It investigates the nature and purpose of government and compares its different forms. It further asks under what circumstances the use of political power is legitimate, rather than a form of simple violence. In this regard, it is concerned with the distribution of political power, social and material goods, and legal rights. Other topics are justice, liberty, equality, sovereignty, and nationalism. Political philosophy involves a general inquiry into normative matters and differs in this respect from political science, which aims to provide empirical descriptions of actually existing states. Political philosophy is often treated as a subfield of ethics. Influential schools of thought in political philosophy are liberalism, conservativism, socialism, and anarchism. Methods Methods of philosophy are ways of conducting philosophical inquiry. They include techniques for arriving at philosophical knowledge and justifying philosophical claims as well as principles used for choosing between competing theories. A great variety of methods have been employed throughout the history of philosophy. Many of them differ significantly from the methods used in the natural sciences in that they do not use experimental data obtained through measuring equipment. The choice of one's method usually has important implications both for how philosophical theories are constructed and for the arguments cited for or against them. This choice is often guided by epistemological considerations about what constitutes philosophical evidence. Methodological disagreements can cause conflicts among philosophical theories or about the answers to philosophical questions. The discovery of new methods has often had important consequences both for how philosophers conduct their research and for what claims they defend. Some philosophers engage in most of their theorizing using one particular method while others employ a wider range of methods based on which one fits the specific problem investigated best. Conceptual analysis is a common method in analytic philosophy. It aims to clarify the meaning of concepts by analyzing them into their component parts. Another method often employed in analytic philosophy is based on common sense. It starts with commonly accepted beliefs and tries to draw unexpected conclusions from them, which it often employs in a negative sense to criticize philosophical theories that are too far removed from how the average person sees the issue. It is similar to how ordinary language philosophy approaches philosophical questions by investigating how ordinary language is used. Various methods in philosophy give particular importance to intuitions, that is, non-inferential impressions about the correctness of specific claims or general principles. For example, they play an important role in thought experiments, which employ counterfactual thinking to evaluate the possible consequences of an imagined situation. These anticipated consequences can then be used to confirm or refute philosophical theories. The method of reflective equilibrium also employs intuitions. It seeks to form a coherent position on a certain issue by examining all the relevant beliefs and intuitions, some of which often have to be deemphasized or reformulated to arrive at a coherent perspective. Pragmatists stress the significance of concrete practical consequences for assessing whether a philosophical theory is true. According to the pragmatic maxim as formulated by Charles Sanders Peirce, the idea a person has of an object is nothing more than the totality of practical consequences they associate with this object. Pragmatists have also used this method to expose disagreements as merely verbal, that is, to show they make no genuine difference on the level of consequences. Phenomenologists seek knowledge of the realm of appearance and the structure of human experience. They insist upon the first-personal character of all experience and proceed by suspending theoretical judgments about the external world. This technique of phenomenological reduction is known as "bracketing" or epoché. The goal is to give an unbiased description of the appearance of things. Methodological naturalism places great emphasis on the empirical approach and the resulting theories found in the natural sciences. In this way, it contrasts with methodologies that give more weight to pure reasoning and introspection. Relation to other fields Philosophy is closely related to many other fields. It is sometimes understood as a meta-discipline that clarifies their nature and limits. It does this by critically examining their basic concepts, background assumptions, and methods. In this regard, it plays a key role in providing an interdisciplinary perspective. It bridges the gap between different disciplines by analyzing which concepts and problems they have in common. It shows how they overlap while also delimiting their scope. Historically, most of the individual sciences originated from philosophy. The influence of philosophy is felt in several fields that require difficult practical decisions. In medicine, philosophical considerations related to bioethics affect issues like whether an embryo is already a person and under what conditions abortion is morally permissible. A closely related philosophical problem is how humans should treat other animals, for instance, whether it is acceptable to use non-human animals as food or for research experiments. In relation to business and professional life, philosophy has contributed by providing ethical frameworks. They contain guidelines on which business practices are morally acceptable and cover the issue of corporate social responsibility. Philosophical inquiry is relevant to many fields that are concerned with what to believe and how to arrive at evidence for one's beliefs. This is a key issue for the sciences, which have as one of their prime objectives the creation of scientific knowledge. Scientific knowledge is based on empirical evidence but it is often not clear whether empirical observations are neutral or already include theoretical assumptions. A closely connected problem is whether the available evidence is sufficient to decide between competing theories. Epistemological problems in relation to the law include what counts as evidence and how much evidence is required to find a person guilty of a crime. A related issue in journalism is how to ensure truth and objectivity when reporting on events. In the fields of theology and religion, there are many doctrines associated with the existence and nature of God as well as rules governing correct behavior. A key issue is whether a rational person should believe these doctrines, for example, whether revelation in the form of holy books and religious experiences of the divine are sufficient evidence for these beliefs. Philosophy in the form of logic has been influential in the fields of mathematics and computer science. Further fields influenced by philosophy include psychology, sociology, linguistics, education, and the arts. The close relation between philosophy and other fields in the contemporary period is reflected in the fact that many philosophy graduates go on to work in related fields rather than in philosophy itself. In the field of politics, philosophy addresses issues such as how to assess whether a government policy is just. Philosophical ideas have prepared and shaped various political developments. For example, ideals formulated in Enlightenment philosophy laid the foundation for constitutional democracy and played a role in the American Revolution and the French Revolution. Marxist philosophy and its exposition of communism was one of the factors in the Russian Revolution and the Chinese Communist Revolution. In India, Mahatma Gandhi's philosophy of non-violence shaped the Indian independence movement. An example of the cultural and critical role of philosophy is found in its influence on the feminist movement through philosophers such as Mary Wollstonecraft, Simone de Beauvoir, and Judith Butler. It has shaped the understanding of key concepts in feminism, for instance, the meaning of gender, how it differs from biological sex, and what role it plays in the formation of personal identity. Philosophers have also investigated the concepts of justice and equality and their implications with respect to the prejudicial treatment of women in male-dominated societies. The idea that philosophy is useful for many aspects of life and society is sometimes rejected. According to one such view, philosophy is mainly undertaken for its own sake and does not make significant contributions to existing practices or external goals. See also References Notes Citations Bibliography External links Internet Encyclopedia of Philosophy – a peer-reviewed online encyclopedia of philosophy Stanford Encyclopedia of Philosophy – an online encyclopedia of philosophy maintained by Stanford University PhilPapers – a comprehensive directory of online philosophical articles and books by academic philosophers Internet Philosophy Ontology Project – a model of relationships between philosophical ideas, thinkers, and journals

Science is a systematic discipline that builds and organizes knowledge in the form of testable hypotheses and predictions about the universe. Modern science is typically divided into two – or three – major branches: the natural sciences, which study the physical world, and the social sciences, which study individuals and societies. While referred to as the formal sciences, the study of logic, mathematics, and theoretical computer science are typically regarded as separate because they rely on deductive reasoning instead of the scientific method as their main methodology. Meanwhile, applied sciences are disciplines that use scientific knowledge for practical purposes, such as engineering and medicine. The history of science spans the majority of the historical record, with the earliest identifiable predecessors to modern science dating to the Bronze Age in Egypt and Mesopotamia (c. 3000–1200 BCE). Their contributions to mathematics, astronomy, and medicine entered and shaped the Greek natural philosophy of classical antiquity and later medieval scholarship, whereby formal attempts were made to provide explanations of events in the physical world based on natural causes; while further advancements, including the introduction of the Hindu–Arabic numeral system, were made during the Golden Age of India and Islamic Golden Age. The recovery and assimilation of Greek works and Islamic inquiries into Western Europe during the Renaissance revived natural philosophy, which was later transformed by the Scientific Revolution that began in the 16th century as new ideas and discoveries departed from previous Greek conceptions and traditions. The scientific method soon played a greater role in the acquisition of knowledge, and in the 19th century, many of the institutional and professional features of science began to take shape, along with the changing of "natural philosophy" to "natural science". New knowledge in science is advanced by research from scientists who are motivated by curiosity about the world and a desire to solve problems. Contemporary scientific research is highly collaborative and is usually done by teams in academic and research institutions, government agencies, and companies. The practical impact of their work has led to the emergence of science policies that seek to influence the scientific enterprise by prioritizing the ethical and moral development of commercial products, armaments, health care, public infrastructure, and environmental protection. Etymology The word science has been used in Middle English since the 14th century in the sense of "the state of knowing". The word was borrowed from the Anglo-Norman language as the suffix -cience, which was borrowed from the Latin word scientia, meaning "knowledge, awareness, understanding", a noun derivative of sciens meaning "knowing", itself the present active participle of sciō, "to know". There are many hypotheses for science's ultimate word origin. According to Michiel de Vaan, Dutch linguist and Indo-Europeanist, sciō may have its origin in the Proto-Italic language as *skije- or *skijo- meaning "to know", which may originate from Proto-Indo-European language as *skh1-ie, *skh1-io, meaning "to incise". The Lexikon der indogermanischen Verben proposed sciō is a back-formation of nescīre, meaning "to not know, be unfamiliar with", which may derive from Proto-Indo-European *sekH- in Latin secāre, or *skh2-, from *sḱʰeh2(i)- meaning "to cut". In the past, science was a synonym for "knowledge" or "study", in keeping with its Latin origin. A person who conducted scientific research was called a "natural philosopher" or "man of science". In 1834, William Whewell introduced the term scientist in a review of Mary Somerville's book On the Connexion of the Physical Sciences, crediting it to "some ingenious gentleman" (possibly himself). History Early history Science has no single origin. Rather, scientific thinking emerged gradually over the course of tens of thousands of years, taking different forms around the world, and few details are known about the very earliest developments. Women likely played a central role in prehistoric science, as did religious rituals. Some scholars use the term "protoscience" to label activities in the past that resemble modern science in some but not all features; however, this label has also been criticized as denigrating, or too suggestive of presentism, thinking about those activities only in relation to modern categories. Direct evidence for scientific processes becomes clearer with the advent of writing systems in the Bronze Age civilizations of Ancient Egypt and Mesopotamia (c. 3000–1200 BCE), creating the earliest written records in the history of science. Although the words and concepts of "science" and "nature" were not part of the conceptual landscape at the time, the ancient Egyptians and Mesopotamians made contributions that would later find a place in Greek and medieval science: mathematics, astronomy, and medicine. From the 3rd millennium BCE, the ancient Egyptians developed a non-positional decimal numbering system, solved practical problems using geometry, and developed a calendar. Their healing therapies involved drug treatments and the supernatural, such as prayers, incantations, and rituals. The ancient Mesopotamians used knowledge about the properties of various natural chemicals for manufacturing pottery, faience, glass, soap, metals, lime plaster, and waterproofing. They studied animal physiology, anatomy, behavior, and astrology for divinatory purposes. The Mesopotamians had an intense interest in medicine and the earliest medical prescriptions appeared in Sumerian during the Third Dynasty of Ur. They seem to have studied scientific subjects which had practical or religious applications and had little interest in satisfying curiosity. Classical antiquity In classical antiquity, there is no real ancient analog of a modern scientist. Instead, well-educated, usually upper-class, and almost universally male individuals performed various investigations into nature whenever they could afford the time. Before the invention or discovery of the concept of phusis or nature by the pre-Socratic philosophers, the same words tend to be used to describe the natural "way" in which a plant grows, and the "way" in which, for example, one tribe worships a particular god. For this reason, it is claimed that these men were the first philosophers in the strict sense and the first to clearly distinguish "nature" and "convention". The early Greek philosophers of the Milesian school, which was founded by Thales of Miletus and later continued by his successors Anaximander and Anaximenes, were the first to attempt to explain natural phenomena without relying on the supernatural. The Pythagoreans developed a complex number philosophy and contributed significantly to the development of mathematical science. The theory of atoms was developed by the Greek philosopher Leucippus and his student Democritus. Later, Epicurus would develop a full natural cosmology based on atomism, and would adopt a "canon" (ruler, standard) which established physical criteria or standards of scientific truth. The Greek doctor Hippocrates established the tradition of systematic medical science and is known as "The Father of Medicine". A turning point in the history of early philosophical science was Socrates' example of applying philosophy to the study of human matters, including human nature, the nature of political communities, and human knowledge itself. The Socratic method as documented by Plato's dialogues is a dialectic method of hypothesis elimination: better hypotheses are found by steadily identifying and eliminating those that lead to contradictions. The Socratic method searches for general commonly held truths that shape beliefs and scrutinizes them for consistency. Socrates criticized the older type of study of physics as too purely speculative and lacking in self-criticism. In the 4th century BCE, Aristotle created a systematic program of teleological philosophy. In the 3rd century BCE, Greek astronomer Aristarchus of Samos was the first to propose a heliocentric model of the universe, with the Sun at the center and all the planets orbiting it. Aristarchus's model was widely rejected because it was believed to violate the laws of physics, while Ptolemy's Almagest, which contains a geocentric description of the Solar System, was accepted through the early Renaissance instead. The inventor and mathematician Archimedes of Syracuse made major contributions to the beginnings of calculus. Pliny the Elder was a Roman writer and polymath, who wrote the seminal encyclopedia Natural History. Positional notation for representing numbers likely emerged between the 3rd and 5th centuries CE along Indian trade routes. This numeral system made efficient arithmetic operations more accessible and would eventually become standard for mathematics worldwide. Middle Ages Due to the collapse of the Western Roman Empire, the 5th century saw an intellectual decline, with knowledge of classical Greek conceptions of the world deteriorating in Western Europe. Latin encyclopedists of the period such as Isidore of Seville preserved the majority of general ancient knowledge. In contrast, because the Byzantine Empire resisted attacks from invaders, they were able to preserve and improve prior learning. John Philoponus, a Byzantine scholar in the 6th century, started to question Aristotle's teaching of physics, introducing the theory of impetus. His criticism served as an inspiration to medieval scholars and Galileo Galilei, who extensively cited his works ten centuries later. During late antiquity and the Early Middle Ages, natural phenomena were mainly examined via the Aristotelian approach. The approach includes Aristotle's four causes: material, formal, moving, and final cause. Many Greek classical texts were preserved by the Byzantine Empire and Arabic translations were made by Christians, mainly Nestorians and Miaphysites. Under the Abbasids, these Arabic translations were later improved and developed by Arabic scientists. By the 6th and 7th centuries, the neighboring Sasanian Empire established the medical Academy of Gondishapur, which was considered by Greek, Syriac, and Persian physicians as the most important medical hub of the ancient world. Islamic study of Aristotelianism flourished in the House of Wisdom established in the Abbasid capital of Baghdad, Iraq and the flourished until the Mongol invasions in the 13th century. Ibn al-Haytham, better known as Alhazen, used controlled experiments in his optical study. Avicenna's compilation of The Canon of Medicine, a medical encyclopedia, is considered to be one of the most important publications in medicine and was used until the 18th century. By the 11th century most of Europe had become Christian, and in 1088, the University of Bologna emerged as the first university in Europe. As such, demand for Latin translation of ancient and scientific texts grew, a major contributor to the Renaissance of the 12th century. Renaissance scholasticism in western Europe flourished, with experiments done by observing, describing, and classifying subjects in nature. In the 13th century, medical teachers and students at Bologna began opening human bodies, leading to the first anatomy textbook based on human dissection by Mondino de Luzzi. Renaissance New developments in optics played a role in the inception of the Renaissance, both by challenging long-held metaphysical ideas on perception, as well as by contributing to the improvement and development of technology such as the camera obscura and the telescope. At the start of the Renaissance, Roger Bacon, Vitello, and John Peckham each built up a scholastic ontology upon a causal chain beginning with sensation, perception, and finally apperception of the individual and universal forms of Aristotle. A model of vision later known as perspectivism was exploited and studied by the artists of the Renaissance. This theory uses only three of Aristotle's four causes: formal, material, and final. In the 16th century, Nicolaus Copernicus formulated a heliocentric model of the Solar System, stating that the planets revolve around the Sun, instead of the geocentric model where the planets and the Sun revolve around the Earth. This was based on a theorem that the orbital periods of the planets are longer as their orbs are farther from the center of motion, which he found not to agree with Ptolemy's model. Johannes Kepler and others challenged the notion that the only function of the eye is perception, and shifted the main focus in optics from the eye to the propagation of light. Kepler is best known, however, for improving Copernicus' heliocentric model through the discovery of Kepler's laws of planetary motion. Kepler did not reject Aristotelian metaphysics and described his work as a search for the Harmony of the Spheres. Galileo had made significant contributions to astronomy, physics and engineering. However, he became persecuted after Pope Urban VIII sentenced him for writing about the heliocentric model. The printing press was widely used to publish scholarly arguments, including some that disagreed widely with contemporary ideas of nature. Francis Bacon and René Descartes published philosophical arguments in favor of a new type of non-Aristotelian science. Bacon emphasized the importance of experiment over contemplation, questioned the Aristotelian concepts of formal and final cause, promoted the idea that science should study the laws of nature and the improvement of all human life. Descartes emphasized individual thought and argued that mathematics rather than geometry should be used to study nature. Age of Enlightenment At the start of the Age of Enlightenment, Isaac Newton formed the foundation of classical mechanics by his Philosophiæ Naturalis Principia Mathematica, greatly influencing future physicists. Gottfried Wilhelm Leibniz incorporated terms from Aristotelian physics, now used in a new non-teleological way. This implied a shift in the view of objects: objects were now considered as having no innate goals. Leibniz assumed that different types of things all work according to the same general laws of nature, with no special formal or final causes. During this time the declared purpose and value of science became producing wealth and inventions that would improve human lives, in the materialistic sense of having more food, clothing, and other things. In Bacon's words, "the real and legitimate goal of sciences is the endowment of human life with new inventions and riches", and he discouraged scientists from pursuing intangible philosophical or spiritual ideas, which he believed contributed little to human happiness beyond "the fume of subtle, sublime or pleasing [speculation]". Science during the Enlightenment was dominated by scientific societies and academies, which had largely replaced universities as centers of scientific research and development. Societies and academies were the backbones of the maturation of the scientific profession. Another important development was the popularization of science among an increasingly literate population. Enlightenment philosophers turned to a few of their scientific predecessors – Galileo, Kepler, Boyle, and Newton principally – as the guides to every physical and social field of the day. The 18th century saw significant advancements in the practice of medicine and physics; the development of biological taxonomy by Carl Linnaeus; a new understanding of magnetism and electricity; and the maturation of chemistry as a discipline. Ideas on human nature, society, and economics evolved during the Enlightenment. Hume and other Scottish Enlightenment thinkers developed A Treatise of Human Nature, which was expressed historically in works by authors including James Burnett, Adam Ferguson, John Millar and William Robertson, all of whom merged a scientific study of how humans behaved in ancient and primitive cultures with a strong awareness of the determining forces of modernity. Modern sociology largely originated from this movement. In 1776, Adam Smith published The Wealth of Nations, which is often considered the first work on modern economics. 19th century During the 19th century, many distinguishing characteristics of contemporary modern science began to take shape. These included the transformation of the life and physical sciences; the frequent use of precision instruments; the emergence of terms such as "biologist", "physicist", and "scientist"; an increased professionalization of those studying nature; scientists gaining cultural authority over many dimensions of society; the industrialization of numerous countries; the thriving of popular science writings; and the emergence of science journals. During the late 19th century, psychology emerged as a separate discipline from philosophy when Wilhelm Wundt founded the first laboratory for psychological research in 1879. During the mid-19th century Charles Darwin and Alfred Russel Wallace independently proposed the theory of evolution by natural selection in 1858, which explained how different plants and animals originated and evolved. Their theory was set out in detail in Darwin's book On the Origin of Species, published in 1859. Separately, Gregor Mendel presented his paper, "Experiments on Plant Hybridization" in 1865, which outlined the principles of biological inheritance, serving as the basis for modern genetics. Early in the 19th century John Dalton suggested the modern atomic theory, based on Democritus's original idea of indivisible particles called atoms. The laws of conservation of energy, conservation of momentum and conservation of mass suggested a highly stable universe where there could be little loss of resources. However, with the advent of the steam engine and the Industrial Revolution there was an increased understanding that not all forms of energy have the same energy qualities, the ease of conversion to useful work or to another form of energy. This realization led to the development of the laws of thermodynamics, in which the free energy of the universe is seen as constantly declining: the entropy of a closed universe increases over time. The electromagnetic theory was established in the 19th century by the works of Hans Christian Ørsted, André-Marie Ampère, Michael Faraday, James Clerk Maxwell, Oliver Heaviside, and Heinrich Hertz. The new theory raised questions that could not easily be answered using Newton's framework. The discovery of X-rays inspired the discovery of radioactivity by Henri Becquerel and Marie Curie in 1896, Marie Curie then became the first person to win two Nobel Prizes. In the next year came the discovery of the first subatomic particle, the electron. 20th century In the first half of the century the development of antibiotics and artificial fertilizers improved human living standards globally. Harmful environmental issues such as ozone depletion, ocean acidification, eutrophication, and climate change came to the public's attention and caused the onset of environmental studies. During this period scientific experimentation became increasingly larger in scale and funding. The extensive technological innovation stimulated by World War I, World War II, and the Cold War led to competitions between global powers, such as the Space Race and nuclear arms race. Substantial international collaborations were also made, despite armed conflicts. In the late 20th century active recruitment of women and elimination of sex discrimination greatly increased the number of women scientists, but large gender disparities remained in some fields. The discovery of the cosmic microwave background in 1964 led to a rejection of the steady-state model of the universe in favor of the Big Bang theory of Georges Lemaître. The century saw fundamental changes within science disciplines. Evolution became a unified theory in the early 20th century when the modern synthesis reconciled Darwinian evolution with classical genetics. Albert Einstein's theory of relativity and the development of quantum mechanics complement classical mechanics to describe physics in extreme length, time and gravity. Widespread use of integrated circuits in the last quarter of the 20th century combined with communications satellites led to a revolution in information technology and the rise of the global internet and mobile computing, including smartphones. The need for mass systematization of long, intertwined causal chains and large amounts of data led to the rise of the fields of systems theory and computer-assisted scientific modelling. 21st century The Human Genome Project was completed in 2003 by identifying and mapping all of the genes of the human genome. The first induced pluripotent human stem cells were made in 2006, allowing adult cells to be transformed into stem cells and turn into any cell type found in the body. With the affirmation of the Higgs boson discovery in 2013, the last particle predicted by the Standard Model of particle physics was found. In 2015, gravitational waves, predicted by general relativity a century before, were first observed. In 2019, the international collaboration Event Horizon Telescope presented the first direct image of a black hole's accretion disc. Branches Modern science is commonly divided into three major branches: natural science, social science, and formal science. Each of these branches comprises various specialised yet overlapping scientific disciplines that often possess their own nomenclature and expertise. Both natural and social sciences are empirical sciences, as their knowledge is based on empirical observations and is capable of being tested for its validity by other researchers working under the same conditions. Natural Natural science is the study of the physical world. It can be divided into two main branches: life science and physical science. These two branches may be further divided into more specialised disciplines. For example, physical science can be subdivided into physics, chemistry, astronomy, and earth science. Modern natural science is the successor to the natural philosophy that began in Ancient Greece. Galileo, Descartes, Bacon, and Newton debated the benefits of using approaches that were more mathematical and more experimental in a methodical way. Still, philosophical perspectives, conjectures, and presuppositions, often overlooked, remain necessary in natural science. Systematic data collection, including discovery science, succeeded natural history, which emerged in the 16th century by describing and classifying plants, animals, minerals, and other biotic beings. Today, "natural history" suggests observational descriptions aimed at popular audiences. Social Social science is the study of human behavior and the functioning of societies. It has many disciplines that include, but are not limited to anthropology, economics, history, human geography, political science, psychology, and sociology. In the social sciences, there are many competing theoretical perspectives, many of which are extended through competing research programs such as the functionalists, conflict theorists, and interactionists in sociology. Due to the limitations of conducting controlled experiments involving large groups of individuals or complex situations, social scientists may adopt other research methods such as the historical method, case studies, and cross-cultural studies. Moreover, if quantitative information is available, social scientists may rely on statistical approaches to better understand social relationships and processes. Formal Formal science is an area of study that generates knowledge using formal systems. A formal system is an abstract structure used for inferring theorems from axioms according to a set of rules. It includes mathematics, systems theory, and theoretical computer science. The formal sciences share similarities with the other two branches by relying on objective, careful, and systematic study of an area of knowledge. They are, however, different from the empirical sciences as they rely exclusively on deductive reasoning, without the need for empirical evidence, to verify their abstract concepts. The formal sciences are therefore a priori disciplines and because of this, there is disagreement on whether they constitute a science. Nevertheless, the formal sciences play an important role in the empirical sciences. Calculus, for example, was initially invented to understand motion in physics. Natural and social sciences that rely heavily on mathematical applications include mathematical physics, chemistry, biology, finance, and economics. Applied Applied science is the use of the scientific method and knowledge to attain practical goals and includes a broad range of disciplines such as engineering and medicine. Engineering is the use of scientific principles to invent, design and build machines, structures and technologies. Science may contribute to the development of new technologies. Medicine is the practice of caring for patients by maintaining and restoring health through the prevention, diagnosis, and treatment of injury or disease. Basic The applied sciences are often contrasted with the basic sciences, which are focused on advancing scientific theories and laws that explain and predict events in the natural world. Blue skies Computational Computational science applies computer simulations to science, enabling a better understanding of scientific problems than formal mathematics alone can achieve. The use of machine learning and artificial intelligence is becoming a central feature of computational contributions to science, for example in agent-based computational economics, random forests, topic modeling and various forms of prediction. However, machines alone rarely advance knowledge as they require human guidance and capacity to reason; and they can introduce bias against certain social groups or sometimes underperform against humans. Computational methods have also raised challenges, including concerns about reproducibility, biases in training data, and the dependence of results on available computational resources. Interdisciplinary Interdisciplinary science involves the combination of two or more disciplines into one, such as bioinformatics, a combination of biology and computer science or cognitive sciences. The concept has existed since the ancient Greek period and it became popular again in the 20th century. Research Scientific research can be labelled as either basic or applied research. Basic research is the search for knowledge and applied research is the search for solutions to practical problems using this knowledge. Most understanding comes from basic research, though sometimes applied research targets specific practical problems. This leads to technological advances that were not previously imaginable. Scientific method Scientific research involves using the scientific method, which seeks to objectively explain the events of nature in a reproducible way. Scientists usually take for granted a set of basic assumptions that are needed to justify the scientific method: there is an objective reality shared by all rational observers; this objective reality is governed by natural laws; these laws were discovered by means of systematic observation and experimentation. Mathematics is essential in the formation of hypotheses, theories, and laws, because it is used extensively in quantitative modelling, observing, and collecting measurements. Statistics is used to summarise and analyse data, which allows scientists to assess the reliability of experimental results. In the scientific method an explanatory thought experiment or hypothesis is put forward as an explanation using parsimony principles and is expected to seek consilience – fitting with other accepted facts related to an observation or scientific question. This tentative explanation is used to make falsifiable predictions, which are typically posted before being tested by experimentation. Disproof of a prediction is evidence of progress. Experimentation is especially important in science to help establish causal relationships to avoid the correlation fallacy, though in some sciences such as astronomy or geology, a predicted observation might be more appropriate. When a hypothesis proves unsatisfactory it is modified or discarded. If the hypothesis survives testing, it may become adopted into the framework of a scientific theory, a validly reasoned, self-consistent model or framework for describing the behavior of certain natural events. A theory typically describes the behavior of much broader sets of observations than a hypothesis; commonly, a large number of hypotheses can be logically bound together by a single theory. Thus, a theory is a hypothesis explaining various other hypotheses. In that vein, theories are formulated according to most of the same scientific principles as hypotheses. Scientists may generate a model, an attempt to describe or depict an observation in terms of a logical, physical or mathematical representation, and to generate new hypotheses that can be tested by experimentation. While performing experiments to test hypotheses, scientists may have a preference for one outcome over another. Eliminating the bias can be achieved through transparency, careful experimental design, and a thorough peer review process of the experimental results and conclusions. After the results of an experiment are announced or published, it is normal practice for independent researchers to double-check how the research was performed, and to follow up by performing similar experiments to determine how dependable the results might be. Taken in its entirety, the scientific method allows for highly creative problem solving while minimising the effects of subjective and confirmation bias. Intersubjective verifiability, the ability to reach a consensus and reproduce results, is fundamental to the creation of all scientific knowledge. Literature Scientific research is published in a range of literature. Scientific journals communicate and document the results of research carried out in universities and various other research institutions, serving as an archival record of science. The first scientific journals, Journal des sçavans followed by Philosophical Transactions, began publication in 1665. Since that time the total number of active periodicals has steadily increased. In 1981, one estimate for the number of scientific and technical journals in publication was 11,500. Most scientific journals cover a single scientific field and publish the research within that field; the research is normally expressed in the form of a scientific paper. Science has become so pervasive in modern societies that it is considered necessary to communicate the achievements, news, and ambitions of scientists to a wider population. Challenges The replication crisis is an ongoing methodological crisis that affects parts of the social and life sciences. In subsequent investigations, the results of many scientific studies have been proven to be unrepeatable. The crisis has long-standing roots; the phrase was coined in the early 2010s as part of a growing awareness of the problem. The replication crisis represents an important body of research in metascience, which aims to improve the quality of all scientific research while reducing waste. An area of study or speculation that masquerades as science in an attempt to claim legitimacy that it would not otherwise be able to achieve is sometimes referred to as pseudoscience, fringe science, or junk science. Physicist Richard Feynman coined the term "cargo cult science" for cases in which researchers believe, and at a glance, look like they are doing science but lack the honesty to allow their results to be rigorously evaluated. Various types of commercial advertising, ranging from hype to fraud, may fall into these categories. Science has been described as "the most important tool" for separating valid claims from invalid ones. There can also be an element of political bias or ideological bias on all sides of scientific debates. Sometimes, research may be characterised as "bad science", research that may be well-intended but is incorrect, obsolete, incomplete, or over-simplified expositions of scientific ideas. The term scientific misconduct refers to situations such as where researchers have intentionally misrepresented their published data or have purposely given credit for a discovery to the wrong person. Philosophy There are different schools of thought in the philosophy of science. The most popular position is empiricism, which holds that knowledge is created by a process involving observation; scientific theories generalise observations. Empiricism generally encompasses inductivism, a position that explains how general theories can be made from the finite amount of empirical evidence available. Many versions of empiricism exist, with the predominant ones being Bayesianism and the hypothetico-deductive method. Empiricism has stood in contrast to rationalism, the position originally associated with Descartes, which holds that knowledge is created by the human intellect, not by observation. Critical rationalism is a contrasting 20th-century approach to science, first defined by Austrian-British philosopher Karl Popper. Popper rejected the way that empiricism describes the connection between theory and observation. He claimed that theories are not generated by observation, but that observation is made in the light of theories, and that the only way theory A can be affected by observation is after theory A were to conflict with observation, but theory B were to survive the observation. Popper proposed replacing verifiability with falsifiability as the landmark of scientific theories, replacing induction with falsification as the empirical method. Popper further claimed that there is actually only one universal method, not specific to science: the negative method of criticism, trial and error, covering all products of the human mind, including science, mathematics, philosophy, and art. Another approach, instrumentalism, emphasizes the utility of theories as instruments for explaining and predicting phenomena. It views scientific theories as black boxes, with only their input (initial conditions) and output (predictions) being relevant. Consequences, theoretical entities, and logical structure are claimed to be things that should be ignored. Close to instrumentalism is constructive empiricism, according to which the main criterion for the success of a scientific theory is whether what it says about observable entities is true. Thomas Kuhn argued that the process of observation and evaluation takes place within a paradigm, a logically consistent "portrait" of the world that is consistent with observations made from its framing. He characterised normal science as the process of observation and "puzzle solving", which takes place within a paradigm, whereas revolutionary science occurs when one paradigm overtakes another in a paradigm shift. Each paradigm has its own distinct questions, aims, and interpretations. The choice between paradigms involves setting two or more "portraits" against the world and deciding which likeness is most promising. A paradigm shift occurs when a significant number of observational anomalies arise in the old paradigm and a new paradigm makes sense of them. That is, the choice of a new paradigm is based on observations, even though those observations are made against the background of the old paradigm. For Kuhn, acceptance or rejection of a paradigm is a social process as much as a logical process. Kuhn's position, however, is not one of relativism. Another approach often cited in debates of scientific scepticism against controversial movements like "creation science" is methodological naturalism. Naturalists maintain that a difference should be made between natural and supernatural, and science should be restricted to natural explanations. Methodological naturalism maintains that science requires strict adherence to empirical study and independent verification. Community The scientific community is a network of interacting scientists who conduct scientific research. The community consists of smaller groups working in scientific fields. By having peer review, through discussion and debate within journals and conferences, scientists maintain the quality of research methodology and objectivity when interpreting results. Scientists Scientists are individuals who conduct scientific research to advance knowledge in an area of interest. Scientists may exhibit a strong curiosity about reality and a desire to apply scientific knowledge for the benefit of public health, nations, the environment, or industries; other motivations include recognition by peers and prestige. In modern times, many scientists study within specific areas of science in academic institutions, often obtaining advanced degrees in the process. Many scientists pursue careers in various fields such as academia, industry, government, and nonprofit organizations. Science has historically been a male-dominated field, with notable exceptions. Women have faced considerable discrimination in science, much as they have in other areas of male-dominated societies. For example, women were frequently passed over for job opportunities and denied credit for their work. The achievements of women in science have been attributed to the defiance of their traditional role as laborers within the domestic sphere. Learned societies Learned societies for the communication and promotion of scientific thought and experimentation have existed since the Renaissance. Many scientists belong to a learned society that promotes their respective scientific discipline, profession, or group of related disciplines. Membership may either be open to all, require possession of scientific credentials, or conferred by election. Most scientific societies are nonprofit organizations, and many are professional associations. Their activities typically include holding regular conferences for the presentation and discussion of new research results and publishing or sponsoring academic journals in their discipline. Some societies act as professional bodies, regulating the activities of their members in the public interest, or the collective interest of the membership. The professionalization of science, begun in the 19th century, was partly enabled by the creation of national distinguished academies of sciences such as the Italian Accademia dei Lincei in 1603, the British Royal Society in 1660, the French Academy of Sciences in 1666, the American National Academy of Sciences in 1863, the German Kaiser Wilhelm Society in 1911, and the Chinese Academy of Sciences in 1949. International scientific organizations, such as the International Science Council, are devoted to international cooperation for science advancement. Awards Science awards are usually given to individuals or organizations that have made significant contributions to a discipline. They are often given by prestigious institutions; thus, it is considered a great honour for a scientist receiving them. Since the early Renaissance, scientists have often been awarded medals, money, and titles. The Nobel Prize, a widely regarded prestigious award, is awarded annually to those who have achieved scientific advances in the fields of medicine, physics, and chemistry. Society Funding and policies Funding of science is often through a competitive process in which potential research projects are evaluated and only the most promising receive funding. Such processes, which are run by government, corporations, or foundations, allocate scarce funds. Total research funding in most developed countries is between 1.5% and 3% of GDP. In the OECD, around two-thirds of research and development in scientific and technical fields is carried out by industry, and 20% and 10%, respectively, by universities and government. The government funding proportion in certain fields is higher, and it dominates research in social science and the humanities. In less developed nations, the government provides the bulk of the funds for their basic scientific research. Many governments have dedicated agencies to support scientific research, such as the National Science Foundation in the United States, the National Scientific and Technical Research Council in Argentina, Commonwealth Scientific and Industrial Research Organisation in Australia, National Centre for Scientific Research in France, the Max Planck Society in Germany, and National Research Council in Spain. In commercial research and development, all but the most research-orientated corporations focus more heavily on near-term commercialization possibilities than research driven by curiosity. Science policy is concerned with policies that affect the conduct of the scientific enterprise, including research funding, often in pursuance of other national policy goals such as technological innovation to promote commercial product development, weapons development, health care, and environmental monitoring. Science policy sometimes refers to the act of applying scientific knowledge and consensus to the development of public policies. In accordance with public policy being concerned about the well-being of its citizens, science policy's goal is to consider how science and technology can best serve the public. Public policy can directly affect the funding of capital equipment and intellectual infrastructure for industrial research by providing tax incentives to those organizations that fund research. Education and awareness Science education for the general public is embedded in the school curriculum, and is supplemented by online pedagogical content (for example, YouTube and Khan Academy), museums, and science magazines and blogs. Major organizations of scientists such as the American Association for the Advancement of Science (AAAS) consider the sciences to be a part of the liberal arts traditions of learning, along with philosophy and history. Scientific literacy is chiefly concerned with an understanding of the scientific method, units and methods of measurement, empiricism, a basic understanding of statistics (correlations, qualitative versus quantitative observations, aggregate statistics), and a basic understanding of core scientific fields such as physics, chemistry, biology, ecology, geology, and computation. As a student advances into higher stages of formal education, the curriculum becomes more in depth. Traditional subjects usually included in the curriculum are natural and formal sciences, although recent movements include social and applied science as well. The mass media face pressures that can prevent them from accurately depicting competing scientific claims in terms of their credibility within the scientific community as a whole. Determining how much weight to give different sides in a scientific debate may require considerable expertise regarding the matter. Few journalists have real scientific knowledge, and even beat reporters who are knowledgeable about certain scientific issues may be ignorant about other scientific issues that they are suddenly asked to cover. Science magazines such as New Scientist, Science & Vie, and Scientific American cater to the needs of a much wider readership and provide a non-technical summary of popular areas of research, including notable discoveries and advances in certain fields of research. The science fiction genre, primarily speculative fiction, can transmit the ideas and methods of science to the general public. Recent efforts to intensify or develop links between science and non-scientific disciplines, such as literature or poetry, include the Creative Writing Science resource developed through the Royal Literary Fund. Anti-science attitudes While the scientific method is broadly accepted in the scientific community, some fractions of society reject certain scientific positions or are sceptical about science. Examples are the common notion that COVID-19 is not a major health threat to the US (held by 39% of Americans in August 2021) or the belief that climate change is not a major threat to the US (also held by 40% of Americans, in late 2019 and early 2020). Psychologists have pointed to four factors driving rejection of scientific results: Scientific authorities are sometimes seen as inexpert, untrustworthy, or biased. Some marginalised social groups hold anti-science attitudes, in part because these groups have often been exploited in unethical experiments. Messages from scientists may contradict deeply held existing beliefs or morals. The delivery of a scientific message may not be appropriately targeted to a recipient's learning style. Anti-science attitudes often seem to be caused by fear of rejection in social groups. For instance, climate change is perceived as a threat by only 22% of Americans on the right side of the political spectrum, but by 85% on the left. That is, if someone on the left would not consider climate change as a threat, this person may face contempt and be rejected in that social group. In fact, people may rather deny a scientifically accepted fact than lose or jeopardise their social status. Politics Attitudes towards science are often determined by political opinions and goals. Government, business and advocacy groups have been known to use legal and economic pressure to influence scientific researchers. Many factors can act as facets of the politicization of science such as anti-intellectualism, perceived threats to religious beliefs, and fear for business interests. Politicization of science is usually accomplished when scientific information is presented in a way that emphasizes the uncertainty associated with the scientific evidence. Tactics such as shifting conversation, failing to acknowledge facts, and capitalising on doubt of scientific consensus have been used to gain more attention for views that have been undermined by scientific evidence. Examples of issues that have involved the politicization of science include the global warming controversy, health effects of pesticides, and health effects of tobacco. See also List of scientific occupations List of years in science Logology (science) Science (Wikiversity) Scientific integrity Notes References == External links ==

Technology is the application of conceptual knowledge to achieve practical goals, especially in a reproducible way. The word technology can also mean the products resulting from such efforts, including both tangible tools such as utensils or machines, and intangible ones such as software. Technology plays a critical role in science, engineering, and everyday life. Technological advancements have led to significant changes in society. The earliest known technology is the stone tool, used during prehistory, followed by the control of fire—which in turn contributed to the growth of the human brain and the development of language during the Ice Age, according to the cooking hypothesis. The invention of the wheel in the Bronze Age allowed greater travel and the creation of more complex machines. More recent technological inventions, including the printing press, telephone, and the Internet, have lowered barriers to communication and ushered in the knowledge economy. While technology contributes to economic development and improves human prosperity, it can also have negative impacts like pollution and resource depletion, and can cause social harms like technological unemployment resulting from automation. As a result, philosophical and political debates about the role and use of technology, the ethics of technology, and ways to mitigate its downsides are ongoing. Etymology Technology is a term dating back to the early 17th century that meant 'systematic treatment' (from Greek Τεχνολογία, from the Greek: τέχνη, romanized: tékhnē, lit. 'craft, art' and -λογία (-logíā), 'study, knowledge'). It is predated in use by the Ancient Greek word τέχνη (tékhnē), used to mean 'knowledge of how to make things', which encompassed activities like architecture. Starting in the 19th century, continental Europeans started using the terms Technik (German) or technique (French) to refer to a 'way of doing', which included all technical arts, such as dancing, navigation, or printing, whether or not they required tools or instruments. At the time, Technologie (German and French) referred either to the academic discipline studying the "methods of arts and crafts", or to the political discipline "intended to legislate on the functions of the arts and crafts." The distinction between Technik and Technologie is absent in English, and so both were translated as technology. The term was previously uncommon in English and mostly referred to the academic discipline, as in the Massachusetts Institute of Technology. In the 20th century, as a result of scientific progress and the Second Industrial Revolution, technology stopped being considered a distinct academic discipline and took on the meaning: the systemic use of knowledge to practical ends. History Prehistoric Tools were initially developed by hominids through observation and trial and error. Around 2 Mya (million years ago), they learned to make the first stone tools by hammering flakes off a pebble, forming a sharp hand axe. This practice was refined 75 kya (thousand years ago) into pressure flaking, enabling much finer work. The discovery of fire was described by Charles Darwin as "possibly the greatest ever made by man". Archaeological, dietary, and social evidence point to "continuous [human] fire-use" at least 1.5 Mya. Fire, fueled with wood and charcoal, allowed early humans to cook their food to increase its digestibility, improving its nutrient value and broadening the number of foods that could be eaten. The cooking hypothesis proposes that the ability to cook promoted an increase in hominid brain size, though some researchers find the evidence inconclusive. Archaeological evidence of hearths was dated to 790 kya; researchers believe this is likely to have intensified human socialization and may have contributed to the emergence of language. Other technological advances made during the Paleolithic era include clothing and shelter. No consensus exists on the approximate time of adoption of either technology, but archaeologists have found archaeological evidence of clothing 90-120 kya and shelter 450 kya. As the Paleolithic era progressed, dwellings became more sophisticated and more elaborate; as early as 380 kya, humans were constructing temporary wood huts. Clothing, adapted from the fur and hides of hunted animals, helped humanity expand into colder regions; humans began to migrate out of Africa around 200 kya, initially moving to Eurasia. Neolithic The Neolithic Revolution (or First Agricultural Revolution) brought about an acceleration of technological innovation, and a consequent increase in social complexity. The invention of the polished stone axe was a major advance that allowed large-scale forest clearance and farming. This use of polished stone axes increased greatly in the Neolithic but was originally used in the preceding Mesolithic in some areas such as Ireland. Agriculture fed larger populations, and the transition to sedentism allowed for the simultaneous raising of more children, as infants no longer needed to be carried around by nomads. Additionally, children could contribute labor to the raising of crops more readily than they could participate in hunter-gatherer activities. With this increase in population and availability of labor came an increase in labor specialization. What triggered the progression from early Neolithic villages to the first cities, such as Uruk, and the first civilizations, such as Sumer, is not specifically known; however, the emergence of increasingly hierarchical social structures and specialized labor, of trade and war among adjacent cultures, and the need for collective action to overcome environmental challenges such as irrigation, are all thought to have played a role. The invention of writing led to the spread of cultural knowledge and became the basis for history, libraries, schools, and scientific research. Continuing improvements led to the furnace and bellows and provided, for the first time, the ability to smelt and forge gold, copper, silver, and lead – native metals found in relatively pure form in nature. The advantages of copper tools over stone, bone and wooden tools were quickly apparent to early humans, and native copper was probably used from near the beginning of Neolithic times (about 10 kya). Native copper does not naturally occur in large amounts, but copper ores are quite common and some of them produce metal easily when burned in wood or charcoal fires. Eventually, the working of metals led to the discovery of alloys such as bronze and brass (about 4,000 BCE). The first use of iron alloys such as steel dates to around 1,800 BCE. Ancient After harnessing fire, humans discovered other forms of energy. The earliest known use of wind power is the sailing ship; the earliest record of a ship under sail is that of a Nile boat dating to around 7,000 BCE. From prehistoric times, Egyptians likely used the power of the annual flooding of the Nile to irrigate their lands, gradually learning to regulate much of it through purposely built irrigation channels and "catch" basins. The ancient Sumerians in Mesopotamia used a complex system of canals and levees to divert water from the Tigris and Euphrates rivers for irrigation. Archaeologists estimate that the wheel was invented independently and concurrently in Mesopotamia (in present-day Iraq), the Northern Caucasus (Maykop culture), and Central Europe. Time estimates range from 5,500 to 3,000 BCE with most experts putting it closer to 4,000 BCE. The oldest artifacts with drawings depicting wheeled carts date from about 3,500 BCE. More recently, the oldest-known wooden wheel in the world as of 2024 was found in the Ljubljana Marsh of Slovenia; Austrian experts have established that the wheel is between 5,100 and 5,350 years old. The invention of the wheel revolutionized trade and war. It did not take long to discover that wheeled wagons could be used to carry heavy loads. The ancient Sumerians used a potter's wheel and may have invented it. A stone pottery wheel found in the city-state of Ur dates to around 3,429 BCE, and even older fragments of wheel-thrown pottery have been found in the same area. Fast (rotary) potters' wheels enabled early mass production of pottery, but it was the use of the wheel as a transformer of energy (through water wheels, windmills, and even treadmills) that revolutionized the application of nonhuman power sources. The first two-wheeled carts were derived from travois and were first used in Mesopotamia and Iran in around 3,000 BCE. The oldest known constructed roadways are the stone-paved streets of the city-state of Ur, dating to c. 4,000 BCE, and timber roads leading through the swamps of Glastonbury, England, dating to around the same period. The first long-distance road, which came into use around 3,500 BCE, spanned 2,400 km from the Persian Gulf to the Mediterranean Sea, but was not paved and was only partially maintained. In around 2,000 BCE, the Minoans on the Greek island of Crete built a 50 km road leading from the palace of Gortyn on the south side of the island, through the mountains, to the palace of Knossos on the north side of the island. Unlike the earlier road, the Minoan road was completely paved. Ancient Minoan private homes had running water. A bathtub virtually identical to modern ones was unearthed at the Palace of Knossos. Several Minoan private homes also had toilets, which could be flushed by pouring water down the drain. The ancient Romans had many public flush toilets, which emptied into an extensive sewage system. The primary sewer in Rome was the Cloaca Maxima; construction began on it in the sixth century BCE and it is still in use today. The ancient Romans also had a complex system of aqueducts, which were used to transport water across long distances. The first Roman aqueduct was built in 312 BCE. The eleventh and final ancient Roman aqueduct was built in 226 CE. Put together, the Roman aqueducts extended over 450 km, but less than 70 km of this was above ground and supported by arches. Pre-modern Innovations continued through the Middle Ages with the introduction of silk production (in Asia and later Europe), the horse collar, and horseshoes. Simple machines (such as the lever, the screw, and the pulley) were combined into more complicated tools, such as the wheelbarrow, windmills, and clocks. A system of universities developed and spread scientific ideas and practices, including Oxford and Cambridge. The Renaissance era produced many innovations, including the introduction of the movable type printing press to Europe, which facilitated the communication of knowledge. Technology became increasingly influenced by science, beginning a cycle of mutual advancement. Modern Starting in the United Kingdom in the 18th century, the discovery of steam power set off the Industrial Revolution, which saw wide-ranging technological discoveries, particularly in the areas of agriculture, manufacturing, mining, metallurgy, and transport, and the widespread application of the factory system. This was followed a century later by the Second Industrial Revolution which led to rapid scientific discovery, standardization, and mass production. New technologies were developed, including sewage systems, electricity, light bulbs, electric motors, railroads, automobiles, and airplanes. These technological advances led to significant developments in medicine, chemistry, physics, and engineering. They were accompanied by consequential social change, with the introduction of skyscrapers accompanied by rapid urbanization. Communication improved with the invention of the telegraph, the telephone, the radio, and television. The 20th century brought a host of innovations. In physics, the discovery of nuclear fission in the Atomic Age led to both nuclear weapons and nuclear power. Analog computers were invented and asserted dominance in processing complex data. While the invention of vacuum tubes allowed for digital computing with computers like the ENIAC, their sheer size precluded widespread use until innovations in quantum physics allowed for the invention of the transistor in 1947, which significantly compacted computers and led the digital transition. Information technology, particularly optical fiber and optical amplifiers, allowed for simple and fast long-distance communication, which ushered in the Information Age and the birth of the Internet. The Space Age began with the launch of Sputnik 1 in 1957, and later the launch of crewed missions to the moon in the 1960s. Organized efforts to search for extraterrestrial intelligence have used radio telescopes to detect signs of technology use, or technosignatures, given off by alien civilizations. In medicine, new technologies were developed for diagnosis (CT, PET, and MRI scanning), treatment (like the dialysis machine, defibrillator, pacemaker, and a wide array of new pharmaceutical drugs), and research (like interferon cloning and DNA microarrays). Complex manufacturing and construction techniques and organizations are needed to make and maintain more modern technologies, and entire industries have arisen to develop succeeding generations of increasingly more complex tools. Modern technology increasingly relies on training and education – their designers, builders, maintainers, and users often require sophisticated general and specific training. Moreover, these technologies have become so complex that entire fields have developed to support them, including engineering, medicine, and computer science; and other fields have become more complex, such as construction, transportation, and architecture. Impact Technological change is the largest cause of long-term economic growth. Throughout human history, energy production was the main constraint on economic development, and new technologies allowed humans to significantly increase the amount of available energy. First came fire, which made edible a wider variety of foods, and made it less physically demanding to digest them. Fire also enabled smelting, and the use of tin, copper, and iron tools, used for hunting or tradesmanship. Then came the agricultural revolution: humans no longer needed to hunt or gather to survive, and began to settle in towns and cities, forming more complex societies, with militaries and more organized forms of religion. Technologies have contributed to human welfare through increased prosperity, improved comfort and quality of life, and medical progress, but they can also disrupt existing social hierarchies, cause pollution, and harm individuals or groups. Recent years have brought about a rise in social media's cultural prominence, with potential repercussions on democracy, and economic and social life. Early on, the internet was seen as a "liberation technology" that would democratize knowledge, improve access to education, and promote democracy. Modern research has turned to investigate the internet's downsides, including disinformation, polarization, hate speech, and propaganda. Since the 1970s, technology's impact on the environment has been criticized, leading to a surge in investment in solar, wind, and other forms of clean energy. Social Jobs Since the invention of the wheel, technologies have helped increase humans' economic output. Past automation has both substituted and complemented labor; machines replaced humans at some lower-paying jobs (for example in agriculture), but this was compensated by the creation of new, higher-paying jobs. Studies have found that computers did not create significant net technological unemployment. Due to artificial intelligence being far more capable than computers, and still being in its infancy, it is not known whether it will follow the same trend; the question has been debated at length among economists and policymakers. A 2017 survey found no clear consensus among economists on whether AI would increase long-term unemployment. According to the World Economic Forum's "The Future of Jobs Report 2020", AI is predicted to replace 85 million jobs worldwide, and create 97 million new jobs by 2025. From 1990 to 2007, a study in the U.S. by MIT economist Daron Acemoglu showed that an addition of one robot for every 1,000 workers decreased the employment-to-population ratio by 0.2%, or about 3.3 workers, and lowered wages by 0.42%. Concerns about technology replacing human labor however are long-lasting. As US president Lyndon Johnson said in 1964, "Technology is creating both new opportunities and new obligations for us, opportunity for greater productivity and progress; obligation to be sure that no workingman, no family must pay an unjust price for progress." upon signing the National Commission on Technology, Automation, and Economic Progress bill. Security With the growing reliance of technology, there have been security and privacy concerns along with it. Billions of people use different online payment methods, such as WeChat Pay, PayPal, Alipay, and much more to help transfer money. Although security measures are placed, some criminals are able to bypass them. In March 2022, North Korea used Blender.io, a mixer which helped them to hide their cryptocurrency exchanges, to launder over $20.5 million in cryptocurrency, from Axie Infinity, and steal over $600 million worth of cryptocurrency from the game's owner. Because of this, the U.S. Treasury Department sanctioned Blender.io, which marked the first time it has taken action against a mixer, to try to crack down on North Korean hackers. The privacy of cryptocurrency has been debated. Although many customers like the privacy of cryptocurrency, many also argue that it needs more transparency and stability. Environmental Technology can have both positive and negative effects on the environment. Environmental technology, describes an array of technologies which seek to reverse, mitigate or halt environmental damage to the environment. This can include measures to halt pollution through environmental regulations, capture and storage of pollution, or using pollutant byproducts in other industries. Other examples of environmental technology include deforestation and the reversing of deforestation. Emerging technologies in the fields of climate engineering may be able to halt or reverse global warming and its environmental impacts, although this remains highly controversial. As technology has advanced, so too has the negative environmental impact, with increased release of greenhouse gases, including methane, nitrous oxide and carbon dioxide, into the atmosphere, causing the greenhouse effect. This continues to gradually heat the earth, causing global warming and climate change. Measures of technological innovation correlates with a rise in greenhouse gas emissions. Pollution Pollution, the presence of contaminants in an environment that causes adverse effects, could have been present as early as the Inca Empire. They used a lead sulfide flux in the smelting of ores, along with the use of a wind-drafted clay kiln, which released lead into the atmosphere and the sediment of rivers. Philosophy Philosophy of technology is a branch of philosophy that studies the "practice of designing and creating artifacts", and the "nature of the things so created." It emerged as a discipline over the past two centuries, and has grown "considerably" since the 1970s. The humanities philosophy of technology is concerned with the "meaning of technology for, and its impact on, society and culture". Initially, technology was seen as an extension of the human organism that replicated or amplified bodily and mental faculties. Marx framed it as a tool used by capitalists to oppress the proletariat, but believed that technology would be a fundamentally liberating force once it was "freed from societal deformations". Second-wave philosophers like Ortega later shifted their focus from economics and politics to "daily life and living in a techno-material culture", arguing that technology could oppress "even the members of the bourgeoisie who were its ostensible masters and possessors." Third-stage philosophers like Don Ihde and Albert Borgmann represent a turn toward de-generalization and empiricism, and considered how humans can learn to live with technology. Early scholarship on technology was split between two arguments: technological determinism, and social construction. Technological determinism is the idea that technologies cause unavoidable social changes. It usually encompasses a related argument, technological autonomy, which asserts that technological progress follows a natural progression and cannot be prevented. Social constructivists argue that technologies follow no natural progression, and are shaped by cultural values, laws, politics, and economic incentives. Modern scholarship has shifted towards an analysis of sociotechnical systems, "assemblages of things, people, practices, and meanings", looking at the value judgments that shape technology. Cultural critic Neil Postman distinguished tool-using societies from technological societies and from what he called "technopolies", societies that are dominated by an ideology of technological and scientific progress to the detriment of other cultural practices, values, and world views. Herbert Marcuse and John Zerzan suggest that technological society will inevitably deprive us of our freedom and psychological health. Ethics The ethics of technology is an interdisciplinary subfield of ethics that analyzes technology's ethical implications and explores ways to mitigate potential negative impacts of new technologies. There is a broad range of ethical issues revolving around technology, from specific areas of focus affecting professionals working with technology to broader social, ethical, and legal issues concerning the role of technology in society and everyday life. Prominent debates have surrounded genetically modified organisms, the use of robotic soldiers, algorithmic bias, and the issue of aligning AI behavior with human values. Technology ethics encompasses several key fields: Bioethics looks at ethical issues surrounding biotechnologies and modern medicine, including cloning, human genetic engineering, and stem cell research. Computer ethics focuses on issues related to computing. Cyberethics explores internet-related issues like intellectual property rights, privacy, and censorship. Nanoethics examines issues surrounding the alteration of matter at the atomic and molecular level in various disciplines including computer science, engineering, and biology. And engineering ethics deals with the professional standards of engineers, including software engineers and their moral responsibilities to the public. A wide branch of technology ethics is concerned with the ethics of artificial intelligence: it includes robot ethics, which deals with ethical issues involved in the design, construction, use, and treatment of robots, as well as machine ethics, which is concerned with ensuring the ethical behavior of artificially intelligent agents. Within the field of AI ethics, significant yet-unsolved research problems include AI alignment (ensuring that AI behaviors are aligned with their creators' intended goals and interests) and the reduction of algorithmic bias. Some researchers have warned against the hypothetical risk of an AI takeover, and have advocated for the use of AI capability control in addition to AI alignment methods. Other fields of ethics have had to contend with technology-related issues, including military ethics, media ethics, and educational ethics. Futures studies Futures studies is the study of social and technological progress. It aims to explore the range of plausible futures and incorporate human values in the development of new technologies. More generally, futures researchers are interested in improving "the freedom and welfare of humankind". It relies on a thorough quantitative and qualitative analysis of past and present technological trends, and attempts to rigorously extrapolate them into the future. Science fiction is often used as a source of ideas. Futures research methodologies include survey research, modeling, statistical analysis, and computer simulations. Existential risk Existential risk researchers analyze risks that could lead to human extinction or civilizational collapse, and look for ways to build resilience against them. Relevant research centers include the Cambridge Center for the Study of Existential Risk, and the Stanford Existential Risk Initiative. Future technologies may contribute to the risks of artificial general intelligence, biological warfare, nuclear warfare, nanotechnology, anthropogenic climate change, global warming, or stable global totalitarianism, though technologies may also help us mitigate asteroid impacts and gamma-ray bursts. In 2019 philosopher Nick Bostrom introduced the notion of a vulnerable world, "one in which there is some level of technological development at which civilization almost certainly gets devastated by default", citing the risks of a pandemic caused by bioterrorists, or an arms race triggered by the development of novel armaments and the loss of mutual assured destruction. He invites policymakers to question the assumptions that technological progress is always beneficial, that scientific openness is always preferable, or that they can afford to wait until a dangerous technology has been invented before they prepare mitigations. Emerging technologies Emerging technologies are novel technologies whose development or practical applications are still largely unrealized. They include nanotechnology, biotechnology, robotics, 3D printing, and blockchains. In 2005, futurist Ray Kurzweil claimed the next technological revolution would rest upon advances in genetics, nanotechnology, and robotics, with robotics being the most impactful of the three technologies. Genetic engineering will allow far greater control over human biological nature through a process called directed evolution. Some thinkers believe that this may shatter our sense of self, and have urged for renewed public debate exploring the issue more thoroughly; others fear that directed evolution could lead to eugenics or extreme social inequality. Nanotechnology will grant us the ability to manipulate matter "at the molecular and atomic scale", which could allow us to reshape ourselves and our environment in fundamental ways. Nanobots could be used within the human body to destroy cancer cells or form new body parts, blurring the line between biology and technology. Autonomous robots have undergone rapid progress, and are expected to replace humans at many dangerous tasks, including search and rescue, bomb disposal, firefighting, and war. Estimates on the advent of artificial general intelligence vary, but half of machine learning experts surveyed in 2018 believe that AI will "accomplish every task better and more cheaply" than humans by 2063, and automate all human jobs by 2140. This expected technological unemployment has led to calls for increased emphasis on computer science education and debates about universal basic income. Political science experts predict that this could lead to a rise in extremism, while others see it as an opportunity to usher in a post-scarcity economy. Movements Appropriate technology Some segments of the 1960s hippie counterculture grew to dislike urban living and developed a preference for locally autonomous, sustainable, and decentralized technology, termed appropriate technology. This later influenced hacker culture and technopaganism. Technological utopianism Technological utopianism refers to the belief that technological development is a moral good, which can and should bring about a utopia, that is, a society in which laws, governments, and social conditions serve the needs of all its citizens. Examples of techno-utopian goals include post-scarcity economics, life extension, mind uploading, cryonics, and the creation of artificial superintelligence. Major techno-utopian movements include transhumanism and singularitarianism. The transhumanism movement is founded upon the "continued evolution of human life beyond its current human form" through science and technology, informed by "life-promoting principles and values." The movement gained wider popularity in the early 21st century. Singularitarians believe that machine superintelligence will "accelerate technological progress" by orders of magnitude and "create even more intelligent entities ever faster", which may lead to a pace of societal and technological change that is "incomprehensible" to us. This event horizon is known as the technological singularity. Major figures of techno-utopianism include Ray Kurzweil and Nick Bostrom. Techno-utopianism has attracted both praise and criticism from progressive, religious, and conservative thinkers. Anti-technology backlash Technology's central role in our lives has drawn concerns and backlash. The backlash against technology is not a uniform movement and encompasses many heterogeneous ideologies. The earliest known revolt against technology was Luddism, a pushback against early automation in textile production. Automation had resulted in a need for fewer workers, a process known as technological unemployment. Between the 1970s and 1990s, American terrorist Ted Kaczynski carried out a series of bombings across America and published the Unabomber Manifesto denouncing technology's negative impacts on nature and human freedom. The essay resonated with a large part of the American public. It was partly inspired by Jacques Ellul's The Technological Society. Some subcultures, like the off-the-grid movement, advocate a withdrawal from technology and a return to nature. The ecovillage movement seeks to reestablish harmony between technology and nature. Relation to science and engineering Engineering is the process by which technology is developed. It often requires problem-solving under strict constraints. Technological development is "action-oriented", while scientific knowledge is fundamentally explanatory. Polish philosopher Henryk Skolimowski framed it like so: "science concerns itself with what is, technology with what is to be." The direction of causality between scientific discovery and technological innovation has been debated by scientists, philosophers and policymakers. Because innovation is often undertaken at the edge of scientific knowledge, most technologies are not derived from scientific knowledge, but instead from engineering, tinkering and chance. For example, in the 1940s and 1950s, when knowledge of turbulent combustion or fluid dynamics was still crude, jet engines were invented through "running the device to destruction, analyzing what broke [...] and repeating the process". Scientific explanations often follow technological developments rather than preceding them. Many discoveries also arose from pure chance, like the discovery of penicillin as a result of accidental lab contamination. Since the 1960s, the assumption that government funding of basic research would lead to the discovery of marketable technologies has lost credibility. Probabilist Nassim Taleb argues that national research programs that implement the notions of serendipity and convexity through frequent trial and error are more likely to lead to useful innovations than research that aims to reach specific outcomes. Despite this, modern technology is increasingly reliant on deep, domain-specific scientific knowledge. In 1975, there was an average of one citation of scientific literature in every three patents granted in the U.S.; by 1989, this increased to an average of one citation per patent. The average was skewed upwards by patents related to the pharmaceutical industry, chemistry, and electronics. A 2021 analysis shows that patents that are based on scientific discoveries are on average 26% more valuable than equivalent non-science-based patents. Other animal species The use of basic technology is also a feature of non-human animal species. Tool use was once considered a defining characteristic of the genus Homo. This view was supplanted after discovering evidence of tool use among chimpanzees and other primates, dolphins, and crows. For example, researchers have observed wild chimpanzees using basic foraging tools, pestles, levers, using leaves as sponges, and tree bark or vines as probes to fish termites. West African chimpanzees use stone hammers and anvils for cracking nuts, as do capuchin monkeys of Boa Vista, Brazil. Tool use is not the only form of animal technology use; for example, beaver dams, built with wooden sticks or large stones, are a technology with "dramatic" impacts on river habitats and ecosystems. In popular culture The relationship of humanity with technology has been explored in science-fiction literature, for example in Brave New World, A Clockwork Orange, Nineteen Eighty-Four, Isaac Asimov's essays, and movies like Minority Report, Total Recall, Gattaca, and Inception. It has spawned the dystopian and futuristic cyberpunk genre, which juxtaposes futuristic technology with societal collapse, dystopia or decay. Notable cyberpunk works include William Gibson's Neuromancer novel, and movies like Blade Runner, and The Matrix. See also References Citations Sources Further reading Gribbin, John, "Alone in the Milky Way: Why we are probably the only intelligent life in the galaxy", Scientific American, vol. 319, no. 3 (September 2018), pp. 94–99. "Is life likely to exist elsewhere in the [Milky Way] galaxy? Almost certainly yes, given the speed with which it appeared on Earth. Is another technological civilization likely to exist today? Almost certainly no, given the chain of circumstances that led to our existence. These considerations suggest that we are unique not just on our planet but in the whole Milky Way. And if our planet is so special, it becomes all the more important to preserve this unique world for ourselves, our descendants and the many creatures that call Earth home." (p. 99.)

History is the systematic study of the past, focusing primarily on the human past. As an academic discipline, it analyses and interprets evidence to construct narratives about what happened and explain why it happened. Some theorists categorize history as a social science, while others see it as part of the humanities or consider it a hybrid discipline. Similar debates surround the purpose of history—for example, whether its main aim is theoretical, to uncover the truth, or practical, to learn lessons from the past. In a more general sense, the term history refers not to an academic field but to the past itself, times in the past, or to individual texts about the past. Historical research relies on primary and secondary sources to reconstruct past events and validate interpretations. Source criticism is used to evaluate these sources, assessing their authenticity, content, and reliability. Historians strive to integrate the perspectives of several sources to develop a coherent narrative. Different schools of thought, such as positivism, the Annales school, Marxism, and postmodernism, have distinct methodological approaches. History is a broad discipline encompassing many branches. Some focus on specific time periods, such as ancient history, while others concentrate on particular geographic regions, such as the history of Africa. Thematic categorizations include political history, military history, social history, and economic history. Branches associated with specific research methods and sources include quantitative history, comparative history, and oral history. History emerged as a field of inquiry in antiquity to replace myth-infused narratives, with influential early traditions originating in Greece, China, and later in the Islamic world. Historical writing evolved throughout the ages and became increasingly professional, particularly during the 19th century, when a rigorous methodology and various academic institutions were established. History is related to many fields, including historiography, philosophy, education, and politics. Definition As an academic discipline, history is the study of the past with the main focus on the human past. It conceptualizes and describes what happened by collecting and analysing evidence to construct narratives. These narratives cover not only how events developed over time but also why they happened and in which contexts, providing an explanation of relevant background conditions and causal mechanisms. History further examines the meaning of historical events and the underlying human motives driving them. In a slightly different sense, history refers to the past events themselves. Under this interpretation, history is what happened rather than the academic field studying what happened. When used as a countable noun, a history is a representation of the past in the form of a history text. History texts are cultural products involving active interpretation and reconstruction. The narratives presented in them can change as historians discover new evidence or reinterpret already-known sources. The past itself, by contrast, is static and unchangeable. Some historians focus on the interpretative and explanatory aspects to distinguish histories from chronicles, arguing that chronicles only catalogue events in chronological order, whereas histories aim at a comprehensive understanding of their causes, contexts, and consequences. History has been primarily concerned with written documents. It focused on recorded history since the invention of writing, leaving prehistory to other fields, such as archaeology. Its scope broadened in the 20th century as historians became interested in the human past before the invention of writing. Historians debate whether history is a social science or forms part of the humanities. Like social scientists, historians formulate hypotheses, gather objective evidence, and present arguments based on this evidence. At the same time, history aligns closely with the humanities because of its reliance on subjective aspects associated with interpretation, storytelling, human experience, and cultural heritage. Some historians strongly support one or the other classification while others characterize history as a hybrid discipline that does not belong to one category at the exclusion of the other. History contrasts with pseudohistory, a label used to describe practices that deviate from historiographical standards by relying on disputed historical evidence, selectively ignoring genuine evidence, or using other means to distort the historical record. Often motivated by specific ideological agendas, pseudohistorical practices mimic historical methodology to promote biased, misleading narratives that lack rigorous analysis and scholarly consensus. Purpose Various suggestions about the purpose or value of history have been made. Some historians propose that its primary function is the pure discovery of truth about the past. This view emphasizes that the disinterested pursuit of truth is an end in itself, while external purposes, associated with ideology or politics, threaten to undermine the accuracy of historical research by distorting the past. In this role, history also challenges traditional myths lacking factual support. A different perspective suggests that the main value of history lies in the lessons it teaches for the present. This view is based on the idea that an understanding of the past can guide decision-making, for example, to avoid repeating previous mistakes. A related perspective focuses on a general understanding of the human condition, making people aware of the diversity of human behaviour across different contexts—similar to what one can learn by visiting foreign countries. History can also foster social cohesion by providing people with a collective identity through a shared past, helping to preserve and cultivate cultural heritage and values across generations. For some scholars, including Whig historians and the Marxist scholar E. H. Carr, history is a key to understanding the present and, in Carr's case, shaping the future. History has sometimes been used for political or ideological purposes, for instance, to justify the status quo by emphasising the respectability of certain traditions or to promote change by highlighting past injustices. In extreme forms, evidence is intentionally ignored or misinterpreted to construct misleading narratives, which can result in pseudohistory or historical denialism. Influential examples are Holocaust denial, Armenian genocide denial, Nanjing Massacre denial, and Holodomor denial. Etymology The word history comes from the Ancient Greek term ἵστωρ (histōr), meaning 'learned, wise man'. It gave rise to the Ancient Greek word ἱστορία (historiā), which had a wide meaning associated with inquiry in general and giving testimony. The term was later adopted into Classical Latin as historia. In Hellenistic and Roman times, the meaning of the term shifted, placing more emphasis on narrative aspects and the art of presentation rather than focusing on investigation and testimony. The word entered Middle English in the 14th century via the Old French term histoire. At this time, it meant 'story, tale', encompassing both factual and fictional narratives. In the 15th century, its meaning shifted to cover the branch of knowledge studying the past in addition to narratives about the past. In the 18th and 19th centuries, the word history became more closely associated with factual accounts and evidence-based inquiry, coinciding with the professionalization of historical inquiry, a meaning still dominant in contemporary usage. The dual meaning, referring to both mere stories and factual accounts of the past, is present in the terms for history in many other European languages. They include the French histoire, the Italian storia, and the German Geschichte. Methods The historical method is a set of techniques historians use to research and interpret the past, covering the processes of collecting, evaluating, and synthesizing evidence. It seeks to ensure scholarly rigour, accuracy, and reliability in how historical evidence is chosen, analysed, and interpreted. Historical research often starts with a research question to define the scope of the inquiry. Some research questions focus on a simple description of what happened. Others aim to explain why a particular event occurred, refute an existing theory, or confirm a new hypothesis. Sources and source criticism To answer research questions, historians rely on various types of evidence to reconstruct the past and support their conclusions. Historical evidence is usually divided into primary and secondary sources. A primary source is a source that originated during the period that is studied. Primary sources can take various forms, such as official documents, letters, diaries, eyewitness accounts, photographs, and audio or video recordings. They also include historical remains examined in archaeology, geology, and the medical sciences, such as artefacts and fossils unearthed from excavations. Primary sources offer the most direct evidence of historical events. A secondary source is a source that analyses or interprets information found in other sources. Whether a document is a primary or a secondary source depends not only on the document itself but also on the purpose for which it is used. For example, if a historian writes a text about slavery based on an analysis of historical documents, then the text is a secondary source on slavery and a primary source on the historian's opinion. Consistency with available sources is one of the main standards of historical works. For instance, the discovery of new sources may lead historians to revise or dismiss previously accepted narratives. To find and access primary and secondary sources, historians consult archives, libraries, and museums. Archives play a central role by preserving countless original sources and making them available to researchers in a systematic and accessible manner. Thanks to technological advances, historians increasingly rely on online resources, which offer vast digital databases with methods to search and access specific documents. Source criticism is the process of analysing and evaluating the information a source provides. Typically, this process begins with external criticism, which evaluates the authenticity of a source. It addresses the questions of when and where the source was created and seeks to identify the author, understand their reasons for producing the source, and determine if it has undergone some type of modification since its creation. Additionally, the process involves distinguishing between original works, copies, and deceptive forgeries. Internal criticism evaluates the content of a source, typically beginning with the clarification of the meaning within the source. This involves disambiguating individual terms that could be misunderstood but may also require a general translation if the source is written in an unfamiliar language. Once the information content of a source is understood, internal criticism is specifically interested in determining accuracy. Critics ask whether the information is reliable or misrepresents the topic and further question whether the source is comprehensive or omits important details. One way to make these assessments is to evaluate whether the author was able, in principle, to provide a faithful presentation of the studied event. Other approaches include the assessment of the influences of the author's intentions and prejudices, and cross-referencing information with other credible sources. Being aware of the inadequacies of a source helps historians decide whether and which aspects of it to trust, and how to use it to construct a narrative. Synthesis and schools of thought The selection, analysis, and criticism of sources result in the validation of a large collection of mostly isolated statements about the past. As a next step, sometimes termed historical synthesis, historians examine how the individual pieces of evidence fit together to form part of a larger story. Constructing this broader perspective is crucial for a comprehensive understanding of the topic as a whole. It is a creative aspect of historical writing that reconstructs, interprets, and explains what happened by showing how different events are connected. In this way, historians address not only which events occurred but also why they occurred and what consequences they had. While there are no universally accepted techniques for this synthesis, historians rely on various interpretative tools and approaches in this process. One tool to provide an accessible overview of complex developments is the use of periodization, which divides a timeframe into different periods, each organized around central themes or developments that shaped the period. For example, the three-age system is traditionally used to divide early human history into Stone Age, Bronze Age, and Iron Age based on the predominant materials and technologies during these periods. Another methodological tool is the examination of silences, gaps or omissions in the historical record of events that occurred but did not leave significant evidential traces. Silences can happen when contemporaries find information too obvious to document but may also occur if there are specific reasons to withhold or destroy information. Conversely, when large datasets are available, quantitative approaches can be used. For instance, economic and social historians commonly employ statistical analysis to identify patterns and trends associated with large groups. Different schools of thought often come with their own methodological implications for how to write history. Positivists emphasize the scientific nature of historical inquiry, focusing on empirical evidence to discover objective truths. In contrast, postmodernists reject grand narratives that claim to offer a single, objective truth. Instead, they highlight the subjective nature of historical interpretation, which leads to a multiplicity of divergent perspectives. Marxists interpret historical developments as expressions of economic forces and class struggles. The Annales school highlights long-term social and economic trends while relying on quantitative and interdisciplinary methods. Feminist historians study the role of gender in history, with a particular interest in analysing the experiences of women to challenge patriarchal perspectives. Areas of study History is a wide field of inquiry encompassing many branches. Some branches focus on a specific time period, while others concentrate on a particular geographic region or a distinct theme. Specializations of different types can usually be combined; for example, a work on economic history in ancient Egypt merges temporal, regional, and thematic perspectives. For topics with a broad scope, the amount of primary sources is often too extensive for an individual historian to review, forcing them to either narrow the scope of their topic or also rely on secondary sources to arrive at a wide overview. By period Chronological division is a common approach to organizing the vast expanse of history into more manageable segments. Different periods are often defined based on dominant themes that characterize a specific time frame and significant events that initiated these developments or brought them to an end. Depending on the selected context and level of detail, a period may be as short as a decade or longer than several centuries. A traditionally influential approach divides human history into prehistory, ancient history, post-classical history, early modern history, and modern history. Depending on the region and theme, the time frames covered by these periods can vary and historians may use entirely different periodizations. For example, traditional periodizations of Chinese history follow the main dynasties, and the division into pre-Columbian, colonial, and post-colonial periods plays a central role in the history of the Americas. The study of prehistory includes the examination of the evolution of human-like species several million years ago, leading to the emergence of anatomically modern humans about 200,000 years ago. Subsequently, humans migrated out of Africa to populate most of the earth. Towards the end of prehistory, technological advances in the form of new and improved tools led many groups to give up their established nomadic lifestyle, based on hunting and gathering, in favour of a sedentary lifestyle supported by early forms of agriculture. The absence of written documents from this period presents researchers with unique challenges. It results in an interdisciplinary approach relying on other forms of evidence from fields such as archaeology, anthropology, palaeontology, and geology. Historians studying the ancient period examine the emergence of the first major civilizations in regions such as Mesopotamia, Egypt, the Indus Valley, China, and Peru, beginning approximately 3500 BCE in some regions. The new social, economic, and political complexities necessitated the development of writing systems. Thanks to advancements in agriculture, surplus food allowed these civilizations to support larger populations, leading to urbanization, the establishment of trade networks, and the emergence of regional empires. In the later part of the ancient period, sometimes termed the classical period, societies in China, India, Persia, and the Mediterranean expanded further, reaching new cultural, scientific, and political heights. Meanwhile, influential religious systems and philosophical ideas were first formulated, such as Hinduism, Buddhism, Confucianism, Judaism, and Greek philosophy. In the study of post-classical or medieval history, which began around 500 CE, historians note the growing influence of major religions. Missionary religions, like Buddhism, Christianity, and Islam, spread rapidly and established themselves as world religions, marking a cultural shift as they gradually replaced other belief systems. Meanwhile, inter-regional trade networks flourished, leading to increased technological and cultural exchange. Conquering many territories in Asia and Europe, the Mongol Empire became a dominant force during the 13th and 14th centuries. Historians focused on early modern history, which started roughly in 1500 CE, commonly highlight how European states rose to global power. As gunpowder empires, they explored and colonized large parts of the world. As a result, the Americas were integrated into the global network, triggering a vast biological exchange of plants, animals, people, and diseases. The Scientific Revolution prompted major discoveries and accelerated technological progress. It was accompanied by other intellectual developments, such as humanism and the Enlightenment, which ushered in secularization. In the study of modern history, which began at the end of the 18th century, historians are interested in how the Industrial Revolution transformed economies by introducing more efficient modes of production. Western powers established vast colonial empires, gaining superiority through industrialized military technology. The increased international exchange of goods, ideas, and people marked the beginning of globalization. Various social revolutions challenged autocratic and colonial regimes, paving the way for democracies. Many developments in fields like science, technology, economy, living standards, and human population accelerated at unprecedented rates. This happened despite the widespread destruction caused by two world wars, which rebalanced international power relations by undermining European dominance. By geographic location Areas of historical study can also be categorized by the geographic locations they examine. Geography plays a central role in history through its influence on food production, natural resources, economic activities, political boundaries, and cultural interactions. Some historical works limit their scope to small regions, such as a village or a settlement. Others focus on broad territories that encompass entire continents, like the histories of Africa, Asia, Europe, the Americas, and Oceania. The history of Africa begins with the examination of the evolution of anatomically modern humans. Ancient historians describe how the invention of writing and the establishment of civilization happened in ancient Egypt in the 4th millennium BCE. Over the next millennia, other notable civilizations and kingdoms formed in Nubia, Axum, Carthage, Ghana, Mali, and Songhay. Islam began spreading across North Africa in the 7th century CE and became the dominant faith in many empires. Meanwhile, trade along the trans-Saharan route intensified. Beginning in the 15th century, millions of Africans were enslaved and forcibly transported to the Americas as part of the Atlantic slave trade. Most of the continent was colonized by European powers in the late 19th and early 20th centuries. Amid rising nationalism, African states gradually gained independence in the aftermath of World War II, a period that saw economic progress, rapid population growth, and struggles for political stability. Historians studying the history of Asia note the arrival of anatomically modern humans around 100,000 years ago. They explore Asia's role as one of the cradles of civilization, with the emergence of some of the first ancient civilizations in Mesopotamia, the Indus Valley, and China beginning in the 4th and 3rd millennia BCE. In the following millennia, civilisations on the Asian continent gave birth to all major world religions and several influential philosophical traditions, such as Hinduism, Buddhism, Confucianism, Taoism, Christianity, and Islam. Other developments were the establishment of the Silk Road, which facilitated trade and cultural exchange across Eurasia, and the formation of powerful empires, such as the Mongol Empire. European influence grew over the following centuries, ushering in the modern era. It culminated in the 19th and early 20th centuries when many parts of Asia came under direct colonial control until the end of World War II. The post-independence period was characterized by modernization, economic growth, and a steep increase in population. In the study of the history of Europe, historians describe the arrival of the first anatomically modern humans about 45,000 years ago. They explore how in the first millennium BCE the Ancient Greeks contributed key elements to the culture, philosophy, and politics associated with the Western world, and how their cultural heritage influenced the Roman and Byzantine Empires. The medieval period began with the fall of the Western Roman Empire in the 5th century CE and was marked by the spread of Christianity. Starting in the 15th century, European exploration and colonization interconnected the globe, while cultural, intellectual, and scientific developments transformed Western societies. From the late 18th to the early 20th centuries, European global dominance was further solidified by the Industrial Revolution and the establishment of large overseas colonies. It came to an end because of the devastating effects of two world wars. In the following Cold War era, the continent was divided into a Western and an Eastern bloc. They pursued political and economic integration in the aftermath of the Cold War. Historians examining the history of the Americas document the arrival of the first humans around 20,000 to 15,000 years ago. The Americas were home to some of the earliest civilizations, like the Norte Chico civilization in South America and the Maya and Olmec civilizations in Central America. Over the next millennia, major empires arose beside them, such as the Teotihuacan, Aztec, and Inca empires. Following the arrival of the Europeans from the late 15th century onwards, the spread of newly introduced diseases drastically reduced the local population. Together with colonization, it led to the collapse of major empires as demographic and cultural landscapes were reshaped. Independence movements in the 18th and 19th centuries led to the formation of new nations across the Americas. In the 20th century, the United States emerged as a dominant global power and a key player in the Cold War. In the study of the history of Oceania, historians note the arrival of humans about 60,000 to 50,000 years ago. They explore the establishment of diverse regional societies and cultures, first in Australia and Papua New Guinea and later also on other Pacific Islands. The arrival of the Europeans in the 16th century prompted significant transformations, and by the end of the 19th century, most of the region had come under Western control. Oceania became involved in various conflicts during the world wars and experienced decolonization in the post-war period. By theme Historians often limit their inquiry to a specific theme. Some propose a general subdivision into three major themes: political history, economic history, and social history. However, the boundaries between these branches are vague and their relation to other thematic branches, such as intellectual history, is not always clear. Political history studies the organization of power in society, examining how power structures arise, develop, and interact. Throughout most of recorded history, states or state-like structures have been central to this field of study. It explores how a state was organized internally, like factions, parties, leaders, and other political institutions. It also examines which policies were implemented and how the state interacted with other states. Political history has been studied since antiquity by historians such as Herodotus and Thucydides, making it one of the oldest branches of history, while other major subfields have only become established branches in the past century. Diplomatic and military history are associated with political history. Diplomatic history examines international relations between states. It covers foreign policy topics such as negotiations, strategic considerations, treaties, and conflicts between nations as well as the role of international organizations in these processes. Military history studies the impact and development of armed conflicts in human history. This includes the examination of specific events, like the analysis of a particular battle and the discussion of the different causes of a war. It also involves more general considerations about the evolution of warfare, including advancements in military technology, strategies, tactics, logistics, and institutions. Economic history examines how commodities are produced, exchanged, and consumed. It covers economic aspects such as the use of land, labour, and capital, the supply and demand of goods, the costs and means of production, and the distribution of income and wealth. Economic historians typically focus on general trends in the form of impersonal forces, such as inflation, rather than the actions and decisions of individuals. If enough data is available, they rely on quantitative methods, like statistical analysis. For periods before the modern era, available data is often limited, forcing economic historians to rely on scarce sources and extrapolate information from them. Social history is a broad field investigating social phenomena, but its precise definition is disputed. Some theorists understand it as the study of everyday life outside the domains of politics and economics, including cultural practices, family structures, community interactions, and education. A closely related approach focuses on experience rather than activities, examining how members of particular social groups, like social classes, races, genders, or age groups, experienced their world. Other definitions see social history as the study of social problems, like poverty, disease, and crime, or take a broader perspective by examining how whole societies developed. Closely related fields include cultural history, gender history, and religious history. Intellectual history is the history of ideas and studies how concepts, philosophies, and ideologies have evolved. It is particularly interested in academic fields but not limited to them, including the study of the beliefs and prejudices of ordinary people. In addition to studying intellectual movements themselves, it also examines the cultural and social contexts that shaped them and their influence on other historical developments. As closely related fields, the history of philosophy investigates the development of philosophical thought while the history of science studies the evolution of scientific theories and practices, such as the scientific contributions of Charles Darwin and Albert Einstein. Art history, another connected discipline, examines historical works of art and the development of artistic activities, styles, and movements. It includes a discussion of the cultural, social, and political contexts of art production. Environmental history studies the relation between humans and their environment. It seeks to understand how humans and the rest of nature have affected each other in the course of history. Other thematic branches include constitutional history, legal history, urban history, business history, history of technology, medical history, history of education, and people's history. Others Some branches of history are characterized by the methods they employ, such as quantitative history and digital history, which rely on quantitative methods and digital media. Comparative history compares historical phenomena from distinct times, regions, or cultures to examine their similarities and differences. Unlike most other branches, oral history relies on oral reports rather than written documents, encompassing eyewitness accounts, hearsay, and communal legends. It reflects the personal experiences, interpretations, and memories of common people, showcasing how people subjectively remember the past. Counterfactual history uses counterfactual thinking to examine alternative courses of history, exploring what could have happened under different circumstances. Certain branches of history are distinguished by their theoretical outlook, such as Marxist and feminist history. Some distinctions focus on the scope of the studied topic. Big History is the branch with the broadest scope, covering everything from the Big Bang to the present, incorporating elements of cosmology, geology, biology, and anthropology. World history is another branch with a wide topic. It examines human history as a whole, starting with the evolution of human-like species. The terms macrohistory, mesohistory, and microhistory refer to different scales of analysis, ranging from large-scale patterns that affect the whole globe to detailed studies of local contexts, small communities, family histories, particular individuals, or specific events. Closely related to microhistory is the genre of historical biography, which recounts an individual's life in its historical context and the legacy it left. Public history involves activities that present history to the general public. It usually happens outside the traditional academic settings in contexts like museums, historical sites, heritage tourism, and popular media. Evolution of the discipline Before the invention of writing, the preservation and transmission of historical knowledge were limited to oral traditions. Early forms of historical writing mixed facts with mythological elements, such as the Epic of Gilgamesh from ancient Mesopotamia and the Odyssey, an ancient Greek text attributed to Homer. Published in the 5th century BCE, the Histories by Herodotus was one of the foundational texts of the Western historical tradition, putting more emphasis on rational and evidence-based inquiry than the stories of Homer and other poets. Thucydides followed and further refined Herodotus's approach but focused more on particular political and military developments in contrast to the wide scope and ethnographic elements of Herodotus's work. Roman historiography was heavily influenced by Greek traditions. It often included not only historical facts but also moral judgments of historical figures. Early Roman historians used an annalistic style, arranging past events by year with little commentary, while later ones preferred a more narrative and analytical approach. Another complex tradition of historical writing emerged in ancient China, with early precursors starting in the late 2nd millennium BCE. It considered annals the highest form of historical writing and emphasized verification through sources. This tradition was associated with Confucian philosophy and closely tied to the government in the form of the ruling dynasty, each responsible for writing the official history of its predecessor. Chinese historians established a coherent and systematic method for recording historical events earlier than other traditions. Of particular influence was the work of Sima Qian, whose meticulous research method and inclusion of alternative viewpoints shaped subsequent historiographical standards. In ancient India, historical narratives were closely associated with religion. They often mixed factual accounts with supernatural elements, as seen in works like the Mahabharata. In Europe during the medieval period, history was primarily documented by the clergy in the form of chronicles. Christian historians drew from Greco-Roman and Jewish traditions and reinterpreted the past from a religious perspective as a narrative highlighting God's divine plan. Influential contributions shaping this tradition were made by the historians Eusebius of Caesarea and Bede and by the theologian Augustine of Hippo. In the Islamic world, historical writing was similarly influenced by religion, interpreting the past from a Muslim perspective. It placed great importance on the chain of transmission to preserve the authority of historical accounts. Al-Tabari wrote a comprehensive history, spanning from the creation of the world to his present day. Ibn Khaldun reflected on philosophical issues underlying the practice of historians, such as universal patterns shaping historical changes and the limits of historical truth. With the emergence of the Tang Dynasty (618–907 CE) in China, historical writing became increasingly institutionalized as a bureau for the writing of history was established in 629 CE. The bureau oversaw the establishment of Veritable Records, a comprehensive compilation serving as the basis of the standard national history. Tang dynasty historians emphasized the difference between actual events that occurred in the past and the way these events are documented in historical texts. Historical writing in the Song dynasty (960–1279 CE) happened in a variety of historical genres, including encyclopedias, biographies, and historical novels, while history became a standard subject in the Chinese educational system. Influenced by the Chinese model, a tradition of historical writing emerged in Japan in the 8th century CE. Like in China, historical writing was closely related to the imperial household, but Japanese historians placed less importance on critical source evaluation than their Chinese counterparts. During the Renaissance and the early modern period (approximately 1500 to 1800), the different historical traditions came increasingly into contact with each other. Starting in 14th-century Europe, the Renaissance led to a shift away from medieval religious outlooks towards a renewed interest in the earlier classical tradition of Greece and Rome. Renaissance humanists used sophisticated text criticism to scrutinize earlier religious historical works, which contributed to the secularization of historical writing. During the 15th to 17th centuries, historians placed greater emphasis on the didactic role of history, using it to promote the established order or argue for a return to an idealised vision of the past. As the invention of the printing press made written documents more accessible and affordable, interest in history expanded outside the clergy and nobility. At the same time, empiricist thought associated with the Scientific Revolution questioned the possibility of arriving at universal historical truths. During the Age of Enlightenment in the 18th century, historical writing was influenced by rationalism and scepticism. Aiming to challenge traditional authority and dogma through reason and empirical methods, historians tried to uncover deeper patterns and meaning in the past, while the scope of historical inquiry expanded with an increased focus on societal and economic topics as well as comparisons between different cultures. In China during the Ming dynasty (1368–1644), public interest in historical writings and their availability also increased. In addition to the continuation of the Veritable Records by official governmental historians, non-official works by private scholars flourished. These scholars tended to use a more creative style and sometimes challenged orthodox accounts. In the Islamic world, new traditions of historical writings emerged in the Safavid, Mughal, and Ottoman empires. Meanwhile, in the Americas, European explorers recorded and interpreted indigenous narratives, which had been passed down through oral and pictographic practices. These views sometimes contested traditional European perspectives. Historical writing was transformed in the 19th century as it became more professional and science-oriented. Following the work of Leopold von Ranke, a systematic method of source criticism was widely accepted while academic institutions dedicated to history were established in the form of university departments, professional associations, and journals. In tune with this scientific outlook, Auguste Comte formulated the school of positivism and aimed to discover general laws of history, similar to the laws of nature studied by physicists. Building on the philosophy of Georg Wilhelm Friedrich Hegel, Karl Marx proposed one such general law in his theory of historical materialism, arguing that economic forces and class struggle are the fundamental drivers of historical change. Another influential development was the spread of European historiographical methods, which became the dominant approach to the academic study of the past worldwide. In the 20th century, traditional historical assumptions and practices were challenged while the scope of historical research broadened. The Annales school used insights from sociology, psychology, and economics to study long-term developments. Authoritarian regimes, like Nazi Germany, the Soviet Union, and China, manipulated historical narratives for ideological purposes. Various historians covered unconventional perspectives, focusing on the experiences of marginalized groups through approaches such as history from below, microhistory, oral history, and feminist history. Postcolonialism aimed to undermine the hegemony of the Western approach and postmodernism rejected the claim to a single universal truth in history. Intellectual historians examined the historical development of ideas. In the second half of the century, renewed attempts to write histories of the world as a whole gained momentum, while technological advances fostered the growth of quantitative and digital history. Related fields Historiography Historiography is the study of the methods and development of historical research. Historiographers examine what historians do, resulting in a metatheory in the form of a history of history. Some theorists use the term historiography in a different sense to refer to written accounts of the past. A central topic in historiography as a metatheory focuses on the standards of evidence and reasoning in historical inquiry. Historiographers examine and codify how historians use sources to construct narratives about the past, including the analysis of the interpretative assumptions from which they proceed. Closely related issues include the style and rhetorical presentation of works of history. By comparing the works of different historians, historiographers identify schools of thought based on shared research methods, assumptions, and styles. For example, they examine the characteristics of the Annales school, like its use of quantitative data from various disciplines and its interest in economic and social developments taking place over extended periods. Comparisons also extend to whole eras from ancient to modern times. This way, historiography traces the development of history as an academic discipline, highlighting how the dominant methods, themes, and research goals have changed over time. Philosophy of history The philosophy of history investigates the theoretical foundations of history. It is interested both in the past itself as a series of interconnected events and in the academic field studying this process. Insights and approaches from various branches of philosophy are relevant to this endeavour, such as metaphysics, epistemology, hermeneutics, and ethics. In examining history as a process, philosophers explore the basic entities that make up historical phenomena. Some approaches rely primarily on the beliefs and actions of individual humans, while others include collective and other general entities, such as civilizations, institutions, ideologies, and social forces. A related topic concerns the nature of causal mechanisms connecting historic events with their causes and consequences. One view holds that there are general laws of history that determine the course of events, similar to the laws of nature studied in the natural sciences. According to another perspective, causal relations between historic events are unique and shaped by contingent factors. Historically, some philosophers have suggested that the general direction of the course of history follows large patterns. According to one proposal, history is cyclic, meaning that on a sufficiently large scale, individual events or general trends repeat. Another such theory asserts that history is a linear, teleological process moving towards a predetermined goal. The topics of philosophy of history and historiography overlap as both are interested in the standards of historical reasoning. Historiographers typically focus more on describing specific methods and developments encountered in the study of history. Philosophers of history, by contrast, tend to explore more general patterns, including evaluative questions about which methods and assumptions are correct. Historical reasoning is sometimes used in philosophy and other disciplines as a method to explain phenomena. This approach, known as historicism, argues that understanding something requires knowledge of its unique history or how it evolved. For instance, historicism about truth states that truth depends on historical circumstances, meaning that there are no transhistorical truths. Historicism contrasts with approaches that seek a timeless and universal understanding of their subject matter. Historical objectivity Diverse debates in the philosophy of history focus on the possibility of an objective account of history. Various theorists argue that this ideal is not achievable, pointing to the subjective nature of interpretation, the narrative aspect of history, and the influence of personal values and biases on the perspective and actions of both historic individuals and historians. According to one view, some particular facts are objective, for example, facts about when a drought occurred or which army was defeated. However, this view does not ensure general objectivity since historians have to interpret and synthesize facts to arrive at an overall narrative describing large trends and developments. As a result, some historians, such as G. M. Trevelyan and Keith Jenkins, assert that all history is biased, arguing that historical narratives are never free of subjective presuppositions and value judgments. Some outlooks associated with realism, empiricism, and reconstructionism, conceptualise history as the search for truth or knowledge, which they see as recoverable through rigorous evaluation and careful interpretation of evidence. Other scholars critique this view, emphasising the subjective and partial nature of historical knowledge. Perspectivists claim that historical perspectives are inherently subjective, as they require selecting particular sources and inquiries, and ascertaining what information can be regarded as historical fact. They argue that statements can only be objective within or relative to one of several competing historical perspectives. A stronger scepticist or relativist outlook states that no historical knowledge can be proven objective. This emphasis on subjectivities has been extended by postmodernist theories that suggest that it is impossible to know the past objectively, adding that meaning is created through human-made texts, the language of which "constitute our world as we perceive it". Neo-realists have responded to this trend by reemphasising the centrality of empiricist methodologies to historical analysis. They acknowledge the influence of subjective evaluations but contend that historical truth is reachable nonetheless. Education History is part of the school curriculum in most countries. Early history education aims to make students interested in the past and familiarize them with fundamental concepts of historical thought. By fostering a basic historical awareness, it seeks to instil a sense of identity by helping them understand their cultural roots. It often takes a narrative form by presenting children with simple stories, which may focus on historic individuals or the origins of local holidays, festivals, and food. More advanced history education encountered in secondary school covers a broader spectrum of topics, ranging from ancient to modern history, at both local and global levels. It further aims to acquaint students with historical research methodologies, including the abilities to interpret and critically evaluate historical claims. History teachers employ a variety of teaching methods. They include narrative presentations of historical developments, questions to engage students and prompt critical thinking, and discussions on historical topics. Students work with historical sources directly to learn how to analyse and interpret evidence, both individually and in group activities. They engage in historical writing to develop the skills of articulating their thoughts clearly and persuasively. Assessment through oral or written tests aims to ensure that learning goals are reached. Traditional methodologies in history education often present numerous facts, like dates of significant events and names of historical figures, which students are expected to memorize. Some modern approaches, by contrast, seek to foster a more active engagement and a deeper interdisciplinary understanding of general patterns, focusing not only on what happened but also on why it happened and its lasting historical significance. History education in state schools serves a variety of purposes. A key skill is historical literacy, the ability to comprehend, critically analyse, and respond to historical claims. By making students aware of significant developments in the past, they can become familiar with various contexts of human life, helping them understand the present and its diverse cultures. At the same time, history education can foster a sense of cultural identity by connecting students with their heritage, traditions, and practices, for example, by introducing them to iconic elements ranging from national landmarks and monuments to historical figures and traditional festivities. Knowledge of a shared past and cultural heritage can contribute to the formation of a national identity and prepares students for active citizenship. This political aspect of history education may spark disputes about which topics school textbooks should cover. In various regions, it has resulted in so-called history wars over the curriculum. It can lead to a biased treatment of controversial topics in an attempt to present their national heritage in a favourable light. In addition to the formal education provided in public schools, history is also taught in informal settings outside the classroom. Public history takes place in locations like museums and memorial sites, where selected artefacts are often used to tell specific stories. It includes popular history, which aims to make the past accessible and appealing to a wide audience of non-specialists in media such as books, television programmes, and online content. Informal history education also happens in oral traditions as narratives about the past are transmitted across generations. Other fields History employs an interdisciplinary methodology, drawing on findings from fields such as archaeology, geology, genetics, anthropology, and linguistics. Archaeologists study human-made historical artefacts and other forms of material culture. Their findings provide crucial insights into past human activities and cultural developments. The interpretation of archaeological evidence presents challenges that differ from standard historical work with written documents. At the same time, it offers new possibilities by presenting information that was not recorded, allowing historians to access the past of non-literate societies and marginalized groups within literate societies by studying the remains of their material culture. Before the advent of modern archaeology in the 19th century, antiquarianism laid the groundwork for this discipline and played a vital role in preserving historical artefacts. Geology and other earth sciences help historians understand the environmental contexts and physical processes that affected past societies, including climate conditions, landscapes, and natural events. Genetics provides key information about the evolutionary origins of humans as a species, human migration, ancestry, and demographic changes. Anthropologists investigate human culture and behaviour, such as social structures, belief systems, and ritual practices. This knowledge offers contexts for the interpretation of historical events. Historical linguistics studies the development of languages over time, which can be crucial for the interpretation of ancient documents and can also provide information about migration patterns and cultural exchanges. Historians further rely on evidence from various other fields belonging to the physical, biological, and social sciences as well as the humanities. In virtue of its relation to ideology and national identity, history is closely connected to politics and historical theories can directly impact political decisions. For example, irredentist attempts by one state to annex territory of another state often rely on historical theories claiming that the disputed territory belonged to the first state in the past. History also plays a central role in so-called historical religions, which base some of their core doctrines on historical events. For instance, Christianity is often categorized as a historical religion because it is centred around historical events surrounding Jesus Christ. History is relevant to many fields through the study of their past, including the history of science, mathematics, philosophy, and art. See also Glossary of history Outline of history References Notes Citations Sources External links Internet History Sourcebooks Project See also Internet History Sourcebooks Project (Collections of public domain and copy-permitted historical texts for educational use)

Geography (from Ancient Greek γεωγραφία geōgraphía; combining gê 'Earth' and gráphō 'write', literally 'Earth writing') is the study of the lands, features, inhabitants, and phenomena of Earth. Geography is an all-encompassing discipline that seeks an understanding of Earth and its human and natural complexities—not merely where objects are, but also how they have changed and come to be. While geography is specific to Earth, many concepts can be applied more broadly to other celestial bodies in the field of planetary science. Geography has been called "a bridge between natural science and social science disciplines." Origins of many of the concepts in geography can be traced to Greek Eratosthenes of Cyrene, who may have coined the term "geographia" (c. 276 BC – c. 195/194 BC). The first recorded use of the word γεωγραφία was as the title of a book by Greek scholar Claudius Ptolemy (100 – 170 AD). This work created the so-called "Ptolemaic tradition" of geography, which included "Ptolemaic cartographic theory." However, the concepts of geography (such as cartography) date back to the earliest attempts to understand the world spatially, with the earliest example of an attempted world map dating to the 9th century BCE in ancient Babylon. The history of geography as a discipline spans cultures and millennia, being independently developed by multiple groups, and cross-pollinated by trade between these groups. The core concepts of geography consistent between all approaches are a focus on space, place, time, and scale. Today, geography is an extremely broad discipline with multiple approaches and modalities. There have been multiple attempts to organize the discipline, including the four traditions of geography, and into branches. Techniques employed can generally be broken down into quantitative and qualitative approaches, with many studies taking mixed-methods approaches. Common techniques include cartography, remote sensing, interviews, and surveying. Fundamentals Geography is a systematic study of the Earth (other celestial bodies are specified, such as "geography of Mars", or given another name, such as areography in the case of Mars, or selenography in the case of the Moon, or planetography for the general case), its features, and phenomena that take place on it. For something to fall into the domain of geography, it generally needs some sort of spatial component that can be placed on a map, such as coordinates, place names, or addresses. This has led to geography being associated with cartography and place names. Although many geographers are trained in toponymy and cartology, this is not their main preoccupation. Geographers study the Earth's spatial and temporal distribution of phenomena, processes, and features as well as the interaction of humans and their environment. Because space and place affect a variety of topics, such as economics, health, climate, plants, and animals, geography is highly interdisciplinary. The interdisciplinary nature of the geographical approach depends on an attentiveness to the relationship between physical and human phenomena and their spatial patterns. While narrowing down geography to a few key concepts is extremely challenging, and subject to tremendous debate within the discipline, several sources have approached the topic. The 1st edition of the book "Key Concepts in Geography" broke down this into chapters focusing on "Space," "Place," "Time," "Scale," and "Landscape." The 2nd edition of the book expanded on these key concepts by adding "Environmental systems," "Social Systems," "Nature," "Globalization," "Development," and "Risk," demonstrating how challenging narrowing the field can be. Another approach used extensively in teaching geography are the Five themes of geography established by "Guidelines for Geographic Education: Elementary and Secondary Schools," published jointly by the National Council for Geographic Education and the Association of American Geographers in 1984. These themes are Location, place, relationships within places (often summarized as Human-Environment Interaction), movement, and regions. The five themes of geography have shaped how American education approaches the topic in the years since. Space Just as all phenomena exist in time and thus have a history, they also exist in space and have a geography. For something to exist in the realm of geography, it must be able to be described spatially. Thus, space is the most fundamental concept at the foundation of geography. The concept is so basic, that geographers often have difficulty defining exactly what it is. Absolute space is the exact site, or spatial coordinates, of objects, persons, places, or phenomena under investigation. We exist in space. Absolute space leads to the view of the world as a photograph, with everything frozen in place when the coordinates were recorded. Today, geographers are trained to recognize the world as a dynamic space where all processes interact and take place, rather than a static image on a map. Place Place is one of the most complex and important terms in geography. In human geography, place is the synthesis of the coordinates on the Earth's surface, the activity and use that occurs, has occurred, and will occur at the coordinates, and the meaning ascribed to the space by human individuals and groups. This can be extraordinarily complex, as different spaces may have different uses at different times and mean different things to different people. In physical geography, a place includes all of the physical phenomena that occur in space, including the lithosphere, atmosphere, hydrosphere, and biosphere. Places do not exist in a vacuum and instead have complex spatial relationships with each other, and place is concerned how a location is situated in relation to all other locations. As a discipline then, the term place in geography includes all spatial phenomena occurring at a location, the diverse uses and meanings humans ascribe to that location, and how that location impacts and is impacted by all other locations on Earth. In one of Yi-Fu Tuan's papers, he explains that in his view, geography is the study of Earth as a home for humanity, and thus place and the complex meaning behind the term is central to the discipline of geography. Time Time is usually thought to be within the domain of history, however, it is of significant concern in the discipline of geography. In physics, space and time are not separated, and are combined into the concept of spacetime. Geography is subject to the laws of physics, and in studying things that occur in space, time must be considered. Time in geography is more than just the historical record of events that occurred at various discrete coordinates; but also includes modeling the dynamic movement of people, organisms, and things through space. Time facilitates movement through space, ultimately allowing things to flow through a system. The amount of time an individual, or group of people, spends in a place will often shape their attachment and perspective to that place. Time constrains the possible paths that can be taken through space, given a starting point, possible routes, and rate of travel. Visualizing time over space is challenging in terms of cartography, and includes Space-Prism, advanced 3D geovisualizations, and animated maps. Scale Scale in the context of a map is the ratio between a distance measured on the map and the corresponding distance as measured on the ground. This concept is fundamental to the discipline of geography, not just cartography, in that phenomena being investigated appear different depending on the scale used. Scale is the frame that geographers use to measure space, and ultimately to understand a place. Laws of geography During the quantitative revolution, geography shifted to an empirical law-making (nomothetic) approach. Several laws of geography have been proposed since then, most notably by Waldo Tobler and can be viewed as a product of the quantitative revolution. In general, some dispute the entire concept of laws in geography and the social sciences. These criticisms have been addressed by Tobler and others, such as Michael Frank Goodchild. However, this is an ongoing source of debate in geography and is unlikely to be resolved anytime soon. Several laws have been proposed, and Tobler's first law of geography is the most generally accepted in geography. Some have argued that geographic laws do not need to be numbered. The existence of a first invites a second, and many have proposed themselves as that. It has also been proposed that Tobler's first law of geography should be moved to the second and replaced with another. A few of the proposed laws of geography are below: Tobler's first law of geography: "Everything is related to everything else, but near things are more related than distant." Tobler's second law of geography: "The phenomenon external to a geographic area of interest affects what goes on inside." Arbia's law of geography: "Everything is related to everything else, but things observed at a coarse spatial resolution are more related than things observed at a finer resolution." Spatial heterogeneity: Geographic variables exhibit uncontrolled variance. The uncertainty principle: "That the geographic world is infinitely complex and that any representation must therefore contain elements of uncertainty, that many definitions used in acquiring geographic data contain elements of vagueness, and that it is impossible to measure location on the Earth's surface exactly." Additionally, several variations or amendments to these laws exist within the literature, although not as well supported. For example, one paper proposed an amended version of Tobler's first law of geography, referred to in the text as the Tobler–von Thünen law, which states: "Everything is related to everything else, but near things are more related than distant things, as a consequence of accessibility." Sub-disciplines Geography is a branch of inquiry that focuses on spatial information on Earth. It is an extremely broad topic and can be broken down multiple ways. There have been several approaches to doing this spanning at least several centuries, including "four traditions of geography" and into distinct branches. The Four traditions of geography are often used to divide the different historical approach theories geographers have taken to the discipline. In contrast, geography's branches describe contemporary applied geographical approaches. Four traditions Geography is an extremely broad field. Because of this, many view the various definitions of geography proposed over the decades as inadequate. To address this, William D. Pattison proposed the concept of the "Four traditions of Geography" in 1964. These traditions are the Spatial or Locational Tradition, the Man-Land or Human-Environment Interaction Tradition (sometimes referred to as Integrated geography), the Area Studies or Regional Tradition, and the Earth Science Tradition. These concepts are broad sets of geography philosophies bound together within the discipline. They are one of many ways geographers organize the major sets of thoughts and philosophies within the discipline. Branches In another approach to the abovementioned four traditions, geography is organized into applied branches. The UNESCO Encyclopedia of Life Support Systems organizes geography into the three categories of human geography, physical geography, and technical geography. Some publications limit the number of branches to physical and human, describing them as the principal branches. Human geography largely focuses on the built environment and how humans create, view, manage, and influence space. Human geographers study people and their communities, cultures, economies, and environmental interactions by studying their relations with and across space and place. Physical geography examines the natural environment and how organisms, climate, soil, water, and landforms produce and interact, studying spatial patterns in the natural environment, atmosphere, hydrosphere, biosphere, and geosphere. The difference between these approaches led to the development of integrated geography, which combines physical and human geography and concerns the interactions between the environment and humans. Technical geography involves studying and developing the tools and techniques used by geographers, such as remote sensing, cartography, and geographic information system. Technical geography is interested in studying and applying techniques and methods to store, process, analyze, visualize, and use spatial data. It is the newest of the branches, the most controversial, and often other terms are used in the literature to describe the emerging category. These branches use similar geographic philosophies, concepts, and tools and often overlap significantly. Geographers rarely focus on just one of these topics, often using one as their primary focus and then incorporating data and methods from the other branches. Often, geographers are asked to describe what they do by individuals outside the discipline and are likely to identify closely with a specific branch, or sub-branch when describing themselves to lay people. Physical Physical geography (or physiography) focuses on geography as an Earth science. It aims to understand the physical problems and the issues of lithosphere, hydrosphere, atmosphere, pedosphere, and global flora and fauna patterns (biosphere). Physical geography is the study of earth's seasons, climate, atmosphere, soil, streams, landforms, and oceans. Physical geographers will often work in identifying and monitoring the use of natural resources. Human Human geography (or anthropogeography) is a branch of geography that focuses on studying patterns and processes that shape human society. It encompasses the human, political, cultural, social, and economic aspects. In industry, human geographers often work in city planning, public health, or business analysis. Various approaches to the study of human geography have also arisen through time and include behavioral geography, culture theory, feminist geography, and geosophy. Technical Technical geography concerns studying and developing tools, techniques, and statistical methods employed to collect, analyze, use, and understand spatial data. Technical geography is the most recently recognized, and controversial, of the branches. Its use dates back to 1749, when a book published by Edward Cave organized the discipline into a section containing content such as cartographic techniques and globes. There are several other terms, often used interchangeably with technical geography to subdivide the discipline, including "techniques of geographic analysis," "Geographic Information Technology," "Geography method's and techniques," "Geographic Information Science," "geoinformatics," "geomatics," and "information geography". There are subtle differences to each concept and term; however, technical geography is one of the broadest, is consistent with the naming convention of the other two branches, has been in use since the 1700s, and has been used by the UNESCO Encyclopedia of Life Support Systems to divide geography into themes. As academic fields increasingly specialize in their nature, technical geography has emerged as a branch of geography specializing in geographic methods and thought. The emergence of technical geography has brought new relevance to the broad discipline of geography by serving as a set of unique methods for managing the interdisciplinary nature of the phenomena under investigation. While human and physical geographers use the techniques employed by technical geographers, technical geography is more concerned with the fundamental spatial concepts and technologies than the nature of the data. It is therefore closely associated with the spatial tradition of geography while being applied to the other two major branches. A technical geographer might work as a GIS analyst, a GIS developer working to make new software tools, or create general reference maps incorporating human and natural features. Methods All geographic research and analysis start with asking the question "where," followed by "why there." Geographers start with the fundamental assumption set forth in Tobler's first law of geography, that "everything is related to everything else, but near things are more related than distant things." As spatial interrelationships are key to this synoptic science, maps are a key tool. Classical cartography has been joined by a more modern approach to geographical analysis, computer-based geographic information systems (GIS). In their study, geographers use four interrelated approaches: Analytical – Asks why we find features and populations in a specific geographic area. Descriptive – Simply specifies the locations of features and populations. Regional – Examines systematic relationships between categories for a specific region or location on the planet. Systematic – Groups geographical knowledge into categories that can be explored globally. Quantitative methods Quantitative methods in geography became particularly influential in the discipline during the quantitative revolution of the 1950s and 60s. These methods revitalized the discipline in many ways, allowing scientific testing of hypotheses and proposing scientific geographic theories and laws. The quantitative revolution heavily influenced and revitalized technical geography, and lead to the development of the subfield of quantitative geography. Quantitative cartography Cartography is the art, science, and technology of making maps. Cartographers study the Earth's surface representation with abstract symbols (map making). Although other subdisciplines of geography rely on maps for presenting their analyses, the actual making of maps is abstract enough to be regarded separately. Cartography has grown from a collection of drafting techniques into an actual science. Cartographers must learn cognitive psychology and ergonomics to understand which symbols convey information about the Earth most effectively and behavioural psychology to induce the readers of their maps to act on the information. They must learn geodesy and fairly advanced mathematics to understand how the shape of the Earth affects the distortion of map symbols projected onto a flat surface for viewing. It can be said, without much controversy, that cartography is the seed from which the larger field of geography grew. Geographic information systems Geographic information systems (GIS) deal with storing information about the Earth for automatic retrieval by a computer in an accurate manner appropriate to the information's purpose. In addition to all of the other subdisciplines of geography, GIS specialists must understand computer science and database systems. GIS has revolutionized the field of cartography: nearly all mapmaking is now done with the assistance of some form of GIS software. The science of using GIS software and GIS techniques to represent, analyse, and predict the spatial relationships is called geographic information science (GISc). Remote sensing Remote sensing is the art, science, and technology of obtaining information about Earth's features from measurements made at a distance. Remotely sensed data can be either passive, such as traditional photography, or active, such as LiDAR. A variety of platforms can be used for remote sensing, including satellite imagery, aerial photography (including consumer drones), and data obtained from hand-held sensors. Products from remote sensing include Digital elevation model and cartographic base maps. Geographers increasingly use remotely sensed data to obtain information about the Earth's land surface, ocean, and atmosphere, because it: (a) supplies objective information at a variety of spatial scales (local to global), (b) provides a synoptic view of the area of interest, (c) allows access to distant and inaccessible sites, (d) provides spectral information outside the visible portion of the electromagnetic spectrum, and (e) facilitates studies of how features/areas change over time. Remotely sensed data may be analyzed independently or in conjunction with other digital data layers (e.g., in a geographic information system). Remote sensing aids in land use, land cover (LULC) mapping, by helping to determine both what is naturally occurring on a piece of land and what human activities are taking place on it. Geostatistics Geostatistics deal with quantitative data analysis, specifically the application of a statistical methodology to the exploration of geographic phenomena. Geostatistics is used extensively in a variety of fields, including hydrology, geology, petroleum exploration, weather analysis, urban planning, logistics, and epidemiology. The mathematical basis for geostatistics derives from cluster analysis, linear discriminant analysis and non-parametric statistical tests, and a variety of other subjects. Applications of geostatistics rely heavily on geographic information systems, particularly for the interpolation (estimate) of unmeasured points. Geographers are making notable contributions to the method of quantitative techniques. Qualitative methods Qualitative methods in geography are descriptive rather than numerical or statistical in nature. They add context to concepts, and explore human concepts like beliefs and perspective that are difficult or impossible to quantify. Human geography is much more likely to employ qualitative methods than physical geography. Increasingly, technical geographers are attempting to employ GIS methods to qualitative datasets. Qualitative cartography Qualitative cartography employs many of the same software and techniques as quantitative cartography. It may be employed to inform on map practices, or to visualize perspectives and ideas that are not strictly quantitative in nature. An example of a form of qualitative cartography is a Chorochromatic map of nominal data, such as land cover or dominant language group in an area. Another example is a deep map, or maps that combine geography and storytelling to produce a product with greater information than a two-dimensional image of places, names, and topography. This approach offers more inclusive strategies than more traditional cartographic approaches for connecting the complex layers that makeup places. Ethnography Ethnographical research techniques are used by human geographers. In cultural geography, there is a tradition of employing qualitative research techniques, also used in anthropology and sociology. Participant observation and in-depth interviews provide human geographers with qualitative data. Geopoetics Geopoetics is an interdisciplinary approach that combines geography and poetry to explore the interconnectedness between humans, space, place, and the environment. Geopoetics is employed as a mixed methods tool to explain the implications of geographic research. It is often employed to address and communicate the implications of complex topics, such as the anthropocene. Interviews Geographers employ interviews to gather data and acquire valuable understandings from individuals or groups regarding their encounters, outlooks, and opinions concerning spatial phenomena. Interviews can be carried out through various mediums, including face-to-face interactions, phone conversations, online platforms, or written exchanges. Geographers typically adopt a structured or semi-structured approach during interviews involving specific questions or discussion points when utilized for research purposes. These questions are designed to extract focused information about the research topic while being flexible enough to allow participants to express their experiences and viewpoints, such as through open-ended questions. Origin and history The concept of geography is present in all cultures, and therefore the history of the discipline is a series of competing narratives, with concepts emerging at various points across space and time. The oldest known world maps date back to ancient Babylon from the 9th century BC. The best known Babylonian world map, however, is the Imago Mundi of 600 BC. The map as reconstructed by Eckhard Unger shows Babylon on the Euphrates, surrounded by a circular landmass showing Assyria, Urartu, and several cities, in turn surrounded by a "bitter river" (Oceanus), with seven islands arranged around it so as to form a seven-pointed star. The accompanying text mentions seven outer regions beyond the encircling ocean. The descriptions of five of them have survived. In contrast to the Imago Mundi, an earlier Babylonian world map dating back to the 9th century BC depicted Babylon as being further north from the center of the world, though it is not certain what that center was supposed to represent. The ideas of Anaximander (c. 610–545 BC): considered by later Greek writers to be the true founder of geography, come to us through fragments quoted by his successors. Anaximander is credited with the invention of the gnomon, the simple, yet efficient Greek instrument that allowed the early measurement of latitude. Thales is also credited with the prediction of eclipses. The foundations of geography can be traced to ancient cultures, such as the ancient, medieval, and early modern Chinese. The Greeks, who were the first to explore geography as both art and science, achieved this through Cartography, Philosophy, and Literature, or through Mathematics. There is some debate about who was the first person to assert that the Earth is spherical in shape, with the credit going either to Parmenides or Pythagoras. Anaxagoras was able to demonstrate that the profile of the Earth was circular by explaining eclipses. However, he still believed that the Earth was a flat disk, as did many of his contemporaries. One of the first estimates of the radius of the Earth was made by Eratosthenes. The first rigorous system of latitude and longitude lines is credited to Hipparchus. He employed a sexagesimal system that was derived from Babylonian mathematics. The meridians were subdivided into 360°, with each degree further subdivided into 60 (minutes). To measure the longitude at different locations on Earth, he suggested using eclipses to determine the relative difference in time. The extensive mapping by the Romans as they explored new lands would later provide a high level of information for Ptolemy to construct detailed atlases. He extended the work of Hipparchus, using a grid system on his maps and adopting a length of 56.5 miles for a degree. From the 3rd century onwards, Chinese methods of geographical study and writing of geographical literature became much more comprehensive than what was found in Europe at the time (until the 13th century). Chinese geographers such as Liu An, Pei Xiu, Jia Dan, Shen Kuo, Fan Chengda, Zhou Daguan, and Xu Xiake wrote important treatises, yet by the 17th century advanced ideas and methods of Western-style geography were adopted in China. During the Middle Ages, the fall of the Roman empire led to a shift in the evolution of geography from Europe to the Islamic world. Muslim geographers such as Muhammad al-Idrisi produced detailed world maps (such as Tabula Rogeriana), while other geographers such as Yaqut al-Hamawi, Abu Rayhan Biruni, Ibn Battuta, and Ibn Khaldun provided detailed accounts of their journeys and the geography of the regions they visited. Turkish geographer Mahmud al-Kashgari drew a world map on a linguistic basis, and later so did Piri Reis (Piri Reis map). Further, Islamic scholars translated and interpreted the earlier works of the Romans and the Greeks and established the House of Wisdom in Baghdad for this purpose. Abū Zayd al-Balkhī, originally from Balkh, founded the "Balkhī school" of terrestrial mapping in Baghdad. Suhrāb, a late tenth century Muslim geographer accompanied a book of geographical coordinates, with instructions for making a rectangular world map with equirectangular projection or cylindrical equidistant projection. Abu Rayhan Biruni (976–1048) first described a polar equi-azimuthal equidistant projection of the celestial sphere. He was regarded as the most skilled when it came to mapping cities and measuring the distances between them, which he did for many cities in the Middle East and the Indian subcontinent. He often combined astronomical readings and mathematical equations to develop methods of pin-pointing locations by recording degrees of latitude and longitude. He also developed similar techniques when it came to measuring the heights of mountains, depths of the valleys, and expanse of the horizon. He also discussed human geography and the planetary habitability of the Earth. He also calculated the latitude of Kath, Khwarezm, using the maximum altitude of the Sun, and solved a complex geodesic equation to accurately compute the Earth's circumference, which was close to modern values of the Earth's circumference. His estimate of 6,339.9 km for the Earth radius was only 16.8 km less than the modern value of 6,356.7 km. In contrast to his predecessors, who measured the Earth's circumference by sighting the Sun simultaneously from two different locations, al-Biruni developed a new method of using trigonometric calculations based on the angle between a plain and mountain top, which yielded more accurate measurements of the Earth's circumference, and made it possible for it to be measured by a single person from a single location. The European Age of Discovery during the 16th and the 17th centuries, where many new lands were discovered and accounts by European explorers such as Christopher Columbus, Marco Polo, and James Cook revived a desire for both accurate geographic detail and more solid theoretical foundations in Europe. In 1650, the first edition of the Geographia Generalis was published by Bernhardus Varenius, which was later edited and republished by others including Isaac Newton. This textbook sought to integrate new scientific discoveries and principles into classical geography and approach the discipline like the other sciences emerging, and is seen by some as the division between ancient and modern geography in the West. The Geographia Generalis contained both theoretical background and practical applications related to ship navigation. The remaining problem facing both explorers and geographers was finding the latitude and longitude of a geographic location. While the problem of latitude was solved long ago, but that of longitude remained; agreeing on what zero meridians should be was only part of the problem. It was left to John Harrison to solve it by inventing the chronometer H-4 in 1760, and later in 1884 for the International Meridian Conference to adopt by convention the Greenwich meridian as zero meridians. The 18th and 19th centuries were the times when geography became recognized as a discrete academic discipline, and became part of a typical university curriculum in Europe (especially Paris and Berlin). The development of many geographic societies also occurred during the 19th century, with the foundations of the Société de Géographie in 1821, the Royal Geographical Society in 1830, Russian Geographical Society in 1845, American Geographical Society in 1851, the Royal Danish Geographical Society in 1876 and the National Geographic Society in 1888. The influence of Immanuel Kant, Alexander von Humboldt, Carl Ritter, and Paul Vidal de la Blache can be seen as a major turning point in geography from philosophy to an academic subject. Geographers such as Richard Hartshorne and Joseph Kerski have regarded both Humboldt and Ritter as the founders of modern geography, as Humboldt and Ritter were the first to establish geography as an independent scientific discipline. Over the past two centuries, the advancements in technology with computers have led to the development of geomatics and new practices such as participant observation and geostatistics being incorporated into geography's portfolio of tools. In the West during the 20th century, the discipline of geography went through four major phases: environmental determinism, regional geography, the quantitative revolution, and critical geography. The strong interdisciplinary links between geography and the sciences of geology and botany, as well as economics, sociology, and demographics, have also grown greatly, especially as a result of earth system science that seeks to understand the world in a holistic view. New concepts and philosophies have emerged from the rapid advancement of computers, quantitative methods, and interdisciplinary approaches. The 1962 book Theoretical Geography by William Bunge, which argued for a nomothetic approach to geography and that from a purely spatial perspective there was no real difference between human and physical geography, has been described by Kevin R. Cox as "perhaps the seminal text of the spatial-quantitative revolution." In 1970, Waldo Tobler proposed the first law of geography, "everything is related to everything else, but near things are more related than distant things." This law summarizes the first assumption geographers make about the world. Related fields Geology The discipline of geography, especially physical geography, and geology have significant overlap. In the past, the two have often shared academic departments at universities, a point that has led to conflict over resources. Both disciplines do seek to understand the rocks on the Earth's surface and the processes that change them over time. Geology employs many of the tools and techniques of technical geographers, such as GIS and remote sensing to aid in geological mapping. However, geology includes research that goes beyond the spatial component, such as the chemical analysis of rocks and biogeochemistry. History The discipline of History has significant overlap with geography, especially human geography. Like geology, history and geography have shared university departments. Geography provides the spatial context within which historical events unfold. The physical geographic features of a region, such as its landforms, climate, and resources, shape human settlements, trade routes, and economic activities, which in turn influence the course of historical events. Thus, a historian must have a strong foundation in geography. Historians employ the techniques of technical geographers to create historical atlases and maps. Planetary science While the discipline of geography is normally concerned with the Earth, the term can also be informally used to describe the study of other worlds, such as the planets of the Solar System and even beyond. The study of systems larger than the Earth itself usually forms part of Astronomy or Cosmology, while the study of other planets is usually called planetary science. Alternative terms such as areography (geography of Mars) have been employed to describe the study of other celestial objects. Ultimately, geography may be considered a subdiscipline within planetary science, and planetary science link geography with fields like astronomy and physics. See also References External links Geography at the Encyclopaedia Britannica website

Mathematics is a field of study that discovers and organizes methods, theories and theorems that are developed and proved for the needs of empirical sciences and mathematics itself. There are many areas of mathematics, which include number theory (the study of numbers), algebra (the study of formulas and related structures), geometry (the study of shapes and spaces that contain them), analysis (the study of continuous changes), and set theory (presently used as a foundation for all mathematics). Mathematics involves the description and manipulation of abstract objects that consist of either abstractions from nature or—in modern mathematics—purely abstract entities that are stipulated to have certain properties, called axioms. Mathematics uses pure reason to prove properties of objects, a proof consisting of a succession of applications of deductive rules to already established results. These results include previously proved theorems, axioms, and—in case of abstraction from nature—some basic properties that are considered true starting points of the theory under consideration. Mathematics is essential in the natural sciences, engineering, medicine, finance, computer science, and the social sciences. Although mathematics is extensively used for modeling phenomena, the fundamental truths of mathematics are independent of any scientific experimentation. Some areas of mathematics, such as statistics and game theory, are developed in close correlation with their applications and are often grouped under applied mathematics. Other areas are developed independently from any application (and are therefore called pure mathematics) but often later find practical applications. Historically, the concept of a proof and its associated mathematical rigour first appeared in Greek mathematics, most notably in Euclid's Elements. Since its beginning, mathematics was primarily divided into geometry and arithmetic (the manipulation of natural numbers and fractions), until the 16th and 17th centuries, when algebra and infinitesimal calculus were introduced as new fields. Since then, the interaction between mathematical innovations and scientific discoveries has led to a correlated increase in the development of both. At the end of the 19th century, the foundational crisis of mathematics led to the systematization of the axiomatic method, which heralded a dramatic increase in the number of mathematical areas and their fields of application. The contemporary Mathematics Subject Classification lists more than sixty first-level areas of mathematics. Areas of mathematics Before the Renaissance, mathematics was divided into two main areas: arithmetic, regarding the manipulation of numbers, and geometry, regarding the study of shapes. Some types of pseudoscience, such as numerology and astrology, were not then clearly distinguished from mathematics. During the Renaissance, two more areas appeared. Mathematical notation led to algebra which, roughly speaking, consists of the study and the manipulation of formulas. Calculus, consisting of the two subfields differential calculus and integral calculus, is the study of continuous functions, which model the typically nonlinear relationships between varying quantities, as represented by variables. This division into four main areas—arithmetic, geometry, algebra, and calculus—endured until the end of the 19th century. Areas such as celestial mechanics and solid mechanics were then studied by mathematicians, but now are considered as belonging to physics. The subject of combinatorics has been studied for much of recorded history, yet did not become a separate branch of mathematics until the seventeenth century. At the end of the 19th century, the foundational crisis in mathematics and the resulting systematization of the axiomatic method led to an explosion of new areas of mathematics. The 2020 Mathematics Subject Classification contains no less than sixty-three first-level areas. Some of these areas correspond to the older division, as is true regarding number theory (the modern name for higher arithmetic) and geometry. Several other first-level areas have "geometry" in their names or are otherwise commonly considered part of geometry. Algebra and calculus do not appear as first-level areas but are respectively split into several first-level areas. Other first-level areas emerged during the 20th century or had not previously been considered as mathematics, such as mathematical logic and foundations. Number theory Number theory began with the manipulation of numbers, that is, natural numbers ( N ) , {\displaystyle (\mathbb {N} ),} and later expanded to integers ( Z ) {\displaystyle (\mathbb {Z} )} and rational numbers ( Q ) . {\displaystyle (\mathbb {Q} ).} Number theory was once called arithmetic, but nowadays this term is mostly used for numerical calculations. Number theory dates back to ancient Babylon and probably China. Two prominent early number theorists were Euclid of ancient Greece and Diophantus of Alexandria. The modern study of number theory in its abstract form is largely attributed to Pierre de Fermat and Leonhard Euler. The field came to full fruition with the contributions of Adrien-Marie Legendre and Carl Friedrich Gauss. Many easily stated number problems have solutions that require sophisticated methods, often from across mathematics. A prominent example is Fermat's Last Theorem. This conjecture was stated in 1637 by Pierre de Fermat, but it was proved only in 1994 by Andrew Wiles, who used tools including scheme theory from algebraic geometry, category theory, and homological algebra. Another example is Goldbach's conjecture, which asserts that every even integer greater than 2 is the sum of two prime numbers. Stated in 1742 by Christian Goldbach, it remains unproven despite considerable effort. Number theory includes several subareas, including analytic number theory, algebraic number theory, geometry of numbers (method oriented), diophantine equations, and transcendence theory (problem oriented). Geometry Geometry is one of the oldest branches of mathematics. It started with empirical recipes concerning shapes, such as lines, angles and circles, which were developed mainly for the needs of surveying and architecture, but has since blossomed out into many other subfields. A fundamental innovation was the ancient Greeks' introduction of the concept of proofs, which require that every assertion must be proved. For example, it is not sufficient to verify by measurement that, say, two lengths are equal; their equality must be proven via reasoning from previously accepted results (theorems) and a few basic statements. The basic statements are not subject to proof because they are self-evident (postulates), or are part of the definition of the subject of study (axioms). This principle, foundational for all mathematics, was first elaborated for geometry, and was systematized by Euclid around 300 BC in his book Elements. The resulting Euclidean geometry is the study of shapes and their arrangements constructed from lines, planes and circles in the Euclidean plane (plane geometry) and the three-dimensional Euclidean space. Euclidean geometry was developed without change of methods or scope until the 17th century, when René Descartes introduced what is now called Cartesian coordinates. This constituted a major change of paradigm: Instead of defining real numbers as lengths of line segments (see number line), it allowed the representation of points using their coordinates, which are numbers. Algebra (and later, calculus) can thus be used to solve geometrical problems. Geometry was split into two new subfields: synthetic geometry, which uses purely geometrical methods, and analytic geometry, which uses coordinates systemically. Analytic geometry allows the study of curves unrelated to circles and lines. Such curves can be defined as the graph of functions, the study of which led to differential geometry. They can also be defined as implicit equations, often polynomial equations (which spawned algebraic geometry). Analytic geometry also makes it possible to consider Euclidean spaces of higher than three dimensions. In the 19th century, mathematicians discovered non-Euclidean geometries, which do not follow the parallel postulate. By questioning that postulate's truth, this discovery has been viewed as joining Russell's paradox in revealing the foundational crisis of mathematics. This aspect of the crisis was solved by systematizing the axiomatic method, and adopting that the truth of the chosen axioms is not a mathematical problem. In turn, the axiomatic method allows for the study of various geometries obtained either by changing the axioms or by considering properties that do not change under specific transformations of the space. Today's subareas of geometry include: Projective geometry, introduced in the 16th century by Girard Desargues, extends Euclidean geometry by adding points at infinity at which parallel lines intersect. This simplifies many aspects of classical geometry by unifying the treatments for intersecting and parallel lines. Affine geometry, the study of properties relative to parallelism and independent from the concept of length. Differential geometry, the study of curves, surfaces, and their generalizations, which are defined using differentiable functions. Manifold theory, the study of shapes that are not necessarily embedded in a larger space. Riemannian geometry, the study of distance properties in curved spaces. Algebraic geometry, the study of curves, surfaces, and their generalizations, which are defined using polynomials. Topology, the study of properties that are kept under continuous deformations. Algebraic topology, the use in topology of algebraic methods, mainly homological algebra. Discrete geometry, the study of finite configurations in geometry. Convex geometry, the study of convex sets, which takes its importance from its applications in optimization. Complex geometry, the geometry obtained by replacing real numbers with complex numbers. Algebra Algebra is the art of manipulating equations and formulas. Diophantus (3rd century) and al-Khwarizmi (9th century) were the two main precursors of algebra. Diophantus solved some equations involving unknown natural numbers by deducing new relations until he obtained the solution. Al-Khwarizmi introduced systematic methods for transforming equations, such as moving a term from one side of an equation into the other side. The term algebra is derived from the Arabic word al-jabr meaning 'the reunion of broken parts' that he used for naming one of these methods in the title of his main treatise. Algebra became an area in its own right only with François Viète (1540–1603), who introduced the use of variables for representing unknown or unspecified numbers. Variables allow mathematicians to describe the operations that have to be done on the numbers represented using mathematical formulas. Until the 19th century, algebra consisted mainly of the study of linear equations (presently linear algebra), and polynomial equations in a single unknown, which were called algebraic equations (a term still in use, although it may be ambiguous). During the 19th century, mathematicians began to use variables to represent things other than numbers (such as matrices, modular integers, and geometric transformations), on which generalizations of arithmetic operations are often valid. The concept of algebraic structure addresses this, consisting of a set whose elements are unspecified, of operations acting on the elements of the set, and rules that these operations must follow. The scope of algebra thus grew to include the study of algebraic structures. This object of algebra was called modern algebra or abstract algebra, as established by the influence and works of Emmy Noether, and popularized by Van der Waerden's book Moderne Algebra. Some types of algebraic structures have useful and often fundamental properties, in many areas of mathematics. Their study became autonomous parts of algebra, and include: group theory field theory vector spaces, whose study is essentially the same as linear algebra ring theory commutative algebra, which is the study of commutative rings, includes the study of polynomials, and is a foundational part of algebraic geometry homological algebra Lie algebra and Lie group theory Boolean algebra, which is widely used for the study of the logical structure of computers The study of types of algebraic structures as mathematical objects is the purpose of universal algebra and category theory. The latter applies to every mathematical structure (not only algebraic ones). At its origin, it was introduced, together with homological algebra for allowing the algebraic study of non-algebraic objects such as topological spaces; this particular area of application is called algebraic topology. Calculus and analysis Calculus, formerly called infinitesimal calculus, was introduced independently and simultaneously by 17th-century mathematicians Newton and Leibniz. It is fundamentally the study of the relationship of variables that depend on each other. Calculus was expanded in the 18th century by Euler with the introduction of the concept of a function and many other results. Presently, "calculus" refers mainly to the elementary part of this theory, and "analysis" is commonly used for advanced parts. Analysis is further subdivided into real analysis, where variables represent real numbers, and complex analysis, where variables represent complex numbers. Analysis includes many subareas shared by other areas of mathematics which include: Multivariable calculus Functional analysis, where variables represent varying functions Integration, measure theory and potential theory, all strongly related with probability theory on a continuum Ordinary differential equations Partial differential equations Numerical analysis, mainly devoted to the computation on computers of solutions of ordinary and partial differential equations that arise in many applications Discrete mathematics Discrete mathematics, broadly speaking, is the study of individual, countable mathematical objects. An example is the set of all integers. Because the objects of study here are discrete, the methods of calculus and mathematical analysis do not directly apply. Algorithms—especially their implementation and computational complexity—play a major role in discrete mathematics. The four color theorem and optimal sphere packing were two major problems of discrete mathematics solved in the second half of the 20th century. The P versus NP problem, which remains open to this day, is also important for discrete mathematics, since its solution would potentially impact a large number of computationally difficult problems. Discrete mathematics includes: Combinatorics, the art of enumerating mathematical objects that satisfy some given constraints. Originally, these objects were elements or subsets of a given set; this has been extended to various objects, which establishes a strong link between combinatorics and other parts of discrete mathematics. For example, discrete geometry includes counting configurations of geometric shapes. Graph theory and hypergraphs Coding theory, including error correcting codes and a part of cryptography Matroid theory Discrete geometry Discrete probability distributions Game theory (although continuous games are also studied, most common games, such as chess and poker are discrete) Discrete optimization, including combinatorial optimization, integer programming, constraint programming Mathematical logic and set theory The two subjects of mathematical logic and set theory have belonged to mathematics since the end of the 19th century. Before this period, sets were not considered to be mathematical objects, and logic, although used for mathematical proofs, belonged to philosophy and was not specifically studied by mathematicians. Before Cantor's study of infinite sets, mathematicians were reluctant to consider actually infinite collections, and considered infinity to be the result of endless enumeration. Cantor's work offended many mathematicians not only by considering actually infinite sets but by showing that this implies different sizes of infinity, per Cantor's diagonal argument. This led to the controversy over Cantor's set theory. In the same period, various areas of mathematics concluded the former intuitive definitions of the basic mathematical objects were insufficient for ensuring mathematical rigour. This became the foundational crisis of mathematics. It was eventually solved in mainstream mathematics by systematizing the axiomatic method inside a formalized set theory. Roughly speaking, each mathematical object is defined by the set of all similar objects and the properties that these objects must have. For example, in Peano arithmetic, the natural numbers are defined by "zero is a number", "each number has a unique successor", "each number but zero has a unique predecessor", and some rules of reasoning. This mathematical abstraction from reality is embodied in the modern philosophy of formalism, as founded by David Hilbert around 1910. The "nature" of the objects defined this way is a philosophical problem that mathematicians leave to philosophers, even if many mathematicians have opinions on this nature, and use their opinion—sometimes called "intuition"—to guide their study and proofs. The approach allows considering "logics" (that is, sets of allowed deducing rules), theorems, proofs, etc. as mathematical objects, and to prove theorems about them. For example, Gödel's incompleteness theorems assert, roughly speaking that, in every consistent formal system that contains the natural numbers, there are theorems that are true (that is provable in a stronger system), but not provable inside the system. This approach to the foundations of mathematics was challenged during the first half of the 20th century by mathematicians led by Brouwer, who promoted intuitionistic logic, which explicitly lacks the law of excluded middle. These problems and debates led to a wide expansion of mathematical logic, with subareas such as model theory (modeling some logical theories inside other theories), proof theory, type theory, computability theory and computational complexity theory. Although these aspects of mathematical logic were introduced before the rise of computers, their use in compiler design, formal verification, program analysis, proof assistants and other aspects of computer science, contributed in turn to the expansion of these logical theories. Statistics and other decision sciences The field of statistics is a mathematical application that is employed for the collection and processing of data samples, using procedures based on mathematical methods especially probability theory. Statisticians generate data with random sampling or randomized experiments. Statistical theory studies decision problems such as minimizing the risk (expected loss) of a statistical action, such as using a procedure in, for example, parameter estimation, hypothesis testing, and selecting the best. In these traditional areas of mathematical statistics, a statistical-decision problem is formulated by minimizing an objective function, like expected loss or cost, under specific constraints. For example, designing a survey often involves minimizing the cost of estimating a population mean with a given level of confidence. Because of its use of optimization, the mathematical theory of statistics overlaps with other decision sciences, such as operations research, control theory, and mathematical economics. Computational mathematics Computational mathematics is the study of mathematical problems that are typically too large for human, numerical capacity. Numerical analysis studies methods for problems in analysis using functional analysis and approximation theory; numerical analysis broadly includes the study of approximation and discretization with special focus on rounding errors. Numerical analysis and, more broadly, scientific computing also study non-analytic topics of mathematical science, especially algorithmic-matrix-and-graph theory. Other areas of computational mathematics include computer algebra and symbolic computation. History Etymology The word mathematics comes from the Ancient Greek word máthēma (μάθημα), meaning 'something learned, knowledge, mathematics', and the derived expression mathēmatikḗ tékhnē (μαθηματικὴ τέχνη), meaning 'mathematical science'. It entered the English language during the Late Middle English period through French and Latin. Similarly, one of the two main schools of thought in Pythagoreanism was known as the mathēmatikoi (μαθηματικοί)—which at the time meant "learners" rather than "mathematicians" in the modern sense. The Pythagoreans were likely the first to constrain the use of the word to just the study of arithmetic and geometry. By the time of Aristotle (384–322 BC) this meaning was fully established. In Latin and English, until around 1700, the term mathematics more commonly meant "astrology" (or sometimes "astronomy") rather than "mathematics"; the meaning gradually changed to its present one from about 1500 to 1800. This change has resulted in several mistranslations: For example, Saint Augustine's warning that Christians should beware of mathematici, meaning "astrologers", is sometimes mistranslated as a condemnation of mathematicians. The apparent plural form in English goes back to the Latin neuter plural mathematica (Cicero), based on the Greek plural ta mathēmatiká (τὰ μαθηματικά) and means roughly "all things mathematical", although it is plausible that English borrowed only the adjective mathematic(al) and formed the noun mathematics anew, after the pattern of physics and metaphysics, inherited from Greek. In English, the noun mathematics takes a singular verb. It is often shortened to maths or, in North America, math. Ancient In addition to recognizing how to count physical objects, prehistoric peoples may have also known how to count abstract quantities, like time—days, seasons, or years. Evidence for more complex mathematics does not appear until around 3000 BC, when the Babylonians and Egyptians began using arithmetic, algebra, and geometry for taxation and other financial calculations, for building and construction, and for astronomy. The oldest mathematical texts from Mesopotamia and Egypt are from 2000 to 1800 BC. Many early texts mention Pythagorean triples and so, by inference, the Pythagorean theorem seems to be the most ancient and widespread mathematical concept after basic arithmetic and geometry. It is in Babylonian mathematics that elementary arithmetic (addition, subtraction, multiplication, and division) first appear in the archaeological record. The Babylonians also possessed a place-value system and used a sexagesimal numeral system which is still in use today for measuring angles and time. In the 6th century BC, Greek mathematics began to emerge as a distinct discipline and some Ancient Greeks such as the Pythagoreans appeared to have considered it a subject in its own right. Around 300 BC, Euclid organized mathematical knowledge by way of postulates and first principles, which evolved into the axiomatic method that is used in mathematics today, consisting of definition, axiom, theorem, and proof. His book, Elements, is widely considered the most successful and influential textbook of all time. The greatest mathematician of antiquity is often held to be Archimedes (c. 287 – c. 212 BC) of Syracuse. He developed formulas for calculating the surface area and volume of solids of revolution and used the method of exhaustion to calculate the area under the arc of a parabola with the summation of an infinite series, in a manner not too dissimilar from modern calculus. Other notable achievements of Greek mathematics are conic sections (Apollonius of Perga, 3rd century BC), trigonometry (Hipparchus of Nicaea, 2nd century BC), and the beginnings of algebra (Diophantus, 3rd century AD). The Hindu–Arabic numeral system and the rules for the use of its operations, in use throughout the world today, evolved over the course of the first millennium AD in India and were transmitted to the Western world via Islamic mathematics. Other notable developments of Indian mathematics include the modern definition and approximation of sine and cosine, and an early form of infinite series. Medieval and later During the Golden Age of Islam, especially during the 9th and 10th centuries, mathematics saw many important innovations building on Greek mathematics. The most notable achievement of Islamic mathematics was the development of algebra. Other achievements of the Islamic period include advances in spherical trigonometry and the addition of the decimal point to the Arabic numeral system. Many notable mathematicians from this period were Persian, such as Al-Khwarizmi, Omar Khayyam and Sharaf al-Dīn al-Ṭūsī. The Greek and Arabic mathematical texts were in turn translated to Latin during the Middle Ages and made available in Europe. During the early modern period, mathematics began to develop at an accelerating pace in Western Europe, with innovations that revolutionized mathematics, such as the introduction of variables and symbolic notation by François Viète (1540–1603), the introduction of logarithms by John Napier in 1614, which greatly simplified numerical calculations, especially for astronomy and marine navigation, the introduction of coordinates by René Descartes (1596–1650) for reducing geometry to algebra, and the development of calculus by Isaac Newton (1643–1727) and Gottfried Leibniz (1646–1716). Leonhard Euler (1707–1783), the most notable mathematician of the 18th century, unified these innovations into a single corpus with a standardized terminology, and completed them with the discovery and the proof of numerous theorems. Perhaps the foremost mathematician of the 19th century was the German mathematician Carl Gauss, who made numerous contributions to fields such as algebra, analysis, differential geometry, matrix theory, number theory, and statistics. In the early 20th century, Kurt Gödel transformed mathematics by publishing his incompleteness theorems, which show in part that any consistent axiomatic system—if powerful enough to describe arithmetic—will contain true propositions that cannot be proved. Mathematics has since been greatly extended, and there has been a fruitful interaction between mathematics and science, to the benefit of both. Mathematical discoveries continue to be made to this very day. According to Mikhail B. Sevryuk, in the January 2006 issue of the Bulletin of the American Mathematical Society, "The number of papers and books included in the Mathematical Reviews (MR) database since 1940 (the first year of operation of MR) is now more than 1.9 million, and more than 75 thousand items are added to the database each year. The overwhelming majority of works in this ocean contain new mathematical theorems and their proofs." Symbolic notation and terminology Mathematical notation is widely used in science and engineering for representing complex concepts and properties in a concise, unambiguous, and accurate way. This notation consists of symbols used for representing operations, unspecified numbers, relations and any other mathematical objects, and then assembling them into expressions and formulas. More precisely, numbers and other mathematical objects are represented by symbols called variables, which are generally Latin or Greek letters, and often include subscripts. Operation and relations are generally represented by specific symbols or glyphs, such as + (plus), × (multiplication), ∫ {\textstyle \int } (integral), = (equal), and < (less than). All these symbols are generally grouped according to specific rules to form expressions and formulas. Normally, expressions and formulas do not appear alone, but are included in sentences of the current language, where expressions play the role of noun phrases and formulas play the role of clauses. Mathematics has developed a rich terminology covering a broad range of fields that study the properties of various abstract, idealized objects and how they interact. It is based on rigorous definitions that provide a standard foundation for communication. An axiom or postulate is a mathematical statement that is taken to be true without need of proof. If a mathematical statement has yet to be proven (or disproven), it is termed a conjecture. Through a series of rigorous arguments employing deductive reasoning, a statement that is proven to be true becomes a theorem. A specialized theorem that is mainly used to prove another theorem is called a lemma. A proven instance that forms part of a more general finding is termed a corollary. Numerous technical terms used in mathematics are neologisms, such as polynomial and homeomorphism. Other technical terms are words of the common language that are used in an accurate meaning that may differ slightly from their common meaning. For example, in mathematics, "or" means "one, the other or both", while, in common language, it is either ambiguous or means "one or the other but not both" (in mathematics, the latter is called "exclusive or"). Finally, many mathematical terms are common words that are used with a completely different meaning. This may lead to sentences that are correct and true mathematical assertions, but appear to be nonsense to people who do not have the required background. For example, "every free module is flat" and "a field is always a ring". Relationship with sciences Mathematics is used in most sciences for modeling phenomena, which then allows predictions to be made from experimental laws. The independence of mathematical truth from any experimentation implies that the accuracy of such predictions depends only on the adequacy of the model. Inaccurate predictions, rather than being caused by invalid mathematical concepts, imply the need to change the mathematical model used. For example, the perihelion precession of Mercury could only be explained after the emergence of Einstein's general relativity, which replaced Newton's law of gravitation as a better mathematical model. There is still a philosophical debate whether mathematics is a science. However, in practice, mathematicians are typically grouped with scientists, and mathematics shares much in common with the physical sciences. Like them, it is falsifiable, which means in mathematics that, if a result or a theory is wrong, this can be proved by providing a counterexample. Similarly as in science, theories and results (theorems) are often obtained from experimentation. In mathematics, the experimentation may consist of computation on selected examples or of the study of figures or other representations of mathematical objects (often mind representations without physical support). For example, when asked how he came about his theorems, Gauss once replied "durch planmässiges Tattonieren" (through systematic experimentation). However, some authors emphasize that mathematics differs from the modern notion of science by not relying on empirical evidence. Pure and applied mathematics Until the 19th century, the development of mathematics in the West was mainly motivated by the needs of technology and science, and there was no clear distinction between pure and applied mathematics. For example, the natural numbers and arithmetic were introduced for the need of counting, and geometry was motivated by surveying, architecture and astronomy. Later, Isaac Newton introduced infinitesimal calculus for explaining the movement of the planets with his law of gravitation. Moreover, most mathematicians were also scientists, and many scientists were also mathematicians. However, a notable exception occurred with the tradition of pure mathematics in Ancient Greece. The problem of integer factorization, for example, which goes back to Euclid in 300 BC, had no practical application before its use in the RSA cryptosystem, now widely used for the security of computer networks. In the 19th century, mathematicians such as Karl Weierstrass and Richard Dedekind increasingly focused their research on internal problems, that is, pure mathematics. This led to split mathematics into pure mathematics and applied mathematics, the latter being often considered as having a lower value among mathematical purists. However, the lines between the two are frequently blurred. The aftermath of World War II led to a surge in the development of applied mathematics in the US and elsewhere. Many of the theories developed for applications were found interesting from the point of view of pure mathematics, and many results of pure mathematics were shown to have applications outside mathematics; in turn, the study of these applications may give new insights on the "pure theory". An example of the first case is the theory of distributions, introduced by Laurent Schwartz for validating computations done in quantum mechanics, which became immediately an important tool of (pure) mathematical analysis. An example of the second case is the decidability of the first-order theory of the real numbers, a problem of pure mathematics that was proved true by Alfred Tarski, with an algorithm that is impossible to implement because of a computational complexity that is much too high. For getting an algorithm that can be implemented and can solve systems of polynomial equations and inequalities, George Collins introduced the cylindrical algebraic decomposition that became a fundamental tool in real algebraic geometry. In the present day, the distinction between pure and applied mathematics is more a question of personal research aim of mathematicians than a division of mathematics into broad areas. The Mathematics Subject Classification has a section for "general applied mathematics" but does not mention "pure mathematics". However, these terms are still used in names of some university departments, such as at the Faculty of Mathematics at the University of Cambridge. Unreasonable effectiveness The unreasonable effectiveness of mathematics is a phenomenon that was named and first made explicit by physicist Eugene Wigner. It is the fact that many mathematical theories (even the "purest") have applications outside their initial object. These applications may be completely outside their initial area of mathematics, and may concern physical phenomena that were completely unknown when the mathematical theory was introduced. Examples of unexpected applications of mathematical theories can be found in many areas of mathematics. A notable example is the prime factorization of natural numbers that was discovered more than 2,000 years before its common use for secure internet communications through the RSA cryptosystem. A second historical example is the theory of ellipses. They were studied by the ancient Greek mathematicians as conic sections (that is, intersections of cones with planes). It was almost 2,000 years later that Johannes Kepler discovered that the trajectories of the planets are ellipses. In the 19th century, the internal development of geometry (pure mathematics) led to definition and study of non-Euclidean geometries, spaces of dimension higher than three and manifolds. At this time, these concepts seemed totally disconnected from the physical reality, but at the beginning of the 20th century, Albert Einstein developed the theory of relativity that uses fundamentally these concepts. In particular, spacetime of special relativity is a non-Euclidean space of dimension four, and spacetime of general relativity is a (curved) manifold of dimension four. A striking aspect of the interaction between mathematics and physics is when mathematics drives research in physics. This is illustrated by the discoveries of the positron and the baryon Ω − . {\displaystyle \Omega ^{-}.} In both cases, the equations of the theories had unexplained solutions, which led to conjecture of the existence of an unknown particle, and the search for these particles. In both cases, these particles were discovered a few years later by specific experiments. Specific sciences Physics Mathematics and physics have influenced each other over their modern history. Modern physics uses mathematics abundantly, and is also considered to be the motivation of major mathematical developments. Computing Computing is closely related to mathematics in several ways. Theoretical computer science is considered to be mathematical in nature. Communication technologies apply branches of mathematics that may be very old (e.g., arithmetic), especially with respect to transmission security, in cryptography and coding theory. Discrete mathematics is useful in many areas of computer science, such as complexity theory, information theory, and graph theory. In 1998, the Kepler conjecture on sphere packing seemed to also be partially proven by computer. Biology and chemistry Biology uses probability extensively in fields such as ecology or neurobiology. Most discussion of probability centers on the concept of evolutionary fitness. Ecology heavily uses modeling to simulate population dynamics, study ecosystems such as the predator-prey model, measure pollution diffusion, or to assess climate change. The dynamics of a population can be modeled by coupled differential equations, such as the Lotka–Volterra equations. Statistical hypothesis testing, is run on data from clinical trials to determine whether a new treatment works. Since the start of the 20th century, chemistry has used computing to model molecules in three dimensions. Earth sciences Structural geology and climatology use probabilistic models to predict the risk of natural catastrophes. Similarly, meteorology, oceanography, and planetology also use mathematics due to their heavy use of models. Social sciences Areas of mathematics used in the social sciences include probability/statistics and differential equations. These are used in linguistics, economics, sociology, and psychology. Often the fundamental postulate of mathematical economics is that of the rational individual actor – Homo economicus (lit. 'economic man'). In this model, the individual seeks to maximize their self-interest, and always makes optimal choices using perfect information. This atomistic view of economics allows it to relatively easily mathematize its thinking, because individual calculations are transposed into mathematical calculations. Such mathematical modeling allows one to probe economic mechanisms. Some reject or criticise the concept of Homo economicus. Economists note that real people have limited information, make poor choices, and care about fairness and altruism, not just personal gain. Without mathematical modeling, it is hard to go beyond statistical observations or untestable speculation. Mathematical modeling allows economists to create structured frameworks to test hypotheses and analyze complex interactions. Models provide clarity and precision, enabling the translation of theoretical concepts into quantifiable predictions that can be tested against real-world data. At the start of the 20th century, there was a development to express historical movements in formulas. In 1922, Nikolai Kondratiev discerned the ~50-year-long Kondratiev cycle, which explains phases of economic growth or crisis. Towards the end of the 19th century, mathematicians extended their analysis into geopolitics. Peter Turchin developed cliodynamics in the 1990s. Mathematization of the social sciences is not without risk. In the controversial book Fashionable Nonsense (1997), Sokal and Bricmont denounced the unfounded or abusive use of scientific terminology, particularly from mathematics or physics, in the social sciences. The study of complex systems (evolution of unemployment, business capital, demographic evolution of a population, etc.) uses mathematical knowledge. However, the choice of counting criteria, particularly for unemployment, or of models, can be subject to controversy. Philosophy Reality The connection between mathematics and material reality has led to philosophical debates since at least the time of Pythagoras. The ancient philosopher Plato argued that abstractions that reflect material reality have themselves a reality that exists outside space and time. As a result, the philosophical view that mathematical objects somehow exist on their own in abstraction is often referred to as Platonism. Independently of their possible philosophical opinions, modern mathematicians may be generally considered as Platonists, since they think of and talk of their objects of study as real objects. Armand Borel summarized this view of mathematics reality as follows, and provided quotations of G. H. Hardy, Charles Hermite, Henri Poincaré and Albert Einstein that support his views. Something becomes objective (as opposed to "subjective") as soon as we are convinced that it exists in the minds of others in the same form as it does in ours and that we can think about it and discuss it together. Because the language of mathematics is so precise, it is ideally suited to defining concepts for which such a consensus exists. In my opinion, that is sufficient to provide us with a feeling of an objective existence, of a reality of mathematics ... Nevertheless, Platonism and the concurrent views on abstraction do not explain the unreasonable effectiveness of mathematics (as Platonism assumes mathematics exists independently, but does not explain why it matches reality). Proposed definitions There is no general consensus about the definition of mathematics or its epistemological status—that is, its place inside knowledge. A great many professional mathematicians take no interest in a definition of mathematics, or consider it undefinable. There is not even consensus on whether mathematics is an art or a science. Some just say, "mathematics is what mathematicians do". A common approach is to define mathematics by its object of study. Aristotle defined mathematics as "the science of quantity" and this definition prevailed until the 18th century. However, Aristotle also noted a focus on quantity alone may not distinguish mathematics from sciences like physics; in his view, abstraction and studying quantity as a property "separable in thought" from real instances set mathematics apart. In the 19th century, when mathematicians began to address topics—such as infinite sets—which have no clear-cut relation to physical reality, a variety of new definitions were given. With the large number of new areas of mathematics that have appeared since the beginning of the 20th century, defining mathematics by its object of study has become increasingly difficult. For example, in lieu of a definition, Saunders Mac Lane in Mathematics, form and function summarizes the basics of several areas of mathematics, emphasizing their inter-connectedness, and observes: the development of Mathematics provides a tightly connected network of formal rules, concepts, and systems. Nodes of this network are closely bound to procedures useful in human activities and to questions arising in science. The transition from activities to the formal Mathematical systems is guided by a variety of general insights and ideas. Another approach for defining mathematics is to use its methods. For example, an area of study is often qualified as mathematics as soon as one can prove theorems—assertions whose validity relies on a proof, that is, a purely logical deduction. Rigor Mathematical reasoning requires rigor. This means that the definitions must be absolutely unambiguous and the proofs must be reducible to a succession of applications of inference rules, without any use of empirical evidence and intuition. Rigorous reasoning is not specific to mathematics, but, in mathematics, the standard of rigor is much higher than elsewhere. Despite mathematics' concision, rigorous proofs can require hundreds of pages to express, such as the 255-page Feit–Thompson theorem. The emergence of computer-assisted proofs has allowed proof lengths to further expand. The result of this trend is a philosophy of the quasi-empiricist proof that can not be considered infallible, but has a probability attached to it. The concept of rigor in mathematics dates back to ancient Greece, where their society encouraged logical, deductive reasoning. However, this rigorous approach would tend to discourage exploration of new approaches, such as irrational numbers and concepts of infinity. The method of demonstrating rigorous proof was enhanced in the sixteenth century through the use of symbolic notation. In the 18th century, social transition led to mathematicians earning their keep through teaching, which led to more careful thinking about the underlying concepts of mathematics. This produced more rigorous approaches, while transitioning from geometric methods to algebraic and then arithmetic proofs. At the end of the 19th century, it appeared that the definitions of the basic concepts of mathematics were not accurate enough for avoiding paradoxes (non-Euclidean geometries and Weierstrass function) and contradictions (Russell's paradox). This was solved by the inclusion of axioms with the apodictic inference rules of mathematical theories; the re-introduction of axiomatic method pioneered by the ancient Greeks. It results that "rigor" is no more a relevant concept in mathematics, as a proof is either correct or erroneous, and a "rigorous proof" is simply a pleonasm. Where a special concept of rigor comes into play is in the socialized aspects of a proof, wherein it may be demonstrably refuted by other mathematicians. After a proof has been accepted for many years or even decades, it can then be considered as reliable. Nevertheless, the concept of "rigor" may remain useful for teaching to beginners what is a mathematical proof. Training and practice Education Mathematics has a remarkable ability to cross cultural boundaries and time periods. As a human activity, the practice of mathematics has a social side, which includes education, careers, recognition, popularization, and so on. In education, mathematics is a core part of the curriculum and forms an important element of the STEM academic disciplines. Prominent careers for professional mathematicians include mathematics teacher or professor, statistician, actuary, financial analyst, economist, accountant, commodity trader, or computer consultant. Archaeological evidence shows that instruction in mathematics occurred as early as the second millennium BCE in ancient Babylonia. Comparable evidence has been unearthed for scribal mathematics training in the ancient Near East and then for the Greco-Roman world starting around 300 BCE. The oldest known mathematics textbook is the Rhind papyrus, dated from c. 1650 BCE in Egypt. Due to a scarcity of books, mathematical teachings in ancient India were communicated using memorized oral tradition since the Vedic period (c. 1500 – c. 500 BCE). In Imperial China during the Tang dynasty (618–907 CE), a mathematics curriculum was adopted for the civil service exam to join the state bureaucracy. Following the Dark Ages, mathematics education in Europe was provided by religious schools as part of the Quadrivium. Formal instruction in pedagogy began with Jesuit schools in the 16th and 17th century. Most mathematical curricula remained at a basic and practical level until the nineteenth century, when it began to flourish in France and Germany. The oldest journal addressing instruction in mathematics was L'Enseignement Mathématique, which began publication in 1899. The Western advancements in science and technology led to the establishment of centralized education systems in many nation-states, with mathematics as a core component—initially for its military applications. While the content of courses varies, in the present day nearly all countries teach mathematics to students for significant amounts of time. During school, mathematical capabilities and positive expectations have a strong association with career interest in the field. Extrinsic factors such as feedback motivation by teachers, parents, and peer groups can influence the level of interest in mathematics. Some students studying mathematics may develop an apprehension or fear about their performance in the subject. This is known as mathematical anxiety, and is considered the most prominent of the disorders impacting academic performance. Mathematical anxiety can develop due to various factors such as parental and teacher attitudes, social stereotypes, and personal traits. Help to counteract the anxiety can come from changes in instructional approaches, by interactions with parents and teachers, and by tailored treatments for the individual. Psychology (aesthetic, creativity and intuition) The validity of a mathematical theorem relies only on the rigor of its proof, which could theoretically be done automatically by a computer program. This does not mean that there is no place for creativity in a mathematical work. On the contrary, many important mathematical results (theorems) are solutions of problems that other mathematicians failed to solve, and the invention of a way for solving them may be a fundamental way of the solving process. An extreme example is Apery's theorem: Roger Apery provided only the ideas for a proof, and the formal proof was given only several months later by three other mathematicians. Creativity and rigor are not the only psychological aspects of the activity of mathematicians. Some mathematicians can see their activity as a game, more specifically as solving puzzles. This aspect of mathematical activity is emphasized in recreational mathematics. Mathematicians can find an aesthetic value to mathematics. Like beauty, it is hard to define, it is commonly related to elegance, which involves qualities like simplicity, symmetry, completeness, and generality. G. H. Hardy in A Mathematician's Apology expressed the belief that the aesthetic considerations are, in themselves, sufficient to justify the study of pure mathematics. He also identified other criteria such as significance, unexpectedness, and inevitability, which contribute to mathematical aesthetics. Paul Erdős expressed this sentiment more ironically by speaking of "The Book", a supposed divine collection of the most beautiful proofs. The 1998 book Proofs from THE BOOK, inspired by Erdős, is a collection of particularly succinct and revelatory mathematical arguments. Some examples of particularly elegant results included are Euclid's proof that there are infinitely many prime numbers and the fast Fourier transform for harmonic analysis. Some feel that to consider mathematics a science is to downplay its artistry and history in the seven traditional liberal arts. One way this difference of viewpoint plays out is in the philosophical debate as to whether mathematical results are created (as in art) or discovered (as in science). The popularity of recreational mathematics is another sign of the pleasure many find in solving mathematical questions. Cultural impact Artistic expression Notes that sound well together to a Western ear are sounds whose fundamental frequencies of vibration are in simple ratios. For example, an octave doubles the frequency and a perfect fifth multiplies it by 3 2 {\displaystyle {\frac {3}{2}}} . Humans, as well as some other animals, find symmetric patterns to be more beautiful. Mathematically, the symmetries of an object form a group known as the symmetry group. For example, the group underlying mirror symmetry is the cyclic group of two elements, Z / 2 Z {\displaystyle \mathbb {Z} /2\mathbb {Z} } . A Rorschach test is a figure invariant by this symmetry, as are butterfly and animal bodies more generally (at least on the surface). Waves on the sea surface possess translation symmetry: moving one's viewpoint by the distance between wave crests does not change one's view of the sea. Fractals possess self-similarity. Popularization Popular mathematics is the act of presenting mathematics without technical terms. Presenting mathematics may be hard since the general public suffers from mathematical anxiety and mathematical objects are highly abstract. However, popular mathematics writing can overcome this by using applications or cultural links. Despite this, mathematics is rarely the topic of popularization in printed or televised media. Awards and prize problems The most prestigious award in mathematics is the Fields Medal, established in 1936 and awarded every four years (except around World War II) to up to four individuals. It is considered the mathematical equivalent of the Nobel Prize. Other prestigious mathematics awards include: The Abel Prize, instituted in 2002 and first awarded in 2003 The Chern Medal for lifetime achievement, introduced in 2009 and first awarded in 2010 The AMS Leroy P. Steele Prize, awarded since 1970 The Wolf Prize in Mathematics, also for lifetime achievement, instituted in 1978 A famous list of 23 open problems, called "Hilbert's problems", was compiled in 1900 by German mathematician David Hilbert. This list has achieved great celebrity among mathematicians, and at least thirteen of the problems (depending how some are interpreted) have been solved. A new list of seven important problems, titled the "Millennium Prize Problems", was published in 2000. Only one of them, the Riemann hypothesis, duplicates one of Hilbert's problems. A solution to any of these problems carries a 1 million dollar reward. To date, only one of these problems, the Poincaré conjecture, has been solved by the Russian mathematician Grigori Perelman. See also Notes References Citations Other sources == Further reading ==

Literature is any collection of written work, but it is also used more narrowly for writings specifically considered to be an art form, especially novels, plays, and poems. It includes both print and digital writing. In recent centuries, the definition has expanded to include oral literature, much of which has been transcribed. Literature is a method of recording, preserving, and transmitting knowledge and entertainment. It can also have a social, psychological, spiritual, or political role. Literary criticism is one of the oldest academic disciplines, and is concerned with the literary merit or intellectual significance of specific texts. The study of books and other texts as artifacts or traditions is instead encompassed by textual criticism or the history of the book. "Literature", as an art form, is sometimes used synonymously with literary fiction, fiction written with the goal of artistic merit, but can also include works in various non-fiction genres, such as biography, diaries, memoirs, letters, and essays. Within this broader definition, literature includes non-fictional books, articles, or other written information on a particular subject. Developments in print technology have allowed an ever-growing distribution and proliferation of written works, while the digital era has blurred the lines between online electronic literature and other forms of modern media. Definitions Definitions of literature have varied over time. In Western Europe, prior to the 18th century, literature denoted all books and writing. It can be seen as returning to older, more inclusive notions, so that cultural studies, for instance, include, in addition to canonical works, popular and minority genres. The word is also used in reference to non-written works: to "oral literature" and "the literature of preliterate culture". Etymologically, the term derives from Latin literatura/litteratura, "learning, writing, grammar," originally "writing formed with letters," from litera/littera, "letter." In spite of this, the term has also been applied to spoken or sung texts. Literature is often referred to synecdochically as "writing," especially creative writing, and poetically as "the craft of writing" (or simply "the craft"). Syd Field described his discipline, screenwriting, as "a craft that occasionally rises to the level of art." A value judgment definition of literature considers it as consisting solely of high quality writing that forms part of the belles-lettres ("fine writing") tradition. An example of this is in the 1910–1911 Encyclopædia Britannica, which classified literature as "the best expression of the best thought reduced to writing". History Oral literature The use of the term "literature" here poses some issues due to its origins in the Latin littera, "letter," essentially writing. Alternatives such as "oral forms" and "oral genres" have been suggested, but the word literature is widely used. Australian Aboriginal culture has thrived on oral traditions and oral histories passed down through tens of thousands of years. In a study published in February 2020, new evidence showed that both Budj Bim and Tower Hill volcanoes erupted between 34,000 and 40,000 years ago. Significantly, this is a "minimum age constraint for human presence in Victoria", and also could be interpreted as evidence for the oral histories of the Gunditjmara people, an Aboriginal Australian people of south-western Victoria, which tell of volcanic eruptions being some of the oldest oral traditions in existence. An axe found underneath volcanic ash in 1947 had already proven that humans inhabited the region before the eruption of Tower Hill. Oral literature is an ancient human tradition found in "all corners of the world." Modern archaeology has been unveiling evidence of the human efforts to preserve and transmit arts and knowledge that depended completely or partially on an oral tradition, across various cultures: The Judeo-Christian Bible reveals its oral traditional roots; medieval European manuscripts are penned by performing scribes; geometric vases from archaic Greece mirror Homer's oral style. (...) Indeed, if these final decades of the millennium have taught us anything, it must be that oral tradition never was the other we accused it of being; it never was the primitive, preliminary technology of communication we thought it to be. Rather, if the whole truth is told, oral tradition stands out as the single most dominant communicative technology of our species as both a historical fact and, in many areas still, a contemporary reality. The earliest poetry is believed to have been recited or sung, employed as a way of remembering history, genealogy, and law. In Asia, the transmission of folklore, mythologies as well as scriptures in ancient India, in different Indian religions, was by oral tradition, preserved with precision with the help of elaborate mnemonic techniques. The early Buddhist texts are also generally believed to be of oral tradition, with the first by comparing inconsistencies in the transmitted versions of literature from various oral societies such as the Greek, Serbia and other cultures, then noting that the Vedic literature is too consistent and vast to have been composed and transmitted orally across generations, without being written down. According to Goody, the Vedic texts likely involved both a written and oral tradition, calling it a "parallel products of a literate society". All ancient Greek literature was to some degree oral in nature, and the earliest literature was completely so. Homer's epic poetry, states Michael Gagarin, was largely composed, performed and transmitted orally. As folklores and legends were performed in front of distant audiences, the singers would substitute the names in the stories with local characters or rulers to give the stories a local flavor and thus connect with the audience by making the historicity embedded in the oral tradition as unreliable. The lack of surviving texts about the Greek and Roman religious traditions have led scholars to presume that these were ritualistic and transmitted as oral traditions, but some scholars disagree that the complex rituals in the ancient Greek and Roman civilizations were an exclusive product of an oral tradition. Writing systems are not known to have existed among Native North Americans (north of Mesoamerica) before contact with Europeans. Oral storytelling traditions flourished in a context without the use of writing to record and preserve history, scientific knowledge, and social practices. While some stories were told for amusement and leisure, most functioned as practical lessons from tribal experience applied to immediate moral, social, psychological, and environmental issues. Stories fuse fictional, supernatural, or otherwise exaggerated characters and circumstances with real emotions and morals as a means of teaching. Plots often reflect real life situations and may be aimed at particular people known by the story's audience. In this way, social pressure could be exerted without directly causing embarrassment or social exclusion. For example, rather than yelling, Inuit parents might deter their children from wandering too close to the water's edge by telling a story about a sea monster with a pouch for children within its reach. The enduring significance of oral traditions is underscored in a systemic literature review on indigenous languages in South Africa, within the framework of contemporary linguistic challenges. Oral literature is crucial for cultural preservation, linguistic diversity, and social justice, as evidenced by the postcolonial struggles and ongoing initiatives to safeguard and promote South African indigenous languages. Oratory Oratory or the art of public speaking was considered a literary art for a significant period of time. From ancient Greece to the late 19th century, rhetoric played a central role in Western education in training orators, lawyers, counselors, historians, statesmen, and poets. Writing Around the 4th millennium BC, the complexity of trade and administration in Mesopotamia outgrew human memory, and writing became a more dependable method of recording and presenting transactions in a permanent form. Though in both ancient Egypt and Mesoamerica, writing may have already emerged because of the need to record historical and environmental events. Subsequent innovations included more uniform, predictable legal systems, sacred texts, and the origins of modern practices of scientific inquiry and knowledge-consolidation, all largely reliant on portable and easily reproducible forms of writing. Early written literature Ancient Egyptian literature, along with Sumerian literature, are considered the world's oldest literatures. The primary genres of the literature of ancient Egypt—didactic texts, hymns and prayers, and tales—were written almost entirely in verse; By the Old Kingdom (26th century BC to 22nd century BC), literary works included funerary texts, epistles and letters, hymns and poems, and commemorative autobiographical texts recounting the careers of prominent administrative officials. It was not until the early Middle Kingdom (21st century BC to 17th century BC) that a narrative Egyptian literature was created. Many works of early periods, even in narrative form, had a covert moral or didactic purpose, such as the Sanskrit Panchatantra (200 BC – 300 AD), based on older oral tradition. Drama and satire also developed as urban cultures, which provided a larger public audience, and later readership for literary production. Lyric poetry (as opposed to epic poetry) was often the speciality of courts and aristocratic circles, particularly in East Asia where songs were collected by the Chinese aristocracy as poems, the most notable being the Shijing or Book of Songs (1046–c. 600 BC). In ancient China, early literature was primarily focused on philosophy, historiography, military science, agriculture, and poetry. China, the origin of modern paper making and woodblock printing, produced the world's first print cultures. Much of Chinese literature originates with the Hundred Schools of Thought period that occurred during the Eastern Zhou dynasty (769‒269 BC). The most important of these include the Classics of Confucianism, of Daoism, of Mohism, of Legalism, as well as works of military science (e.g. Sun Tzu's The Art of War, c. 5th century BC) and Chinese history (e.g. Sima Qian's Records of the Grand Historian, c. 94 BC). Ancient Chinese literature had a heavy emphasis on historiography, with often very detailed court records. An exemplary piece of narrative history of ancient China was the Zuo Zhuan, which was compiled no later than 389 BC, and attributed to the blind 5th-century BC historian Zuo Qiuming. In ancient India, literature originated from stories that were originally orally transmitted. Early genres included drama, fables, sutras and epic poetry. Sanskrit literature begins with the Vedas, dating back to 1500–1000 BC, and continues with the Sanskrit Epics of Iron Age India. The Vedas are among the oldest sacred texts. The Samhitas (vedic collections) date to roughly 1500–1000 BC, and the "circum-Vedic" texts, as well as the redaction of the Samhitas, date to c. 1000‒500 BC, resulting in a Vedic period, spanning the mid-2nd to mid-1st millennium BC, or the Late Bronze Age and the Iron Age. The period between approximately the 6th to 1st centuries BC saw the composition and redaction of the two most influential Indian epics, the Mahabharata and the Ramayana, with subsequent redaction progressing down to the 4th century AD such as Ramcharitmanas. The earliest known Greek writings are Mycenaean (c. 1600–1100 BC), written in the Linear B syllabary on clay tablets. These documents contain prosaic records largely concerned with trade (lists, inventories, receipts, etc.); no real literature has been discovered. Michael Ventris and John Chadwick, the original decipherers of Linear B, state that literature almost certainly existed in Mycenaean Greece, but it was either not written down or, if it was, it was on parchment or wooden tablets, which did not survive the destruction of the Mycenaean palaces in the twelfth century BC. Homer's epic poems, the Iliad and the Odyssey, are central works of ancient Greek literature. It is generally accepted that the poems were composed at some point around the late eighth or early seventh century BC. Modern scholars consider these accounts legendary. Most researchers believe that the poems were originally transmitted orally. From antiquity until the present day, the influence of Homeric epic on Western civilization has been significant, inspiring many of its most famous works of literature, music, art and film. The Homeric epics were the greatest influence on ancient Greek culture and education; to Plato, Homer was simply the one who "has taught Greece" – ten Hellada pepaideuken. Hesiod's Works and Days (c.700 BC) and Theogony are some of the earliest and most influential works of ancient Greek literature. Classical Greek genres included philosophy, poetry, historiography, comedies and dramas. Plato (428/427 or 424/423 – 348/347 BC) and Aristotle (384–322 BC) authored philosophical texts that are regarded as the foundation of Western philosophy, Sappho (c. 630 – c. 570 BC) and Pindar were influential lyric poets, and Herodotus (c. 484 – c. 425 BC) and Thucydides were early Greek historians. Although drama was popular in ancient Greece, of the hundreds of tragedies written and performed during the classical age, only a limited number of plays by three authors still exist: Aeschylus, Sophocles, and Euripides. The plays of Aristophanes (c. 446 – c. 386 BC) provide the only real examples of a genre of comic drama known as Old Comedy, the earliest form of Greek Comedy, and are in fact used to define the genre. The Hebrew religious text, the Torah, is widely seen as a product of the Persian period (539–333 BC, probably 450–350 BC). This consensus echoes a traditional Jewish view which gives Ezra, the leader of the Jewish community on its return from Babylon, a pivotal role in its promulgation. This represents a major source of Christianity's Bible, which has had a major influence on Western literature. The beginning of Roman literature dates to 240 BC, when a Roman audience saw a Latin version of a Greek play. Literature in Latin would flourish for the next six centuries, and includes essays, histories, poems, plays, and other writings. The Qur'an (610 AD to 632 AD), the main holy book of Islam, had a significant influence on the Arab language, and marked the beginning of Islamic literature. Muslims believe it was transcribed in the Arabic dialect of the Quraysh, the tribe of Muhammad. As Islam spread, the Quran had the effect of unifying and standardizing Arabic. Theological works in Latin were the dominant form of literature in Europe typically found in libraries during the Middle Ages. Western Vernacular literature includes the Poetic Edda and the sagas, or heroic epics, of Iceland, the Anglo-Saxon Beowulf, and the German Song of Hildebrandt. A later form of medieval fiction was the romance, an adventurous and sometimes magical narrative with strong popular appeal. Controversial, religious, political and instructional literature proliferated during the European Renaissance as a result of the Johannes Gutenberg's invention of the printing press around 1440, while the Medieval romance developed into the novel. Publishing Publishing became possible with the invention of writing but became more practical with the invention of printing. Prior to printing, distributed works were copied manually, by scribes. The Chinese inventor Bi Sheng made movable type of earthenware c. 1045 and was spread to Korea later. Around 1230, Koreans invented a metal type movable printing. East metal movable type was spread to Europe between the late 14th century and early 15th century. In c. 1450, Johannes Gutenberg invented movable type in Europe. This invention gradually made books less expensive to produce and more widely available. Early printed books, single sheets, and images created before 1501 in Europe are known as incunables or incunabula. "A man born in 1453, the year of the fall of Constantinople, could look back from his fiftieth year on a lifetime in which about eight million books had been printed, more perhaps than all the scribes of Europe had produced since Constantine founded his city in A.D. 330." Eventually, printing enabled other forms of publishing besides books. The history of newspaper publishing began in Germany in 1609, with the publishing of magazines following in 1663. University discipline In England In late 1820s England, growing political and social awareness, "particularly among the utilitarians and Benthamites, promoted the possibility of including courses in English literary study in the newly formed London University". This further developed into the idea of the study of literature being "the ideal carrier for the propagation of the humanist cultural myth of a well educated, culturally harmonious nation". America Women and literature The widespread education of women was not common until the nineteenth century, and because of this, literature until recently was mostly male dominated. There were few English-language women poets whose names are remembered until the twentieth century. In the nineteenth century some notable individuals include Emily Brontë, Elizabeth Barrett Browning, and Emily Dickinson (see American poetry). But while generally women are absent from the European canon of Romantic literature, there is one notable exception, the French novelist and memoirist Amantine Dupin (1804 – 1876) best known by her pen name George Sand. One of the more popular writers in Europe in her lifetime, being more renowned than both Victor Hugo and Honoré de Balzac in England in the 1830s and 1840s, Sand is recognised as one of the most notable writers of the European Romantic era. Jane Austen (1775 – 1817) is the first major English woman novelist, while Aphra Behn is an early female dramatist. Nobel Prizes in Literature have been awarded between 1901 and 2020 to 117 individuals: 101 men and 16 women. Selma Lagerlöf (1858 – 1940) was the first woman to win the Nobel Prize in Literature, which she was awarded in 1909. Additionally, she was the first woman to be granted a membership in The Swedish Academy in 1914. Feminist scholars have since the twentieth century sought to expand the literary canon to include more women writers. Children's literature A separate genre of children's literature only began to emerge in the eighteenth century, with the development of the concept of childhood. The earliest of these books were educational books, books on conduct, and simple ABCs—often decorated with animals, plants, and anthropomorphic letters. Study and criticism Literary theory A fundamental question of literary theory is "what is literature?" – although many contemporary theorists and literary scholars believe either that "literature" cannot be defined or that it can refer to any use of language. Literary fiction Literary fiction is a term used to describe fiction that explores any facet of the human condition, and may involve social commentary. It is often regarded as having more artistic merit than genre fiction, especially the most commercially oriented types, but this has been contested in recent years, with the serious study of genre fiction within universities. The following, by the British author William Boyd on the short story, might be applied to all prose fiction: [short stories] seem to answer something very deep in our nature as if, for the duration of its telling, something special has been created, some essence of our experience extrapolated, some temporary sense has been made of our common, turbulent journey towards the grave and oblivion. The very best in literature is annually recognized by the Nobel Prize in Literature, which is awarded to an author from any country who has, in the words of the will of Swedish industrialist Alfred Nobel, produced "in the field of literature the most outstanding work in an ideal direction" (original Swedish: den som inom litteraturen har producerat det mest framstående verket i en idealisk riktning). The value of imaginative literature Some researchers suggest that literary fiction can play a role in an individual's psychological development. Psychologists have also been using literature as a therapeutic tool. Psychologist Hogan argues for the value of the time and emotion that a person devotes to understanding a character's situation in literature; that it can unite a large community by provoking universal emotions, as well as allowing readers access to different cultures, and new emotional experiences. One study, for example, suggested that the presence of familiar cultural values in literary texts played an important impact on the performance of minority students. Psychologist Maslow's ideas help literary critics understand how characters in literature reflect their personal culture and the history. The theory suggests that literature helps an individual's struggle for self-fulfillment. Aesthetic value The influence of religious texts Religion has had a major influence on literature, through works like the Vedas, the Torah, the Bible, and the Quran. The King James Version of the Bible has been called "the most influential version of the most influential book in the world, in what is now its most influential language", "the most important book in English religion and culture", and "arguably the most celebrated book in the English-speaking world," principally because of its literary style and widespread distribution. Prominent atheist figures such as the late Christopher Hitchens and Richard Dawkins have praised the King James Version as being "a giant step in the maturing of English literature" and "a great work of literature", respectively, with Dawkins adding: "A native speaker of English who has never read a word of the King James Bible is verging on the barbarian". Societies in which preaching has great importance, and those in which religious structures and authorities have a near-monopoly of reading and writing and/or a censorship role (as, for example, in the European Middle Ages), may impart a religious gloss to much of the literature those societies produce or retain. The traditions of close study of religious texts has furthered the development of techniques and theories in literary studies. Types Poetry Poetry has traditionally been distinguished from prose by its greater use of the aesthetic qualities of language, including musical devices such as assonance, alliteration, rhyme, and rhythm, and by being set in lines and verses rather than paragraphs, and more recently its use of other typographical elements. This distinction is complicated by various hybrid forms such as digital poetry, sound poetry, concrete poetry and prose poem, and more generally by the fact that prose possesses rhythm. Abram Lipsky refers to it as an "open secret" that "prose is not distinguished from poetry by lack of rhythm". Prior to the 19th century, poetry was commonly understood to be something set in metrical lines: "any kind of subject consisting of Rhythm or Verses". Possibly as a result of Aristotle's influence (his Poetics), "poetry" before the 19th century was usually less a technical designation for verse than a normative category of fictive or rhetorical art. As a form it may pre-date literacy, with the earliest works being composed within and sustained by an oral tradition; hence it constitutes the earliest example of literature. Prose As noted above, prose generally makes far less use of the aesthetic qualities of language than poetry. However, developments in modern literature, including free verse and prose poetry have tended to blur the differences, and poet T.S. Eliot suggested that while "the distinction between verse and prose is clear, the distinction between poetry and prose is obscure". There are verse novels, a type of narrative poetry in which a novel-length narrative is told through the medium of poetry rather than prose. Eugene Onegin (1831) by Alexander Pushkin is the most famous example. On the historical development of prose, Richard Graff notes that, in the case of ancient Greece, "recent scholarship has emphasized the fact that formal prose was a comparatively late development, an 'invention' properly associated with the classical period". Latin was a major influence on the development of prose in many European countries. Especially important was the great Roman orator Cicero. It was the lingua franca among literate Europeans until quite recent times, and the great works of Descartes (1596 – 1650), Francis Bacon (1561 – 1626), and Baruch Spinoza (1632 – 1677) were published in Latin. Among the last important books written primarily in Latin prose were the works of Swedenborg (d. 1772), Linnaeus (d. 1778), Euler (d. 1783), Gauss (d. 1855), and Isaac Newton (d. 1727). Novel A novel is a long fictional narrative, usually written in prose. In English, the term emerged from the Romance languages in the late 15th century, with the meaning of "news"; it came to indicate something new, without a distinction between fact or fiction. The romance is a closely related long prose narrative. Walter Scott defined it as "a fictitious narrative in prose or verse; the interest of which turns upon marvelous and uncommon incidents", whereas in the novel "the events are accommodated to the ordinary train of human events and the modern state of society". Other European languages do not distinguish between romance and novel: "a novel is le roman, der Roman, il romanzo", indicates the proximity of the forms. Although there are many historical prototypes, so-called "novels before the novel", the modern novel form emerges late in cultural history—roughly during the eighteenth century. Initially subject to much criticism, the novel has acquired a dominant position amongst literary forms, both popularly and critically. Novella The publisher Melville House classifies the novella as "too short to be a novel, too long to be a short story". Publishers and literary award societies typically consider a novella to be between 17,000 and 40,000 words. Short story A dilemma in defining the "short story" as a literary form is how to, or whether one should, distinguish it from any short narrative and its contested origin, that include the Bible, and Edgar Allan Poe. Graphic novel Graphic novels and comic books present stories told in a combination of artwork, dialogue, and text. Electronic literature Electronic literature is a literary genre consisting of works created exclusively on and for digital devices. Non-fiction Common literary examples of non-fiction include, the essay; travel literature; biography, autobiography and memoir; journalism; letter; diary; history, philosophy, economics; scientific, nature, and technical writings. Non-fiction can fall within the broad category of literature as "any collection of written work", but some works fall within the narrower definition "by virtue of the excellence of their writing, their originality and their general aesthetic and artistic merits". Drama Drama is literature intended for performance. The form is combined with music and dance in opera and musical theatre (see libretto). A play is a written dramatic work by a playwright that is intended for performance in a theatre; it comprises chiefly dialogue between characters. A closet drama, by contrast, is written to be read rather than to be performed; the meaning of which can be realized fully on the page. Nearly all drama took verse form until comparatively recently. The earliest form of which there exists substantial knowledge is Greek drama. This developed as a performance associated with religious and civic festivals, typically enacting or developing upon well-known historical, or mythological themes, In the twentieth century, scripts written for non-stage media have been added to this form, including radio, television and film. Law Law and literature The law and literature movement focuses on the interdisciplinary connection between law and literature. Copyright Copyright is a type of intellectual property that gives its owner the exclusive right to make copies of a creative work, usually for a limited time. The creative work may be in a literary, artistic, educational, or musical form. Copyright is intended to protect the original expression of an idea in the form of a creative work, but not the idea itself. United Kingdom Literary works have been protected by copyright law from unauthorized reproduction since at least 1710. Literary works are defined by copyright law to mean "any work, other than a dramatic or musical work, which is written, spoken or sung, and accordingly includes (a) a table or compilation (other than a database), (b) a computer program, (c) preparatory design material for a computer program, and (d) a database." Literary works are all works of literature; that is all works expressed in print or writing (other than dramatic or musical works). United States The copyright law of the United States has a long and complicated history, dating back to colonial times. It was established as federal law with the Copyright Act of 1790. This act was updated many times, including a major revision in 1976. European Union The copyright law of the European Union is the copyright law applicable within the European Union. Copyright law is largely harmonized in the Union, although country to country differences exist. The body of law was implemented in the EU through a number of directives, which the member states need to enact into their national law. The main copyright directives are the Copyright Term Directive, the Information Society Directive and the Directive on Copyright in the Digital Single Market. Copyright in the Union is furthermore dependent on international conventions to which the European Union is a member (such as the TRIPS Agreement and conventions to which all Member States are parties (such as the Berne Convention)). Copyright in communist countries Copyright in Japan Japan was a party to the original Berne convention in 1899, so its copyright law is in sync with most international regulations. The convention protected copyrighted works for 50 years after the author's death (or 50 years after publication for unknown authors and corporations). However, in 2004 Japan extended the copyright term to 70 years for cinematographic works. At the end of 2018, as a result of the Trans-Pacific Partnership negotiations, the 70-year term was applied to all works. This new term is not applied retroactively; works that had entered the public domain between 1999 and 2018 by expiration would remain in the public domain. Censorship Censorship of literature is employed by states, religious organizations, educational institutions, etc., to control what can be portrayed, spoken, performed, or written. Generally such bodies attempt to ban works for political reasons, or because they deal with other controversial matters such as race, or sex. A notable example of censorship is James Joyce's novel Ulysses, which has been described by Russian-American novelist Vladimir Nabokov as a "divine work of art" and the greatest masterpiece of 20th century prose. It was banned in the United States from 1921 until 1933 on the grounds of obscenity. Nowadays it is a central literary text in English literature courses, throughout the world. Awards There are numerous awards recognizing achievements and contributions in literature. Given the diversity of the field, awards are typically limited in scope, usually on: form, genre, language, nationality and output (e.g. for first-time writers or debut novels). The Nobel Prize in Literature was one of the six Nobel Prizes established by the will of Alfred Nobel in 1895, and is awarded to an author on the basis of their body of work, rather than to, or for, a particular work itself. Other literary prizes for which all nationalities are eligible include: the Neustadt International Prize for Literature, the Man Booker International Prize, Pulitzer Prize, Hugo Award, Guardian First Book Award and the Franz Kafka Prize. See also Index of literature articles Notes References Bibliography Further reading External links Project Gutenberg Online Library Internet Book List similar to IMDb but for books (archived 7 February 2007) Digital eBook Collection – Internet Archive

Art is a diverse range of cultural activity centered around works utilizing creative or imaginative talents, which are expected to evoke a worthwhile experience, generally through an expression of emotional power, conceptual ideas, technical proficiency, or beauty. There is no generally agreed definition of what constitutes art, and its interpretation has varied greatly throughout history and across cultures. In the Western tradition, the three classical branches of visual art are painting, sculpture, and architecture. Theatre, dance, and other performing arts, as well as literature, music, film and other media such as interactive media, are included in a broader definition of "the arts". Until the 17th century, art referred to any skill or mastery and was not differentiated from crafts or sciences. In modern usage after the 17th century, where aesthetic considerations are paramount, the fine arts are separated and distinguished from acquired skills in general, such as the decorative or applied arts. The nature of art and related concepts, such as creativity and interpretation, are explored in a branch of philosophy known as aesthetics. The resulting artworks are studied in the professional fields of art criticism and the history of art. Overview In the perspective of the history of art, artistic works have existed for almost as long as humankind: from early prehistoric art to contemporary art; however, some theorists think that the typical concept of "artistic works" does not fit well outside modern Western societies. One early sense of the definition of art is closely related to the older Latin meaning, which roughly translates to "skill" or "craft", as associated with words such as "artisan". English words derived from this meaning include artifact, artificial, artifice, medical arts, and military arts. However, there are many other colloquial uses of the word, all with some relation to its etymology. Over time, philosophers like Plato, Aristotle, Socrates and Immanuel Kant, among others, questioned the meaning of art. Several dialogues in Plato tackle questions about art, while Socrates says that poetry is inspired by the muses and is not rational. He speaks approvingly of this, and other forms of divine madness (drunkenness, eroticism, and dreaming) in the Phaedrus (265a–c), and yet in the Republic wants to outlaw Homer's great poetic art, and laughter as well. In Ion, Socrates gives no hint of the disapproval of Homer that he expresses in the Republic. The dialogue Ion suggests that Homer's Iliad functioned in the ancient Greek world as the Bible does today in the modern Christian world: as divinely inspired literary art that can provide moral guidance, if only it can be properly interpreted. With regards to the literary art and the musical arts, Aristotle considered epic poetry, tragedy, comedy, Dithyrambic poetry and music to be mimetic or imitative art, each varying in imitation by medium, object, and manner. For example, music imitates with the media of rhythm and harmony, whereas dance imitates with rhythm alone, and poetry with language. The forms also differ in their object of imitation. Comedy, for instance, is a dramatic imitation of men worse than average; whereas tragedy imitates men slightly better than average. Lastly, the forms differ in their manner of imitation—through narrative or character, through change or no change, and through drama or no drama. Aristotle believed that imitation is natural to mankind and constitutes one of mankind's advantages over animals. The more recent and specific sense of the word art as an abbreviation for creative art or fine art emerged in the early 17th century. Fine art refers to a skill used to express the artist's creativity, or to engage the audience's aesthetic sensibilities, or to draw the audience towards consideration of more refined or finer works of art. Within this latter sense, the word art may refer to several things: (i) a study of a creative skill, (ii) a process of using the creative skill, (iii) a product of the creative skill, or (iv) the audience's experience with the creative skill. The creative arts (art as discipline) are a collection of disciplines which produce artworks (art as objects) that are compelled by a personal drive (art as activity) and convey a message, mood, or symbolism for the perceiver to interpret (art as experience). Art is something that stimulates an individual's thoughts, emotions, beliefs, or ideas through the senses. Works of art can be explicitly made for this purpose or interpreted on the basis of images or objects. For some scholars, such as Kant, the sciences and the arts could be distinguished by taking science as representing the domain of knowledge and the arts as representing the domain of the freedom of artistic expression. Often, if the skill is being used in a common or practical way, people will consider it a craft instead of art. Likewise, if the skill is being used in a commercial or industrial way, it may be considered commercial art instead of fine art. On the other hand, crafts and design are sometimes considered applied art. Some art followers have argued that the difference between fine art and applied art has more to do with value judgments made about the art than any clear definitional difference. However, even fine art often has goals beyond pure creativity and self-expression. The purpose of works of art may be to communicate ideas, such as in politically, spiritually, or philosophically motivated art; to create a sense of beauty (see aesthetics); to explore the nature of perception; for pleasure; or to generate strong emotions. The purpose may also be seemingly nonexistent. The nature of art has been described by philosopher Richard Wollheim as "one of the most elusive of the traditional problems of human culture". Art has been defined as a vehicle for the expression or communication of emotions and ideas, a means for exploring and appreciating formal elements for their own sake, and as mimesis or representation. Art as mimesis has deep roots in the philosophy of Aristotle. Leo Tolstoy identified art as a use of indirect means to communicate from one person to another. Benedetto Croce and R. G. Collingwood advanced the idealist view that art expresses emotions, and that the work of art therefore essentially exists in the mind of the creator. The theory of art as form has its roots in the philosophy of Kant, and was developed in the early 20th century by Roger Fry and Clive Bell. More recently, thinkers influenced by Martin Heidegger have interpreted art as the means by which a community develops for itself a medium for self-expression and interpretation. George Dickie has offered an institutional theory of art that defines a work of art as any artifact upon which a qualified person or persons acting on behalf of the social institution commonly referred to as "the art world" has conferred "the status of candidate for appreciation". Larry Shiner has described fine art as "not an essence or a fate but something we have made. Art as we have generally understood it is a European invention barely two hundred years old." Art may be characterized in terms of mimesis (its representation of reality), narrative (storytelling), expression, communication of emotion, or other qualities. During the Romantic period, art came to be seen as "a special faculty of the human mind to be classified with religion and science". History A shell engraved by Homo erectus was determined to be between 430,000 and 540,000 years old. A set of eight 130,000 years old white-tailed eagle talons bear cut marks and abrasion that indicate manipulation by neanderthals, possibly for using it as jewelry. A series of tiny, drilled snail shells about 75,000 years old—were discovered in a South African cave. Containers that may have been used to hold paints have been found dating as far back as 100,000 years. The oldest piece of art found in Europe is the Riesenhirschknochen der Einhornhöhle, dating back 51,000 years and made by Neanderthals. Sculptures, cave paintings, rock paintings and petroglyphs from the Upper Paleolithic dating to roughly 40,000 years ago have been found, but the precise meaning of such art is often disputed because so little is known about the cultures that produced them. The first undisputed sculptures and similar art pieces, like the Venus of Hohle Fels, are the numerous objects found at the Caves and Ice Age Art in the Swabian Jura UNESCO World Heritage Site, where the oldest non-stationary works of human art yet discovered were found, in the form of carved animal and humanoid figurines, in addition to the oldest musical instruments unearthed so far, with the artifacts dating between 43,000 and 35,000 BC, so being the first centre of human art. Many great traditions in art have a foundation in the art of one of the great ancient civilizations: Ancient Egypt, Mesopotamia, Persia, India, China, Ancient Greece, Rome, as well as Inca, Maya, and Olmec. Each of these centers of early civilization developed a unique and characteristic style in its art. Because of the size and duration of these civilizations, more of their art works have survived and more of their influence has been transmitted to other cultures and later times. Some also have provided the first records of how artists worked. For example, this period of Greek art saw a veneration of the human physical form and the development of equivalent skills to show musculature, poise, beauty, and anatomically correct proportions. In Byzantine and Medieval art of the Western Middle Ages, much art focused on the expression of subjects about biblical and religious culture, and used styles that showed the higher glory of a heavenly world, such as the use of gold in the background of paintings, or glass in mosaics or windows, which also presented figures in idealized, patterned (flat) forms. Nevertheless, a classical realist tradition persisted in small Byzantine works, and realism steadily grew in the art of Catholic Europe. Renaissance art had a greatly increased emphasis on the realistic depiction of the material world, and the place of humans in it, reflected in the corporeality of the human body, and development of a systematic method of graphical perspective to depict recession in a three-dimensional picture space. In the east, Islamic art's rejection of iconography led to emphasis on geometric patterns, calligraphy, and architecture. Further east, religion dominated artistic styles and forms too. India and Tibet saw emphasis on painted sculptures and dance, while religious painting borrowed many conventions from sculpture and tended to bright contrasting colors with emphasis on outlines. China saw the flourishing of many art forms: jade carving, bronzework, pottery (including the stunning Terracotta Army of Emperor Qin), poetry, calligraphy, music, painting, drama, fiction, etc. Chinese styles vary greatly from era to era and each one is traditionally named after the ruling dynasty. So, for example, Tang dynasty paintings are monochromatic and sparse, emphasizing idealized landscapes, but Ming dynasty paintings are busy and colorful, and focus on telling stories via setting and composition. Japan names its styles after imperial dynasties too, and also saw much interplay between the styles of calligraphy and painting. Woodblock printing became important in Japan after the 17th century. The western Age of Enlightenment in the 18th century saw artistic depictions of physical and rational certainties of the clockwork universe, as well as politically revolutionary visions of a post-monarchist world, such as Blake's portrayal of Newton as a divine geometer, or David's propagandistic paintings. This led to Romantic rejections of this in favor of pictures of the emotional side and individuality of humans, exemplified in the novels of Goethe. The late 19th century then saw a host of artistic movements, such as academic art, Symbolism, impressionism and fauvism among others. The history of 20th-century art is a narrative of endless possibilities and the search for new standards, each being torn down in succession by the next. Thus the parameters of Impressionism, Expressionism, Fauvism, Cubism, Dadaism, Surrealism, etc. cannot be maintained very much beyond the time of their invention. Increasing global interaction during this time saw an equivalent influence of other cultures into Western art. Thus, Japanese woodblock prints (themselves influenced by Western Renaissance draftsmanship) had an immense influence on impressionism and subsequent development. Later, African sculptures were taken up by Picasso and to some extent by Matisse. Similarly, in the 19th and 20th centuries the West has had huge impacts on Eastern art with originally western ideas like Communism and Post-Modernism exerting a powerful influence. Modernism, the idealistic search for truth, gave way in the latter half of the 20th century to a realization of its unattainability. Theodor W. Adorno said in 1970, "It is now taken for granted that nothing which concerns art can be taken for granted any more: neither art itself, nor art in relationship to the whole, nor even the right of art to exist." Relativism was accepted as an unavoidable truth, which led to the period of contemporary art and postmodern criticism, where cultures of the world and of history are seen as changing forms, which can be appreciated and drawn from only with skepticism and irony. Furthermore, the separation of cultures is increasingly blurred and some argue it is now more appropriate to think in terms of a global culture, rather than of regional ones. In The Origin of the Work of Art, Martin Heidegger, a German philosopher and seminal thinker, describes the essence of art in terms of the concepts of being and truth. He argues that art is not only a way of expressing the element of truth in a culture, but the means of creating it and providing a springboard from which "that which is" can be revealed. Works of art are not merely representations of the way things are, but actually produce a community's shared understanding. Each time a new artwork is added to any culture, the meaning of what it is to exist is inherently changed. Historically, art and artistic skills and ideas have often been spread through trade. An example of this is the Silk Road, where Hellenistic, Iranian, Indian and Chinese influences could mix. Greco Buddhist art is one of the most vivid examples of this interaction. The meeting of different cultures and worldviews also influenced artistic creation. An example of this is the multicultural port metropolis of Trieste at the beginning of the 20th century, where James Joyce met writers from Central Europe and the artistic development of New York City as a cultural melting pot. Forms, genres, media, and styles The creative arts are often divided into more specific categories, typically along perceptually distinguishable categories such as media, genre, styles, and form. Art form refers to the elements of art that are independent of its interpretation or significance. It covers the methods adopted by the artist and the physical composition of the artwork, primarily non-semantic aspects of the work (i.e., figurae), such as color, contour, dimension, medium, melody, space, texture, and value. Form may also include Design principles, such as arrangement, balance, contrast, emphasis, harmony, proportion, proximity, and rhythm. In general there are three schools of philosophy regarding art, focusing respectively on form, content, and context. Extreme Formalism is the view that all aesthetic properties of art are formal (that is, part of the art form). Philosophers almost universally reject this view and hold that the properties and aesthetics of art extend beyond materials, techniques, and form. Unfortunately, there is little consensus on terminology for these informal properties. Some authors refer to subject matter and content—i.e., denotations and connotations—while others prefer terms like meaning and significance. Extreme Intentionalism holds that authorial intent plays a decisive role in the meaning of a work of art, conveying the content or essential main idea, while all other interpretations can be discarded. It defines the subject as the persons or idea represented, and the content as the artist's experience of that subject. For example, the composition of Napoleon I on his Imperial Throne is partly borrowed from the Statue of Zeus at Olympia. As evidenced by the title, the subject is Napoleon, and the content is Ingres's representation of Napoleon as "Emperor-God beyond time and space". Similarly to extreme formalism, philosophers typically reject extreme intentionalism, because art may have multiple ambiguous meanings and authorial intent may be unknowable and thus irrelevant. Its restrictive interpretation is "socially unhealthy, philosophically unreal, and politically unwise". Finally, the developing theory of post-structuralism studies art's significance in a cultural context, such as the ideas, emotions, and reactions prompted by a work. The cultural context often reduces to the artist's techniques and intentions, in which case analysis proceeds along lines similar to formalism and intentionalism. However, in other cases historical and material conditions may predominate, such as religious and philosophical convictions, sociopolitical and economic structures, or even climate and geography. Art criticism continues to grow and develop alongside art. Skill and craft Art can connote a sense of trained ability or mastery of a medium. Art can also refer to the developed and efficient use of a language to convey meaning with immediacy or depth. Art can be defined as an act of expressing feelings, thoughts, and observations. There is an understanding that is reached with the material as a result of handling it, which facilitates one's thought processes. A common view is that the epithet art, particular in its elevated sense, requires a certain level of creative expertise by the artist, whether this be a demonstration of technical ability, an originality in stylistic approach, or a combination of these two. Traditionally skill of execution was viewed as a quality inseparable from art and thus necessary for its success; for Leonardo da Vinci, art, neither more nor less than his other endeavors, was a manifestation of skill. Rembrandt's work, now praised for its ephemeral virtues, was most admired by his contemporaries for its virtuosity. At the turn of the 20th century, the adroit performances of John Singer Sargent were alternately admired and viewed with skepticism for their manual fluency, yet at nearly the same time the artist who would become the era's most recognized and peripatetic iconoclast, Pablo Picasso, was completing a traditional academic training at which he excelled. A common contemporary criticism of some modern art occurs along the lines of objecting to the apparent lack of skill or ability required in the production of the artistic object. In conceptual art, Marcel Duchamp's Fountain is among the first examples of pieces wherein the artist used found objects ("ready-made") and exercised no traditionally recognised set of skills. Tracey Emin's My Bed, or Damien Hirst's The Physical Impossibility of Death in the Mind of Someone Living follow this example. Emin slept (and engaged in other activities) in her bed before placing the result in a gallery as work of art. Hirst came up with the conceptual design for the artwork but has left most of the eventual creation of many works to employed artisans. Hirst's celebrity is founded entirely on his ability to produce shocking concepts. The actual production in many conceptual and contemporary works of art is a matter of assembly of found objects. However, there are many modernist and contemporary artists who continue to excel in the skills of drawing and painting and in creating hands-on works of art. Purpose Art has had a great number of different functions throughout its history, making its purpose difficult to abstract or quantify to any single concept. This does not imply that the purpose of art is "vague", but that it has had many unique, different reasons for being created. Some of these functions of art are provided in the following outline. The different purposes of art may be grouped according to those that are non-motivated, and those that are motivated (Lévi-Strauss). Non-motivated functions The non-motivated purposes of art are those that are integral to being human, transcend the individual, or do not fulfill a specific external purpose. In this sense, Art, as creativity, is something humans must do by their very nature (i.e., no other species creates art), and is therefore beyond utility. Basic human instinct for harmony, balance, rhythm. Art at this level is not an action or an object, but an internal appreciation of balance and harmony (beauty), and therefore an aspect of being human beyond utility.Imitation, then, is one instinct of our nature. Next, there is the instinct for 'harmony' and rhythm, meters being manifestly sections of rhythm. Persons, therefore, starting with this natural gift developed by degrees their special aptitudes, till their rude improvisations gave birth to Poetry. – Aristotle Experience of the mysterious. Art provides a way to experience one's self in relation to the universe. This experience may often come unmotivated, as one appreciates art, music or poetry.The most beautiful thing we can experience is the mysterious. It is the source of all true art and science. – Albert Einstein Expression of the imagination. Art provides a means to express the imagination in non-grammatic ways that are not tied to the formality of spoken or written language. Unlike words, which come in sequences and each of which have a definite meaning, art provides a range of forms, symbols and ideas with meanings that are malleable.Jupiter's eagle [as an example of art] is not, like logical (aesthetic) attributes of an object, the concept of the sublimity and majesty of creation, but rather something else—something that gives the imagination an incentive to spread its flight over a whole host of kindred representations that provoke more thought than admits of expression in a concept determined by words. They furnish an aesthetic idea, which serves the above rational idea as a substitute for logical presentation, but with the proper function, however, of animating the mind by opening out for it a prospect into a field of kindred representations stretching beyond its ken. – Immanuel Kant Ritualistic and symbolic functions. In many cultures, art is used in rituals, performances and dances as a decoration or symbol. While these often have no specific utilitarian (motivated) purpose, anthropologists know that they often serve a purpose at the level of meaning within a particular culture. This meaning is not furnished by any one individual, but is often the result of many generations of change, and of a cosmological relationship within the culture.Most scholars who deal with rock paintings or objects recovered from prehistoric contexts that cannot be explained in utilitarian terms and are thus categorized as decorative, ritual or symbolic, are aware of the trap posed by the term 'art'. – Silva Tomaskova Motivated functions Motivated purposes of art refer to intentional, conscious actions on the part of the artists or creator. These may be to bring about political change, to comment on an aspect of society, to convey a specific emotion or mood, to address personal psychology, to illustrate another discipline, to (with commercial arts) sell a product, or used as a form of communication. Communication. Art, at its simplest, is a form of communication. As most forms of communication have an intent or goal directed toward another individual, this is a motivated purpose. Illustrative arts, such as scientific illustration, are a form of art as communication. Maps are another example. However, the content need not be scientific. Emotions, moods and feelings are also communicated through art.[Art is a set of] artefacts or images with symbolic meanings as a means of communication. – Steve Mithen Art as entertainment. Art may seek to bring about a particular emotion or mood, for the purpose of relaxing or entertaining the viewer. This is often the function of the art industries of motion pictures and video games. The Avant-Garde. Art for political change. One of the defining functions of early 20th-century art has been to use visual images to bring about political change. Art movements that had this goal—Dadaism, Surrealism, Russian constructivism, and Abstract Expressionism, among others—are collectively referred to as the avant-garde arts.By contrast, the realistic attitude, inspired by positivism, from Saint Thomas Aquinas to Anatole France, clearly seems to me to be hostile to any intellectual or moral advancement. I loathe it, for it is made up of mediocrity, hate, and dull conceit. It is this attitude which today gives birth to these ridiculous books, these insulting plays. It constantly feeds on and derives strength from the newspapers and stultifies both science and art by assiduously flattering the lowest of tastes; clarity bordering on stupidity, a dog's life. – André Breton (Surrealism) Art as a "free zone", removed from the action of the social censure. Unlike the avant-garde movements, which wanted to erase cultural differences in order to produce new universal values, contemporary art has enhanced its tolerance towards cultural differences as well as its critical and liberating functions (social inquiry, activism, subversion, deconstruction, etc.), becoming a more open place for research and experimentation. Art for social inquiry, subversion or anarchy. While similar to art for political change, subversive or deconstructivist art may seek to question aspects of society without any specific political goal. In this case, the function of art may be used to criticize some aspect of society. Graffiti art and other types of street art are graphics and images that are spray-painted or stencilled on publicly viewable walls, buildings, buses, trains, and bridges, usually without permission. Certain art forms, such as graffiti, may also be illegal when they break laws (in this case vandalism). Art for social causes. Art can be used to raise awareness for a large variety of causes. A number of art activities were aimed at raising awareness of autism, cancer, human trafficking, and a variety of other topics, such as ocean conservation, human rights in Darfur, murdered and missing Aboriginal women, elder abuse, and pollution. Trashion, using trash to make fashion, practiced by artists such as Marina DeBris is one example of using art to raise awareness about pollution. Art for psychological and healing purposes. Art is also used by art therapists, psychotherapists and clinical psychologists as art therapy. The Diagnostic Drawing Series, for example, is used to determine the personality and emotional functioning of a patient. The end product is not the principal goal in this case, but rather a process of healing, through creative acts, is sought. The resultant piece of artwork may also offer insight into the troubles experienced by the subject and may suggest suitable approaches to be used in more conventional forms of psychiatric therapy. Art for propaganda, or commercialism. Art is often used as a form of propaganda, and thus can be used to subtly influence popular conceptions or mood. In a similar way, art that tries to sell a product also influences mood and emotion. In both cases, the purpose of art here is to subtly manipulate the viewer into a particular emotional or psychological response toward a particular idea or object. Art as a fitness indicator. It has been argued that the ability of the human brain by far exceeds what was needed for survival in the ancestral environment. One evolutionary psychology explanation for this is that the human brain and associated traits (such as artistic ability and creativity) are the human equivalent of the peacock's tail. The purpose of the male peacock's extravagant tail has been argued to be to attract females (see also Fisherian runaway and handicap principle). According to this theory superior execution of art was evolutionarily important because it attracted mates. The functions of art described above are not mutually exclusive, as many of them may overlap. For example, art for the purpose of entertainment may also seek to sell a product, i.e. the movie or video game. Steps Art can be divided into any number of steps one can make an argument for. This section divides the creative process into broad three steps, but there is no consensus on an exact number. Preparation In the first step, the artist envisions the art in their mind. By imagining what their art would look like, the artist begins the process of bringing the art into existence. Preparation of art may involve approaching and researching the subject matter. Artistic inspiration is one of the main drivers of art, and may be considered to stem from instinct, impressions, and feelings. Creation In the second step, the artist executes the creation of their work. The creation of a piece can be affected by factors such as the artist's mood, surroundings, and mental state. For example, The Black Paintings by Francisco de Goya, created in the elder years of his life, are thought to be so bleak because he was in isolation and because of his experience with war. He painted them directly on the walls of his apartment in Spain, and most likely never discussed them with anyone. The Beatles stated drugs such as LSD and cannabis influenced some of their greatest hits, such as Revolver. Trial and error are considered an integral part of the creation process. Appreciation The last step is art appreciation, which has the sub-topic of critique. In one study, over half of visual arts students agreed that reflection is an essential step of the art process. According to education journals, the reflection of art is considered an essential part of the experience. However an important aspect of art is that others may view and appreciate it as well. While many focus on whether those viewing/listening/etc. believe the art to be good/successful or not, art has profound value beyond its commercial success as a provider of information and health in society. Art enjoyment can bring about a wide spectrum of emotion due to beauty. Some art is meant to be practical, with its analysis studious, meant to stimulate discourse. Public access Since ancient times, much of the finest art has represented a deliberate display of wealth or power, often achieved by using massive scale and expensive materials. Much art has been commissioned by political rulers or religious establishments, with more modest versions only available to the most wealthy in society. Nevertheless, there have been many periods where art of very high quality was available, in terms of ownership, across large parts of society, above all in cheap media such as pottery, which persists in the ground, and perishable media such as textiles and wood. In many different cultures, the ceramics of indigenous peoples of the Americas are found in such a wide range of graves that they were clearly not restricted to a social elite, though other forms of art may have been. Reproductive methods such as moulds made mass-production easier, and were used to bring high-quality Ancient Roman pottery and Greek Tanagra figurines to a very wide market. Cylinder seals were both artistic and practical, and very widely used by what can be loosely called the middle class in the Ancient Near East. Once coins were widely used, these also became an art form that reached the widest range of society. Another important innovation came in the 15th century in Europe, when printmaking began with small woodcuts, mostly religious, that were often very small and hand-colored, and affordable even by peasants who glued them to the walls of their homes. Printed books were initially very expensive, but fell steadily in price until by the 19th century even the poorest could afford some with printed illustrations. Popular prints of many different sorts have decorated homes and other places for centuries. In 1661, the city of Basel, in Switzerland, opened the first public museum of art in the world, the Kunstmuseum Basel. Today, its collection is distinguished by an impressively wide historic span, from the early 15th century up to the immediate present. Its various areas of emphasis give it international standing as one of the most significant museums of its kind. These encompass: paintings and drawings by artists active in the Upper Rhine region between 1400 and 1600, and on the art of the 19th to 21st centuries. Public buildings and monuments, secular and religious, by their nature normally address the whole of society, and visitors as viewers, and display to the general public has long been an important factor in their design. Egyptian temples are typical in that the most largest and most lavish decoration was placed on the parts that could be seen by the general public, rather than the areas seen only by the priests. Many areas of royal palaces, castles and the houses of the social elite were often generally accessible, and large parts of the art collections of such people could often be seen, either by anybody, or by those able to pay a small price, or those wearing the correct clothes, regardless of who they were, as at the Palace of Versailles, where the appropriate extra accessories (silver shoe buckles and a sword) could be hired from shops outside. Special arrangements were made to allow the public to see many royal or private collections placed in galleries, as with the Orleans Collection mostly housed in a wing of the Palais Royal in Paris, which could be visited for most of the 18th century. In Italy the art tourism of the Grand Tour became a major industry from the Renaissance onwards, and governments and cities made efforts to make their key works accessible. The British Royal Collection remains distinct, but large donations such as the Old Royal Library were made from it to the British Museum, established in 1753. The Uffizi in Florence opened entirely as a gallery in 1765, though this function had been gradually taking the building over from the original civil servants' offices for a long time before. The building now occupied by the Prado in Madrid was built before the French Revolution for the public display of parts of the royal art collection, and similar royal galleries open to the public existed in Vienna, Munich and other capitals. The opening of the Musée du Louvre during the French Revolution (in 1793) as a public museum for much of the former French royal collection certainly marked an important stage in the development of public access to art, transferring ownership to a republican state, but was a continuation of trends already well established. Most modern public museums and art education programs for children in schools can be traced back to this impulse to have art available to everyone. However, museums do not only provide availability to art, but do also influence the way art is being perceived by the audience, as studies found. Thus, the museum itself is not only a blunt stage for the presentation of art, but plays an active and vital role in the overall perception of art in modern society. Museums in the United States tend to be gifts from the very rich to the masses. (The Metropolitan Museum of Art in New York City, for example, was created by John Taylor Johnston, a railroad executive whose personal art collection seeded the museum.) But despite all this, at least one of the important functions of art in the 21st century remains as a marker of wealth and social status. There have been attempts by artists to create art that can not be bought by the wealthy as a status object. One of the prime original motivators of much of the art of the late 1960s and 1970s was to create art that could not be bought and sold. It is "necessary to present something more than mere objects" said the major post war German artist Joseph Beuys. This time period saw the rise of such things as performance art, video art, and conceptual art. The idea was that if the artwork was a performance that would leave nothing behind, or was an idea, it could not be bought and sold. "Democratic precepts revolving around the idea that a work of art is a commodity impelled the aesthetic innovation which germinated in the mid-1960s and was reaped throughout the 1970s. Artists broadly identified under the heading of Conceptual art ... substituting performance and publishing activities for engagement with both the material and materialistic concerns of painted or sculptural form ... [have] endeavored to undermine the art object qua object." In the decades since, these ideas have been somewhat lost as the art market has learned to sell limited edition DVDs of video works, invitations to exclusive performance art pieces, and the objects left over from conceptual pieces. Many of these performances create works that are only understood by the elite who have been educated as to why an idea or video or piece of apparent garbage may be considered art. The marker of status becomes understanding the work instead of necessarily owning it, and the artwork remains an upper-class activity. "With the widespread use of DVD recording technology in the early 2000s, artists, and the gallery system that derives its profits from the sale of artworks, gained an important means of controlling the sale of video and computer artworks in limited editions to collectors." Controversies Art has long been controversial, that is to say disliked by some viewers, for a wide variety of reasons, though most pre-modern controversies are dimly recorded, or completely lost to a modern view. Iconoclasm is the destruction of art that is disliked for a variety of reasons, including religious ones. Aniconism is a general dislike of either all figurative images, or often just religious ones, and has been a thread in many major religions. It has been a crucial factor in the history of Islamic art, where depictions of Muhammad remain especially controversial. Much art has been disliked purely because it depicted or otherwise stood for unpopular rulers, parties or other groups. Artistic conventions have often been conservative and taken very seriously by art critics, though often much less so by a wider public. The iconographic content of art could cause controversy, as with late medieval depictions of the new motif of the Swoon of the Virgin in scenes of the Crucifixion of Jesus. The Last Judgment by Michelangelo was controversial for various reasons, including breaches of decorum through nudity and the Apollo-like pose of Christ. The content of much formal art through history was dictated by the patron or commissioner rather than just the artist, but with the advent of Romanticism, and economic changes in the production of art, the artists' vision became the usual determinant of the content of his art, increasing the incidence of controversies, though often reducing their significance. Strong incentives for perceived originality and publicity also encouraged artists to court controversy. Théodore Géricault's Raft of the Medusa (c. 1820), was in part a political commentary on a recent event. Édouard Manet's Le Déjeuner sur l'Herbe (1863), was considered scandalous not because of the nude woman, but because she is seated next to men fully dressed in the clothing of the time, rather than in robes of the antique world. John Singer Sargent's Madame Pierre Gautreau (Madam X) (1884), caused a controversy over the reddish pink used to color the woman's ear lobe, considered far too suggestive and supposedly ruining the high-society model's reputation. The gradual abandonment of naturalism and the depiction of realistic representations of the visual appearance of subjects in the 19th and 20th centuries led to a rolling controversy lasting for over a century. In the 20th century, Pablo Picasso's Guernica (1937) used arresting cubist techniques and stark monochromatic oils, to depict the harrowing consequences of a contemporary bombing of a small, ancient Basque town. Leon Golub's Interrogation III (1981), depicts a female nude, hooded detainee strapped to a chair, her legs open to reveal her sexual organs, surrounded by two tormentors dressed in everyday clothing. Andres Serrano's Piss Christ (1989) is a photograph of a crucifix, sacred to the Christian religion and representing Christ's sacrifice and final suffering, submerged in a glass of the artist's own urine. The resulting uproar led to comments in the United States Senate about public funding of the arts. Theory Before Modernism, aesthetics in Western art was greatly concerned with achieving the appropriate balance between different aspects of realism or truth to nature and the ideal; ideas as to what the appropriate balance is have shifted to and fro over the centuries. This concern is largely absent in other traditions of art. The aesthetic theorist John Ruskin, who championed what he saw as the naturalism of J. M. W. Turner, saw art's role as the communication by artifice of an essential truth that could only be found in nature. The definition and evaluation of art has become especially problematic since the 20th century. Richard Wollheim distinguishes three approaches to assessing the aesthetic value of art: the Realist, whereby aesthetic quality is an absolute value independent of any human view; the Objectivist, whereby it is also an absolute value, but is dependent on general human experience; and the Relativist position, whereby it is not an absolute value, but depends on, and varies with, the human experience of different humans. Arrival of Modernism The arrival of Modernism in the late 19th century led to a radical break in the conception of the function of art, and then again in the late 20th century with the advent of postmodernism. Clement Greenberg's 1960 article "Modernist Painting" defines modern art as "the use of characteristic methods of a discipline to criticize the discipline itself". Greenberg originally applied this idea to the Abstract Expressionist movement and used it as a way to understand and justify flat (non-illusionistic) abstract painting: Realistic, naturalistic art had dissembled the medium, using art to conceal art; modernism used art to call attention to art. The limitations that constitute the medium of painting—the flat surface, the shape of the support, the properties of the pigment—were treated by the Old Masters as negative factors that could be acknowledged only implicitly or indirectly. Under Modernism these same limitations came to be regarded as positive factors, and were acknowledged openly. After Greenberg, several important art theorists emerged, such as Michael Fried, T. J. Clark, Rosalind Krauss, Linda Nochlin and Griselda Pollock among others. Though only originally intended as a way of understanding a specific set of artists, Greenberg's definition of modern art is important to many of the ideas of art within the various art movements of the 20th century and early 21st century. Pop artists like Andy Warhol became both noteworthy and influential through work including and possibly critiquing popular culture, as well as the art world. Artists of the 1980s, 1990s, and 2000s expanded this technique of self-criticism beyond high art to all cultural image-making, including fashion images, comics, billboards and pornography. Duchamp once proposed that art is any activity of any kind-everything. However, the way that only certain activities are classified today as art is a social construction. There is evidence that there may be an element of truth to this. In The Invention of Art: A Cultural History, Larry Shiner examines the construction of the modern system of the arts, i.e. fine art. He finds evidence that the older system of the arts before our modern system (fine art) held art to be any skilled human activity; for example, Ancient Greek society did not possess the term art, but techne. Techne can be understood neither as art or craft, the reason being that the distinctions of art and craft are historical products that came later on in human history. Techne included painting, sculpting and music, but also cooking, medicine, horsemanship, geometry, carpentry, prophecy, and farming, etc. New Criticism and the "intentional fallacy" Following Duchamp during the first half of the 20th century, a significant shift to general aesthetic theory took place which attempted to apply aesthetic theory between various forms of art, including the literary arts and the visual arts, to each other. This resulted in the rise of the New Criticism school and debate concerning the intentional fallacy. At issue was the question of whether the aesthetic intentions of the artist in creating the work of art, whatever its specific form, should be associated with the criticism and evaluation of the final product of the work of art, or, if the work of art should be evaluated on its own merits independent of the intentions of the artist. In 1946, William K. Wimsatt and Monroe Beardsley published a classic and controversial New Critical essay entitled "The Intentional Fallacy", in which they argued strongly against the relevance of an author's intention, or "intended meaning" in the analysis of a literary work. For Wimsatt and Beardsley, the words on the page were all that mattered; importation of meanings from outside the text was considered irrelevant, and potentially distracting. In another essay, "The Affective Fallacy", which served as a kind of sister essay to "The Intentional Fallacy" Wimsatt and Beardsley also discounted the reader's personal/emotional reaction to a literary work as a valid means of analyzing a text. This fallacy would later be repudiated by theorists from the reader-response school of literary theory. Ironically, one of the leading theorists from this school, Stanley Fish, was himself trained by New Critics. Fish criticizes Wimsatt and Beardsley in his 1970 essay "Literature in the Reader". As summarized by Berys Gaut and Paisley Livingston in their essay "The Creation of Art": "Structuralist and post-structuralists theorists and critics were sharply critical of many aspects of New Criticism, beginning with the emphasis on aesthetic appreciation and the so-called autonomy of art, but they reiterated the attack on biographical criticisms' assumption that the artist's activities and experience were a privileged critical topic." These authors contend that: "Anti-intentionalists, such as formalists, hold that the intentions involved in the making of art are irrelevant or peripheral to correctly interpreting art. So details of the act of creating a work, though possibly of interest in themselves, have no bearing on the correct interpretation of the work." Gaut and Livingston define the intentionalists as distinct from formalists stating that: "Intentionalists, unlike formalists, hold that reference to intentions is essential in fixing the correct interpretation of works." They quote Richard Wollheim as stating that, "The task of criticism is the reconstruction of the creative process, where the creative process must in turn be thought of as something not stopping short of, but terminating on, the work of art itself." "Linguistic turn" and its debate The end of the 20th century fostered an extensive debate known as the linguistic turn controversy, or the "innocent eye debate" in the philosophy of art. This debate discussed the encounter of the work of art as being determined by the relative extent to which the conceptual encounter with the work of art dominates over the perceptual encounter with the work of art. Decisive for the linguistic turn debate in art history and the humanities were the works of yet another tradition, namely the structuralism of Ferdinand de Saussure and the ensuing movement of poststructuralism. In 1981, the artist Mark Tansey created a work of art titled The Innocent Eye as a criticism of the prevailing climate of disagreement in the philosophy of art during the closing decades of the 20th century. Influential theorists include Judith Butler, Luce Irigaray, Julia Kristeva, Michel Foucault and Jacques Derrida. The power of language, more specifically of certain rhetorical tropes, in art history and historical discourse was explored by Hayden White. The fact that language is not a transparent medium of thought had been stressed by a very different form of philosophy of language which originated in the works of Johann Georg Hamann and Wilhelm von Humboldt. Ernst Gombrich and Nelson Goodman in his book Languages of Art: An Approach to a Theory of Symbols came to hold that the conceptual encounter with the work of art predominated exclusively over the perceptual and visual encounter with the work of art during the 1960s and 1970s. He was challenged on the basis of research done by the Nobel prize winning psychologist Roger Sperry who maintained that the human visual encounter was not limited to concepts represented in language alone (the linguistic turn) and that other forms of psychological representations of the work of art were equally defensible and demonstrable. Sperry's view eventually prevailed by the end of the 20th century with aesthetic philosophers such as Nick Zangwill strongly defending a return to moderate aesthetic formalism among other alternatives. Classification disputes Disputes as to whether or not to classify something as a work of art are referred to as classificatory disputes about art. Classificatory disputes in the 20th century have included cubist and impressionist paintings, Duchamp's Fountain, the movies, J. S. G. Boggs' superlative imitations of banknotes, conceptual art, and video games. Philosopher David Novitz has argued that disagreement about the definition of art are rarely the heart of the problem. Rather, "the passionate concerns and interests that humans vest in their social life" are "so much a part of all classificatory disputes about art." According to Novitz, classificatory disputes are more often disputes about societal values and where society is trying to go than they are about theory proper. For example, when the Daily Mail criticized Hirst's and Emin's work by arguing "For 1,000 years art has been one of our great civilising forces. Today, pickled sheep and soiled beds threaten to make barbarians of us all" they are not advancing a definition or theory about art, but questioning the value of Hirst's and Emin's work. In 1998, Arthur Danto, suggested a thought experiment showing that "the status of an artifact as work of art results from the ideas a culture applies to it, rather than its inherent physical or perceptible qualities. Cultural interpretation (an art theory of some kind) is therefore constitutive of an object's arthood." Anti-art is a label for art that intentionally challenges the established parameters and values of art; it is a term associated with Dadaism and attributed to Marcel Duchamp just before World War I, when he was making art from found objects. One of these, Fountain (1917), an ordinary urinal, has achieved considerable prominence and influence on art. Anti-art is a feature of work by Situationist International, the lo-fi Mail art movement, and the Young British Artists, though it is a form still rejected by the Stuckists, who describe themselves as anti-anti-art. Architecture is often included as one of the visual arts; however, like the decorative arts, or advertising, it involves the creation of objects where the practical considerations of use are essential in a way that they usually are not in a painting, for example. Value judgment Somewhat in relation to the above, the word art is also used to apply judgments of value, as in such expressions as "that meal was a work of art" (the cook is an artist), or "the art of deception" (the highly attained level of skill of the deceiver is praised). It is this use of the word as a measure of high quality and high value that gives the term its flavor of subjectivity. Making judgments of value requires a basis for criticism. At the simplest level, a way to determine whether the impact of the object on the senses meets the criteria to be considered art is whether it is perceived to be attractive or repulsive. Though perception is always colored by experience, and is necessarily subjective, it is commonly understood that what is not somehow aesthetically satisfying cannot be art. However, "good" art is not always or even regularly aesthetically appealing to a majority of viewers. In other words, an artist's prime motivation need not be the pursuit of the aesthetic. Also, art often depicts terrible images made for social, moral, or thought-provoking reasons. For example, Francisco Goya's painting depicting the Spanish shootings of 3 May 1808 is a graphic depiction of a firing squad executing several pleading civilians. Yet at the same time, the horrific imagery demonstrates Goya's keen artistic ability in composition and execution and produces fitting social and political outrage. Thus, the debate continues as to what mode of aesthetic satisfaction, if any, is required to define 'art'. The assumption of new values or the rebellion against accepted notions of what is aesthetically superior need not occur concurrently with a complete abandonment of the pursuit of what is aesthetically appealing. Indeed, the reverse is often true, that the revision of what is popularly conceived of as being aesthetically appealing allows for a re-invigoration of aesthetic sensibility, and a new appreciation for the standards of art itself. Countless schools have proposed their own ways to define quality, yet they all seem to agree in at least one point: once their aesthetic choices are accepted, the value of the work of art is determined by its capacity to transcend the limits of its chosen medium to strike some universal chord by the rarity of the skill of the artist or in its accurate reflection in what is termed the zeitgeist. Art is often intended to appeal to and connect with human emotion. It can arouse aesthetic or moral feelings, and can be understood as a way of communicating these feelings. Artists express something so that their audience is aroused to some extent, but they do not have to do so consciously. Art may be considered an exploration of the human condition; that is, what it is to be human. By extension, it has been argued by Emily L. Spratt that the development of generative artificial intelligence, especially in regard to artificial intelligence art, necessitates a re-evaluation of aesthetic theory in art history today and a reconsideration of the limits of human creativity. Music and artificial intelligence has taken a similar path. So too has the use of large language models in generating creative texts. Art and law An essential legal issue are art forgeries, plagiarism, replicas and works that are strongly based on other works of art. Intellectual property law plays a significant role in the art world. Copyright protection is granted to artists for their original works, providing them with exclusive rights to reproduce, distribute, and display their creations. This safeguard empowers artists to govern the usage of their work and safeguard against unauthorized copying or infringement. The trade in works of art or the export from a country may be subject to legal regulations. Internationally there are also extensive efforts to protect the works of art created. The UN, UNESCO and Blue Shield International try to ensure effective protection at the national level and to intervene directly in the event of armed conflicts or disasters. This can particularly affect museums, archives, art collections and excavation sites. This should also secure the economic basis of a country, especially because works of art are often of tourist importance. The founding president of Blue Shield International, Karl von Habsburg, explained an additional connection between the destruction of cultural property and the cause of flight during a mission in Lebanon in April 2019: "Cultural goods are part of the identity of the people who live in a certain place. If you destroy their culture, you also destroy their identity. Many people are uprooted, often no longer have any prospects and as a result flee from their homeland." In order to preserve the diversity of cultural identity, UNESCO protects the living human treasure through the Convention for the Safeguarding of the Intangible Cultural Heritage. See also Works cited Bibliography Intentions by Oscar Wilde Katharine Everett Gilbert and Helmut Kuhn, A History of Esthetics. Edition 2, revised. Indiana: Indiana University Press, 1953. Stephen Davies, Definitions of Art, 1991 Nina Felshin, ed. But is it Art?, 1995 Catherine de Zegher (ed.). Inside the Visible. MIT Press, 1996 Evelyn Hatcher, ed. Art as Culture: An Introduction to the Anthropology of Art, 1999 Noel Carroll, Theories of Art Today, 2000 John Whitehead. Grasping for the Wind, 2001 Michael Ann Holly and Keith Moxey (eds.) Art History Aesthetics Visual Studies. New Haven: Yale University Press, 2002. ISBN 0300097891 Shiner, Larry. The Invention of Art: A Cultural History. Chicago: University of Chicago Press, 2003. ISBN 978-0-226-75342-3 Arthur Danto, The Abuse of Beauty: Aesthetics and the Concept of Art. 2003 Dana Arnold and Margaret Iversen, eds. Art and Thought. London: Blackwell, 2003. ISBN 0631227156 Jean Robertson and Craig McDaniel, Themes of Contemporary Art, Visual Art after 1980, 2005 References Further reading Cole, Ina, From the Sculptor’s Studio: Conversations with Twenty Seminal Artists (London: Laurence King Publishing Ltd, 2021) ISBN 9781913947590 OCLC 1420954826. Antony Briant and Griselda Pollock, eds. Digital and Other Virtualities: Renegotiating the image. London and NY: I.B. Tauris, 2010. ISBN 978-1441676313 Augros, Robert M., Stanciu, George N. The New Story of Science: mind and the universe, Lake Bluff, Ill.: Regnery Gateway, 1984. ISBN 0-89526-833-7 (this book has significant material on art and science) Benedetto Croce. Aesthetic as Science of Expression and General Linguistic, 2002 Botar, Oliver A.I. Technical Detours: The Early Moholy-Nagy Reconsidered. Art Gallery of The Graduate Center, The City University of New York and The Salgo Trust for Education, 2006. ISBN 978-1599713571 Burguete, Maria, and Lam, Lui, eds. (2011). Arts: A Science Matter. World Scientific: Singapore. ISBN 978-981-4324-93-9 Carol Armstrong and Catherine de Zegher, eds. Women Artists at the Millennium. Massachusetts: October Books/The MIT Press, 2006. ISBN 026201226X Colvin, Sidney (1911). "Art" . In Chisholm, Hugh (ed.). Encyclopædia Britannica. Vol. 2 (11th ed.). Cambridge University Press. pp. 657–660. Carl Jung, Man and His Symbols. London: Pan Books, 1978. ISBN 0330253212 E.H. Gombrich, The Story of Art. London: Phaidon Press, 1995. ISBN 978-0714832470 Florian Dombois, Ute Meta Bauer, Claudia Mareis and Michael Schwab, eds. Intellectual Birdhouse. Artistic Practice as Research. London: Koening Books, 2012. ISBN 978-3863351182 Kristine Stiles and Peter Selz, eds. Theories and Documents of Contemporary Art. Berkeley: University of California Press, 1986 Kleiner, Gardner, Mamiya and Tansey. Art Through the Ages, Twelfth Edition (2 volumes) Wadsworth, 2004. ISBN 0-534-64095-8 (vol 1) and ISBN 0-534-64091-5 (vol 2) Richard Wollheim, Art and its Objects: An introduction to aesthetics. New York: Harper & Row, 1968. OCLC 1077405 Will Gompertz. What Are You Looking At?: 150 Years of Modern Art in the Blink of an Eye. New York: Viking, 2012. ISBN 978-0670920495 Władysław Tatarkiewicz, A History of Six Ideas: an Essay in Aesthetics, translated from the Polish by Christopher Kasparek, The Hague, Martinus Nijhoff, 1980 External links Art and Play from the Dictionary of the History of ideas In-depth directory of art Art and Artist Files in the Smithsonian Libraries Collection (2005) Smithsonian Digital Libraries Visual Arts Data Service (VADS) – online collections from UK museums, galleries, universities RevolutionArt – Art magazines with worldwide exhibitions, callings and competitions Adajian, Thomas. "The Definition of Art". In Zalta, Edward N. (ed.). Stanford Encyclopedia of Philosophy. "Definition of Art". Internet Encyclopedia of Philosophy.

Music is the arrangement of sound to create some combination of form, harmony, melody, rhythm, or otherwise expressive content. Music is generally agreed to be a cultural universal that is present in all human societies. Definitions of music vary widely in substance and approach. While scholars agree that music is defined by a small number of specific elements, there is no consensus as to what these necessary elements are. Music is often characterized as a highly versatile medium for expressing human creativity. Diverse activities are involved in the creation of music, and are often divided into categories of composition, improvisation, and performance. Music may be performed using a wide variety of musical instruments, including the human voice. It can also be composed, sequenced, or otherwise produced to be indirectly played mechanically or electronically, such as via a music box, barrel organ, or digital audio workstation software on a computer. Music often plays a key role in social events and religious ceremonies. The techniques of making music are often transmitted as part of a cultural tradition. Music is played in public and private contexts, highlighted at events such as festivals and concerts for various different types of ensembles. Music is used in the production of other media, such as in soundtracks to films, TV shows, operas, and video games. Listening to music is a common means of entertainment. The culture surrounding music extends into areas of academic study, journalism, philosophy, psychology, and therapy. The music industry includes songwriters, performers, sound engineers, producers, tour organizers, distributors of instruments, accessories, and publishers of sheet music and recordings. Technology facilitating the recording and reproduction of music has historically included sheet music, microphones, phonographs, and tape machines, with playback of digital music being a common use for MP3 players, CD players, and smartphones. Etymology and terminology The modern English word 'music' came into use in the 1630s. It is derived from a long line of successive precursors: the Old English 'musike' of the mid-13th century; the Old French musique of the 12th century; and the Latin mūsica. The Latin word itself derives from the Ancient Greek mousiké (technē)—μουσική (τέχνη)—literally meaning "(art) of the Muses". The Muses were nine deities in Ancient Greek mythology who presided over the arts and sciences. They were included in tales by the earliest Western authors, Homer and Hesiod, and eventually came to be associated with music specifically. Over time, Polyhymnia would reside over music more prominently than the other muses. The Latin word musica was also the originator for both the Spanish música and French musique via spelling and linguistic adjustment, though other European terms were probably loanwords, including the Italian musica, German Musik, Dutch muziek, Norwegian musikk, Polish muzyka and Russian muzïka. The modern Western world usually defines music as an all-encompassing term used to describe diverse genres, styles, and traditions. This is not the case worldwide, and languages such as modern Indonesian (musik) and Shona (musakazo) have recently adopted words to reflect this universal conception, as they did not have words that fit exactly the Western scope. Before Western contact in East Asia, neither Japan nor China had a single word that encompasses music in a broad sense, but culturally, they often regarded music in such a fashion. The closest word to mean music in Chinese, yue, shares a character with le, meaning joy, and originally referred to all the arts before narrowing in meaning. Africa is too diverse to make firm generalizations, but the musicologist J. H. Kwabena Nketia has emphasized African music's often inseparable connection to dance and speech in general. Some African cultures, such as the Songye people of the Democratic Republic of the Congo and the Tiv people of Nigeria, have a strong and broad conception of 'music' but no corresponding word in their native languages. Other words commonly translated as 'music' often have more specific meanings in their respective cultures: the Hindi word for music, sangita, properly refers to art music, while the many Indigenous languages of the Americas have words for music that refer specifically to song but describe instrumental music regardless. Though the Arabic musiqi can refer to all music, it is usually used for instrumental and metric music, while khandan identifies vocal and improvised music. History Origins and prehistory It is often debated to what extent the origins of music will ever be understood, and there are competing theories that aim to explain it. Many scholars highlight a relationship between the origin of music and the origin of language, and there is disagreement surrounding whether music developed before, after, or simultaneously with language. A similar source of contention surrounds whether music was the intentional result of natural selection or was a byproduct spandrel of evolution. The earliest influential theory was proposed by Charles Darwin in 1871, who stated that music arose as a form of sexual selection, perhaps via mating calls. Darwin's original perspective has been heavily criticized for its inconsistencies with other sexual selection methods, though many scholars in the 21st century have developed and promoted the theory. Other theories include that music arose to assist in organizing labor, improving long-distance communication, benefiting communication with the divine, assisting in community cohesion or as a defense to scare off predators. Prehistoric music can only be theorized based on findings from paleolithic archaeology sites. The disputed Divje Babe flute, a perforated cave bear femur, is at least 40,000 years old, though there is considerable debate surrounding whether it is truly a musical instrument or an object formed by animals. The earliest objects whose designations as musical instruments are widely accepted are bone flutes from the Swabian Jura, Germany, namely from the Geissenklösterle, Hohle Fels and Vogelherd caves. Dated to the Aurignacian (of the Upper Paleolithic) and used by Early European modern humans, from all three caves there are eight examples, four made from the wing bones of birds and four from mammoth ivory; three of these are near complete. Three flutes from the Geissenklösterle are dated as the oldest, c. 43,150–39,370 BP. Antiquity The earliest material and representational evidence of Egyptian musical instruments dates to the Predynastic period, but the evidence is more securely attested in the Old Kingdom when harps, flutes and double clarinets were played. Percussion instruments, lyres, and lutes were added to orchestras by the Middle Kingdom. Cymbals frequently accompanied music and dance, much as they still do in Egypt today. Egyptian folk music, including the traditional Sufi dhikr rituals, are the closest contemporary music genre to ancient Egyptian music, having preserved many of its features, rhythms and instruments. The "Hurrian Hymn to Nikkal", found on clay tablets in the ancient Syrian city of Ugarit, is the oldest surviving notated work of music, dating back to approximately 1400 BCE. Music was an important part of social and cultural life in ancient Greece, in fact it was one of the main subjects taught to children. Musical education was considered important for the development of an individual's soul. Musicians and singers played a prominent role in Greek theater, and those who received a musical education were seen as nobles and in perfect harmony (as can be read in the Republic, Plato). Mixed gender choruses performed for entertainment, celebration, and spiritual ceremonies. Instruments included the double-reed aulos and a plucked string instrument, the lyre, principally a special kind called a kithara. Music was an important part of education, and boys were taught music starting at age six. Greek musical literacy created significant musical development. Greek music theory included the Greek musical modes, that eventually became the basis for Western religious and classical music. Later, influences from the Roman Empire, Eastern Europe, and the Byzantine Empire changed Greek music. The Seikilos epitaph is the oldest surviving example of a complete musical composition, including musical notation, from anywhere in the world. The oldest surviving work written about music theory is Harmonika Stoicheia by Aristoxenus. Asian cultures Asian music covers a swath of music cultures surveyed in the articles on Arabia, Central Asia, East Asia, South Asia, and Southeast Asia. Several have traditions reaching into antiquity. Indian classical music is one of the oldest musical traditions in the world. Sculptures from the Indus Valley civilization show dance and old musical instruments, like the seven-holed flute. Stringed instruments and drums have been recovered from Harappa and Mohenjo Daro by excavations carried out by Mortimer Wheeler. The Rigveda, an ancient Hindu text, has elements of present Indian music, with musical notation to denote the meter and mode of chanting. Indian classical music (marga) is monophonic, and based on a single melody line or raga rhythmically organized through talas. The poem Cilappatikaram provides information about how new scales can be formed by modal shifting of the tonic from an existing scale. Present day Hindi music was influenced by Persian traditional music and Afghan Mughals. Carnatic music, popular in the southern states, is largely devotional; the majority of the songs are addressed to the Hindu deities. There are songs emphasizing love and other social issues. Indonesian music has been formed since the Bronze Age culture migrated to the Indonesian archipelago in the 2nd-3rd centuries BCE. Indonesian traditional music uses percussion instruments, especially kendang and gongs. Some of them developed elaborate and distinctive instruments, such as the sasando stringed instrument on the island of Rote, the Sundanese angklung, and the complex and sophisticated Javanese and Balinese gamelan orchestras. Indonesia is the home of gong chime, a general term for a set of small, high pitched pot gongs. Gongs are usually placed in order of note, with the boss up on a string held in a low wooden frame. The most popular form of Indonesian music is gamelan, an ensemble of tuned percussion instruments that include metallophones, drums, gongs and spike fiddles along with bamboo suling (like a flute). Chinese classical music, the traditional art or court music of China, has a history stretching over about 3,000 years. It has its own unique systems of musical notation, as well as musical tuning and pitch, musical instruments and styles or genres. Chinese music is pentatonic-diatonic, having a scale of twelve notes to an octave (5 + 7 = 12) as does European-influenced music. Western classical Early music The medieval music era (500 to 1400), which took place during the Middle Ages, started with the introduction of monophonic (single melodic line) chanting into Catholic Church services. Musical notation was used since ancient times in Greek culture, but in the Middle Ages, notation was first introduced by the Catholic Church, so chant melodies could be written down, to facilitate the use of the same melodies for religious music across the Catholic empire. The only European Medieval repertory that has been found, in written form, from before 800 is the monophonic liturgical plainsong chant of the Catholic Church, the central tradition of which was called Gregorian chant. Alongside these traditions of sacred and church music there existed a vibrant tradition of secular song (non-religious songs). Examples of composers from this period are Léonin, Pérotin, Guillaume de Machaut, and Walther von der Vogelweide. Renaissance music (c. 1400 to 1600) was more focused on secular themes, such as courtly love. Around 1450, the printing press was invented, which made printed sheet music much less expensive and easier to mass-produce (prior to the invention of the press, all notated music was hand-copied). The increased availability of sheet music spread musical styles quicker and across a larger area. Musicians and singers often worked for the church, courts and towns. Church choirs grew in size, and the church remained an important patron of music. By the middle of the 15th century, composers wrote richly polyphonic sacred music, in which different melody lines were interwoven simultaneously. Prominent composers from this era include Guillaume Du Fay, Giovanni Pierluigi da Palestrina, Thomas Morley, Orlando di Lasso and Josquin des Prez. As musical activity shifted from the church to aristocratic courts, kings, queens and princes competed for the finest composers. Many leading composers came from the Netherlands, Belgium, and France; they are called the Franco-Flemish composers. They held important positions throughout Europe, especially in Italy. Other countries with vibrant musical activity included Germany, England, and Spain. Common practice period Baroque The Baroque era of music took place from 1600 to 1750, coinciding with the flourishing of the Baroque artistic style in Europe. The start of the Baroque era was marked by the penning of the first operas. Polyphonic contrapuntal music (music with separate, simultaneous melodic lines) remained important during this period. German Baroque composers wrote for small ensembles including strings, brass, and woodwinds, as well as for choirs and keyboard instruments such as pipe organ, harpsichord, and clavichord. Musical complexity increased during this time. Several major musical forms were created, some of them which persisted into later periods, seeing further development. These include the fugue, the invention, the sonata, and the concerto. The late Baroque style was polyphonically complex and richly ornamented. Important composers from the Baroque era include Johann Sebastian Bach (Cello suites), George Frideric Handel (Messiah), Georg Philipp Telemann and Antonio Vivaldi (The Four Seasons). Classicism The music of the Classical period (1730 to 1820) aimed to imitate what were seen as the key elements of the art and philosophy of Ancient Greece and Rome: the ideals of balance, proportion and disciplined expression. (Note: the music from the Classical period should not be confused with Classical music in general, a term which refers to Western art music from the 5th century to the 2000s, which includes the Classical period as one of a number of periods). Music from the Classical period has a lighter, clearer and considerably simpler texture than the Baroque music which preceded it. The main style was homophony, where a prominent melody and a subordinate chordal accompaniment part are clearly distinct. Classical instrumental melodies tended to be almost voicelike and singable. New genres were developed, and the fortepiano, the forerunner to the modern piano, replaced the Baroque era harpsichord and pipe organ as the main keyboard instrument (though pipe organ continued to be used in sacred music, such as Masses). Importance was given to instrumental music. It was dominated by further development of musical forms initially defined in the Baroque period: the sonata, the concerto, and the symphony. Other main kinds were the trio, string quartet, serenade and divertimento. The sonata was the most important and developed form. Although Baroque composers also wrote sonatas, the Classical style of sonata is completely distinct. All of the main instrumental forms of the Classical era, from string quartets to symphonies and concertos, were based on the structure of the sonata. The instruments used chamber music and orchestra became more standardized. In place of the basso continuo group of the Baroque era, which consisted of harpsichord, organ or lute along with a number of bass instruments selected at the discretion of the group leader (e.g., viol, cello, theorbo, serpent), Classical chamber groups used specified, standardized instruments (e.g., a string quartet would be performed by two violins, a viola and a cello). The practice of improvised chord-playing by the continuo keyboardist or lute player, a hallmark of Baroque music, underwent a gradual decline between 1750 and 1800. One of the most important changes made in the Classical period was the development of public concerts. The aristocracy still played a significant role in the sponsorship of concerts and compositions, but it was now possible for composers to survive without being permanent employees of queens or princes. The increasing popularity of classical music led to a growth in the number and types of orchestras. The expansion of orchestral concerts necessitated the building of large public performance spaces. Symphonic music including symphonies, musical accompaniment to ballet and mixed vocal/instrumental genres, such as opera and oratorio, became more popular. The best known composers of Classicism are Carl Philipp Emanuel Bach, Christoph Willibald Gluck, Johann Christian Bach, Joseph Haydn, Wolfgang Amadeus Mozart, Ludwig van Beethoven and Franz Schubert. Beethoven and Schubert are also considered to be composers in the later part of the Classical era, as it began to move towards Romanticism. Romanticism Romantic music (c. 1820 to 1900) from the 19th century had many elements in common with the Romantic styles in literature and painting of the era. Romanticism was an artistic, literary, and intellectual movement was characterized by its emphasis on emotion and individualism as well as glorification of all the past and nature. Romantic music expanded beyond the rigid styles and forms of the Classical era into more passionate, dramatic expressive pieces and songs. Romantic composers such as Wagner and Brahms attempted to increase emotional expression and power in their music to describe deeper truths or human feelings. With symphonic tone poems, composers tried to tell stories and evoke images or landscapes using instrumental music. Some composers promoted nationalistic pride with patriotic orchestral music inspired by folk music. The emotional and expressive qualities of music came to take precedence over tradition. Romantic composers grew in idiosyncrasy, and went further in the syncretism of exploring different art-forms in a musical context, (such as literature), history (historical figures and legends), or nature itself. Romantic love or longing was a prevalent theme in many works composed during this period. In some cases, the formal structures from the classical period continued to be used (e.g., the sonata form used in string quartets and symphonies), but these forms were expanded and altered. In many cases, new approaches were explored for existing genres, forms, and functions. Also, new forms were created that were deemed better suited to the new subject matter. Composers continued to develop opera and ballet music, exploring new styles and themes. In the years after 1800, the music developed by Ludwig van Beethoven and Franz Schubert introduced a more dramatic, expressive style. In Beethoven's case, short motifs, developed organically, came to replace melody as the most significant compositional unit (an example is the distinctive four note figure used in his Fifth Symphony). Later Romantic composers such as Pyotr Ilyich Tchaikovsky, Antonín Dvořák, and Gustav Mahler used more unusual chords and more dissonance to create dramatic tension. They generated complex and often much longer musical works. During the late Romantic period, composers explored dramatic chromatic alterations of tonality, such as extended chords and altered chords, which created new sound "colors." The late 19th century saw a dramatic expansion in the size of the orchestra, and the Industrial Revolution helped to create better instruments, creating a more powerful sound. Public concerts became an important part of well-to-do urban society. It also saw a new diversity in theatre music, including operetta, and musical comedy and other forms of musical theatre. 20th and 21st century In the 19th century, a key way new compositions became known to the public was by the sales of sheet music, which middle class amateur music lovers would perform at home, on their piano or other common instruments, such as the violin. With 20th-century music, the invention of new electric technologies such as radio broadcasting and mass market availability of gramophone records meant sound recordings heard by listeners (on the radio or record player) became the main way to learn about new songs and pieces. There was a vast increase in music listening as the radio gained popularity and phonographs were used to replay and distribute music; anyone with a radio or record player could hear operas, symphonies and big bands in their own living room. During the 19th century, the focus on sheet music had restricted access to new music to middle and upper-class people who could read music and who owned pianos and other instruments. Radios and record players allowed lower-income people, who could not afford an opera or symphony concert ticket, to hear this music. As well, people could hear music from different parts of the country, or even different parts of the world, even if they could not afford to travel to these locations. This helped to spread musical styles. The focus of art music in the 20th century was characterized by exploration of new rhythms, styles, and sounds. The horrors of World War I influenced many of the arts, including music, and composers began exploring darker, harsher sounds. Traditional music styles such as jazz and folk music were used by composers as a source of ideas for classical music. Igor Stravinsky, Arnold Schoenberg, and John Cage were influential composers in 20th-century art music. The invention of sound recording and the ability to edit music gave rise to new subgenres of classical music, including the acousmatic and musique concrète schools of electronic composition. Sound recording was a major influence on the development of popular music genres, because it enabled recordings of songs and bands to be widely distributed. The introduction of the multitrack recording system had a major influence on rock music, because it could do more than record a band's performance. Using a multitrack system, a band and their music producer could overdub many layers of instrument tracks and vocals, creating new sounds that would not be possible in a live performance. Jazz evolved and became an important genre of music over the course of the 20th century, and during the second half, rock music did the same. Jazz is an American musical artform that originated in the late 19th and early 20th centuries in African American communities in the Southern United States from a confluence of African and European music traditions. The style's West African pedigree is evident in its use of blue notes, improvisation, polyrhythms, syncopation, and the swung note. Rock music is a genre of popular music that developed in the 1950s from rock and roll, rockabilly, blues, and country music. The sound of rock often revolves around the electric or acoustic guitar, and it uses a strong back beat laid down by a rhythm section. Along with the guitar or keyboards, saxophone and blues-style harmonica are used as soloing instruments. In its "purest form", it "has three chords, a strong, insistent back beat, and a catchy melody." The traditional rhythm section for popular music is rhythm guitar, electric bass guitar, drums. Some bands have keyboard instruments such as organ, piano, or, since the 1970s, analog synthesizers. In the 1980s, pop musicians began using digital synthesizers, such as the DX-7 synthesizer, electronic drum machines such as the TR-808 and synth bass devices (such as the TB-303) or synth bass keyboards. In the 1990s, an increasingly large range of computerized hardware musical devices and instruments and software (e.g. digital audio workstations) were used. In the 2020s, soft synths and computer music apps make it possible for bedroom producers to create and record types of music, such as electronic dance music, in their home, adding sampled and digital instruments and editing the recording digitally. In the 1990s, bands in genres such as nu metal began including DJs in their bands. DJs create music by manipulating recorded music, using a DJ mixer. Creation Composition "Composition" is the act or practice of creating a song, an instrumental music piece, a work with both singing and instruments, or another type of music. In many cultures, including Western classical music, the act of composing also includes the creation of music notation, such as a sheet music "score", which is then performed by the composer or by other singers or musicians. In popular music and traditional music, the act of composing, which is typically called songwriting, may involve the creation of a basic outline of the song, called the lead sheet, which sets out the melody, lyrics and chord progression. In classical music, the composer typically orchestrates his or her own compositions, but in musical theatre and in pop music, songwriters may hire an arranger to do the orchestration. In some cases, a songwriter may not use notation at all, and instead, compose the song in her mind and then play or record it from memory. In jazz and popular music, notable recordings by influential performers are given the weight that written scores play in classical music. Even when music is notated relatively precisely, as in classical music, there are many decisions that a performer has to make, because notation does not specify all of the elements of music precisely. The process of deciding how to perform music that has been previously composed and notated is termed "interpretation". Different performers' interpretations of the same work of music can vary widely, in terms of the tempos that are chosen and the playing or singing style or phrasing of the melodies. Composers and songwriters who present their own music are interpreting their songs, just as much as those who perform the music of others. The standard body of choices and techniques present at a given time and a given place is referred to as performance practice, whereas interpretation is generally used to mean the individual choices of a performer. Although a musical composition often uses musical notation and has a single author, this is not always the case. A work of music can have multiple composers, which often occurs in popular music when a band collaborates to write a song, or in musical theatre, when one person writes the melodies, a second person writes the lyrics, and a third person orchestrates the songs. In some styles of music, such as the blues, a composer/songwriter may create, perform and record new songs or pieces without ever writing them down in music notation. A piece of music can also be composed with words, images, or computer programs that explain or notate how the singer or musician should create musical sounds. Examples range from avant-garde music that uses graphic notation, to text compositions such as Aus den sieben Tagen, to computer programs that select sounds for musical pieces. Music that makes heavy use of randomness and chance is called aleatoric music, and is associated with contemporary composers active in the 20th century, such as John Cage, Morton Feldman, and Witold Lutosławski. A commonly known example of chance-based music is the sound of wind chimes jingling in a breeze. The study of composition has traditionally been dominated by examination of methods and practice of Western classical music, but the definition of composition is broad enough to include the creation of popular music and traditional music songs and instrumental pieces as well as spontaneously improvised works like those of free jazz performers and African percussionists such as Ewe drummers. Performance Performance is the physical expression of music, which occurs when a song is sung or piano piece, guitar melody, symphony, drum beat or other musical part is played. In classical music, a work is written in music notation by a composer and then performed once the composer is satisfied with its structure and instrumentation. However, as it gets performed, the interpretation of a song or piece can evolve and change. In classical music, instrumental performers, singers or conductors may gradually make changes to the phrasing or tempo of a piece. In popular and traditional music, the performers have more freedom to make changes to the form of a song or piece. As such, in popular and traditional music styles, even when a band plays a cover song, they can make changes such as adding a guitar solo or inserting an introduction. A performance can either be planned out and rehearsed (practiced)—which is the norm in classical music, jazz big bands, and many popular music styles–or improvised over a chord progression (a sequence of chords), which is the norm in small jazz and blues groups. Rehearsals of orchestras, concert bands and choirs are led by a conductor. Rock, blues and jazz bands are usually led by the bandleader. A rehearsal is a structured repetition of a song or piece by the performers until it can be sung or played correctly and, if it is a song or piece for more than one musician, until the parts are together from a rhythmic and tuning perspective. Many cultures have strong traditions of solo performance (in which one singer or instrumentalist performs), such as in Indian classical music, and in the Western art-music tradition. Other cultures, such as in Bali, include strong traditions of group performance. All cultures include a mixture of both, and performance may range from improvised solo playing to highly planned and organized performances such as the modern classical concert, religious processions, classical music festivals or music competitions. Chamber music, which is music for a small ensemble with only one or a few of each type of instrument, is often seen as more intimate than large symphonic works. Improvisation Musical improvisation is the creation of spontaneous music, often within (or based on) a pre-existing harmonic framework, chord progression, or riffs. Improvisers use the notes of the chord, various scales that are associated with each chord, and chromatic ornaments and passing tones which may be neither chord tones nor from the typical scales associated with a chord. Musical improvisation can be done with or without preparation. Improvisation is a major part of some types of music, such as blues, jazz, and jazz fusion, in which instrumental performers improvise solos, melody lines, and accompaniment parts. In the Western art music tradition, improvisation was an important skill during the Baroque era and during the Classical era. In the Baroque era, performers improvised ornaments, and basso continuo keyboard players improvised chord voicings based on figured bass notation. As well, the top soloists were expected to be able to improvise pieces such as preludes. In the Classical era, solo performers and singers improvised virtuoso cadenzas during concerts. However, in the 20th and early 21st century, as "common practice" Western art music performance became institutionalized in symphony orchestras, opera houses, and ballets, improvisation has played a smaller role, as more and more music was notated in scores and parts for musicians to play. At the same time, some 20th and 21st century art music composers have increasingly included improvisation in their creative work. In Indian classical music, improvisation is a core component and an essential criterion of performances. Art and entertainment Music is composed and performed for many purposes, ranging from aesthetic pleasure, religious or ceremonial purposes, or as an entertainment product for the marketplace. When music was only available through sheet music scores, such as during the Classical and Romantic eras, music lovers would buy the sheet music of their favourite pieces and songs so that they could perform them at home on the piano. With the advent of the phonograph, records of popular songs, rather than sheet music became the dominant way that music lovers would enjoy their favourite songs. With the advent of home tape recorders in the 1980s and digital music in the 1990s, music lovers could make tapes or playlists of favourite songs and take them with them on a portable cassette player or MP3 player. Some music lovers create mix tapes of favourite songs, which serve as a "self-portrait, a gesture of friendship, prescription for an ideal party... [and] an environment consisting solely of what is most ardently loved". Amateur musicians can compose or perform music for their own pleasure and derive income elsewhere. Professional musicians are employed by institutions and organisations, including armed forces (in marching bands, concert bands and popular music groups), religious institutions, symphony orchestras, broadcasting or film production companies, and music schools. Professional musicians sometimes work as freelancers or session musicians, seeking contracts and engagements in a variety of settings. There are often many links between amateur and professional musicians. Beginning amateur musicians take lessons with professional musicians. In community settings, advanced amateur musicians perform with professional musicians in a variety of ensembles such as community concert bands and community orchestras. A distinction is often made between music performed for a live audience and music that is performed in a studio so that it can be recorded and distributed through the music retail system or the broadcasting system. However, there are also many cases where a live performance in front of an audience is also recorded and distributed. Live concert recordings are popular in both classical music and in popular music forms such as rock, where illegally taped live concerts are prized by music lovers. In the jam band scene, live, improvised jam sessions are preferred to studio recordings. Notation Music notation typically means the written expression of music notes and rhythms on paper using symbols. When music is written down, the pitches and rhythm of the music, such as the notes of a melody, are notated. Music notation often provides instructions on how to perform the music. For example, the sheet music for a song may state the song is a "slow blues" or a "fast swing", which indicates the tempo and the genre. To read notation, a person must have an understanding of music theory, harmony and the performance practice associated with a particular song or piece's genre. Written notation varies with the style and period of music. Nowadays, notated music is produced as sheet music or, for individuals with computer scorewriter programs, as an image on a computer screen. In ancient times, music notation was put onto stone or clay tablets. To perform music from notation, a singer or instrumentalist requires an understanding of the rhythmic and pitch elements embodied in the symbols and the performance practice that is associated with a piece of music or genre. In genres requiring musical improvisation, the performer often plays from music where only the chord changes and form of the song are written, requiring the performer to have a great understanding of the music's structure, harmony and the styles of a particular genre e.g., jazz or country music. In Western art music, the most common types of written notation are scores, which include all the music parts of an ensemble piece, and parts, which are the music notation for the individual performers or singers. In popular music, jazz, and blues, the standard musical notation is the lead sheet, which notates the melody, chords, lyrics (if it is a vocal piece), and structure of the music. Fake books are also used in jazz; they may consist of lead sheets or simply chord charts, which permit rhythm section members to improvise an accompaniment part to jazz songs. Scores and parts are also used in popular music and jazz, particularly in large ensembles such as jazz "big bands." In popular music, guitarists and electric bass players often read music notated in tablature (often abbreviated as "tab"), which indicates the location of the notes to be played on the instrument using a diagram of the guitar or bass fingerboard. Tablature was used in the Baroque era to notate music for the lute, a stringed, fretted instrument. Oral and aural tradition Many types of music, such as traditional blues and folk music were not written down in sheet music; instead, they were originally preserved in the memory of performers, and the songs were handed down orally, from one musician or singer to another, or aurally, in which a performer learns a song "by ear". When the composer of a song or piece is no longer known, this music is often classified as "traditional" or as a "folk song". Different musical traditions have different attitudes towards how and where to make changes to the original source material, from quite strict, to those that demand improvisation or modification to the music. A culture's history and stories may also be passed on by ear through song. Elements Music has many different fundamentals or elements. Depending on the definition of "element" being used, these can include pitch, beat or pulse, tempo, rhythm, melody, harmony, texture, style, allocation of voices, timbre or color, dynamics, expression, articulation, form, and structure. The elements of music feature prominently in the music curriculums of Australia, the UK, and the US. All three curriculums identify pitch, dynamics, timbre, and texture as elements, but the other identified elements of music are far from universally agreed upon. Below is a list of the three official versions of the "elements of music": Australia: pitch, timbre, texture, dynamics and expression, rhythm, form and structure. UK: pitch, timbre, texture, dynamics, duration, tempo, structure. USA: pitch, timbre, texture, dynamics, rhythm, form, harmony, style/articulation. In relation to the UK curriculum, in 2013 the term: "appropriate musical notations" was added to their list of elements and the title of the list was changed from the "elements of music" to the "inter-related dimensions of music". The inter-related dimensions of music are listed as: pitch, duration, dynamics, tempo, timbre, texture, structure, and appropriate musical notations. The phrase "the elements of music" is used in a number of different contexts. The two most common contexts can be differentiated by describing them as the "rudimentary elements of music" and the "perceptual elements of music". Pitch Pitch is an aspect of a sound that we can hear, reflecting whether one musical sound, note, or tone is "higher" or "lower" than another musical sound, note, or tone. We can talk about the highness or lowness of pitch in the more general sense, such as the way a listener hears a piercingly high piccolo note or whistling tone as higher in pitch than a deep thump of a bass drum. We also talk about pitch in the precise sense associated with musical melodies, basslines and chords. Precise pitch can only be determined in sounds that have a frequency that is clear and stable enough to distinguish from noise. For example, it is much easier for listeners to discern the pitch of a single note played on a piano than to try to discern the pitch of a crash cymbal that is struck. Melody A melody, also called a "tune", is a series of pitches (notes) sounding in succession (one after the other), often in a rising and falling pattern. The notes of a melody are typically created using pitch systems such as scales or modes. Melodies also often contain notes from the chords used in the song. The melodies in simple folk songs and traditional songs may use only the notes of a single scale, the scale associated with the tonic note or key of a given song. For example, a folk song in the key of C (also referred to as C major) may have a melody that uses only the notes of the C major scale (the individual notes C, D, E, F, G, A, B, and C; these are the "white notes" on a piano keyboard. On the other hand, Bebop-era jazz from the 1940s and contemporary music from the 20th and 21st centuries may use melodies with many chromatic notes (i.e., notes in addition to the notes of the major scale; on a piano, a chromatic scale would include all the notes on the keyboard, including the "white notes" and "black notes" and unusual scales, such as the whole tone scale (a whole tone scale in the key of C would contain the notes C, D, E, F♯, G♯ and A♯). A low musical line played by bass instruments, such as double bass, electric bass, or tuba, is called a bassline. Harmony Harmony refers to the "vertical" sounds of pitches in music, which means pitches that are played or sung together at the same time creates a chord. Usually, this means the notes are played at the same time, although harmony may also be implied by a melody that outlines a harmonic structure (i.e., by using melody notes that are played one after the other, outlining the notes of a chord). In music written using the system of major-minor tonality ("keys"), which includes most classical music written from 1600 to 1900 and most Western pop, rock, and traditional music, the key of a piece determines the "home note" or tonic to which the piece generally resolves, and the character (e.g. major or minor) of the scale in use. Simple classical pieces and many pop and traditional music songs are written so that all the music is in a single key. More complex Classical, pop, and traditional music songs and pieces may have two keys (and in some cases three or more keys). Classical music from the Romantic era (written from about 1820–1900) often contains multiple keys, as does jazz, especially Bebop jazz from the 1940s, in which the key or "home note" of a song may change every four bars or even every two bars. Rhythm Rhythm is the arrangement of sounds and silences in time. Meter animates time in regular pulse groupings, called measures or bars, which in Western classical, popular, and traditional music often group notes in sets of two (e.g., 2/4 time), three (e.g., 3/4 time, also known as Waltz time, or 3/8 time), or four (e.g., 4/4 time). Meters are made easier to hear because songs and pieces often (but not always) place an emphasis on the first beat of each grouping. Notable exceptions exist, such as the backbeat used in much Western pop and rock, in which a song that uses a measure that consists of four beats (called 4/4 time or common time) will have accents on beats two and four, which are typically performed by the drummer on the snare drum, a loud and distinctive-sounding percussion instrument. In pop and rock, the rhythm parts of a song are played by the rhythm section, which includes chord-playing instruments (e.g., electric guitar, acoustic guitar, piano, or other keyboard instruments), a bass instrument (typically electric bass or for some styles such as jazz and bluegrass, double bass) and a drum kit player. Texture Musical texture is the overall sound of a piece of music or song. The texture of a piece or song is determined by how the melodic, rhythmic, and harmonic materials are combined in a composition, thus determining the overall nature of the sound in a piece. Texture is often described in regard to the density, or thickness, and range, or width, between lowest and highest pitches, in relative terms as well as more specifically distinguished according to the number of voices, or parts, and the relationship between these voices (see common types below). For example, a thick texture contains many 'layers' of instruments. One layer can be a string section or another brass. The thickness is affected by the amount and the richness of the instruments. Texture is commonly described according to the number of and relationship between parts or lines of music: monophony: a single melody (or "tune") with neither instrumental accompaniment nor a harmony part. A mother singing a lullaby to her baby would be an example. heterophony: two or more instruments or singers playing/singing the same melody, but with each performer slightly varying the rhythm or speed of the melody or adding different ornaments to the melody. Two bluegrass fiddlers playing the same traditional fiddle tune together will typically each vary the melody by some degree and each add different ornaments. polyphony: multiple independent melody lines that interweave together, which are sung or played at the same time. Choral music written in the Renaissance music era was typically written in this style. A round, which is a song such as "Row, Row, Row Your Boat", which different groups of singers all start to sing at a different time, is an example of polyphony. homophony: a clear melody supported by chordal accompaniment. Most Western popular music songs from the 19th century onward are written in this texture. Music that contains a large number of independent parts (e.g., a double concerto accompanied by 100 orchestral instruments with many interweaving melodic lines) is generally said to have a "thicker" or "denser" texture than a work with few parts (e.g., a solo flute melody accompanied by a single cello). Timbre Timbre, sometimes called "color" or "tone color" is the quality or sound of a voice or instrument. Timbre is what makes a particular musical sound different from another, even when they have the same pitch and loudness. For example, a 440 Hz A note sounds different when it is played on oboe, piano, violin, or electric guitar. Even if different players of the same instrument play the same note, their notes might sound different due to differences in instrumental technique (e.g., different embouchures), different types of accessories (e.g., mouthpieces for brass players, reeds for oboe and bassoon players) or strings made out of different materials for string players (e.g., gut strings versus steel strings). Even two instrumentalists playing the same note on the same instrument (one after the other) may sound different due to different ways of playing the instrument (e.g., two string players might hold the bow differently). The physical characteristics of sound that determine the perception of timbre include the spectrum, envelope, and overtones of a note or musical sound. For electric instruments developed in the 20th century, such as electric guitar, electric bass and electric piano, the performer can also change the tone by adjusting equalizer controls, tone controls on the instrument, and by using electronic effects units such as distortion pedals. The tone of the electric Hammond organ is controlled by adjusting drawbars. Expression Expressive qualities are those elements in music that create change in music without changing the main pitches or substantially changing the rhythms of the melody and its accompaniment. Performers, including singers and instrumentalists, can add musical expression to a song or piece by adding phrasing, by adding effects such as vibrato (with voice and some instruments, such as guitar, violin, brass instruments, and woodwinds), dynamics (the loudness or softness of piece or a section of it), tempo fluctuations (e.g., ritardando or accelerando, which are, respectively slowing down and speeding up the tempo), by adding pauses or fermatas on a cadence, and by changing the articulation of the notes (e.g., making notes more pronounced or accented, by making notes more legato, which means smoothly connected, or by making notes shorter). Expression is achieved through the manipulation of pitch (such as inflection, vibrato, slides etc.), volume (dynamics, accent, tremolo etc.), duration (tempo fluctuations, rhythmic changes, changing note duration such as with legato and staccato, etc.), timbre (e.g. changing vocal timbre from a light to a resonant voice) and sometimes even texture (e.g. doubling the bass note for a richer effect in a piano piece). Expression therefore can be seen as a manipulation of all elements to convey "an indication of mood, spirit, character etc." and as such cannot be included as a unique perceptual element of music, although it can be considered an important rudimentary element of music. Form In music, form describes the overall structure or plan of a song or piece of music, and it describes the layout of a composition as divided into sections. In the early 20th century, Tin Pan Alley songs and Broadway musical songs were often in AABA thirty-two-bar form, in which the A sections repeated the same eight bar melody (with variation) and the B section provided a contrasting melody or harmony for eight bars. From the 1960s onward, Western pop and rock songs are often in verse-chorus form, which comprises a sequence of verse and chorus ("refrain") sections, with new lyrics for most verses and repeating lyrics for the choruses. Popular music often makes use of strophic form, sometimes in conjunction with the twelve bar blues. In the tenth edition of The Oxford Companion to Music, Percy Scholes defines musical form as "a series of strategies designed to find a successful mean between the opposite extremes of unrelieved repetition and unrelieved alteration." Examples of common forms of Western music include the fugue, the invention, sonata-allegro, canon, strophic, theme and variations, and rondo. Scholes states that European classical music had only six stand-alone forms: simple binary, simple ternary, compound binary, rondo, air with variations, and fugue (although musicologist Alfred Mann emphasized that the fugue is primarily a method of composition that has sometimes taken on certain structural conventions.) Where a piece cannot readily be broken into sectional units (though it might borrow some form from a poem, story or programme), it is said to be through-composed. Such is often the case with a fantasia, prelude, rhapsody, etude (or study), symphonic poem, Bagatelle, impromptu or similar composition. Professor Charles Keil classified forms and formal detail as "sectional, developmental, or variational." Philosophy The philosophy of music is the study of fundamental questions regarding music and has connections with questions in metaphysics and aesthetics. Questions include: What is the definition of music? (What are the necessary and sufficient conditions for classifying something as music?) What is the relationship between music and mind? What does music history reveal to us about the world? What is the connection between music and emotions? What is meaning in relation to music? In ancient times, such as with the Ancient Greeks, the aesthetics of music explored the mathematical and cosmological dimensions of rhythmic and harmonic organization. In the 18th century, focus shifted to the experience of hearing music, and thus to questions about its beauty and human enjoyment (plaisir and jouissance) of music. The origin of this philosophic shift is sometimes attributed to Alexander Gottlieb Baumgarten in the 18th century, followed by Immanuel Kant. Through their writing, the ancient term 'aesthetics', meaning sensory perception, received its present-day connotation. In the 2000s, philosophers have tended to emphasize issues besides beauty and enjoyment. For example, music's capacity to express emotion has been foregrounded. In the 20th century, important contributions were made by Peter Kivy, Jerrold Levinson, Roger Scruton, and Stephen Davies. However, many musicians, music critics, and other non-philosophers have contributed to the aesthetics of music. In the 19th century, a significant debate arose between Eduard Hanslick, a music critic and musicologist, and composer Richard Wagner regarding whether music can express meaning. Harry Partch and some other musicologists, such as Kyle Gann, have studied and tried to popularize microtonal music and the usage of alternate musical scales. Modern composers like La Monte Young, Rhys Chatham and Glenn Branca paid much attention to a scale called just intonation. It is often thought that music has the ability to affect our emotions, intellect, and psychology; it can assuage our loneliness or incite our passions. The philosopher Plato suggests in The Republic that music has a direct effect on the soul. Therefore, he proposes that in the ideal regime music would be closely regulated by the state (Book VII). In Ancient China, the philosopher Confucius believed that music and rituals or rites are interconnected and harmonious with nature; he stated that music was the harmonization of heaven and earth, while the order was brought by the rites order, making them extremely crucial functions in society. Psychology Modern music psychology aims to explain and understand musical behavior and experience. Research in this field and its subfields are primarily empirical; their knowledge tends to advance on the basis of interpretations of data collected by systematic observation of and interaction with human participants. In addition to its focus on fundamental perceptions and cognitive processes, music psychology is a field of research with practical relevance for many areas, including music performance, composition, education, criticism, and therapy, as well as investigations of human aptitude, skill, intelligence, creativity, and social behavior. Neuroscience Cognitive neuroscience of music is the scientific study of brain-based mechanisms involved in the cognitive processes underlying music. These behaviours include music listening, performing, composing, reading, writing, and ancillary activities. It also is increasingly concerned with the brain basis for musical aesthetics and musical emotion. The field is distinguished by its reliance on direct observations of the brain, using such techniques as functional magnetic resonance imaging (fMRI), transcranial magnetic stimulation (TMS), magnetoencephalography (MEG), electroencephalography (EEG), and positron emission tomography (PET). Cognitive musicology Cognitive musicology is a branch of cognitive science concerned with computationally modeling musical knowledge with the goal of understanding both music and cognition. The use of computer models provides an exacting, interactive medium in which to formulate and test theories and has roots in artificial intelligence and cognitive science. Cognitive musicology investigates topics such as the parallels between language and music in the brain. Research often includes biologically inspired models of computation, such as neural networks and evolutionary programs. This field seeks to model how musical knowledge is represented, stored, perceived, performed, and generated. By using a well-structured computer environment, the systematic structures of these cognitive phenomena can be investigated. Psychoacoustics Psychoacoustics is the scientific study of sound perception. More specifically, it is the branch of science studying the psychological and physiological responses associated with sound (including speech and music). It can be further categorized as a branch of psychophysics. Evolutionary musicology Evolutionary musicology concerns the "origins of music, the question of animal song, selection pressures underlying music evolution", and "music evolution and human evolution". It seeks to understand music perception and activity in the context of evolutionary theory. Charles Darwin speculated that music may have held an adaptive advantage and functioned as a protolanguage, a view which has spawned several competing theories of music evolution. An alternate view sees music as a by-product of linguistic evolution; a type of "auditory cheesecake" that pleases the senses without providing any adaptive function. This view has been directly countered by numerous music researchers. Cultural effects An individual's culture or ethnicity plays a role in their music cognition, including their preferences, emotional reaction, and musical memory. Musical preferences are biased toward culturally familiar musical traditions beginning in infancy, and adults' classification of the emotion of a musical piece depends on both culturally specific and universal structural features. Additionally, individuals' musical memory abilities are greater for culturally familiar music than for culturally unfamiliar music. Perceptual Since the emergence of the study of psychoacoustics in the 1930s, most lists of elements of music have related more to how we hear music than how we learn to play it or study it. C.E. Seashore, in his book Psychology of Music, identified four "psychological attributes of sound". These were: "pitch, loudness, time, and timbre" (p. 3). He did not call them the "elements of music" but referred to them as "elemental components" (p. 2). Nonetheless, these elemental components link precisely with four of the most common musical elements: "Pitch" and "timbre" match exactly, "loudness" links with dynamics, and "time" links with the time-based elements of rhythm, duration, and tempo. This usage of the phrase "the elements of music" links more closely with Webster's New 20th Century Dictionary definition of an element as: "a substance which cannot be divided into a simpler form by known methods" and educational institutions' lists of elements generally align with this definition as well. Although writers of lists of "rudimentary elements of music" can vary their lists depending on their personal (or institutional) priorities, the perceptual elements of music should consist of an established (or proven) list of discrete elements which can be independently manipulated to achieve an intended musical effect. It seems at this stage that there is still research to be done in this area. A slightly different way of approaching the identification of the elements of music, is to identify the "elements of sound" as: pitch, duration, loudness, timbre, sonic texture and spatial location, and then to define the "elements of music" as: sound, structure, and artistic intent. Sociological aspects Ethnographic studies demonstrate that music is a participatory, community-based activity. Music is experienced by individuals in a range of social settings from being alone, to attending a large concert, forming a music community, which cannot be understood as a function of individual will or accident; it includes both commercial and non-commercial participants with a shared set of common values. Musical performances take different forms in different cultures and socioeconomic milieus. In Europe and North America, there was a divide between what types of music were viewed as "high culture" and "low culture." "High culture" included Baroque, Classical, Romantic, and modern-era symphonies, concertos, and solo works, and are typically heard in formal concerts in concert halls and churches, with the audience sitting quietly. Other types of music—including jazz, blues, soul, and country—are often performed in bars, nightclubs, and theatres, where the audience may drink, dance and cheer. Until the 20th century, the division between "high" and "low" musical forms was accepted as a valid distinction that separated out "art music", from popular music heard in bars and dance halls. Musicologists, such as David Brackett, note a "redrawing of high-low cultural-aesthetic boundaries" in the 20th century. And, "when industry and public discourses link categories of music with categories of people, they tend to conflate stereotypes with actual listening communities." Stereotypes can be based on socioeconomic standing, or social class, of the performers or audience of the different types of music. When composers introduce styles of music that break with convention, there can be strong resistance from academics and others. Late-period Beethoven string quartets, Stravinsky ballet scores, serialism, bebop, hip hop, punk rock, and electronica were controversial and criticised, when they were first introduced. Such themes are examined in the sociology of music, sometimes called sociomusicology, which is pursued in departments of sociology, media studies, or music, and is closely related to ethnomusicology. Role of women Women have played a major role in music throughout history, as composers, songwriters, instrumental performers, singers, conductors, music scholars, music educators, music critics/music journalists and other musical professions. In the 2010s, while women comprise a significant proportion of popular music and classical music singers, and a significant proportion of songwriters (many of them being singer-songwriters), there are few women record producers, rock critics and rock instrumentalists. Although there have been a huge number of women composers in classical music, from the medieval period to the present day, women composers are significantly underrepresented in the commonly performed classical music repertoire, music history textbooks and music encyclopedias; for example, in the Concise Oxford History of Music, Clara Schumann is one of the few female composers who is mentioned. Women comprise a significant proportion of instrumental soloists in classical music and the percentage of women in orchestras is increasing. A 2015 article on concerto soloists in major Canadian orchestras, however, indicated that 84% of the soloists with the Montreal Symphony Orchestra were men. In 2012, women still made up just 6% of the top-ranked Vienna Philharmonic orchestra. Women are less common as instrumental players in popular music genres such as rock and heavy metal, although there have been a number of notable female instrumentalists and all-female bands. Women are particularly underrepresented in extreme metal genres. In the 1960s pop-music scene, "[l]ike most aspects of the...music business, [in the 1960s,] songwriting was a male-dominated field. Though there were plenty of female singers on the radio, women ...were primarily seen as consumers:... Singing was sometimes an acceptable pastime for a girl, but playing an instrument, writing songs, or producing records simply wasn't done." Young women "...were not socialized to see themselves as people who create [music]." Women are also underrepresented in orchestral conducting, music criticism/music journalism, music producing, and sound engineering. While women were discouraged from composing in the 19th century, and there are few women musicologists, women became involved in music education "...to such a degree that women dominated [this field] during the later half of the 19th century and well into the 20th century." According to Jessica Duchen, a music writer for London's The Independent, women musicians in classical music are "...too often judged for their appearances, rather than their talent" and they face pressure "...to look sexy onstage and in photos." Duchen states that while "[t]here are women musicians who refuse to play on their looks,...the ones who do tend to be more materially successful." According to the UK's Radio 3 editor, Edwina Wolstencroft, the music industry has long been open to having women in performance or entertainment roles, but women are much less likely to have positions of authority, such as being the conductor of an orchestra. In popular music, while there are many women singers recording songs, there are very few women behind the audio console acting as music producers, the individuals who direct and manage the recording process. One of the most recorded artists is Asha Bhosle, an Indian singer best known as a playback singer in Hindi cinema. Media and technology Since the 20th century, live music can be broadcast over the radio, television or the Internet, or recorded and listened to on a CD player or MP3 player. In the early 20th century (in the late 1920s), as talking pictures emerged in the early 20th century, with their prerecorded musical tracks, an increasing number of moviehouse orchestra musicians found themselves out of work. During the 1920s, live musical performances by orchestras, pianists, and theater organists were common at first-run theaters. With the coming of the talking motion pictures, those featured performances were largely eliminated. The American Federation of Musicians (AFM) took out newspaper advertisements protesting the replacement of live musicians with mechanical playing devices. One 1929 ad that appeared in the Pittsburgh Press features an image of a can labeled "Canned Music / Big Noise Brand / Guaranteed to Produce No Intellectual or Emotional Reaction Whatever" Sometimes, live performances incorporate prerecorded sounds. For example, a disc jockey uses disc records for scratching, and some 20th-century works have a solo for an instrument or voice that is performed along with music that is prerecorded onto a tape. Some pop bands use recorded backing tracks. Computers and many keyboards can be programmed to produce and play Musical Instrument Digital Interface (MIDI) music. Audiences can also become performers by participating in karaoke, an activity of Japanese origin centered on a device that plays voice-eliminated versions of well-known songs. Most karaoke machines also have video screens that show lyrics to songs being performed; performers can follow the lyrics as they sing over the instrumental tracks. The advent of the Internet and widespread high-speed broadband access has transformed the experience of music, partly through the increased ease of access to recordings of music via streaming video and vastly increased choice of music for consumers. Another effect of the Internet arose with online communities and social media websites like YouTube and Facebook, a social networking service. These sites make it easier for aspiring singers and amateur bands to distribute videos of their songs, connect with other musicians, and gain audience interest. Professional musicians also use YouTube as a free publisher of promotional material. YouTube users, for example, no longer only download and listen to MP3s, but also actively create their own. According to Don Tapscott and Anthony D. Williams, in their book Wikinomics, there has been a shift from a traditional consumer role to what they call a "prosumer" role, a consumer who both creates content and consumes. Manifestations of this in music include the production of mashes, remixes, and music videos by fans. Education Non-institutional The incorporation of music into general education from preschool to post secondary education, is common in North America and Europe. Involvement in playing and singing music is thought to teach basic skills such as concentration, counting, listening, and cooperation while also promoting understanding of language, improving the ability to recall information, and creating an environment more conducive to learning in other areas. In elementary schools, children often learn to play instruments such as the recorder, sing in small choirs, and learn about the history of Western art music and traditional music. Some elementary school children also learn about popular music styles. In religious schools, children sing hymns and other religious music. In secondary schools (but rarely in primary schools), students may have the opportunity to perform in some types of musical ensembles, such as choirs (a group of singers), marching bands, concert bands, jazz bands, or orchestras. In some school systems, music lessons on how to play instruments may be provided. Some students also take private music lessons after school with a singing teacher or instrument teacher. Amateur musicians typically learn basic musical rudiments (e.g., learning about musical notation for musical scales and rhythms) and beginner- to intermediate-level singing or instrument-playing techniques. At the university level, students in most arts and humanities programs can receive credit for taking a few music courses, which typically take the form of an overview course on the history of music, or a music appreciation course that focuses on listening to music and learning about different musical styles. In addition, most North American and European universities have some types of musical ensembles that students in arts and humanities are able to participate in, such as choirs, marching bands, concert bands, or orchestras. The study of Western art music is increasingly common outside of North America and Europe, such as the Indonesian Institute of the Arts in Yogyakarta, Indonesia, or the classical music programs that are available in Asian countries such as South Korea, Japan, and China. At the same time, Western universities and colleges are widening their curriculum to include music of non-Western cultures, such as the music of Africa or Bali (e.g. Gamelan music). Institutional People aiming to become professional musicians, singers, composers, songwriters, music teachers and practitioners of other music-related professions such as music history professors, sound engineers, and so on study in specialized post-secondary programs offered by colleges, universities and music conservatories. Some institutions that train individuals for careers in music offer training in a wide range of professions, as is the case with many of the top U.S. universities, which offer degrees in music performance (including singing and playing instruments), music history, music theory, music composition, music education (for individuals aiming to become elementary or high school music teachers) and, in some cases, conducting. On the other hand, some small colleges may only offer training in a single profession (e.g., sound recording). While most university and conservatory music programs focus on training students in classical music, there are universities and colleges that train musicians for careers as jazz or popular music musicians and composers, with notable U.S. examples including the Manhattan School of Music and the Berklee College of Music. Two schools in Canada which offer professional jazz training are McGill University and Humber College. Individuals aiming at careers in some types of music, such as heavy metal music, country music or blues are unlikely to become professionals by completing degrees or diplomas. Instead, they typically learn about their style of music by singing or playing in bands (often beginning in amateur bands, cover bands and tribute bands), studying recordings on DVD and the Internet, and working with already-established professionals in their style of music, either through informal mentoring or regular music lessons. Since the 2000s, the increasing popularity and availability of Internet forums and YouTube "how-to" videos have enabled singers and musicians from metal, blues and similar genres to improve their skills. Many pop, rock and country singers train informally with vocal coaches and voice teachers. Academic study Musicology Musicology, the academic study of music, is studied in universities and music conservatories. The earliest definitions from the 19th century defined three sub-disciplines of musicology: systematic musicology, historical musicology, and comparative musicology or ethnomusicology. In 2010-era scholarship, one is more likely to encounter a division into music theory, music history, and ethnomusicology. Research in musicology has often been enriched by cross-disciplinary work, for example in the field of psychoacoustics. The study of music of non-Western cultures, and cultural study of music, is called ethnomusicology. Students can pursue study of musicology, ethnomusicology, music history, and music theory through different types of degrees, including bachelor's, master's and PhD. Music theory Music theory is the study of music, generally in a highly technical manner outside of other disciplines. More broadly it refers to any study of music, usually related in some form with compositional concerns, and may include mathematics, physics, and anthropology. What is most commonly taught in beginning music theory classes are guidelines to write in the style of the common practice period, or tonal music. Theory, even of music of the common practice period, may take other forms. Musical set theory is the application of mathematical set theory to music, first applied to atonal music. Speculative music theory, contrasted with analytic music theory, is devoted to the analysis and synthesis of music materials, for example tuning systems, generally as preparation for composition. Zoomusicology Zoomusicology is the study of the music of non-human animals, or the musical aspects of sounds produced by non-human animals. As George Herzog (1941) asked, "do animals have music?" François-Bernard Mâche's Musique, mythe, nature, ou les Dauphins d'Arion (1983), a study of "ornitho-musicology" using a technique of Nicolas Ruwet's Language, musique, poésie (1972) paradigmatic segmentation analysis, shows that bird songs are organised according to a repetition-transformation principle. Jean-Jacques Nattiez (1990), argues that "in the last analysis, it is a human being who decides what is and is not musical, even when the sound is not of human origin. If we acknowledge that sound is not organised and conceptualised (that is, made to form music) merely by its producer, but by the mind that perceives it, then music is uniquely human." Ethnomusicology In the West, much of the history of music that is taught deals with the Western civilization's art music, known as classical music. The history of music in non-Western cultures ("world music" or the field of "ethnomusicology") is also taught in Western universities. This includes the documented classical traditions of Asian countries outside the influence of Western Europe, as well as the folk or indigenous music of various other cultures. Popular or folk styles of music in non-Western countries varied from culture to culture, and period to period. Different cultures emphasised different instruments, techniques, singing styles and uses for music. Music has been used for entertainment, ceremonies, rituals, religious purposes and for practical and artistic communication. Non-Western music has also been used for propaganda purposes, as was the case with Chinese opera during the Cultural Revolution. There is a host of music classifications for non-Western music, many of which are caught up in the argument over the definition of music. Among the largest of these is the division between classical music (or "art" music), and popular music (or commercial music – including non-Western styles of rock, country, and pop music-related styles). Some genres do not fit neatly into one of these "big two" classifications, (such as folk music, world music, or jazz-related music). As world cultures have come into greater global contact, their indigenous musical styles have often merged with other styles, which produces new styles. For example, the United States bluegrass style contains elements from Anglo-Irish, Scottish, Irish, German and African instrumental and vocal traditions, which were able to fuse in the United States' multi-ethnic "melting pot" society. Some types of world music contain a mixture of non-Western indigenous styles with Western pop music elements. Genres of music are determined as much by tradition and presentation as by the actual music. Some works, like George Gershwin's Rhapsody in Blue, are claimed by both jazz and classical music, while Gershwin's Porgy and Bess and Leonard Bernstein's West Side Story are claimed by both opera and the Broadway musical tradition. Many music festivals for non-Western music, include bands and singers from a particular musical genre, such as world music. Indian music, for example, is one of the oldest and longest living types of music, and is still widely heard and performed in South Asia, as well as internationally (especially since the 1960s). Indian music has mainly three forms of classical music, Hindustani, Carnatic, and Dhrupad styles. It has also a large repertoire of styles, which involve only percussion music such as the talavadya performances famous in South India. Therapy Music therapy is an interpersonal process in which a trained therapist uses music and all of its facets—physical, emotional, mental, social, aesthetic, and spiritual—to help clients to improve or maintain their health. In some instances, the client's needs are addressed directly through music; in others they are addressed through the relationships that develop between the client and therapist. Music therapy is used with individuals of all ages and with a variety of conditions, including: psychiatric disorders, medical problems, physical disabilities, sensory impairments, developmental disabilities, substance abuse issues, communication disorders, interpersonal problems, and aging. It is also used to improve learning, build self-esteem, reduce stress, support physical exercise, and facilitate a host of other health-related activities. Music therapists may encourage clients to sing, play instruments, create songs, or do other musical activities. In the 10th century, the philosopher Al-Farabi described how vocal music can stimulate the feelings and souls of listeners. Music has long been used to help people deal with their emotions. In the 17th century, the scholar Robert Burton's The Anatomy of Melancholy argued that music and dance were critical in treating mental illness, especially melancholia. He noted that music has an "excellent power ...to expel many other diseases" and he called it "a sovereign remedy against despair and melancholy." He pointed out that in Antiquity, Canus, a Rhodian fiddler, used music to "make a melancholy man merry, ...a lover more enamoured, a religious man more devout." In the Ottoman Empire, mental illnesses were treated with music. In November 2006, Michael J. Crawford and his colleagues also found that music therapy helped schizophrenic patients. See also Glossary of music terminology Lists of musicians List of musicology topics Music and emotion Music archaeology Music history Music-specific disorders References Notes Citations Sources Further reading External links Grove Music Online — online version of The New Grove Dictionary of Music and Musicians. All ten volumes of the Garland Encyclopedia of World Music (subscription required) Dolmetsch free online music dictionary, complete, with references to a list of specialised music dictionaries (by continent, by instrument, by genre, etc.) Some books on music by Carl Van Vechten (1880–1964)

Linguistics is the scientific study of language. The areas of linguistic analysis are syntax (rules governing the structure of sentences), semantics (meaning), morphology (structure of words), phonetics (speech sounds and equivalent gestures in sign languages), phonology (the abstract sound system of a particular language, and analogous systems of sign languages), and pragmatics (how the context of use contributes to meaning). Subdisciplines such as biolinguistics (the study of the biological variables and evolution of language) and psycholinguistics (the study of psychological factors in human language) bridge many of these divisions. Linguistics encompasses many branches and subfields that span both theoretical and practical applications. Theoretical linguistics is concerned with understanding the universal and fundamental nature of language and developing a general theoretical framework for describing it. Applied linguistics seeks to utilize the scientific findings of the study of language for practical purposes, such as developing methods of improving language education and literacy. Linguistic features may be studied through a variety of perspectives: synchronically (by describing the structure of a language at a specific point in time) or diachronically (through the historical development of a language over a period of time), in monolinguals or in multilinguals, among children or among adults, in terms of how it is being learnt or how it was acquired, as abstract objects or as cognitive structures, through written texts or through oral elicitation, and finally through mechanical data collection or practical fieldwork. Linguistics emerged from the field of philology, of which some branches are more qualitative and holistic in approach. Today, philology and linguistics are variably described as related fields, subdisciplines, or separate fields of language study, but, by and large, linguistics can be seen as an umbrella term. Linguistics is also related to the philosophy of language, stylistics, rhetoric, semiotics, lexicography, and translation. Major subdisciplines Historical linguistics Historical linguistics is the study of how language changes over history, particularly with regard to a specific language or a group of languages. Western trends in historical linguistics date back to roughly the late 18th century, when the discipline grew out of philology, the study of ancient texts and oral traditions. Historical linguistics emerged as one of the first few sub-disciplines in the field, and was most widely practised during the late 19th century. Despite a shift in focus in the 20th century towards formalism and generative grammar, which studies the universal properties of language, historical research today still remains a significant field of linguistic inquiry. Subfields of the discipline include language change and grammaticalization. Historical linguistics studies language change either diachronically (through a comparison of different time periods in the past and present) or in a synchronic manner (by observing developments between different variations that exist within the current linguistic stage of a language). At first, historical linguistics was the cornerstone of comparative linguistics, which involves a study of the relationship between different languages. At that time, scholars of historical linguistics were only concerned with creating different categories of language families, and reconstructing prehistoric proto-languages by using both the comparative method and the method of internal reconstruction. Internal reconstruction is the method by which an element that contains a certain meaning is re-used in different contexts or environments where there is a variation in either sound or analogy. The reason for this had been to describe well-known Indo-European languages, many of which had detailed documentation and long written histories. Scholars of historical linguistics also studied Uralic languages, another European language family for which very little written material existed back then. After that, there also followed significant work on the corpora of other languages, such as the Austronesian languages and the Native American language families. In historical work, the uniformitarian principle is generally the underlying working hypothesis, occasionally also clearly expressed. The principle was expressed early by William Dwight Whitney, who considered it imperative, a "must", of historical linguistics to "look to find the same principle operative also in the very outset of that [language] history." The above approach of comparativism in linguistics is now, however, only a small part of the much broader discipline called historical linguistics. The comparative study of specific Indo-European languages is considered a highly specialized field today, while comparative research is carried out over the subsequent internal developments in a language: in particular, over the development of modern standard varieties of languages, and over the development of a language from its standardized form to its varieties. For instance, some scholars also tried to establish super-families, linking, for example, Indo-European, Uralic, and other language families to a hypothetical Nostratic language group. While these attempts are still not widely accepted as credible methods, they provide necessary information to establish relatedness in language change. This is generally hard to find for events long ago, due to the occurrence of chance word resemblances and variations between language groups. A limit of around 10,000 years is often assumed for the functional purpose of conducting research. It is also hard to date various proto-languages. Even though several methods are available, these languages can be dated only approximately. In modern historical linguistics, we examine how languages change over time, focusing on the relationships between dialects within a specific period. This includes studying morphological, syntactical, and phonetic shifts. Connections between dialects in the past and present are also explored. Syntax Syntax is the study of how words and morphemes combine to form larger units such as phrases and sentences. Central concerns of syntax include word order, grammatical relations, constituency, agreement, the nature of crosslinguistic variation, and the relationship between form and meaning. There are numerous approaches to syntax that differ in their central assumptions and goals. Morphology Morphology is the study of words, including the principles by which they are formed, and how they relate to one another within a language. Most approaches to morphology investigate the structure of words in terms of morphemes, which are the smallest units in a language with some independent meaning. Morphemes include roots that can exist as words by themselves, but also categories such as affixes that can only appear as part of a larger word. For example, in English the root catch and the suffix -ing are both morphemes; catch may appear as its own word, or it may be combined with -ing to form the new word catching. Morphology also analyzes how words behave as parts of speech, and how they may be inflected to express grammatical categories including number, tense, and aspect. Concepts such as productivity are concerned with how speakers create words in specific contexts, which evolves over the history of a language. The discipline that deals specifically with the sound changes occurring within morphemes is morphophonology. Semantics and pragmatics Semantics and pragmatics are branches of linguistics concerned with meaning. These subfields have traditionally been divided according to aspects of meaning: "semantics" refers to grammatical and lexical meanings, while "pragmatics" is concerned with meaning in context. Within linguistics, the subfield of formal semantics studies the denotations of sentences and how they are composed from the meanings of their constituent expressions. Formal semantics draws heavily on philosophy of language and uses formal tools from logic and computer science. On the other hand, cognitive semantics explains linguistic meaning via aspects of general cognition, drawing on ideas from cognitive science such as prototype theory. Pragmatics focuses on phenomena such as speech acts, implicature, and talk in interaction. Unlike semantics, which examines meaning that is conventional or "coded" in a given language, pragmatics studies how the transmission of meaning depends not only on the structural and linguistic knowledge (grammar, lexicon, etc.) of the speaker and listener, but also on the context of the utterance, any pre-existing knowledge about those involved, the inferred intent of the speaker, and other factors. Phonetics and phonology Phonetics and phonology are branches of linguistics concerned with sounds (or the equivalent aspects of sign languages). Phonetics is largely concerned with the physical aspects of sounds such as their articulation, acoustics, production, and perception. Phonology is concerned with the linguistic abstractions and categorizations of sounds, and it tells us what sounds are in a language, how they do and can combine into words, and explains why certain phonetic features are important to identifying a word. Typology Structures Linguistic structures are pairings of meaning and form. Any particular pairing of meaning and form is a Saussurean linguistic sign. For instance, the meaning "cat" is represented worldwide with a wide variety of different sound patterns (in oral languages), movements of the hands and face (in sign languages), and written symbols (in written languages). Linguistic patterns have proven their importance for the knowledge engineering field especially with the ever-increasing amount of available data. Linguists focusing on structure attempt to understand the rules regarding language use that native speakers know (not always consciously). All linguistic structures can be broken down into component parts that are combined according to (sub)conscious rules, over multiple levels of analysis. For instance, consider the structure of the word "tenth" on two different levels of analysis. On the level of internal word structure (known as morphology), the word "tenth" is made up of one linguistic form indicating a number and another form indicating ordinality. The rule governing the combination of these forms ensures that the ordinality marker "th" follows the number "ten." On the level of sound structure (known as phonology), structural analysis shows that the "n" sound in "tenth" is made differently from the "n" sound in "ten" spoken alone. Although most speakers of English are consciously aware of the rules governing internal structure of the word pieces of "tenth", they are less often aware of the rule governing its sound structure. Linguists focused on structure find and analyze rules such as these, which govern how native speakers use language. Grammar Grammar is a system of rules which governs the production and use of utterances in a given language. These rules apply to sound as well as meaning, and include componential subsets of rules, such as those pertaining to phonology (the organization of phonetic sound systems), morphology (the formation and composition of words), and syntax (the formation and composition of phrases and sentences). Modern frameworks that deal with the principles of grammar include structural and functional linguistics, and generative linguistics. Sub-fields that focus on a grammatical study of language include the following: Phonetics, the study of the physical properties of speech sound production and perception, and delves into their acoustic and articulatory properties Phonology, the study of sounds as abstract elements in the speaker's mind that distinguish meaning (phonemes) Morphology, the study of morphemes, or the internal structures of words and how they can be modified Syntax, the study of how words combine to form grammatical phrases and sentences Semantics, the study of lexical and grammatical aspects of meaning Pragmatics, the study of how utterances are used in communicative acts, and the role played by situational context and non-linguistic knowledge in the transmission of meaning Discourse analysis, the analysis of language use in texts (spoken, written, or signed) Stylistics, the study of linguistic factors (rhetoric, diction, stress) that place a discourse in context Semiotics, the study of signs and sign processes (semiosis), indication, designation, likeness, analogy, metaphor, symbolism, signification, and communication Discourse Discourse is language as social practice (Baynham, 1995) and is a multilayered concept. As a social practice, discourse embodies different ideologies through written and spoken texts. Discourse analysis can examine or expose these ideologies. Discourse not only influences genre, which is selected based on specific contexts but also, at a micro level, shapes language as text (spoken or written) down to the phonological and lexico-grammatical levels. Grammar and discourse are linked as parts of a system. A particular discourse becomes a language variety when it is used in this way for a particular purpose, and is referred to as a register. There may be certain lexical additions (new words) that are brought into play because of the expertise of the community of people within a certain domain of specialization. Thus, registers and discourses distinguish themselves not only through specialized vocabulary but also, in some cases, through distinct stylistic choices. People in the medical fraternity, for example, may use some medical terminology in their communication that is specialized to the field of medicine. This is often referred to as being part of the "medical discourse", and so on. Lexicon The lexicon is a catalogue of words and terms that are stored in a speaker's mind. The lexicon consists of words and bound morphemes, which are parts of words that can not stand alone, like affixes. In some analyses, compound words and certain classes of idiomatic expressions and other collocations are also considered to be part of the lexicon. Dictionaries represent attempts at listing, in alphabetical order, the lexicon of a given language; usually, however, bound morphemes are not included. Lexicography, closely linked with the domain of semantics, is the science of mapping the words into an encyclopedia or a dictionary. The creation and addition of new words (into the lexicon) is called coining or neologization, and the new words are called neologisms. It is often believed that a speaker's capacity for language lies in the quantity of words stored in the lexicon. However, this is often considered a myth by linguists. The capacity for the use of language is considered by many linguists to lie primarily in the domain of grammar, and to be linked with competence, rather than with the growth of vocabulary. Even a very small lexicon is theoretically capable of producing an infinite number of sentences. Vocabulary size is relevant as a measure of comprehension. There is general consensus that reading comprehension of a written text in English requires 98% coverage, meaning that the person understands 98% of the words in the text. The question of how much vocabulary is needed is therefore related to which texts or conversations need to be understood. A common estimate is 6-7,000 word families to understand a wide range of conversations and 8-9,000 word families to be able to read a wide range of written texts. Style Stylistics also involves the study of written, signed, or spoken discourse through varying speech communities, genres, and editorial or narrative formats in the mass media. It involves the study and interpretation of texts for aspects of their linguistic and tonal style. Stylistic analysis entails the analysis of description of particular dialects and registers used by speech communities. Stylistic features include rhetoric, diction, stress, satire, irony, dialogue, and other forms of phonetic variations. Stylistic analysis can also include the study of language in canonical works of literature, popular fiction, news, advertisements, and other forms of communication in popular culture as well. It is usually seen as a variation in communication that changes from speaker to speaker and community to community. In short, Stylistics is the interpretation of text. In the 1960s, Jacques Derrida, for instance, further distinguished between speech and writing, by proposing that written language be studied as a linguistic medium of communication in itself. Palaeography is therefore the discipline that studies the evolution of written scripts (as signs and symbols) in language. The formal study of language also led to the growth of fields like psycholinguistics, which explores the representation and function of language in the mind; neurolinguistics, which studies language processing in the brain; biolinguistics, which studies the biology and evolution of language; and language acquisition, which investigates how children and adults acquire the knowledge of one or more languages. Approaches Humanistic The fundamental principle of humanistic linguistics, especially rational and logical grammar, is that language is an invention created by people. A semiotic tradition of linguistic research considers language a sign system which arises from the interaction of meaning and form. The organization of linguistic levels is considered computational. Linguistics is essentially seen as relating to social and cultural studies because different languages are shaped in social interaction by the speech community. Frameworks representing the humanistic view of language include structural linguistics, among others. Structural analysis means dissecting each linguistic level: phonetic, morphological, syntactic, and discourse, to the smallest units. These are collected into inventories (e.g. phoneme, morpheme, lexical classes, phrase types) to study their interconnectedness within a hierarchy of structures and layers. Functional analysis adds to structural analysis the assignment of semantic and other functional roles that each unit may have. For example, a noun phrase may function as the subject or object of the sentence; or the agent or patient. Functional linguistics, or functional grammar, is a branch of structural linguistics. In the humanistic reference, the terms structuralism and functionalism are related to their meaning in other human sciences. The difference between formal and functional structuralism lies in the way that the two approaches explain why languages have the properties they have. Functional explanation entails the idea that language is a tool for communication, or that communication is the primary function of language. Linguistic forms are consequently explained by an appeal to their functional value, or usefulness. Other structuralist approaches take the perspective that form follows from the inner mechanisms of the bilateral and multilayered language system. Biological Approaches such as cognitive linguistics and generative grammar study linguistic cognition with a view towards uncovering the biological underpinnings of language. In Generative Grammar, these underpinning are understood as including innate domain-specific grammatical knowledge. Thus, one of the central concerns of the approach is to discover what aspects of linguistic knowledge are innate and which are not. Cognitive linguistics, in contrast, rejects the notion of innate grammar, and studies how the human mind creates linguistic constructions from event schemas, and the impact of cognitive constraints and biases on human language. In cognitive linguistics, language is approached via the senses. A closely related approach is evolutionary linguistics which includes the study of linguistic units as cultural replicators. It is possible to study how language replicates and adapts to the mind of the individual or the speech community. Construction grammar is a framework which applies the meme concept to the study of syntax. The generative versus evolutionary approach are sometimes called formalism and functionalism, respectively. This reference is however different from the use of the terms in human sciences. Methodology Modern linguistics is primarily descriptive. Linguists describe and explain features of language without making subjective judgments on whether a particular feature or usage is "good" or "bad". This is analogous to practice in other sciences: a zoologist studies the animal kingdom without making subjective judgments on whether a particular species is "better" or "worse" than another. Prescription, on the other hand, is an attempt to promote particular linguistic usages over others, often favoring a particular dialect or "acrolect". This may have the aim of establishing a linguistic standard, which can aid communication over large geographical areas. It may also, however, be an attempt by speakers of one language or dialect to exert influence over speakers of other languages or dialects (see Linguistic imperialism). An extreme version of prescriptivism can be found among censors, who attempt to eradicate words and structures that they consider to be destructive to society. Prescription, however, may be practised appropriately in language instruction, like in ELT, where certain fundamental grammatical rules and lexical items need to be introduced to a second-language speaker who is attempting to acquire the language. Sources Most contemporary linguists work under the assumption that spoken data and signed data are more fundamental than written data. This is because Speech appears to be universal to all human beings capable of producing and perceiving it, while there have been many cultures and speech communities that lack written communication; Features appear in speech which are not always recorded in writing, including phonological rules, sound changes, and speech errors; All natural writing systems reflect a spoken language (or potentially a signed one), even with pictographic scripts like Dongba writing Naxi homophones with the same pictogram, and text in writing systems used for two languages changing to fit the spoken language being recorded; Speech evolved before human beings invented writing; Individuals learn to speak and process spoken language more easily and earlier than they do with writing. Nonetheless, linguists agree that the study of written language can be worthwhile and valuable. For research that relies on corpus linguistics and computational linguistics, written language is often much more convenient for processing large amounts of linguistic data. Large corpora of spoken language are difficult to create and hard to find, and are typically transcribed and written. In addition, linguists have turned to text-based discourse occurring in various formats of computer-mediated communication as a viable site for linguistic inquiry. The study of writing systems themselves, graphemics, is, in any case, considered a branch of linguistics. Analysis Before the 20th century, linguists analysed language on a diachronic plane, which was historical in focus. This meant that they would compare linguistic features and try to analyse language from the point of view of how it had changed between then and later. However, with the rise of Saussurean linguistics in the 20th century, the focus shifted to a more synchronic approach, where the study was geared towards analysis and comparison between different language variations, which existed at the same given point of time. At another level, the syntagmatic plane of linguistic analysis entails the comparison between the way words are sequenced, within the syntax of a sentence. For example, the article "the" is followed by a noun, because of the syntagmatic relation between the words. The paradigmatic plane, on the other hand, focuses on an analysis that is based on the paradigms or concepts that are embedded in a given text. In this case, words of the same type or class may be replaced in the text with each other to achieve the same conceptual understanding. History The earliest activities in the description of language have been attributed to the 6th-century BC Indian grammarian Pāṇini who composed a formal description of the Sanskrit language in his Aṣṭādhyāyī. Today, modern-day theories on grammar employ many of the principles that were laid down then. Nomenclature Before the 20th century, the term philology, first attested in 1716, was commonly used to refer to the study of language, which was then predominantly historical in focus. Since Ferdinand de Saussure's insistence on the importance of synchronic analysis, however, this focus has shifted and the term philology is now generally used for the "study of a language's grammar, history, and literary tradition", especially in the United States (where philology has never been very popularly considered as the "science of language"). Although the term linguist in the sense of "a student of language" dates from 1641, the term linguistics is first attested in 1847. It is now the usual term in English for the scientific study of language, though linguistic science is sometimes used. Linguistics is a multi-disciplinary field of research that combines tools from natural sciences, social sciences, formal sciences, and the humanities. Many linguists, such as David Crystal, conceptualize the field as being primarily scientific. The term linguist applies to someone who studies language or is a researcher within the field, or to someone who uses the tools of the discipline to describe and analyse specific languages. Early grammarians An early formal study of language was undertaken in India by the 6th-century BC grammarian Pāṇini, who formulated 3,959 rules of Sanskrit morphology. Pāṇini's systematic classification of the sounds of Sanskrit into consonants and vowels, and word classes, such as nouns and verbs, was the first known instance of its kind. In the Middle East, Sibawayh, a Persian, made a detailed description of Arabic in AD 760 in his monumental work, Al-kitab fii an-naħw (الكتاب في النحو, The Book on Grammar), the first known author to distinguish between sounds and phonemes (sounds as units of a linguistic system). Western interest in the study of languages began somewhat later than in the East, but the grammarians of the classical languages did not use the same methods or reach the same conclusions as their contemporaries in the Indic world. Early interest in language in the West was a part of philosophy, not of grammatical description. The first insights into semantic theory were made by Plato in his Cratylus dialogue, where he argues that words denote concepts that are eternal and exist in the world of ideas. This work is the first to use the word etymology to describe the history of a word's meaning. Around 280 BC, one of Alexander the Great's successors founded a university (see Musaeum) in Alexandria, where a school of philologists studied the ancient texts in Greek, and taught Greek to speakers of other languages. While this school was the first to use the word "grammar" in its modern sense, Plato had used the word in its original meaning as "téchnē grammatikḗ" (Τέχνη Γραμματική), the "art of writing", which is also the title of one of the most important works of the Alexandrine school by Dionysius Thrax. Throughout the Middle Ages, the study of language was subsumed under the topic of philology, the study of ancient languages and texts, practised by such educators as Roger Ascham, Wolfgang Ratke, and John Amos Comenius. Comparative philology In the 18th century, the first use of the comparative method by William Jones sparked the rise of comparative linguistics. Bloomfield attributes "the first great scientific linguistic work of the world" to Jacob Grimm, who wrote Deutsche Grammatik. It was soon followed by other authors writing similar comparative studies on other language groups of Europe. The study of language was broadened from Indo-European to language in general by Wilhelm von Humboldt, of whom Bloomfield asserts: This study received its foundation at the hands of the Prussian statesman and scholar Wilhelm von Humboldt (1767–1835), especially in the first volume of his work on Kavi, the literary language of Java, entitled Über die Verschiedenheit des menschlichen Sprachbaues und ihren Einfluß auf die geistige Entwickelung des Menschengeschlechts (On the Variety of the Structure of Human Language and its Influence upon the Mental Development of the Human Race). 20th-century developments There was a shift of focus from historical and comparative linguistics to synchronic analysis in early 20th century. Structural analysis was improved by Leonard Bloomfield, Louis Hjelmslev; and Zellig Harris who also developed methods of discourse analysis. Functional analysis was developed by the Prague linguistic circle and André Martinet. As sound recording devices became commonplace in the 1960s, dialectal recordings were made and archived, and the audio-lingual method provided a technological solution to foreign language learning. The 1960s also saw a new rise of comparative linguistics: the study of language universals in linguistic typology. Towards the end of the century the field of linguistics became divided into further areas of interest with the advent of language technology and digitalized corpora. Areas of research Sociolinguistics Sociolinguistics is the study of how language is shaped by social factors. This sub-discipline focuses on the synchronic approach of linguistics, and looks at how a language in general, or a set of languages, display variation and varieties at a given point in time. The study of language variation and the different varieties of language through dialects, registers, and idiolects can be tackled through a study of style, as well as through analysis of discourse. Sociolinguists research both style and discourse in language, as well as the theoretical factors that are at play between language and society. Developmental linguistics Developmental linguistics is the study of the development of linguistic ability in individuals, particularly the acquisition of language in childhood. Some of the questions that developmental linguistics looks into are how children acquire different languages, how adults can acquire a second language, and what the process of language acquisition is. Neurolinguistics Neurolinguistics is the study of the structures in the human brain that underlie grammar and communication. Researchers are drawn to the field from a variety of backgrounds, bringing along a variety of experimental techniques as well as widely varying theoretical perspectives. Much work in neurolinguistics is informed by models in psycholinguistics and theoretical linguistics, and is focused on investigating how the brain can implement the processes that theoretical and psycholinguistics propose are necessary in producing and comprehending language. Neurolinguists study the physiological mechanisms by which the brain processes information related to language, and evaluate linguistic and psycholinguistic theories, using aphasiology, brain imaging, electrophysiology, and computer modelling. Amongst the structures of the brain involved in the mechanisms of neurolinguistics, the cerebellum which contains the highest numbers of neurons has a major role in terms of predictions required to produce language. Applied linguistics Linguists are largely concerned with finding and describing the generalities and varieties both within particular languages and among all languages. Applied linguistics takes the results of those findings and "applies" them to other areas. Linguistic research is commonly applied to areas such as language education, lexicography, translation, language planning, which involves governmental policy implementation related to language use, and natural language processing. "Applied linguistics" has been argued to be something of a misnomer. Applied linguists actually focus on making sense of and engineering solutions for real-world linguistic problems, and not literally "applying" existing technical knowledge from linguistics. Moreover, they commonly apply technical knowledge from multiple sources, such as sociology (e.g., conversation analysis) and anthropology. (Constructed language fits under Applied linguistics.) Today, computers are widely used in many areas of applied linguistics. Speech synthesis and speech recognition use phonetic and phonemic knowledge to provide voice interfaces to computers. Applications of computational linguistics in machine translation, computer-assisted translation, and natural language processing are areas of applied linguistics that have come to the forefront. Their influence has had an effect on theories of syntax and semantics, as modelling syntactic and semantic theories on computers constraints. Linguistic analysis is a sub-discipline of applied linguistics used by many governments to verify the claimed nationality of people seeking asylum who do not hold the necessary documentation to prove their claim. This often takes the form of an interview by personnel in an immigration department. Depending on the country, this interview is conducted either in the asylum seeker's native language through an interpreter or in an international lingua franca like English. Australia uses the former method, while Germany employs the latter; the Netherlands uses either method depending on the languages involved. Tape recordings of the interview then undergo language analysis, which can be done either by private contractors or within a department of the government. In this analysis, linguistic features of the asylum seeker are used by analysts to make a determination about the speaker's nationality. The reported findings of the linguistic analysis can play a critical role in the government's decision on the refugee status of the asylum seeker. Language documentation Language documentation combines anthropological inquiry (into the history and culture of language) with linguistic inquiry, in order to describe languages and their grammars. Lexicography involves the documentation of words that form a vocabulary. Such a documentation of a linguistic vocabulary from a particular language is usually compiled in a dictionary. Computational linguistics is concerned with the statistical or rule-based modeling of natural language from a computational perspective. Specific knowledge of language is applied by speakers during the act of translation and interpretation, as well as in language education – the teaching of a second or foreign language. Policy makers work with governments to implement new plans in education and teaching which are based on linguistic research. Since the inception of the discipline of linguistics, linguists have been concerned with describing and analysing previously undocumented languages. Starting with Franz Boas in the early 1900s, this became the main focus of American linguistics until the rise of formal linguistics in the mid-20th century. This focus on language documentation was partly motivated by a concern to document the rapidly disappearing languages of indigenous peoples. The ethnographic dimension of the Boasian approach to language description played a role in the development of disciplines such as sociolinguistics, anthropological linguistics, and linguistic anthropology, which investigate the relations between language, culture, and society. The emphasis on linguistic description and documentation has also gained prominence outside North America, with the documentation of rapidly dying indigenous languages becoming a focus in some university programs in linguistics. Language description is a work-intensive endeavour, usually requiring years of field work in the language concerned, so as to equip the linguist to write a sufficiently accurate reference grammar. Further, the task of documentation requires the linguist to collect a substantial corpus in the language in question, consisting of texts and recordings, both sound and video, which can be stored in an accessible format within open repositories, and used for further research. Translation The sub-field of translation includes the translation of written and spoken texts across media, from digital to print and spoken. To translate literally means to transmute the meaning from one language into another. Translators are often employed by organizations such as travel agencies and governmental embassies to facilitate communication between two speakers who do not know each other's language. Translators are also employed to work within computational linguistics setups like Google Translate, which is an automated program to translate words and phrases between any two or more given languages. Translation is also conducted by publishing houses, which convert works of writing from one language to another in order to reach varied audiences. Cross-national and cross-cultural survey research studies employ translation to collect comparable data among multilingual populations. Academic translators specialize in or are familiar with various other disciplines such as technology, science, law, economics, etc. Clinical linguistics Clinical linguistics is the application of linguistic theory to the field of speech-language pathology. Speech language pathologists work on corrective measures to treat communication and swallowing disorders. Computational linguistics Computational linguistics is the study of linguistic issues in a way that is "computationally responsible", i.e., taking careful note of computational consideration of algorithmic specification and computational complexity, so that the linguistic theories devised can be shown to exhibit certain desirable computational properties and their implementations. Computational linguists also work on computer language and software development. Evolutionary linguistics Evolutionary linguistics is a sociobiological approach to analyzing the emergence of the language faculty through human evolution, and also the application of evolutionary theory to the study of cultural evolution among different languages. It is also a study of the dispersal of various languages across the globe, through movements among ancient communities. Forensic linguistics Forensic linguistics is the application of linguistic analysis to forensics. Forensic analysis investigates the style, language, lexical use, and other linguistic and grammatical features used in the legal context to provide evidence in courts of law. Forensic linguists have also used their expertise in the framework of criminal cases. See also References Bibliography External links The Linguist List, a global online linguistics community with news and information updated daily Glossary of Linguistic Terms Archived February 9, 2025, at the Wayback Machine by SIL Global (last revised 2003) Glottopedia, MediaWiki-based encyclopedia of linguistics, under construction Linguistic sub-fields – according to the Linguistic Society of America Linguistics and language-related wiki articles on Scholarpedia and Citizendium "Linguistics" section – A Bibliography of Literary Theory, Criticism and Philology, ed. J.A. García Landa (University of Zaragoza, Spain) Isac, Daniela; Charles Reiss (2013). I-language: An Introduction to Linguistics as Cognitive Science (2nd ed.). Oxford University Press. ISBN 978-0-19-953420-3.

Politics (from Ancient Greek πολιτικά (politiká) 'affairs of the cities') is the set of activities that are associated with making decisions in groups, or other forms of power relations among individuals, such as the distribution of status or resources. The branch of social science that studies politics and government is referred to as political science. Politics may be used positively in the context of a "political solution" which is compromising and non-violent, or descriptively as "the art or science of government", but the word often also carries a negative connotation. The concept has been defined in various ways, and different approaches have fundamentally differing views on whether it should be used extensively or in a limited way, empirically or normatively, and on whether conflict or co-operation is more essential to it. A variety of methods are deployed in politics, which include promoting one's own political views among people, negotiation with other political subjects, making laws, and exercising internal and external force, including warfare against adversaries. Politics is exercised on a wide range of social levels, from clans and tribes of traditional societies, through modern local governments, companies and institutions up to sovereign states, to the international level. In modern states, people often form political parties to represent their ideas. Members of a party often agree to take the same position on many issues and agree to support the same changes to law and the same leaders. An election is usually a competition between different parties. A political system is a framework which defines acceptable political methods within a society. The history of political thought can be traced back to early antiquity, with seminal works such as Plato's Republic, Aristotle's Politics, Confucius's political manuscripts and Chanakya's Arthashastra. Etymology The English word politics has its roots in the name of Aristotle's classic work, Politiká, which introduced the Ancient Greek term politiká (Πολιτικά, 'affairs of the cities'). In the mid-15th century, Aristotle's composition was rendered in Early Modern English as Polettiques [sic], which became Politics in Modern English. The singular politic first attested in English in 1430, coming from Middle French politique—itself taking from politicus, a Latinization of the Greek πολιτικός (politikos) from πολίτης (polites, 'citizen') and πόλις (polis, 'city'). Definitions Harold Lasswell: "who gets what, when, how" David Easton: "the authoritative allocation of values for a society" Vladimir Lenin: "the most concentrated expression of economics" Otto von Bismarck: "the capacity of always choosing at each instant, in constantly changing situations, the least harmful, the most useful" Bernard Crick: "a distinctive form of rule whereby people act together through institutionalized procedures to resolve differences" Adrian Leftwich: "comprises all the activities of co-operation, negotiation and conflict within and between societies" Approaches There are several ways in which approaching politics has been conceptualized. Extensive and limited Adrian Leftwich has differentiated views of politics based on how extensive or limited their perception of what accounts as 'political' is. The extensive view sees politics as present across the sphere of human social relations, while the limited view restricts it to certain contexts. For example, in a more restrictive way, politics may be viewed as primarily about governance, while a feminist perspective could argue that sites which have been viewed traditionally as non-political, should indeed be viewed as political as well. This latter position is encapsulated in the slogan "the personal is political", which disputes the distinction between private and public issues. Politics may also be defined by the use of power, as has been argued by Robert A. Dahl. Moralism and realism Some perspectives on politics view it empirically as an exercise of power, while others see it as a social function with a normative basis. This distinction has been called the difference between political moralism and political realism. For moralists, politics is closely linked to ethics, and is at its extreme in utopian thinking. For example, according to Hannah Arendt, the view of Aristotle was that, "to be political…meant that everything was decided through words and persuasion and not through violence"; while according to Bernard Crick, "politics is the way in which free societies are governed. Politics is politics, and other forms of rule are something else." In contrast, for realists, represented by those such as Niccolò Machiavelli, Thomas Hobbes, and Harold Lasswell, politics is based on the use of power, irrespective of the ends being pursued. Conflict and co-operation Agonism argues that politics essentially comes down to conflict between conflicting interests. Political scientist Elmer Schattschneider argued that "at the root of all politics is the universal language of conflict", while for Carl Schmitt the essence of politics is the distinction of 'friend' from 'foe'. This is in direct contrast to the more co-operative views of politics by Aristotle and Crick. However, a more mixed view between these extremes is provided by Irish political scientist Michael Laver, who noted that:Politics is about the characteristic blend of conflict and co-operation that can be found so often in human interactions. Pure conflict is war. Pure co-operation is true love. Politics is a mixture of both. History The history of politics spans human history and is not limited to modern institutions of government. Prehistoric Early human forms of social organization—bands and tribes—lacked centralized political structures. These are sometimes referred to as stateless societies. Early states In ancient history, civilizations did not have definite boundaries as states have today, and their borders could be more accurately described as frontiers. Early dynastic Sumer, and early dynastic Egypt were the first civilizations to define their borders. Moreover, up to the 12th century, many people lived in non-state societies. These range from relatively egalitarian bands and tribes to complex and highly stratified chiefdoms. State formation There are a number of different theories and hypotheses regarding early state formation that seek generalizations to explain why the state developed in some places but not others. Other scholars believe that generalizations are unhelpful and that each case of early state formation should be treated on its own. Voluntary theories contend that diverse groups of people came together to form states as a result of some shared rational interest. The theories largely focus on the development of agriculture, and the population and organizational pressure that followed and resulted in state formation. One of the most prominent theories of early and primary state formation is the hydraulic hypothesis, which contends that the state was a result of the need to build and maintain large-scale irrigation projects. Conflict theories of state formation regard conflict and dominance of some population over another population as key to the formation of states. In contrast with voluntary theories, these arguments believe that people do not voluntarily agree to create a state to maximize benefits, but that states form due to some form of oppression by one group over others. Some theories in turn argue that warfare was critical for state formation. Ancient history The first states of sorts were those of early dynastic Sumer and early dynastic Egypt, which arose from the Uruk period and Predynastic Egypt respectively around approximately 3000 BC. Early dynastic Egypt was based around the Nile River in the north-east of Africa, the kingdom's boundaries being based around the Nile and stretching to areas where oases existed. Early dynastic Sumer was located in southern Mesopotamia, with its borders extending from the Persian Gulf to parts of the Euphrates and Tigris rivers. Egyptians, Romans, and the Greeks were the first people known to have explicitly formulated a political philosophy of the state, and to have rationally analyzed political institutions. Prior to this, states were described and justified in terms of religious myths. Several important political innovations of classical antiquity came from the Greek city-states (polis) and the Roman Republic. The Greek city-states before the 4th century granted citizenship rights to their free population; in Athens these rights were combined with a directly democratic form of government that was to have a long afterlife in political thought and history. Modern states The Peace of Westphalia (1648) is considered by political scientists to be the beginning of the modern international system, in which external powers should avoid interfering in another country's domestic affairs. The principle of non-interference in other countries' domestic affairs was laid out in the mid-18th century by Swiss jurist Emer de Vattel. States became the primary institutional agents in an interstate system of relations. The Peace of Westphalia is said to have ended attempts to impose supranational authority on European states. The "Westphalian" doctrine of states as independent agents was bolstered by the rise in 19th century thought of nationalism, under which legitimate states were assumed to correspond to nations—groups of people united by language and culture. In Europe, during the 18th century, the classic non-national states were the multinational empires: the Austrian Empire, Kingdom of France, Kingdom of Hungary, the Russian Empire, the Spanish Empire, the Ottoman Empire, and the British Empire. Such empires also existed in Asia, Africa, and the Americas; in the Muslim world, immediately after the death of Muhammad in 632, Caliphates were established, which developed into multi-ethnic transnational empires. The multinational empire was an absolute monarchy ruled by a king, emperor or sultan. The population belonged to many ethnic groups, and they spoke many languages. The empire was dominated by one ethnic group, and their language was usually the language of public administration. The ruling dynasty was usually, but not always, from that group. Some of the smaller European states were not so ethnically diverse, but were also dynastic states, ruled by a royal house. A few of the smaller states survived, such as the independent principalities of Liechtenstein, Andorra, Monaco, and the republic of San Marino. Most theories see the nation state as a 19th-century European phenomenon, facilitated by developments such as state-mandated education, mass literacy, and mass media. However, historians also note the early emergence of a relatively unified state and identity in Portugal and the Dutch Republic. Scholars such as Steven Weber, David Woodward, Michel Foucault, and Jeremy Black have advanced the hypothesis that the nation state did not arise out of political ingenuity or an unknown undetermined source, nor was it an accident of history or political invention. Rather, the nation state is an inadvertent byproduct of 15th-century intellectual discoveries in political economy, capitalism, mercantilism, political geography, and geography combined with cartography and advances in map-making technologies. Some nation states, such as Germany and Italy, came into existence at least partly as a result of political campaigns by nationalists, during the 19th century. In both cases, the territory was previously divided among other states, some of them very small. Liberal ideas of free trade played a role in German unification, which was preceded by a customs union, the Zollverein. National self-determination was a key aspect of United States President Woodrow Wilson's Fourteen Points, leading to the dissolution of the Austro-Hungarian Empire and the Ottoman Empire after the First World War, while the Russian Empire became the Soviet Union after the Russian Civil War. Decolonization lead to the creation of new nation states in place of multinational empires in the Third World. Globalization Political globalization began in the 20th century through intergovernmental organizations and supranational unions. The League of Nations was founded after World War I, and after World War II it was replaced by the United Nations. Various international treaties have been signed through it. Regional integration has been pursued by the African Union, ASEAN, the European Union, and Mercosur. International political institutions on the international level include the International Criminal Court, the International Monetary Fund, and the World Trade Organization. Political science The study of politics is called political science. It comprises numerous subfields, namely three: Comparative politics, international relations and political philosophy. Political science is related to, and draws upon, the fields of economics, law, sociology, history, philosophy, geography, psychology, psychiatry, anthropology, and neurosciences. Comparative politics is the science of comparison and teaching of different types of constitutions, political actors, legislature and associated fields. International relations deals with the interaction between nation-states as well as intergovernmental and transnational organizations. Political philosophy is more concerned with contributions of various classical and contemporary thinkers and philosophers. Political science is methodologically diverse and appropriates many methods originating in psychology, social research, and cognitive neuroscience. Approaches include positivism, interpretivism, rational choice theory, behavioralism, structuralism, post-structuralism, realism, institutionalism, and pluralism. Political science, as one of the social sciences, uses methods and techniques that relate to the kinds of inquiries sought: primary sources such as historical documents and official records, secondary sources such as scholarly journal articles, survey research, statistical analysis, case studies, experimental research, and model building. Political system The political system defines the process for making official government decisions. It is usually compared to the legal system, economic system, cultural system, and other social systems. According to David Easton, "A political system can be designated as the interactions through which values are authoritatively allocated for a society." Each political system is embedded in a society with its own political culture, and they in turn shape their societies through public policy. The interactions between different political systems are the basis for global politics. Forms of government Forms of government can be classified by several ways. In terms of the structure of power, there are monarchies (including constitutional monarchies) and republics (usually presidential, semi-presidential, or parliamentary). The separation of powers describes the degree of horizontal integration between the legislature, the executive, the judiciary, and other independent institutions. Source of power The source of power determines the difference between democracies, oligarchies, and autocracies. In a democracy, political legitimacy is based on popular sovereignty. Forms of democracy include representative democracy, direct democracy, and demarchy. These are separated by the way decisions are made, whether by elected representatives, referendums, or by citizen juries. Democracies can be either republics or constitutional monarchies. Oligarchy is a power structure where a minority rules. These may be in the form of anocracy, aristocracy, ergatocracy, geniocracy, gerontocracy, kakistocracy, kleptocracy, meritocracy, noocracy, particracy, plutocracy, stratocracy, technocracy, theocracy, or timocracy. Autocracies are either dictatorships (including military dictatorships) or absolute monarchies. Vertical integration In terms of level of vertical integration, political systems can be divided into (from least to most integrated) confederations, federations, and unitary states. A federation (also known as a federal state) is a political entity characterized by a union of partially self-governing provinces, states, or other regions under a central federal government (federalism). In a federation, the self-governing status of the component states, as well as the division of power between them and the central government, is typically constitutionally entrenched and may not be altered by a unilateral decision of either party, the states or the federal political body. Federations were formed first in Switzerland, then in the United States in 1776, in Canada in 1867 and in Germany in 1871 and in 1901, Australia. Compared to a federation, a confederation has less centralized power. State All the above forms of government are variations of the same basic polity, the sovereign state. The state has been defined by Max Weber as a political entity that has monopoly on violence within its territory, while the Montevideo Convention holds that states need to have a defined territory; a permanent population; a government; and a capacity to enter into international relations. A stateless society is a society that is not governed by a state. In stateless societies, there is little concentration of authority; most positions of authority that do exist are very limited in power and are generally not permanently held positions; and social bodies that resolve disputes through predefined rules tend to be small. Stateless societies are highly variable in economic organization and cultural practices. While stateless societies were the norm in human prehistory, few stateless societies exist today; almost the entire global population resides within the jurisdiction of a sovereign state. In some regions nominal state authorities may be very weak and wield little or no actual power. Over the course of history most stateless peoples have been integrated into the state-based societies around them. Some political philosophies consider the state undesirable, and thus consider the formation of a stateless society a goal to be achieved. A central tenet of anarchism is the advocacy of society without states. The type of society sought for varies significantly between anarchist schools of thought, ranging from extreme individualism to complete collectivism. In Marxism, Marx's theory of the state considers that in a post-capitalist society the state, an undesirable institution, would be unnecessary and wither away. A related concept is that of stateless communism, a phrase sometimes used to describe Marx's anticipated post-capitalist society. Constitutions Constitutions are written documents that specify and limit the powers of the different branches of government. Although a constitution is a written document, there is also an unwritten constitution. The unwritten constitution is continually being written by the legislative and judiciary branch of government; this is just one of those cases in which the nature of the circumstances determines the form of government that is most appropriate. England did set the fashion of written constitutions during the Civil War but after the Restoration abandoned them to be taken up later by the American Colonies after their emancipation and then France after the Revolution and the rest of Europe including the European colonies. Constitutions often set out separation of powers, dividing the government into the executive, the legislature, and the judiciary (together referred to as the trias politica), in order to achieve checks and balances within the state. Additional independent branches may also be created, including civil service commissions, election commissions, and supreme audit institutions. Political culture Political culture describes how culture impacts politics. Every political system is embedded in a particular political culture. Lucian Pye's definition is that, "Political culture is the set of attitudes, beliefs, and sentiments, which give order and meaning to a political process and which provide the underlying assumptions and rules that govern behavior in the political system." Trust is a major factor in political culture, as its level determines the capacity of the state to function. Postmaterialism is the degree to which a political culture is concerned with issues which are not of immediate physical or material concern, such as human rights and environmentalism. Religion has also an impact on political culture. Political dysfunction Political corruption Political corruption is the use of powers for illegitimate private gain, conducted by government officials or their network contacts. Forms of political corruption include bribery, cronyism, nepotism, and political patronage. Forms of political patronage, in turn, includes clientelism, earmarking, pork barreling, slush funds, and spoils systems; as well as political machines, which is a political system that operates for corrupt ends. When corruption is embedded in political culture, this may be referred to as patrimonialism or neopatrimonialism. A form of government that is built on corruption is called a kleptocracy ('rule of thieves'). Insincere politics The words "politics" and "political" are sometimes used as pejoratives to mean political action that is deemed to be overzealous, performative, or insincere. Levels of politics Macropolitics Macropolitics can either describe political issues that affect an entire political system (e.g. the nation state), or refer to interactions between political systems (e.g. international relations). Global politics (or world politics) covers all aspects of politics that affect multiple political systems, in practice meaning any political phenomenon crossing national borders. This can include cities, nation-states, multinational corporations, non-governmental organizations or international organizations. An important element is international relations: the relations between nation-states may be peaceful when they are conducted through diplomacy, or they may be violent, which is described as war. States that are able to exert strong international influence are referred to as superpowers, whereas less-powerful ones may be called regional or middle powers. The international system of power is called the world order, which is affected by the balance of power that defines the degree of polarity in the system. Emerging powers are potentially destabilizing to it, especially if they display revanchism or irredentism. Politics inside the limits of political systems, which in contemporary context correspond to national borders, are referred to as domestic politics. This includes most forms of public policy, such as social policy, economic policy, or law enforcement, which are executed by the state bureaucracy. Mesopolitics Mesopolitics describes the politics of intermediary structures within a political system, such as national political parties or movements. A political party is a political organization that typically seeks to attain and maintain political power within government, usually by participating in political campaigns, educational outreach, or protest actions. Parties often espouse an expressed ideology or vision, bolstered by a written platform with specific goals, forming a coalition among disparate interests. Political parties within a particular political system together form the party system, which can be either multiparty, two-party, dominant-party, or one-party, depending on the level of pluralism. This is affected by characteristics of the political system, including its electoral system. According to Duverger's law, first-past-the-post systems are likely to lead to two-party systems, while proportional representation systems are more likely to create a multiparty system. Micropolitics Micropolitics describes the actions of individual actors within the political system. This is often described as political participation. Political participation may take many forms, including: Activism Boycott Civil disobedience Demonstration Petition Picketing Strike action Tax resistance Voting (or its opposite, abstentionism) Political values Democracy Democracy is a system of processing conflicts in which outcomes depend on what participants do, but no single force controls what occurs and its outcomes. The uncertainty of outcomes is inherent in democracy. Democracy makes all forces struggle repeatedly to realize their interests and devolves power from groups of people to sets of rules. Among modern political theorists, there are three contending conceptions of democracy: aggregative, deliberative, and radical. Aggregation The theory of aggregative democracy claims that the aim of the democratic processes is to solicit the preferences of citizens, and aggregate them together to determine what social policies the society should adopt. Therefore, proponents of this view hold that democratic participation should primarily focus on voting, where the policy with the most votes gets implemented. Different variants of aggregative democracy exist. Under minimalism, democracy is a system of government in which citizens have given teams of political leaders the right to rule in periodic elections. According to this minimalist conception, citizens cannot and should not "rule" because, for example, on most issues, most of the time, they have no clear views or their views are not well-founded. Joseph Schumpeter articulated this view most famously in his book Capitalism, Socialism, and Democracy. Contemporary proponents of minimalism include William H. Riker, Adam Przeworski, and Richard Posner. According to the theory of direct democracy, on the other hand, citizens should vote directly, not through their representatives, on legislative proposals. Proponents of direct democracy offer varied reasons to support this view. Political activity can be valuable in itself, it socialises and educates citizens, and popular participation can check powerful elites. Most importantly, citizens do not rule themselves unless they directly decide laws and policies. Governments will tend to produce laws and policies that are close to the views of the median voter—with half to their left and the other half to their right. This is not a desirable outcome as it represents the action of self-interested and somewhat unaccountable political elites competing for votes. Anthony Downs suggests that ideological political parties are necessary to act as a mediating broker between individual and governments. Downs laid out this view in his 1957 book An Economic Theory of Democracy. Polyarchy Robert A. Dahl argues that the fundamental democratic principle is that, when it comes to binding collective decisions, each person in a political community is entitled to have his/her interests be given equal consideration (not necessarily that all people are equally satisfied by the collective decision). He uses the term polyarchy to refer to societies in which there exists a certain set of institutions and procedures which are perceived as leading to such democracy. First and foremost among these institutions is the regular occurrence of free and open elections which are used to select representatives who then manage all or most of the public policy of the society. However, these polyarchic procedures may not create a full democracy if, for example, poverty prevents political participation. Similarly, Ronald Dworkin argues that "democracy is a substantive, not a merely procedural, ideal". Deliberation Deliberative democracy is based on the notion that democracy is government by deliberation. Unlike aggregative democracy, deliberative democracy holds that, for a democratic decision to be legitimate, it must be preceded by authentic deliberation, not merely the aggregation of preferences that occurs in voting. Authentic deliberation is deliberation among decision-makers that is free from distortions of unequal political power, such as power a decision-maker obtained through economic wealth or the support of interest groups. If the decision-makers cannot reach consensus after authentically deliberating on a proposal, then they vote on the proposal using a form of majority rule. Equality Equality is a state of affairs in which all people within a specific society or isolated group have the same social status, especially socioeconomic status, including protection of human rights and dignity, as well as access to certain social goods and social services. Furthermore, it may also include health equality, economic equality and other social securities. Social equality requires the absence of legally enforced social class or caste boundaries and the absence of discrimination based on by an inalienable aspect of a person's identity. To this end, there must be equal justice under law, and equal opportunity regardless of, sex, gender, ethnicity, age, sexual orientation, origin, caste or class, income or property, language, religion, convictions, opinions, health or disability. Left–right spectrum A common way of understanding politics is through the left–right political spectrum, which ranges from left-wing politics via centrism to right-wing politics. This classification is comparatively recent and dates from the French Revolution, when those members of the National Assembly who supported the republic, the common people and a secular society sat on the left and supporters of the monarchy, aristocratic privilege and the Church sat on the right. Today, the left is generally progressivist, seeking social progress in society. The more extreme elements of the left, named the far-left, tend to support revolutionary means for achieving this. This includes ideologies such as Communism and Marxism. The center-left, on the other hand, advocates for more reformist approaches, for example that of social democracy. In contrast, the right is generally motivated by conservatism, which seeks to conserve what it sees as the important elements of society such as law and order, limited government and preserving individual freedoms. The far-right goes beyond this, and often represents a reactionary turn against progress, seeking to undo it. Examples of such ideologies have included Fascism and Nazism. The center-right may be less clear-cut and more mixed in this regard, with neoconservatives supporting the spread of free markets and capitalism, and one-nation conservatives more open to social welfare programs. According to Norberto Bobbio, one of the major exponents of this distinction, the left believes in attempting to eradicate social inequality—believing it to be unethical or unnatural, while the right regards most social inequality as the result of ineradicable natural inequalities, and sees attempts to enforce social equality as utopian or authoritarian. Some ideologies, notably Christian Democracy, claim to combine left and right-wing politics; according to Geoffrey K. Roberts and Patricia Hogwood, "In terms of ideology, Christian Democracy has incorporated many of the views held by liberals, conservatives and socialists within a wider framework of moral and Christian principles." Movements which claim or formerly claimed to be above the left-right divide include Fascist Terza Posizione economic politics in Italy and Peronism in Argentina. Freedom Political freedom (also known as political liberty or autonomy) is a central concept in political thought and one of the most important features of democratic societies. Negative liberty has been described as freedom from oppression or coercion and unreasonable external constraints on action, often enacted through civil and political rights, while positive liberty is the absence of disabling conditions for an individual and the fulfillment of enabling conditions, e.g. economic compulsion, in a society. This capability approach to freedom requires economic, social and cultural rights in order to be realized. Authoritarianism and libertarianism Authoritarianism and libertarianism disagree the amount of individual freedom each person possesses in that society relative to the state. One author describes authoritarian political systems as those where "individual rights and goals are subjugated to group goals, expectations and conformities", while libertarians generally oppose the state and hold the individual as sovereign. In their purest form, libertarians are anarchists, who argue for the total abolition of the state, of political parties and of other political entities, while the purest authoritarians are, by definition, totalitarians who support state control over all aspects of society. For instance, classical liberalism (also known as laissez-faire liberalism) is a doctrine stressing individual freedom and limited government. This includes the importance of human rationality, individual property rights, free markets, natural rights, the protection of civil liberties, constitutional limitation of government, and individual freedom from restraint as exemplified in the writings of John Locke, Adam Smith, David Hume, David Ricardo, Voltaire, Montesquieu and others. According to the libertarian Institute for Humane Studies, "the libertarian, or 'classical liberal', perspective is that individual well-being, prosperity, and social harmony are fostered by 'as much liberty as possible' and 'as little government as necessary'." For anarchist political philosopher L. Susan Brown (1993), "liberalism and anarchism are two political philosophies that are fundamentally concerned with individual freedom yet differ from one another in very distinct ways. Anarchism shares with liberalism a radical commitment to individual freedom while rejecting liberalism's competitive property relations." See also Historic recurrence Horseshoe theory Index of politics articles – alphabetical list of political subjects List of banned political parties List of politics awards List of years in politics Outline of political science – structured list of political topics, arranged by subject area Political lists – lists of political topics Political censorship Political corruption Political polarization Political representation of nature Political scandal References Notes Citations Bibliography == Further reading ==

Law is a set of rules that are created and are enforceable by social or governmental institutions to regulate behavior, with its precise definition a matter of longstanding debate. It has been variously described as a science and as the art of justice. State-enforced laws can be made by a legislature, resulting in statutes; by the executive through decrees and regulations; or by judges' decisions, which form precedent in common law jurisdictions. An autocrat may exercise those functions within their realm. The creation of laws themselves may be influenced by a constitution, written or tacit, and the rights encoded therein. The law shapes politics, economics, history and society in various ways and also serves as a mediator of relations between people. Legal systems vary between jurisdictions, with their differences analysed in comparative law. In civil law jurisdictions, a legislature or other central body codifies and consolidates the law. In common law systems, judges may make binding case law through precedent, although on occasion this may be overturned by a higher court or the legislature. Religious law is in use in some religious communities and states, and has historically influenced secular law. The scope of law can be divided into two domains: public law concerns government and society, including constitutional law, administrative law, and criminal law; while private law deals with legal disputes between parties in areas such as contracts, property, torts, delicts and commercial law. This distinction is stronger in civil law countries, particularly those with a separate system of administrative courts; by contrast, the public-private law divide is less pronounced in common law jurisdictions. Law provides a source of scholarly inquiry into legal history, philosophy, economic analysis and sociology. Law also raises important and complex issues concerning equality, fairness, and justice. Etymology The word law, attested in Old English as lagu, comes from the Old Norse word lǫg. The singular form lag meant 'something laid or fixed' while its plural meant 'law'. Philosophy of law The philosophy of law is commonly known as jurisprudence. Normative jurisprudence asks "what should law be?", while analytic jurisprudence asks "what is law?" Analytical jurisprudence There have been several attempts to produce "a universally acceptable definition of law". In 1972, Baron Hampstead suggested that no such definition could be produced. McCoubrey and White said that the question "what is law?" has no simple answer. Glanville Williams said that the meaning of the word "law" depends on the context in which that word is used. He said that, for example, "early customary law" and "municipal law" were contexts where the word "law" had two different and irreconcilable meanings. Thurman Arnold said that it is obvious that it is impossible to define the word "law" and that it is also equally obvious that the struggle to define that word should not ever be abandoned. It is possible to take the view that there is no need to define the word "law" (e.g. "let's forget about generalities and get down to cases"). One definition is that law is a system of rules and guidelines which are enforced through social institutions to govern behaviour. In The Concept of Law, H. L. A. Hart argued that law is a "system of rules"; John Austin said law was "the command of a sovereign, backed by the threat of a sanction"; Ronald Dworkin describes law as an "interpretive concept" to achieve justice in his text titled Law's Empire; and Joseph Raz argues law is an "authority" to mediate people's interests. Oliver Wendell Holmes defined law as "the prophecies of what the courts will do in fact, and nothing more pretentious." In his Treatise on Law, Thomas Aquinas argues that law is a rational ordering of things, which concern the common good, that is promulgated by whoever is charged with the care of the community. This definition has both positivist and naturalist elements. Connection to morality and justice Definitions of law often raise the question of the extent to which law incorporates morality. John Austin's utilitarian answer was that law is "commands, backed by threat of sanctions, from a sovereign, to whom people have a habit of obedience". Natural lawyers, on the other hand, such as Jean-Jacques Rousseau, argue that law reflects essentially moral and unchangeable laws of nature. The concept of "natural law" emerged in ancient Greek philosophy concurrently and in connection with the notion of justice, and re-entered the mainstream of Western culture through the writings of Thomas Aquinas, notably his Treatise on Law. Hugo Grotius, the founder of a purely rationalistic system of natural law, argued that law arises from both a social impulse—as Aristotle had indicated—and reason. Immanuel Kant believed a moral imperative requires laws "be chosen as though they should hold as universal laws of nature". Jeremy Bentham and his student Austin, following David Hume, believed that this conflated the "is" and what "ought to be" problem. Bentham and Austin argued for law's positivism; that real law is entirely separate from "morality". Kant was also criticised by Friedrich Nietzsche, who rejected the principle of equality, and believed that law emanates from the will to power, and cannot be labeled as "moral" or "immoral". In 1934, the Austrian philosopher Hans Kelsen continued the positivist tradition in his book the Pure Theory of Law. Kelsen believed that although law is separate from morality, it is endowed with "normativity", meaning we ought to obey it. While laws are positive "is" statements (e.g. the fine for reversing on a highway is €500); law tells us what we "should" do. Thus, each legal system can be hypothesised to have a 'basic norm' (German: Grundnorm) instructing us to obey. Kelsen's major opponent, Carl Schmitt, rejected both positivism and the idea of the rule of law because he did not accept the primacy of abstract normative principles over concrete political positions and decisions. Therefore, Schmitt advocated a jurisprudence of the exception (state of emergency), which denied that legal norms could encompass all of the political experience. Later in the 20th century, H. L. A. Hart attacked Austin for his simplifications and Kelsen for his fiction in The Concept of Law. Hart argued law is a system of rules, divided into primary (rules of conduct) and secondary ones (rules addressed to officials to administer primary rules). Secondary rules are further divided into rules of adjudication (to resolve legal disputes), rules of change (allowing laws to be varied) and the rule of recognition (allowing laws to be identified as valid). Two of Hart's students continued the debate: In his book Law's Empire, Ronald Dworkin attacked Hart and the positivists for their refusal to treat law as a moral issue. Dworkin argues that law is an "interpretive concept" that requires judges to find the best fitting and most just solution to a legal dispute, given their Anglo-American constitutional traditions. Joseph Raz, on the other hand, defended the positivist outlook and criticised Hart's "soft social thesis" approach in The Authority of Law. Raz argues that law is authority, identifiable purely through social sources and without reference to moral reasoning. In his view, any categorisation of rules beyond their role as authoritative instruments in mediation is best left to sociology, rather than jurisprudence. History The history of law links closely to the development of civilization. Ancient Egyptian law, dating as far back as 3000 BC, was based on the concept of Ma'at and characterised by tradition, rhetorical speech, social equality and impartiality. By the 22nd century BC, the ancient Sumerian ruler Ur-Nammu had formulated the first law code, which consisted of casuistic statements ("if … then ..."). Around 1760 BC, King Hammurabi further developed Babylonian law, by codifying and inscribing it in stone. Hammurabi placed several copies of his law code throughout the kingdom of Babylon as stelae, for the entire public to see; this became known as the Codex Hammurabi. The most intact copy of these stelae was discovered in the 19th century by British Assyriologists, and has since been fully transliterated and translated into various languages, including English, Italian, German, and French. The Old Testament dates back to 1280 BC and takes the form of moral imperatives as recommendations for a good society. The small Greek city-state, ancient Athens, from about the 8th century BC was the first society to be based on broad inclusion of its citizenry, excluding women and enslaved people. However, Athens had no legal science or single word for "law", relying instead on the three-way distinction between divine law (thémis), human decree (nómos) and custom (díkē). Yet Ancient Greek law contained major constitutional innovations in the development of democracy. Roman law was heavily influenced by Greek philosophy, but its detailed rules were developed by professional jurists and were highly sophisticated. Over the centuries between the rise and decline of the Roman Empire, law was adapted to cope with the changing social situations and underwent major codification under Theodosius II and Justinian I. Although codes were replaced by custom and case law during the Early Middle Ages, Roman law was rediscovered around the 11th century when medieval legal scholars began to research Roman codes and adapt their concepts to the canon law, giving birth to the jus commune. Latin legal maxims (called brocards) were compiled for guidance. In medieval England, royal courts developed a body of precedent which later became the common law. A Europe-wide Law Merchant was formed so that merchants could trade with common standards of practice rather than with the many splintered facets of local laws. The Law Merchant, a precursor to modern commercial law, emphasised the freedom to contract and alienability of property. As nationalism grew in the 18th and 19th centuries, the Law Merchant was incorporated into countries' local law under new civil codes. The Napoleonic and German Codes became the most influential. In contrast to English common law, which consists of enormous tomes of case law, codes in small books are easy to export and easy for judges to apply. However, today there are signs that civil and common law are converging. EU law is codified in treaties, but develops through de facto precedent laid down by the European Court of Justice. Ancient India and China represent distinct traditions of law, and have historically had independent schools of legal theory and practice. The Arthashastra, probably compiled around 100 AD (although it contains older material), and the Manusmriti (c. 100–300 AD) were foundational treatises in India, and comprise texts considered authoritative legal guidance. Manu's central philosophy was tolerance and pluralism, and was cited across Southeast Asia. During the Muslim conquests in the Indian subcontinent, sharia was established by the Muslim sultanates and empires, most notably Mughal Empire's Fatawa-e-Alamgiri, compiled by emperor Aurangzeb and various scholars of Islam. In India, the Hindu legal tradition, along with Islamic law, were both supplanted by common law when India became part of the British Empire. Malaysia, Brunei, Singapore and Hong Kong also adopted the common law system. The Eastern Asia legal tradition reflects a unique blend of secular and religious influences. Japan was the first country to begin modernising its legal system along Western lines, by importing parts of the French, but mostly the German Civil Code. This partly reflected Germany's status as a rising power in the late 19th century. Similarly, traditional Chinese law gave way to westernisation towards the final years of the Qing Dynasty in the form of six private law codes based mainly on the Japanese model of German law. Today Taiwanese law retains the closest affinity to the codifications from that period, because of the split between Chiang Kai-shek's nationalists, who fled there, and Mao Zedong's communists who won control of the mainland in 1949. The current legal infrastructure in the People's Republic of China was heavily influenced by Soviet Socialist law, which essentially prioritises administrative law at the expense of private law rights. Due to rapid industrialisation, today China is undergoing a process of reform, at least in terms of economic, if not social and political, rights. A new contract code in 1999 represented a move away from administrative domination. Furthermore, after negotiations lasting fifteen years, in 2001 China joined the World Trade Organization. Legal systems In general, legal systems can be split between civil law and common law systems. Modern scholars argue that the significance of this distinction has progressively declined. The numerous legal transplants, typical of modern law, result in the sharing of many features traditionally considered typical of either common law or civil law. The third type of legal system is religious law, based on scriptures. The specific system that a country is ruled by is often determined by its history, connections with other countries, or its adherence to international standards. The sources that jurisdictions adopt as authoritatively binding are the defining features of any legal system. Civil law Civil law is the legal system used in most countries around the world today. In civil law the sources recognised as authoritative are, primarily, legislation—especially codifications in constitutions or statutes passed by government—and custom. Codifications date back millennia, with one early example being the Babylonian Codex Hammurabi. Modern civil law systems essentially derive from legal codes issued by Byzantine Emperor Justinian I in the 6th century, which were rediscovered by 11th century Italy. Roman law in the days of the Roman Republic and Empire was heavily procedural, and lacked a professional legal class. Instead a lay magistrate, iudex, was chosen to adjudicate. Decisions were not published in any systematic way, so any case law that developed was disguised and almost unrecognised. Each case was to be decided afresh from the laws of the State, which mirrors the (theoretical) unimportance of judges' decisions for future cases in civil law systems today. From 529 to 534 AD the Byzantine Emperor Justinian I codified and consolidated Roman law up until that point, so that what remained was one-twentieth of the mass of legal texts from before. This became known as the Corpus Juris Civilis. As one legal historian wrote, "Justinian consciously looked back to the golden age of Roman law and aimed to restore it to the peak it had reached three centuries before." The Justinian Code remained in force in the East until the fall of the Byzantine Empire. Western Europe, meanwhile, relied on a mix of the Theodosian Code and Germanic customary law until the Justinian Code was rediscovered in the 11th century, which scholars at the University of Bologna used to interpret their own laws. Civil law codifications based closely on Roman law, alongside some influences from religious laws such as canon law, continued to spread throughout Europe until the Enlightenment. Then, in the 19th century, both France, with the Code Civil, and Germany, with the Bürgerliches Gesetzbuch, modernised their legal codes. Both these codes heavily influenced not only the law systems of the countries in continental Europe but also the Japanese and Korean legal traditions. A central doctrine in continental European legal thinking, originating in German jurisprudence, is the cocpet of a Rechtsstaat, meaning that everyone is subjected to the law, especially governments. Today, countries that have civil law systems range from Russia and Turkey to most of Central and Latin America. Common law and equity In common law legal systems, decisions by courts are explicitly acknowledged as "law" on equal footing with legislative statutes and executive regulations. The "doctrine of precedent", or stare decisis (Latin for "to stand by decisions") means that decisions by higher courts bind lower courts to assure that similar cases reach similar results. In contrast, in civil law systems, legislative statutes are typically more detailed, and judicial decisions are shorter and less detailed because the adjudicator is only writing to decide the single case, rather than to set out reasoning that will guide future courts. Common law originated from England and has been inherited by almost every country once tied to the British Empire (except Malta, Scotland, the U.S. state of Louisiana, and the Canadian province of Quebec). In medieval England during the Norman Conquest, the law varied shire-to-shire based on disparate tribal customs. The concept of a "common law" developed during the reign of Henry II during the late 12th century, when Henry appointed judges who had the authority to create an institutionalised and unified system of law common to the country. The next major step in the evolution of the common law came when King John was forced by his barons to sign a document limiting his authority to pass laws. This "great charter" or Magna Carta of 1215 also required that the King's entourage of judges hold their courts and judgments at "a certain place" rather than dispensing autocratic justice in unpredictable places about the country. A concentrated and elite group of judges acquired a dominant role in law-making under this system, and compared to its European counterparts the English judiciary became highly centralised. In 1297, for instance, while the highest court in France had fifty-one judges, the English Court of Common Pleas had five. This powerful and tight-knit judiciary gave rise to a systematised process of developing common law. As time went on, many felt that the common law was overly systematised and inflexible, and increasing numbers of citizens petitioned the King to override the common law. On the King's behalf, the Lord Chancellor started giving judgments to do what was equitable in a case. From the time of Sir Thomas More, the first lawyer to be appointed as Lord Chancellor, a systematic body of equity grew up alongside the rigid common law, and developed its own Court of Chancery. At first, equity was often criticised as erratic. Over time, courts of equity developed solid principles, especially under Lord Eldon. In the 19th century in England, and in 1937 in the U.S., the two systems were merged. In developing the common law, academic writings have always played an important part, both to collect overarching principles from dispersed case law and to argue for change. William Blackstone, from around 1760, was the first scholar to collect, describe, and teach the common law. But merely in describing, scholars who sought explanations and underlying structures slowly changed the way the law actually worked. Religious law Religious law is explicitly based on religious precepts. Examples include the Jewish Halakha and Islamic Sharia—both of which translate as the "path to follow". Christian canon law also survives in some church communities. Often the implication of religion for law is unalterability because the word of God cannot be amended or legislated against by judges or governments. Nonetheless, most religious jurisdictions rely on further human elaboration to provide for thorough and detailed legal systems. For instance, the Quran has some law, and it acts as a source of further law through interpretation, Qiyas (reasoning by analogy), Ijma (consensus) and precedent. This is mainly contained in a body of law and jurisprudence known as Sharia and Fiqh respectively. Another example is the Torah or Old Testament, in the Pentateuch or Five Books of Moses. This contains the basic code of Jewish law, which some Israeli communities choose to use. The Halakha is a code of Jewish law that summarizes some of the Talmud's interpretations. A number of countries are sharia jurisdictions. Israeli law allows litigants to use religious laws only if they choose. Canon law is only in use by members of the Catholic Church, the Eastern Orthodox Church and the Anglican Communion. Canon law Canon law (Ancient Greek: κανών, romanized: kanon, lit. 'a straight measuring rod; a ruler') is a set of ordinances and regulations made by ecclesiastical authority, for the government of a Christian organisation or church and its members. It is the internal ecclesiastical law governing the Catholic Church, the Eastern Orthodox Church, the Oriental Orthodox Churches, and the individual national churches within the Anglican Communion. The way that such church law is legislated, interpreted and at times adjudicated varies widely among these three bodies of churches. In all three traditions, a canon was originally a rule adopted by a church council; these canons formed the foundation of canon law. The Catholic Church has the oldest continuously functioning legal system in the western world, predating the evolution of modern European civil law and common law systems. The 1983 Code of Canon Law governs the Latin Church sui juris. The Eastern Catholic Churches, which developed different disciplines and practices, are governed by the Code of Canons of the Eastern Churches. The canon law of the Catholic Church influenced the common law during the medieval period through its preservation of Roman law doctrine such as the presumption of innocence. Roman Catholic canon law is a fully developed legal system, with all the necessary elements: courts, lawyers, judges, a fully articulated legal code, principles of legal interpretation, and coercive penalties, though it lacks civilly-binding force in most secular jurisdictions. Sharia law Until the 18th century, Sharia law was practiced throughout the Muslim world in a non-codified form, with the Ottoman Empire's Mecelle code in the 19th century being a first attempt at codifying elements of Sharia law. Since the mid-1940s, efforts have been made, in country after country, to bring Sharia law more into line with modern conditions and conceptions. In modern times, the legal systems of many Muslim countries draw upon both civil and common law traditions as well as Islamic law and custom. The constitutions of certain Muslim states, such as Egypt and Afghanistan, recognise Islam as the religion of the state, obliging legislature to adhere to Sharia. Saudi Arabia recognises the Quran as its constitution, and is governed on the basis of Islamic law. Iran has also witnessed a reiteration of Islamic law into its legal system after 1979. During the last few decades, one of the fundamental features of the movement of Islamic resurgence has been the call to restore the Sharia, which has generated a vast amount of literature and affected world politics. Socialist law Socialist law is the legal systems in communist states such as the former Soviet Union and the People's Republic of China. Academic opinion is divided on whether it is a separate system from civil law, given major deviations based on Marxist–Leninist ideology, such as subordinating the judiciary to the executive ruling party. Legal methods There are distinguished methods of legal reasoning (applying the law) and methods of interpreting (construing) the law. The former are legal syllogism, which holds sway in civil law legal systems, analogy, which is present in common law legal systems, especially in the US, and argumentative theories that occur in both systems. The latter are different rules (directives) of legal interpretation such as directives of linguistic interpretation, teleological interpretation or systemic interpretation as well as more specific rules, for instance, golden rule or mischief rule. There are also many other arguments and cannons of interpretation which altogether make statutory interpretation possible. Law professor and former United States Attorney General Edward H. Levi noted that the "basic pattern of legal reasoning is reasoning by example"—that is, reasoning by comparing outcomes in cases resolving similar legal questions. In a U.S. Supreme Court case regarding procedural efforts taken by a debt collection company to avoid errors, Justice Sotomayor cautioned that "legal reasoning is not a mechanical or strictly linear process". Jurimetrics is the formal application of quantitative methods, especially probability and statistics, to legal questions. The use of statistical methods in court cases and law review articles has grown massively in importance in the last few decades. Legal institutions The main institutions of law in industrialised countries are independent courts, representative parliaments, an accountable executive, the military and police, bureaucratic organisation, the legal profession and civil society itself. John Locke, in his Two Treatises of Government, and Baron de Montesquieu in The Spirit of the Laws, advocated for a separation of powers between the political, legislature and executive bodies. Their principle was that no person should be able to usurp all powers of the state, in contrast to the absolutist theory of Thomas Hobbes' Leviathan. Sun Yat-sen's Five Power Constitution for the Republic of China took the separation of powers further by having two additional branches of government—a Control Yuan for auditing oversight and an Examination Yuan to manage the employment of public officials. Max Weber and others reshaped thinking on the extension of state. Modern military, policing and bureaucratic power over ordinary citizens' daily lives pose special problems for accountability that earlier writers such as Locke or Montesquieu could not have foreseen. The custom and practice of the legal profession is an important part of people's access to justice, whilst civil society is a term used to refer to the social institutions, communities and partnerships that form law's political basis. Judiciary Legislature Prominent examples of legislatures are the Houses of Parliament in London, the Congress in Washington, D.C., the Bundestag in Berlin, the Duma in Moscow, the Parlamento Italiano in Rome and the Assemblée nationale in Paris. By the principle of representative government people vote for politicians to carry out their wishes. Although countries like Israel, Greece, Sweden and China are unicameral, most countries are bicameral, meaning they have two separately appointed legislative houses. In the 'lower house' politicians are elected to represent smaller constituencies. The 'upper house' is usually elected to represent states in a federal system (as in Australia, Germany or the United States) or different voting configuration in a unitary system (as in France). In the UK the upper house is appointed by the government as a house of review. One criticism of bicameral systems with two elected chambers is that the upper and lower houses may simply mirror one another. The traditional justification of bicameralism is that an upper chamber acts as a house of review. This can minimise arbitrariness and injustice in governmental action. To pass legislation, a majority of the members of a legislature must vote for a bill (proposed law) in each house. Normally there will be several readings and amendments proposed by the different political factions. If a country has an entrenched constitution, a special majority for changes to the constitution may be required, making changes to the law more difficult. A government usually leads the process, which can be formed from Members of Parliament (e.g. the UK or Germany). However, in a presidential system, the government is usually formed by an executive and his or her appointed cabinet officials (e.g. the United States or Brazil). Executive The executive in a legal system serves as the centre of political authority of the State. In a parliamentary system, as with Britain, Italy, Germany, India, and Japan, the executive is known as the cabinet, and composed of members of the legislature. The executive is led by the head of government, whose office holds power under the confidence of the legislature. Because popular elections appoint political parties to govern, the leader of a party can change in between elections. The head of state is apart from the executive, and symbolically enacts laws and acts as representative of the nation. Examples include the President of Germany (appointed by members of federal and state legislatures), the Queen of the United Kingdom (an hereditary office), and the President of Austria (elected by popular vote). The other important model is the presidential system, found in the United States and in Brazil. In presidential systems, the executive acts as both head of state and head of government, and has power to appoint an unelected cabinet. Under a presidential system, the executive branch is separate from the legislature to which it is not accountable. Although the role of the executive varies from country to country, usually it will propose the majority of legislation, and propose government agenda. In presidential systems, the executive often has the power to veto legislation. Most executives in both systems are responsible for foreign relations, the law enforcement, and the bureaucracy. Ministers or other officials head a country's public offices, such as a foreign ministry or defence ministry. The election of a different executive is therefore capable of revolutionising an entire country's approach to government. Law enforcement Max Weber famously argued that the state is that which controls the monopoly on the legitimate use of force. The military and police carry out law enforcement at the request of the government or the courts. The term failed state refers to states that cannot implement or enforce policies; their police and military no longer control security and order and society moves into anarchy, the absence of government. While military organisations have existed as long as government itself, the idea of a standing police force is a relatively modern concept. For example, Medieval England's system of travelling criminal courts, or assizes, used show trials and public executions to instill communities with fear to maintain control. The first modern police were probably those in 17th-century Paris, in the court of Louis XIV, although the Paris Prefecture of Police claim they were the world's first uniformed policemen. Bureaucracy The etymology of bureaucracy derives from the French word for office (bureau) and the Ancient Greek for word power (kratos). Like the military and police, a legal system's government servants and bodies that make up its bureaucracy carry out the directives of the executive. One of the earliest references to the concept was made by Baron de Grimm, a German author who lived in France. In 1765, he wrote: The real spirit of the laws in France is that bureaucracy of which the late Monsieur de Gournay used to complain so greatly; here the offices, clerks, secretaries, inspectors and intendants are not appointed to benefit the public interest, indeed the public interest appears to have been established so that offices might exist. Cynicism over "officialdom" is still common, and the workings of public servants is typically contrasted to private enterprise motivated by profit. In fact private companies, especially large ones, also have bureaucracies. Negative perceptions of "red tape" aside, public services such as schooling, health care, policing or public transport are considered a crucial state function making public bureaucratic action the locus of government power. Writing in the early 20th century, Max Weber believed that a definitive feature of a developed state had come to be its bureaucratic support. Weber wrote that the typical characteristics of modern bureaucracy are that officials define its mission, the scope of work is bound by rules, and management is composed of career experts who manage top down, communicating through writing and binding public servants' discretion with rules. Legal profession A corollary of the rule of law is the existence of a legal profession sufficiently autonomous to invoke the authority of the independent judiciary; the right to assistance of a barrister in a court proceeding emanates from this corollary—in England the function of barrister or advocate is distinguished from legal counselor. As the European Court of Human Rights has stated, the law should be adequately accessible to everyone and people should be able to foresee how the law affects them. In order to maintain professionalism, the practice of law is typically overseen by either a government or independent regulating body such as a bar association, bar council or law society. Modern lawyers achieve distinct professional identity through specified legal procedures (e.g. successfully passing a qualifying examination), are required by law to have a special qualification (a legal education earning the student a Bachelor of Laws, a Bachelor of Civil Law, or a Juris Doctor degree. Higher academic degrees may also be pursued. Examples include a Master of Laws, a Master of Legal Studies, a Bar Professional Training Course or a Doctor of Laws.), and are constituted in office by legal forms of appointment (being admitted to the bar). There are few titles of respect to signify famous lawyers, such as Esquire, to indicate barristers of greater dignity, and Doctor of law, to indicate a person who obtained a PhD in Law. Many Muslim countries have developed similar rules about legal education and the legal profession, but some still allow lawyers with training in traditional Islamic law to practice law before personal status law courts. In China and other developing countries there are not sufficient professionally trained people to staff the existing judicial systems, and, accordingly, formal standards are more relaxed. Once accredited, a lawyer will often work in a law firm, in a chambers as a sole practitioner, in a government post or in a private corporation as an internal counsel. In addition a lawyer may become a legal researcher who provides on-demand legal research through a library, a commercial service or freelance work. Many people trained in law put their skills to use outside the legal field entirely. Significant to the practice of law in the common law tradition is the legal research to determine the current state of the law. This usually entails exploring case-law reports, legal periodicals and legislation. Law practice also involves drafting documents such as court pleadings, persuasive briefs, contracts, or wills and trusts. Negotiation and dispute resolution skills (including ADR techniques) are also important to legal practice, depending on the field. Civil society The Classical republican concept of "civil society" dates back to Hobbes and Locke. Locke saw civil society as people who have "a common established law and judicature to appeal to, with authority to decide controversies between them." German philosopher Georg Wilhelm Friedrich Hegel distinguished the "state" from "civil society" (German: bürgerliche Gesellschaft) in Elements of the Philosophy of Right. Hegel believed that civil society and the state were polar opposites, within the scheme of his dialectic theory of history. The modern dipole state–civil society was reproduced in the theories of Alexis de Tocqueville and Karl Marx. In post-modern theory, civil society is necessarily a source of law, by being the basis from which people form opinions and lobby for what they believe law should be. As Australian barrister and author Geoffrey Robertson QC wrote of international law, "one of its primary modern sources is found in the responses of ordinary men and women, and of the non-governmental organizations which many of them support, to the human rights abuses they see on the television screen in their living rooms." Freedom of speech, freedom of association and many other individual rights allow people to gather, discuss, criticise and hold to account their governments, from which the basis of a deliberative democracy is formed. The more people are involved with, concerned by and capable of changing how political power is exercised over their lives, the more acceptable and legitimate the law becomes to the people. The most familiar institutions of civil society include economic markets, profit-oriented firms, families, trade unions, hospitals, universities, schools, charities, debating clubs, non-governmental organisations, neighbourhoods, churches, and religious associations. There is no clear legal definition of the civil society, and of the institutions it includes. Most of the institutions and bodies who try to give a list of institutions (such as the European Economic and Social Committee) exclude the political parties. Areas of law All legal systems deal with the same basic issues, but jurisdictions categorise and identify their legal topics in different ways. A common distinction is that between "public law" (a term related closely to the state, and including constitutional, administrative and criminal law), and "private law" (which covers contract, tort and property). In civil law systems, contract and tort fall under a general law of obligations, while trusts law is dealt with under statutory regimes or international conventions. International, constitutional and administrative law, criminal law, contract, tort, property law and trusts are regarded as the "traditional core subjects", although there are many further disciplines. International law Constitutional and administrative law Constitutional and administrative law govern the affairs of the state. Constitutional law concerns both the relationships between the executive, legislature and judiciary and the human rights or civil liberties of individuals against the state. Most jurisdictions, like the United States and France, have a single codified constitution with a bill of rights. A few, like the United Kingdom, have no such document. A "constitution" is simply those laws which constitute the body politic, from statute, case law and convention. The fundamental constitutional principle, inspired by John Locke, holds that the individual can do anything except that which is forbidden by law, and the state may do nothing except that which is authorised by law. Administrative law is the chief method for people to hold state bodies to account. People (wheresoever allowed) may potentially have prerogative to legally challenge (or sue) an agency, local council, public service, or government ministry for judicial review of the offending edict (law, ordinance, policy order). Such challenge vets the ability of actionable authority under the law, and that the government entity observed required procedure. The first specialist administrative court was the Conseil d'État set up in 1799, as Napoleon assumed power in France. A sub-discipline of constitutional law is election law. It along with Elections commissions, councils, or committees deal with policy and procedures facilitating elections. These rules settle disputes or enable the translation of the will of the people into functioning democracies. Election law addresses issues who is entitled to vote, voter registration, ballot access, campaign finance and party funding, redistricting, apportionment, electronic voting and voting machines, accessibility of elections, election systems and formulas, vote counting, election disputes, referendums, and issues such as electoral fraud and electoral silence. Criminal law Criminal law, also known as penal law, pertains to crimes and punishment. It thus regulates the definition of and penalties for offences found to have a sufficiently deleterious social impact but, in itself, makes no moral judgment on an offender nor imposes restrictions on society that physically prevent people from committing a crime in the first place. Investigating, apprehending, charging, and trying suspected offenders is regulated by the law of criminal procedure. The paradigm case of a crime lies in the proof, beyond reasonable doubt, that a person is guilty of two things. First, the accused must commit an act which is deemed by society to be criminal, or actus reus (guilty act). Second, the accused must have the requisite malicious intent to do a criminal act, or mens rea (guilty mind). However, for so called "strict liability" crimes, an actus reus is enough. Criminal systems of the civil law tradition distinguish between intention in the broad sense (dolus directus and dolus eventualis), and negligence. Negligence does not carry criminal responsibility unless a particular crime provides for its punishment. Examples of crimes include murder, assault, fraud and theft. In exceptional circumstances defences can apply to specific acts, such as killing in self defence, or pleading insanity. Another example is in the 19th-century English case of R v Dudley and Stephens, which tested whether a defence of "necessity" could justify murder and cannibalism to survive a shipwreck. Criminal law offences are viewed as offences against not just individual victims, but the community as well. The state, usually with the help of police, takes the lead in prosecution, which is why in common law countries cases are cited as "The People v ..." or "R (for Rex or Regina) v ...". Also, lay juries are often used to determine the guilt of defendants on points of fact: juries cannot change legal rules. Some developed countries still condone capital punishment for criminal activity, but the normal punishment for a crime will be imprisonment, fines, state supervision (such as probation), or community service. Modern criminal law has been affected considerably by the social sciences, especially with respect to sentencing, legal research, legislation, and rehabilitation. On the international field, 111 countries are members of the International Criminal Court, which was established to try people for crimes against humanity. Contract law Contract law concerns enforceable promises, and can be summed up in the Latin phrase pacta sunt servanda (agreements must be kept). In common law jurisdictions, three key elements to the creation of a contract are necessary: offer and acceptance, consideration and the intention to create legal relations. Consideration indicates the fact that all parties to a contract have exchanged something of value. Some common law systems, including Australia, are moving away from the idea of consideration as a requirement. The idea of estoppel or culpa in contrahendo, can be used to create obligations during pre-contractual negotiations. Civil law jurisdictions treat contracts differently in a number of respects, with a more interventionist role for the state in both the formation and enforcement of contracts. Compared to common law jurisdictions, civil law systems incorporate more mandatory terms into contracts, allow greater latitude for courts to interpret and revise contract terms and impose a stronger duty of good faith, but are also more likely to enforce penalty clauses and specific performance of contracts. They also do not require consideration for a contract to be binding. In France, an ordinary contract is said to form simply on the basis of a "meeting of the minds" or a "concurrence of wills". Germany has a special approach to contracts, which ties into property law. Their 'abstraction principle' (Abstraktionsprinzip) means that the personal obligation of contract forms separately from the title of property being conferred. When contracts are invalidated for some reason (e.g. a car buyer is so drunk that he lacks legal capacity to contract) the contractual obligation to pay can be invalidated separately from the proprietary title of the car. Unjust enrichment law, rather than contract law, is then used to restore title to the rightful owner. Torts and delicts Certain civil wrongs are grouped together as torts under common law systems and delicts under civil law systems. To have acted tortiously, one must have breached a duty to another person, or infringed some pre-existing legal right. A simple example might be unintentionally hitting someone with a ball. Under the law of negligence, the most common form of tort, the injured party could potentially claim compensation for their injuries from the party responsible. The principles of negligence are illustrated by Donoghue v Stevenson. A friend of Donoghue ordered an opaque bottle of ginger beer (intended for the consumption of Donoghue) in a café in Paisley. Having consumed half of it, Donoghue poured the remainder into a tumbler. The decomposing remains of a snail floated out. She claimed to have suffered from shock, fell ill with gastroenteritis and sued the manufacturer for carelessly allowing the drink to be contaminated. The House of Lords decided that the manufacturer was liable for Mrs Donoghue's illness. Lord Atkin took a distinctly moral approach and said: The liability for negligence [...] is no doubt based upon a general public sentiment of moral wrongdoing for which the offender must pay. [...] The rule that you are to love your neighbour becomes in law, you must not injure your neighbour; and the lawyer's question, Who is my neighbour? receives a restricted reply. You must take reasonable care to avoid acts or omissions which you can reasonably foresee would be likely to injure your neighbour. This became the basis for the four principles of negligence, namely that: Stevenson owed Donoghue a duty of care to provide safe drinks; he breached his duty of care; the harm would not have occurred but for his breach; and his act was the proximate cause of her harm. Another example of tort might be a neighbour making excessively loud noises with machinery on his property. Under a nuisance claim the noise could be stopped. Torts can also involve intentional acts such as assault, battery or trespass. A better known tort is defamation, which occurs, for example, when a newspaper makes unsupportable allegations that damage a politician's reputation. More infamous are economic torts, which form the basis of labour law in some countries by making trade unions liable for strikes, when statute does not provide immunity. Property law Property law governs ownership and possession. Real property, sometimes called 'real estate', refers to ownership of land and things attached to it. Personal property, refers to everything else; movable objects, such as computers, cars, jewelry or intangible rights, such as stocks and shares. A right in rem is a right to a specific piece of property, contrasting to a right in personam which allows compensation for a loss, but not a particular thing back. Land law forms the basis for most kinds of property law, and is the most complex. It concerns mortgages, rental agreements, licences, covenants, easements and the statutory systems for land registration. Regulations on the use of personal property fall under intellectual property, company law, trusts and commercial law. A representative example of property law is the 1722 suit of Armory v Delamirie, applying English law. A child was deprived of possession of the gemstones that had been set in piece of jewellery, by the businessperson entrusted to appraise the piece. The court articulated that, according to the view of property in common law jurisdictions, the person who can show the best claim to a piece of property, against any contesting party, is the owner. By contrast, the classic civil law approach to property, propounded by Friedrich Carl von Savigny, is that it is a right good against the world. Obligations, like contracts and torts, are conceptualised as rights good between individuals. The idea of property raises many further philosophical and political issues. Locke argued that our "lives, liberties and estates" are our property because we own our bodies and mix our labour with our surroundings. Trusts In historical English law, the common law did not permit dividing the ownership from the control of one piece of property—but the law of equity did recognize this through an arrangement known as a trust. Trustees control property whereas the beneficial, or equitable, ownership of trust property is held by people known as beneficiaries. Trustees owe duties to their beneficiaries to take good care of the entrusted property. Another example of a trustee's duty might be to invest property wisely or sell it. This is especially the case for pension funds, the most important form of trust, where investors are trustees for people's savings until retirement. But trusts can also be set up for charitable purposes. Some international norms for the structure and regulation of trusts are set out in the Hague Trust Convention of 1985. Intersection with other fields Economics In the 18th century, Adam Smith presented a philosophical foundation for explaining the relationship between law and economics. The discipline arose partly out of a critique of trade unions and U.S. antitrust law. The most prominent economic analyst of law is Ronald Coase, whose first major article, The Nature of the Firm (1937), argued that the reason for the existence of firms (companies, partnerships, etc.) is the existence of transaction costs. Rational individuals trade through bilateral contracts on open markets until the costs of transactions mean that using corporations to produce things is more cost-effective. His second major article, The Problem of Social Cost (1960), argued that if we lived in a world without transaction costs, people would bargain with one another to create the same allocation of resources, regardless of the way a court might rule in property disputes. He contended that law ought to be pre-emptive, and be guided by the most efficient solution. Many members of the so-called Chicago School are generally advocates of deregulation and privatisation, and are hostile to state regulation or what they see as restrictions on the operation of free markets. Sociology The sociology of law examines the interaction of law with society and overlaps with jurisprudence, philosophy of law, social theory and more specialised subjects such as criminology. It is a transdisciplinary and multidisciplinary study focused on the theorisation and empirical study of legal practices and experiences as social phenomena. The institutions of social construction, social norms, dispute processing and legal culture are key areas for inquiry in this knowledge field. In the United States, the field is usually called law and society studies; in Europe, it is more often referred to as socio-legal studies. At first, jurists and legal philosophers were suspicious of sociology of law. Kelsen attacked one of its founders, Eugen Ehrlich, who sought to make clear the differences and connections between positive law, which lawyers learn and apply, and other forms of 'law' or social norms that regulate everyday life, generally preventing conflicts from reaching lawyers and courts. Contemporary research in the sociology of law is concerned with the way that law develops outside discrete state jurisdictions, being produced through social interaction in social arenas, and acquiring a diversity of sources of authority in national and transnational communal networks. Around 1900, Max Weber defined his "scientific" approach to law, identifying the "legal rational form" as a type of domination, not attributable to personal authority but to the authority of abstract norms. Formal legal rationality was his term for the key characteristic of the kind of coherent and calculable law that was a precondition for modern political developments and the modern bureaucratic state. Weber saw this law as having developed in parallel with the growth of capitalism. Another sociologist, Émile Durkheim, wrote in his classic work The Division of Labour in Society that as society becomes more complex, the body of civil law concerned primarily with restitution and compensation grows at the expense of criminal laws and penal sanctions. Other notable early legal sociologists included Hugo Sinzheimer, Theodor Geiger, Georges Gurvitch and Leon Petrażycki in Europe, and William Graham Sumner in the U.S. See also By-law Justice Law dictionary Legal industry by country Legal research in the United States Legal treatise Outline of law Political science Pseudolaw Public interest law Social law Sources of law Translating "law" to other European languages Notes References Bibliography Further reading "House of Lords Judgments". House of Lords. Archived from the original on 10 November 2006. Retrieved 10 November 2006. Opinions of the Supreme Court of the United States "law". Law.com Dictionary. Archived from the original on 5 January 2009. Retrieved 10 February 2007. "law". Online Etymology Dictionary. Archived from the original on 2 July 2017. Retrieved 9 February 2007. "legal". Merriam-Webster's Online Dictionary. Archived from the original on 26 December 2005. Retrieved 9 February 2007. External links DRAGNET: Search of free legal databases from New York Law School. Archived 3 September 2013 at the Wayback Machine. World Legal Information Institute Commonwealth Legal Information Institute Asian Legal Information Institute Australasian Legal Information Institute British and Irish Legal Information Institute Canadian Legal Information Institute (archived 4 October 2006) New Zealand Legal Information Institute Pacific Islands Legal Information Institute Southern African Legal Information Institute

Economics () is a behavioral science that studies the production, distribution, and consumption of goods and services. Economics focuses on the behaviour and interactions of economic agents and how economies work. Microeconomics analyses what is viewed as basic elements within economies, including individual agents and markets, their interactions, and the outcomes of interactions. Individual agents may include, for example, households, firms, buyers, and sellers. Macroeconomics analyses economies as systems where production, distribution, consumption, savings, and investment expenditure interact; and the factors of production affecting them, such as: labour, capital, land, and enterprise, inflation, economic growth, and public policies that impact these elements. It also seeks to analyse and describe the global economy. Other broad distinctions within economics include those between positive economics, describing "what is", and normative economics, advocating "what ought to be"; between economic theory and applied economics; between rational and behavioural economics; and between mainstream economics and heterodox economics. Economic analysis can be applied throughout society, including business, finance, cybersecurity, health care, engineering and government. It is also applied to such diverse subjects as crime, education, the family, feminism, law, philosophy, politics, religion, social institutions, war, science, and the environment. Definitions of economics The earlier term for the discipline was "political economy", but since the late 19th century, it has commonly been called "economics". The term is ultimately derived from Ancient Greek οἰκονομία (oikonomia) which is a term for the "way (nomos) to run a household (oikos)", or in other words the know-how of an οἰκονομικός (oikonomikos), or "household or homestead manager". Derived terms such as "economy" can therefore often mean "frugal" or "thrifty". By extension then, "political economy" was the way to manage a polis or state. There are a variety of modern definitions of economics; some reflect evolving views of the subject or different views among economists. Scottish philosopher Adam Smith (1776) defined what was then called political economy as "an inquiry into the nature and causes of the wealth of nations", in particular as: a branch of the science of a statesman or legislator [with the twofold objectives of providing] a plentiful revenue or subsistence for the people ... [and] to supply the state or commonwealth with a revenue for the public services. Jean-Baptiste Say (1803), distinguishing the subject matter from its public-policy uses, defined it as the science of production, distribution, and consumption of wealth. On the satirical side, Thomas Carlyle (1849) coined "the dismal science" as an epithet for classical economics, in this context, commonly linked to the pessimistic analysis of Malthus (1798). John Stuart Mill (1844) delimited the subject matter further: The science which traces the laws of such of the phenomena of society as arise from the combined operations of mankind for the production of wealth, in so far as those phenomena are not modified by the pursuit of any other object. Alfred Marshall provided a still widely cited definition in his textbook Principles of Economics (1890) that extended analysis beyond wealth and from the societal to the microeconomic level: Economics is a study of man in the ordinary business of life. It enquires how he gets his income and how he uses it. Thus, it is on the one side, the study of wealth and on the other and more important side, a part of the study of man. Lionel Robbins (1932) developed implications of what has been termed "[p]erhaps the most commonly accepted current definition of the subject": Economics is the science which studies human behaviour as a relationship between ends and scarce means which have alternative uses. Robbins described the definition as not classificatory in "pick[ing] out certain kinds of behaviour" but rather analytical in "focus[ing] attention on a particular aspect of behaviour, the form imposed by the influence of scarcity." He affirmed that previous economists have usually centred their studies on the analysis of wealth: how wealth is created (production), distributed, and consumed; and how wealth can grow. But he said that economics can be used to study other things, such as war, that are outside its usual focus. This is because war has as the goal winning it (as a sought-after end), generates both cost and benefits; and, resources (human life and other costs) are used to attain the goal. If the war is not winnable or if the expected costs outweigh the benefits, the deciding actors (assuming they are rational) may never go to war (a decision) but rather explore other alternatives. Economics cannot be defined as the science that studies wealth, war, crime, education, and any other field economic analysis can be applied to; but, as the science that studies a particular common aspect of each of those subjects (they all use scarce resources to attain a sought-after end). Some subsequent comments criticised the definition as overly broad in failing to limit its subject matter to analysis of markets. From the 1960s, however, such comments abated as the economic theory of maximizing behaviour and rational-choice modelling expanded the domain of the subject to areas previously treated in other fields. There are other criticisms as well, such as in scarcity not accounting for the macroeconomics of high unemployment. Gary Becker, a contributor to the expansion of economics into new areas, described the approach he favoured as "combin[ing the] assumptions of maximizing behaviour, stable preferences, and market equilibrium, used relentlessly and unflinchingly." One commentary characterises the remark as making economics an approach rather than a subject matter but with great specificity as to the "choice process and the type of social interaction that [such] analysis involves." The same source reviews a range of definitions included in principles of economics textbooks and concludes that the lack of agreement need not affect the subject-matter that the texts treat. Among economists more generally, it argues that a particular definition presented may reflect the direction toward which the author believes economics is evolving, or should evolve. Many economists including Nobel Prize winners James M. Buchanan and Ronald Coase reject the method-based definition of Robbins and continue to prefer definitions like those of Say, in terms of its subject matter. Ha-Joon Chang has for example argued that the definition of Robbins would make economics very peculiar because all other sciences define themselves in terms of the area of inquiry or object of inquiry rather than the methodology. In the biology department, it is not said that all biology should be studied with DNA analysis. People study living organisms in many different ways, so some people will perform DNA analysis, others might analyse anatomy, and still others might build game theoretic models of animal behaviour. But they are all called biology because they all study living organisms. According to Ha Joon Chang, this view that the economy can and should be studied in only one way (for example by studying only rational choices), and going even one step further and basically redefining economics as a theory of everything, is peculiar. History of economic thought From antiquity through the physiocrats Questions regarding distribution of resources are found throughout the writings of the Boeotian poet Hesiod and several economic historians have described him as the "first economist". However, the Greek word oikos was used for issues regarding how to manage a household (which was understood to be the landowner, his family, and his slaves) rather than to refer to some normative societal system of distribution of resources, which is a far more recent phenomenon. Although Xenophon, the author of the Oeconomicus, is credited by philologues as the source of the word "economy", modern scholarship often credits Aristotle as the first author writing on economics proper in some scattered passages, particularly in the Nicomachean Ethics, where the topic of use value vs exchange value is discussed. Joseph Schumpeter described 16th and 17th century scholastic writers, including Tomás de Mercado, Luis de Molina, and Juan de Lugo, as "coming nearer than any other group to being the 'founders' of scientific economics" as to monetary, interest, and value theory within a natural-law perspective. Two groups, who later were called "mercantilists" and "physiocrats", more directly influenced the subsequent development of the subject. Both groups were associated with the rise of economic nationalism and modern capitalism in Europe. Mercantilism was an economic doctrine that flourished from the 16th to 18th century in a prolific pamphlet literature, whether of merchants or statesmen. It held that a nation's wealth depended on its accumulation of gold and silver. Nations without access to mines could obtain gold and silver from trade only by selling goods abroad and restricting imports other than of gold and silver. The doctrine called for importing inexpensive raw materials to be used in manufacturing goods, which could be exported, and for state regulation to impose protective tariffs on foreign manufactured goods and prohibit manufacturing in the colonies. Physiocrats, a group of 18th-century French thinkers and writers, developed the idea of the economy as a circular flow of income and output. Physiocrats believed that only agricultural production generated a clear surplus over cost, so that agriculture was the basis of all wealth. Thus, they opposed the mercantilist policy of promoting manufacturing and trade at the expense of agriculture, including import tariffs. Physiocrats advocated replacing administratively costly tax collections with a single tax on income of land owners. In reaction against copious mercantilist trade regulations, the physiocrats advocated a policy of laissez-faire, which called for minimal government intervention in the economy. Adam Smith (1723–1790) was an early economic theorist. Smith was harshly critical of the mercantilists but described the physiocratic system "with all its imperfections" as "perhaps the purest approximation to the truth that has yet been published" on the subject. Classical political economy The publication of Adam Smith's The Wealth of Nations in 1776, has been described as "the effective birth of economics as a separate discipline." The book identified land, labour, and capital as the three factors of production and the major contributors to a nation's wealth, as distinct from the physiocratic idea that only agriculture was productive. Smith discusses potential benefits of specialisation by division of labour, including increased labour productivity and gains from trade, whether between town and country or across countries. His "theorem" that "the division of labor is limited by the extent of the market" has been described as the "core of a theory of the functions of firm and industry" and a "fundamental principle of economic organization." To Smith has also been ascribed "the most important substantive proposition in all of economics" and foundation of resource-allocation theory—that, under competition, resource owners (of labour, land, and capital) seek their most profitable uses, resulting in an equal rate of return for all uses in equilibrium (adjusted for apparent differences arising from such factors as training and unemployment). In an argument that includes "one of the most famous passages in all economics," Smith represents every individual as trying to employ any capital they might command for their own advantage, not that of the society, and for the sake of profit, which is necessary at some level for employing capital in domestic industry, and positively related to the value of produce. In this: He generally, indeed, neither intends to promote the public interest, nor knows how much he is promoting it. By preferring the support of domestic to that of foreign industry, he intends only his own security; and by directing that industry in such a manner as its produce may be of the greatest value, he intends only his own gain, and he is in this, as in many other cases, led by an invisible hand to promote an end which was no part of his intention. Nor is it always the worse for the society that it was no part of it. By pursuing his own interest he frequently promotes that of the society more effectually than when he really intends to promote it. The Reverend Thomas Robert Malthus (1798) used the concept of diminishing returns to explain low living standards. Human population, he argued, tended to increase geometrically, outstripping the production of food, which increased arithmetically. The force of a rapidly growing population against a limited amount of land meant diminishing returns to labour. The result, he claimed, was chronically low wages, which prevented the standard of living for most of the population from rising above the subsistence level. Economist Julian Simon has criticised Malthus's conclusions. While Adam Smith emphasised production and income, David Ricardo (1817) focused on the distribution of income among landowners, workers, and capitalists. Ricardo saw an inherent conflict between landowners on the one hand and labour and capital on the other. He posited that the growth of population and capital, pressing against a fixed supply of land, pushes up rents and holds down wages and profits. Ricardo was also the first to state and prove the principle of comparative advantage, according to which each country should specialise in producing and exporting goods in that it has a lower relative cost of production, rather relying only on its own production. It has been termed a "fundamental analytical explanation" for gains from trade. Coming at the end of the classical tradition, John Stuart Mill (1848) parted company with the earlier classical economists on the inevitability of the distribution of income produced by the market system. Mill pointed to a distinct difference between the market's two roles: allocation of resources and distribution of income. The market might be efficient in allocating resources but not in distributing income, he wrote, making it necessary for society to intervene. Value theory was important in classical theory. Smith wrote that the "real price of every thing ... is the toil and trouble of acquiring it". Smith maintained that, with rent and profit, other costs besides wages also enter the price of a commodity. Other classical economists presented variations on Smith, termed the 'labour theory of value'. Classical economics focused on the tendency of any market economy to settle in a final stationary state made up of a constant stock of physical wealth (capital) and a constant population size. Marxian economics Marxist (later, Marxian) economics descends from classical economics and it derives from the work of Karl Marx. The first volume of Marx's major work, Das Kapital, was published in 1867. Marx focused on the labour theory of value and theory of surplus value. Marx wrote that they were mechanisms used by capital to exploit labour. The labour theory of value held that the value of an exchanged commodity was determined by the labour that went into its production, and the theory of surplus value demonstrated how workers were only paid a proportion of the value their work had created. Marxian economics was further developed by Karl Kautsky (1854–1938)'s The Economic Doctrines of Karl Marx and The Class Struggle (Erfurt Program), Rudolf Hilferding's (1877–1941) Finance Capital, Vladimir Lenin (1870–1924)'s The Development of Capitalism in Russia and Imperialism, the Highest Stage of Capitalism, and Rosa Luxemburg (1871–1919)'s The Accumulation of Capital. Neoclassical economics At its inception as a social science, economics was defined and discussed at length as the study of production, distribution, and consumption of wealth by Jean-Baptiste Say in his Treatise on Political Economy or, The Production, Distribution, and Consumption of Wealth (1803). These three items were considered only in relation to the increase or diminution of wealth, and not in reference to their processes of execution. Say's definition has survived in part up to the present, modified by substituting the word "wealth" for "goods and services" meaning that wealth may include non-material objects as well. One hundred and thirty years later, Lionel Robbins noticed that this definition no longer sufficed, because many economists were making theoretical and philosophical inroads in other areas of human activity. In his Essay on the Nature and Significance of Economic Science, he proposed a definition of economics as a study of human behaviour, subject to and constrained by scarcity, which forces people to choose, allocate scarce resources to competing ends, and economise (seeking the greatest welfare while avoiding the wasting of scarce resources). According to Robbins: "Economics is the science which studies human behavior as a relationship between ends and scarce means which have alternative uses". Robbins' definition eventually became widely accepted by mainstream economists, and found its way into current textbooks. Although far from unanimous, most mainstream economists would accept some version of Robbins' definition, even though many have raised serious objections to the scope and method of economics, emanating from that definition. A body of theory later termed "neoclassical economics" formed from about 1870 to 1910. The term "economics" was popularised by such neoclassical economists as Alfred Marshall and Mary Paley Marshall as a concise synonym for "economic science" and a substitute for the earlier "political economy". This corresponded to the influence on the subject of mathematical methods used in the natural sciences. Neoclassical economics systematically integrated supply and demand as joint determinants of both price and quantity in market equilibrium, influencing the allocation of output and income distribution. It rejected the classical economics' labour theory of value in favour of a marginal utility theory of value on the demand side and a more comprehensive theory of costs on the supply side. In the 20th century, neoclassical theorists departed from an earlier idea that suggested measuring total utility for a society, opting instead for ordinal utility, which posits behaviour-based relations across individuals. In microeconomics, neoclassical economics represents incentives and costs as playing a pervasive role in shaping decision making. An immediate example of this is the consumer theory of individual demand, which isolates how prices (as costs) and income affect quantity demanded. In macroeconomics it is reflected in an early and lasting neoclassical synthesis with Keynesian macroeconomics. Neoclassical economics is occasionally referred as orthodox economics whether by its critics or sympathisers. Modern mainstream economics builds on neoclassical economics but with many refinements that either supplement or generalise earlier analysis, such as econometrics, game theory, analysis of market failure and imperfect competition, and the neoclassical model of economic growth for analysing long-run variables affecting national income. Neoclassical economics studies the behaviour of individuals, households, and organisations (called economic actors, players, or agents), when they manage or use scarce resources, which have alternative uses, to achieve desired ends. Agents are assumed to act rationally, have multiple desirable ends in sight, limited resources to obtain these ends, a set of stable preferences, a definite overall guiding objective, and the capability of making a choice. There exists an economic problem, subject to study by economic science, when a decision (choice) is made by one or more players to attain the best possible outcome. Keynesian economics Keynesian economics derives from John Maynard Keynes, in particular his book The General Theory of Employment, Interest and Money (1936), which ushered in contemporary macroeconomics as a distinct field. The book focused on determinants of national income in the short run when prices are relatively inflexible. Keynes attempted to explain in broad theoretical detail why high labour-market unemployment might not be self-correcting due to low "effective demand" and why even price flexibility and monetary policy might be unavailing. The term "revolutionary" has been applied to the book in its impact on economic analysis. During the following decades, many economists followed Keynes' ideas and expanded on his works. John Hicks and Alvin Hansen developed the IS–LM model which was a simple formalisation of some of Keynes' insights on the economy's short-run equilibrium. Franco Modigliani and James Tobin developed important theories of private consumption and investment, respectively, two major components of aggregate demand. Lawrence Klein built the first large-scale macroeconometric model, applying the Keynesian thinking systematically to the US economy. Post-WWII economics Immediately after World War II, Keynesian was the dominant economic view of the United States establishment and its allies, Marxian economics was the dominant economic view of the Soviet Union nomenklatura and its allies. Monetarism Monetarism appeared in the 1950s and 1960s, its intellectual leader being Milton Friedman. Monetarists contended that monetary policy and other monetary shocks, as represented by the growth in the money stock, was an important cause of economic fluctuations, and consequently that monetary policy was more important than fiscal policy for purposes of stabilisation. Friedman was also skeptical about the ability of central banks to conduct a sensible active monetary policy in practice, advocating instead using simple rules such as a steady rate of money growth. Monetarism rose to prominence in the 1970s and 1980s, when several major central banks followed a monetarist-inspired policy, but was later abandoned because the results were unsatisfactory. New classical economics A more fundamental challenge to the prevailing Keynesian paradigm came in the 1970s from new classical economists like Robert Lucas, Thomas Sargent and Edward Prescott. They introduced the notion of rational expectations in economics, which had profound implications for many economic discussions, among which were the so-called Lucas critique and the presentation of real business cycle models. New Keynesians During the 1980s, a group of researchers appeared being called New Keynesian economists, including among others George Akerlof, Janet Yellen, Gregory Mankiw and Olivier Blanchard. They adopted the principle of rational expectations and other monetarist or new classical ideas such as building upon models employing micro foundations and optimizing behaviour, but simultaneously emphasised the importance of various market failures for the functioning of the economy, as had Keynes. Not least, they proposed various reasons that potentially explained the empirically observed features of price and wage rigidity, usually made to be endogenous features of the models, rather than simply assumed as in older Keynesian-style ones. New neoclassical synthesis After decades of often heated discussions between Keynesians, monetarists, new classical and new Keynesian economists, a synthesis emerged by the 2000s, often given the name the new neoclassical synthesis. It integrated the rational expectations and optimizing framework of the new classical theory with a new Keynesian role for nominal rigidities and other market imperfections like imperfect information in goods, labour and credit markets. The monetarist importance of monetary policy in stabilizing the economy and in particular controlling inflation was recognised as well as the traditional Keynesian insistence that fiscal policy could also play an influential role in affecting aggregate demand. Methodologically, the synthesis led to a new class of applied models, known as dynamic stochastic general equilibrium or DSGE models, descending from real business cycles models, but extended with several new Keynesian and other features. These models proved useful and influential in the design of modern monetary policy and are now standard workhorses in most central banks. After the 2008 financial crisis After the 2008 financial crisis, macroeconomic research has put greater emphasis on understanding and integrating the financial system into models of the general economy and shedding light on the ways in which problems in the financial sector can turn into major macroeconomic recessions. In this and other research branches, inspiration from behavioural economics has started playing a more important role in mainstream economic theory. Also, heterogeneity among the economic agents, e.g. differences in income, plays an increasing role in recent economic research. Other schools and approaches Other schools or trends of thought referring to a particular style of economics practised at and disseminated from well-defined groups of academicians that have become known worldwide, include the Freiburg School, the School of Lausanne, the Stockholm school and the Chicago school of economics. During the 1970s and 1980s mainstream economics was sometimes separated into the Saltwater approach of those universities along the Eastern and Western coasts of the US, and the Freshwater, or Chicago school approach. Within macroeconomics there is, in general order of their historical appearance in the literature; classical economics, neoclassical economics, Keynesian economics, the neoclassical synthesis, monetarism, new classical economics, New Keynesian economics and the new neoclassical synthesis. Beside the mainstream development of economic thought, various alternative or heterodox economic theories have evolved over time, positioning themselves in contrast to mainstream theory. These include: Austrian School, emphasizing human action, property rights and the freedom to contract and transact to have a thriving and successful economy. It also emphasises that the state should play as small role as possible (if any role) in the regulation of economic activity between two transacting parties. Friedrich Hayek and Ludwig von Mises are the two most prominent representatives of the Austrian school. Post-Keynesian economics concentrates on macroeconomic rigidities and adjustment processes. It is generally associated with the University of Cambridge and the work of Joan Robinson. Ecological economics like environmental economics studies the interactions between human economies and the ecosystems in which they are embedded, but in contrast to environmental economics takes an oppositional position towards general mainstream economic principles. A major difference between the two subdisciplines is their assumptions about the substitution possibilities between human-made and natural capital. Additionally, alternative developments include Marxian economics, constitutional economics, institutional economics, evolutionary economics, dependency theory, structuralist economics, world systems theory, econophysics, econodynamics, feminist economics and biophysical economics. Feminist economics emphasises the role that gender plays in economies, challenging analyses that render gender invisible or support gender-oppressive economic systems. The goal is to create economic research and policy analysis that is inclusive and gender-aware to encourage gender equality and improve the well-being of marginalised groups. Methodology Theoretical research Mainstream economic theory relies upon analytical economic models. When creating theories, the objective is to find assumptions which are at least as simple in information requirements, more precise in predictions, and more fruitful in generating additional research than prior theories. While neoclassical economic theory constitutes both the dominant or orthodox theoretical as well as methodological framework, economic theory can also take the form of other schools of thought such as in heterodox economic theories. In microeconomics, principal concepts include supply and demand, marginalism, rational choice theory, opportunity cost, budget constraints, utility, and the theory of the firm. Early macroeconomic models focused on modelling the relationships between aggregate variables, but as the relationships appeared to change over time macroeconomists, including new Keynesians, reformulated their models with microfoundations, in which microeconomic concepts play a major part. Sometimes an economic hypothesis is only qualitative, not quantitative. Expositions of economic reasoning often use two-dimensional graphs to illustrate theoretical relationships. At a higher level of generality, mathematical economics is the application of mathematical methods to represent theories and analyse problems in economics. Paul Samuelson's treatise Foundations of Economic Analysis (1947) exemplifies the method, particularly as to maximizing behavioural relations of agents reaching equilibrium. The book focused on examining the class of statements called operationally meaningful theorems in economics, which are theorems that can conceivably be refuted by empirical data. Empirical research Economic theories are frequently tested empirically, largely through the use of econometrics using economic data. The controlled experiments common to the physical sciences are difficult and uncommon in economics, and instead broad data is observationally studied; this type of testing is typically regarded as less rigorous than controlled experimentation, and the conclusions typically more tentative. However, the field of experimental economics is growing, and increasing use is being made of natural experiments. Statistical methods such as regression analysis are common. Practitioners use such methods to estimate the size, economic significance, and statistical significance ("signal strength") of the hypothesised relation(s) and to adjust for noise from other variables. By such means, a hypothesis may gain acceptance, although in a probabilistic, rather than certain, sense. Acceptance is dependent upon the falsifiable hypothesis surviving tests. Use of commonly accepted methods need not produce a final conclusion or even a consensus on a particular question, given different tests, data sets, and prior beliefs. Experimental economics has promoted the use of scientifically controlled experiments. This has reduced the long-noted distinction of economics from natural sciences because it allows direct tests of what were previously taken as axioms. In some cases these have found that the axioms are not entirely correct. In behavioural economics, psychologist Daniel Kahneman won the Nobel Prize in economics in 2002 for his and Amos Tversky's empirical discovery of several cognitive biases and heuristics. Similar empirical testing occurs in neuroeconomics. Another example is the assumption of narrowly selfish preferences versus a model that tests for selfish, altruistic, and cooperative preferences. These techniques have led some to argue that economics is a "genuine science". Microeconomics Microeconomics examines how entities, forming a market structure, interact within a market to create a market system. These entities include private and public players with various classifications, typically operating under scarcity of tradable units and regulation. The item traded may be a tangible product such as apples or a service such as repair services, legal counsel, or entertainment. Various market structures exist. In perfectly competitive markets, no participants are large enough to have the market power to set the price of a homogeneous product. In other words, every participant is a "price taker" as no participant influences the price of a product. In the real world, markets often experience imperfect competition. Forms of imperfect competition include monopoly (in which there is only one seller of a good), duopoly (in which there are only two sellers of a good), oligopoly (in which there are few sellers of a good), monopolistic competition (in which there are many sellers producing highly differentiated goods), monopsony (in which there is only one buyer of a good), and oligopsony (in which there are few buyers of a good). Firms under imperfect competition have the potential to be "price makers", which means that they can influence the prices of their products. In partial equilibrium method of analysis, it is assumed that activity in the market being analysed does not affect other markets. This method aggregates (the sum of all activity) in only one market. General-equilibrium theory studies various markets and their behaviour. It aggregates (the sum of all activity) across all markets. This method studies both changes in markets and their interactions leading towards equilibrium. Production, cost, and efficiency In microeconomics, production is the conversion of inputs into outputs. It is an economic process that uses inputs to create a commodity or a service for exchange or direct use. Production is a flow and thus a rate of output per period of time. Distinctions include such production alternatives as for consumption (food, haircuts, etc.) vs. investment goods (new tractors, buildings, roads, etc.), public goods (national defence, smallpox vaccinations, etc.) or private goods, and "guns" vs "butter". Inputs used in the production process include such primary factors of production as labour services, capital (durable produced goods used in production, such as an existing factory), and land (including natural resources). Other inputs may include intermediate goods used in production of final goods, such as the steel in a new car. Economic efficiency measures how well a system generates desired output with a given set of inputs and available technology. Efficiency is improved if more output is generated without changing inputs. A widely accepted general standard is Pareto efficiency, which is reached when no further change can make someone better off without making someone else worse off. The production–possibility frontier (PPF) is an expository figure for representing scarcity, cost, and efficiency. In the simplest case, an economy can produce just two goods (say "guns" and "butter"). The PPF is a table or graph (as at the right) that shows the different quantity combinations of the two goods producible with a given technology and total factor inputs, which limit feasible total output. Each point on the curve shows potential total output for the economy, which is the maximum feasible output of one good, given a feasible output quantity of the other good. Scarcity is represented in the figure by people being willing but unable in the aggregate to consume beyond the PPF (such as at X) and by the negative slope of the curve. If production of one good increases along the curve, production of the other good decreases, an inverse relationship. This is because increasing output of one good requires transferring inputs to it from production of the other good, decreasing the latter. The slope of the curve at a point on it gives the trade-off between the two goods. It measures what an additional unit of one good costs in units forgone of the other good, an example of a real opportunity cost. Thus, if one more Gun costs 100 units of butter, the opportunity cost of one Gun is 100 Butter. Along the PPF, scarcity implies that choosing more of one good in the aggregate entails doing with less of the other good. Still, in a market economy, movement along the curve may indicate that the choice of the increased output is anticipated to be worth the cost to the agents. By construction, each point on the curve shows productive efficiency in maximizing output for given total inputs. A point inside the curve (as at A), is feasible but represents production inefficiency (wasteful use of inputs), in that output of one or both goods could increase by moving in a northeast direction to a point on the curve. Examples cited of such inefficiency include high unemployment during a business-cycle recession or economic organisation of a country that discourages full use of resources. Being on the curve might still not fully satisfy allocative efficiency (also called Pareto efficiency) if it does not produce a mix of goods that consumers prefer over other points. Much applied economics in public policy is concerned with determining how the efficiency of an economy can be improved. Recognizing the reality of scarcity and then figuring out how to organise society for the most efficient use of resources has been described as the "essence of economics", where the subject "makes its unique contribution." Specialisation Specialisation is considered key to economic efficiency based on theoretical and empirical considerations. Different individuals or nations may have different real opportunity costs of production, say from differences in stocks of human capital per worker or capital/labour ratios. According to theory, this may give a comparative advantage in production of goods that make more intensive use of the relatively more abundant, thus relatively cheaper, input. Even if one region has an absolute advantage as to the ratio of its outputs to inputs in every type of output, it may still specialise in the output in which it has a comparative advantage and thereby gain from trading with a region that lacks any absolute advantage but has a comparative advantage in producing something else. It has been observed that a high volume of trade occurs among regions even with access to a similar technology and mix of factor inputs, including high-income countries. This has led to investigation of economies of scale and agglomeration to explain specialisation in similar but differentiated product lines, to the overall benefit of respective trading parties or regions. The general theory of specialisation applies to trade among individuals, farms, manufacturers, service providers, and economies. Among each of these production systems, there may be a corresponding division of labour with different work groups specializing, or correspondingly different types of capital equipment and differentiated land uses. An example that combines features above is a country that specialises in the production of high-tech knowledge products, as developed countries do, and trades with developing nations for goods produced in factories where labour is relatively cheap and plentiful, resulting in different in opportunity costs of production. More total output and utility thereby results from specializing in production and trading than if each country produced its own high-tech and low-tech products. Theory and observation set out the conditions such that market prices of outputs and productive inputs select an allocation of factor inputs by comparative advantage, so that (relatively) low-cost inputs go to producing low-cost outputs. In the process, aggregate output may increase as a by-product or by design. Such specialisation of production creates opportunities for gains from trade whereby resource owners benefit from trade in the sale of one type of output for other, more highly valued goods. A measure of gains from trade is the increased income levels that trade may facilitate. Supply and demand Prices and quantities have been described as the most directly observable attributes of goods produced and exchanged in a market economy. The theory of supply and demand is an organizing principle for explaining how prices coordinate the amounts produced and consumed. In microeconomics, it applies to price and output determination for a market with perfect competition, which includes the condition of no buyers or sellers large enough to have price-setting power. For a given market of a commodity, demand is the relation of the quantity that all buyers would be prepared to purchase at each unit price of the good. Demand is often represented by a table or a graph showing price and quantity demanded (as in the figure). Demand theory describes individual consumers as rationally choosing the most preferred quantity of each good, given income, prices, tastes, etc. A term for this is "constrained utility maximisation" (with income and wealth as the constraints on demand). Here, utility refers to the hypothesised relation of each individual consumer for ranking different commodity bundles as more or less preferred. The law of demand states that, in general, price and quantity demanded in a given market are inversely related. That is, the higher the price of a product, the less of it people would be prepared to buy (other things unchanged). As the price of a commodity falls, consumers move toward it from relatively more expensive goods (the substitution effect). In addition, purchasing power from the price decline increases ability to buy (the income effect). Other factors can change demand; for example an increase in income will shift the demand curve for a normal good outward relative to the origin, as in the figure. All determinants are predominantly taken as constant factors of demand and supply. Supply is the relation between the price of a good and the quantity available for sale at that price. It may be represented as a table or graph relating price and quantity supplied. Producers, for example business firms, are hypothesised to be profit maximisers, meaning that they attempt to produce and supply the amount of goods that will bring them the highest profit. Supply is typically represented as a function relating price and quantity, if other factors are unchanged. That is, the higher the price at which the good can be sold, the more of it producers will supply, as in the figure. The higher price makes it profitable to increase production. Just as on the demand side, the position of the supply can shift, say from a change in the price of a productive input or a technical improvement. The "Law of Supply" states that, in general, a rise in price leads to an expansion in supply and a fall in price leads to a contraction in supply. Here as well, the determinants of supply, such as price of substitutes, cost of production, technology applied and various factors inputs of production are all taken to be constant for a specific time period of evaluation of supply. Market equilibrium occurs where quantity supplied equals quantity demanded, the intersection of the supply and demand curves in the figure above. At a price below equilibrium, there is a shortage of quantity supplied compared to quantity demanded. This is posited to bid the price up. At a price above equilibrium, there is a surplus of quantity supplied compared to quantity demanded. This pushes the price down. The model of supply and demand predicts that for given supply and demand curves, price and quantity will stabilise at the price that makes quantity supplied equal to quantity demanded. Similarly, demand-and-supply theory predicts a new price-quantity combination from a shift in demand (as to the figure), or in supply. Firms People frequently do not trade directly on markets. Instead, on the supply side, they may work in and produce through firms. The most obvious kinds of firms are corporations, partnerships and trusts. According to Ronald Coase, people begin to organise their production in firms when the costs of doing business becomes lower than doing it on the market. Firms combine labour and capital, and can achieve far greater economies of scale (when the average cost per unit declines as more units are produced) than individual market trading. In perfectly competitive markets studied in the theory of supply and demand, there are many producers, none of which significantly influence price. Industrial organisation generalises from that special case to study the strategic behaviour of firms that do have significant control of price. It considers the structure of such markets and their interactions. Common market structures studied besides perfect competition include monopolistic competition, various forms of oligopoly, and monopoly. Managerial economics applies microeconomic analysis to specific decisions in business firms or other management units. It draws heavily from quantitative methods such as operations research and programming and from statistical methods such as regression analysis in the absence of certainty and perfect knowledge. A unifying theme is the attempt to optimise business decisions, including unit-cost minimisation and profit maximisation, given the firm's objectives and constraints imposed by technology and market conditions. Uncertainty and game theory Uncertainty in economics is an unknown prospect of gain or loss, whether quantifiable as risk or not. Without it, household behaviour would be unaffected by uncertain employment and income prospects, financial and capital markets would reduce to exchange of a single instrument in each market period, and there would be no communications industry. Given its different forms, there are various ways of representing uncertainty and modelling economic agents' responses to it. Game theory is a branch of applied mathematics that considers strategic interactions between agents, one kind of uncertainty. It provides a mathematical foundation of industrial organisation, discussed above, to model different types of firm behaviour, for example in a solipsistic industry (few sellers), but equally applicable to wage negotiations, bargaining, contract design, and any situation where individual agents are few enough to have perceptible effects on each other. In behavioural economics, it has been used to model the strategies agents choose when interacting with others whose interests are at least partially adverse to their own. In this, it generalises maximisation approaches developed to analyse market actors such as in the supply and demand model and allows for incomplete information of actors. The field dates from the 1944 classic Theory of Games and Economic Behavior by John von Neumann and Oskar Morgenstern. It has significant applications seemingly outside of economics in such diverse subjects as the formulation of nuclear strategies, ethics, political science, and evolutionary biology. Risk aversion may stimulate activity that in well-functioning markets smooths out risk and communicates information about risk, as in markets for insurance, commodity futures contracts, and financial instruments. Financial economics or simply finance describes the allocation of financial resources. It also analyses the pricing of financial instruments, the financial structure of companies, the efficiency and fragility of financial markets, financial crises, and related government policy or regulation. Some market organisations may give rise to inefficiencies associated with uncertainty. Based on George Akerlof's "Market for Lemons" article, the paradigm example is of a dodgy second-hand car market. Customers without knowledge of whether a car is a "lemon" depress its price below what a quality second-hand car would be. Information asymmetry arises here, if the seller has more relevant information than the buyer but no incentive to disclose it. Related problems in insurance are adverse selection, such that those at most risk are most likely to insure (say reckless drivers), and moral hazard, such that insurance results in riskier behaviour (say more reckless driving). Both problems may raise insurance costs and reduce efficiency by driving otherwise willing transactors from the market ("incomplete markets"). Moreover, attempting to reduce one problem, say adverse selection by mandating insurance, may add to another, say moral hazard. Information economics, which studies such problems, has relevance in subjects such as insurance, contract law, mechanism design, monetary economics, and health care. Applied subjects include market and legal remedies to spread or reduce risk, such as warranties, government-mandated partial insurance, restructuring or bankruptcy law, inspection, and regulation for quality and information disclosure. Market failure The term "market failure" encompasses several problems which may undermine standard economic assumptions. Although economists categorise market failures differently, the following categories emerge in the main texts. Information asymmetries and incomplete markets may result in economic inefficiency but also a possibility of improving efficiency through market, legal, and regulatory remedies, as discussed above. Natural monopoly, or the overlapping concepts of "practical" and "technical" monopoly, is an extreme case of failure of competition as a restraint on producers. Extreme economies of scale are one possible cause. Public goods are goods which are under-supplied in a typical market. The defining features are that people can consume public goods without having to pay for them and that more than one person can consume the good at the same time. Externalities occur where there are significant social costs or benefits from production or consumption that are not reflected in market prices. For example, air pollution may generate a negative externality, and education may generate a positive externality (less crime, etc.). Governments often tax and otherwise restrict the sale of goods that have negative externalities and subsidise or otherwise promote the purchase of goods that have positive externalities in an effort to correct the price distortions caused by these externalities. Elementary demand-and-supply theory predicts equilibrium but not the speed of adjustment for changes of equilibrium due to a shift in demand or supply. In many areas, some form of price stickiness is postulated to account for quantities, rather than prices, adjusting in the short run to changes on the demand side or the supply side. This includes standard analysis of the business cycle in macroeconomics. Analysis often revolves around causes of such price stickiness and their implications for reaching a hypothesised long-run equilibrium. Examples of such price stickiness in particular markets include wage rates in labour markets and posted prices in markets deviating from perfect competition. Some specialised fields of economics deal in market failure more than others. The economics of the public sector is one example. Much environmental economics concerns externalities or "public bads". Policy options include regulations that reflect cost–benefit analysis or market solutions that change incentives, such as emission fees or redefinition of property rights. Welfare Welfare economics uses microeconomics techniques to evaluate well-being from allocation of productive factors as to desirability and economic efficiency within an economy, often relative to competitive general equilibrium. It analyses social welfare, however measured, in terms of economic activities of the individuals that compose the theoretical society considered. Accordingly, individuals, with associated economic activities, are the basic units for aggregating to social welfare, whether of a group, a community, or a society, and there is no "social welfare" apart from the "welfare" associated with its individual units. Macroeconomics Macroeconomics, another branch of economics, examines the economy as a whole to explain broad aggregates and their interactions "top down", that is, using a simplified form of general-equilibrium theory. Such aggregates include national income and output, the unemployment rate, and price inflation and subaggregates like total consumption and investment spending and their components. It also studies effects of monetary policy and fiscal policy. Since at least the 1960s, macroeconomics has been characterised by further integration as to micro-based modelling of sectors, including rationality of players, efficient use of market information, and imperfect competition. This has addressed a long-standing concern about inconsistent developments of the same subject. Macroeconomic analysis also considers factors affecting the long-term level and growth of national income. Such factors include capital accumulation, technological change and labour force growth. Growth Growth economics studies factors that explain economic growth – the increase in output per capita of a country over a long period of time. The same factors are used to explain differences in the level of output per capita between countries, in particular why some countries grow faster than others, and whether countries converge at the same rates of growth. Much-studied factors include the rate of investment, population growth, and technological change. These are represented in theoretical and empirical forms (as in the neoclassical and endogenous growth models) and in growth accounting. Business cycle The economics of a depression spurred the creation of "macroeconomics" as a separate discipline. During the Great Depression of the 1930s, John Maynard Keynes authored a book entitled The General Theory of Employment, Interest and Money, outlining the key theories of Keynesian economics. Keynes contended that aggregate demand for goods might be insufficient during economic downturns, leading to unnecessarily high unemployment and losses of potential output. He therefore advocated active policy responses by the public sector, including monetary policy actions by the central bank and fiscal policy actions by the government, to stabilize output over the business cycle. Thus, a central conclusion of Keynesian economics is that, in some situations, no strong automatic mechanism moves output and employment towards full employment levels. John Hicks' IS/LM model has been the most influential interpretation of The General Theory. Over the years, the understanding of the business cycle has branched into various research programs, mostly related to or distinct from Keynesianism. The neoclassical synthesis refers to the reconciliation of Keynesian economics with classical economics, stating that Keynesianism is correct in the short run but qualified by classical-like considerations in the intermediate and long run. New classical macroeconomics, as distinct from the Keynesian view of the business cycle, posits market clearing with imperfect information. It includes Friedman's permanent income hypothesis on consumption and "rational expectations" theory, led by Robert Lucas, and real business cycle theory. In contrast, the new Keynesian approach retains the rational expectations assumption; however, it assumes a variety of market failures. In particular, New Keynesians assume prices and wages are "sticky", which means they do not adjust instantaneously to changes in economic conditions. Thus, the new classical economists assume that prices and wages adjust automatically to attain full employment. In contrast, the new Keynesians see full employment as being automatically achieved only in the long run. Hence, government and central-bank policies are needed because the "long run" may be very long. Unemployment The amount of unemployment in an economy is measured by the unemployment rate, the percentage of workers without jobs in the labour force. The labour force only includes workers actively looking for jobs. People who are retired, pursuing education, or discouraged from seeking work by a lack of job prospects are excluded from the labour force. Unemployment can be generally broken down into several types that are related to different causes. Classical models of unemployment occurs when wages are too high for employers to be willing to hire more workers. Consistent with classical unemployment, frictional unemployment occurs when appropriate job vacancies exist for a worker, but the length of time needed to search for and find the job leads to a period of unemployment. Structural unemployment covers a variety of possible causes of unemployment including a mismatch between workers' skills and the skills required for open jobs. Large amounts of structural unemployment can occur when an economy is transitioning industries and workers find their previous set of skills are no longer in demand. Structural unemployment is similar to frictional unemployment since both reflect the problem of matching workers with job vacancies, but structural unemployment covers the time needed to acquire new skills not just the short term search process. While some types of unemployment may occur regardless of the condition of the economy, cyclical unemployment occurs when growth stagnates. Okun's law represents the empirical relationship between unemployment and economic growth. The original version of Okun's law states that a 3% increase in output would lead to a 1% decrease in unemployment. Money and monetary policy Money is a means of final payment for goods in most price system economies, and is the unit of account in which prices are typically stated. Money has general acceptability, relative consistency in value, divisibility, durability, portability, elasticity in supply, and longevity with mass public confidence. It includes currency held by the nonbank public and checkable deposits. It has been described as a social convention, like language, useful to one largely because it is useful to others. In the words of Francis Amasa Walker, a well-known 19th-century economist, "Money is what money does" ("Money is that money does" in the original). As a medium of exchange, money facilitates trade. It is essentially a measure of value and more importantly, a store of value being a basis for credit creation. Its economic function can be contrasted with barter (non-monetary exchange). Given a diverse array of produced goods and specialised producers, barter may entail a hard-to-locate double coincidence of wants as to what is exchanged, say apples and a book. Money can reduce the transaction cost of exchange because of its ready acceptability. Then it is less costly for the seller to accept money in exchange, rather than what the buyer produces. Monetary policy is the policy that central banks conduct to accomplish their broader objectives. Most central banks in developed countries follow inflation targeting, whereas the main objective for many central banks in development countries is to uphold a fixed exchange rate system. The primary monetary tool is normally the adjustment of interest rates, either directly via administratively changing the central bank's own interest rates or indirectly via open market operations. Via the monetary transmission mechanism, interest rate changes affect investment, consumption and net export, and hence aggregate demand, output and employment, and ultimately the development of wages and inflation. Fiscal policy Governments implement fiscal policy to influence macroeconomic conditions by adjusting spending and taxation policies to alter aggregate demand. When aggregate demand falls below the potential output of the economy, there is an output gap where some productive capacity is left unemployed. Governments increase spending and cut taxes to boost aggregate demand. Resources that have been idled can be used by the government. For example, unemployed home builders can be hired to expand highways. Tax cuts allow consumers to increase their spending, which boosts aggregate demand. Both tax cuts and spending have multiplier effects where the initial increase in demand from the policy percolates through the economy and generates additional economic activity. The effects of fiscal policy can be limited by crowding out. When there is no output gap, the economy is producing at full capacity and there are no excess productive resources. If the government increases spending in this situation, the government uses resources that otherwise would have been used by the private sector, so there is no increase in overall output. Some economists think that crowding out is always an issue while others do not think it is a major issue when output is depressed. Sceptics of fiscal policy also make the argument of Ricardian equivalence. They argue that an increase in debt will have to be paid for with future tax increases, which will cause people to reduce their consumption and save money to pay for the future tax increase. Under Ricardian equivalence, any boost in demand from tax cuts will be offset by the increased saving intended to pay for future higher taxes. Inequality Economic inequality includes income inequality, measured using the distribution of income (the amount of money people receive), and wealth inequality measured using the distribution of wealth (the amount of wealth people own), and other measures such as consumption, land ownership, and human capital. Inequality exists at different extents between countries or states, groups of people, and individuals. There are many methods for measuring inequality, the Gini coefficient being widely used for income differences among individuals. An example measure of inequality between countries is the Inequality-adjusted Human Development Index, a composite index that takes inequality into account. Important concepts of equality include equity, equality of outcome, and equality of opportunity. Research has linked economic inequality to political and social instability, including revolution, democratic breakdown and civil conflict. Research suggests that greater inequality hinders economic growth and macroeconomic stability, and that land and human capital inequality reduce growth more than inequality of income. Inequality is at the centre stage of economic policy debate across the globe, as government tax and spending policies have significant effects on income distribution. In advanced economies, taxes and transfers decrease income inequality by one-third, with most of this being achieved via public social spending (such as pensions and family benefits.) Other branches of economics Public economics Public economics is the field of economics that deals with economic activities of a public sector, usually government. The subject addresses such matters as tax incidence (who really pays a particular tax), cost–benefit analysis of government programmes, effects on economic efficiency and income distribution of different kinds of spending and taxes, and fiscal politics. The latter, an aspect of public choice theory, models public-sector behaviour analogously to microeconomics, involving interactions of self-interested voters, politicians, and bureaucrats. Much of economics is positive, seeking to describe and predict economic phenomena. Normative economics seeks to identify what economies ought to be like. Welfare economics is a normative branch of economics that uses microeconomic techniques to simultaneously determine the allocative efficiency within an economy and the income distribution associated with it. It attempts to measure social welfare by examining the economic activities of the individuals that comprise society. International economics International trade studies determinants of goods-and-services flows across international boundaries. It also concerns the size and distribution of gains from trade. Policy applications include estimating the effects of changing tariff rates and trade quotas. International finance is a macroeconomic field which examines the flow of capital across international borders, and the effects of these movements on exchange rates. Increased trade in goods, services and capital between countries is a major effect of contemporary globalisation. Labour economics Labour economics seeks to understand the functioning and dynamics of the markets for wage labour. Labour markets function through the interaction of workers and employers. Labour economics looks at the suppliers of labour services (workers), the demands of labour services (employers), and attempts to understand the resulting pattern of wages, employment, and income. In economics, labour is a measure of the work done by human beings. It is conventionally contrasted with such other factors of production as land and capital. There are theories which have developed a concept called human capital (referring to the skills that workers possess, not necessarily their actual work), although there are also counter posing macro-economic system theories that think human capital is a contradiction in terms. Development economics Development economics examines economic aspects of the economic development process in relatively low-income countries focusing on structural change, poverty, and economic growth. Approaches in development economics frequently incorporate social and political factors. Related subjects Economics is one social science among several and has fields bordering on other areas, including economic geography, economic history, public choice, energy economics, cultural economics, family economics and institutional economics. Law and economics, or economic analysis of law, is an approach to legal theory that applies methods of economics to law. It includes the use of economic concepts to explain the effects of legal rules, to assess which legal rules are economically efficient, and to predict what the legal rules will be. A seminal article by Ronald Coase published in 1961 suggested that well-defined property rights could overcome the problems of externalities. Political economy is the interdisciplinary study that combines economics, law, and political science in explaining how political institutions, the political environment, and the economic system (capitalist, socialist, mixed) influence each other. It studies questions such as how monopoly, rent-seeking behaviour, and externalities should impact government policy. Historians have employed political economy to explore the ways in the past that persons and groups with common economic interests have used politics to effect changes beneficial to their interests. Energy economics is a broad scientific subject area which includes topics related to energy supply and energy demand. Georgescu-Roegen reintroduced the concept of entropy in relation to economics and energy from thermodynamics, as distinguished from what he viewed as the mechanistic foundation of neoclassical economics drawn from Newtonian physics. His work contributed significantly to thermoeconomics and to ecological economics. He also did foundational work which later developed into evolutionary economics. The sociological subfield of economic sociology arose, primarily through the work of Émile Durkheim, Max Weber and Georg Simmel, as an approach to analysing the effects of economic phenomena in relation to the overarching social paradigm (i.e. modernity). Classic works include Max Weber's The Protestant Ethic and the Spirit of Capitalism (1905) and Georg Simmel's The Philosophy of Money (1900). More recently, the works of James S. Coleman, Mark Granovetter, Peter Hedstrom and Richard Swedberg have been influential in this field. Gary Becker in 1974 presented an economic theory of social interactions, whose applications included the family, charity, merit goods and multiperson interactions, and envy and hatred. He and Kevin Murphy authored a book in 2001 that analysed market behaviour in a social environment. Profession The professionalisation of economics, reflected in the growth of graduate programmes on the subject, has been described as "the main change in economics since around 1900". Most major universities and many colleges have a major, school, or department in which academic degrees are awarded in the subject, whether in the liberal arts, business, or for professional study. See Bachelor of Economics and Master of Economics. In the private sector, professional economists are employed as consultants and in industry, including banking and finance. Economists also work for various government departments and agencies, for example, the national treasury, central bank or National Bureau of Statistics. See Economic analyst. There are dozens of prizes awarded to economists each year for outstanding intellectual contributions to the field, the most prominent of which is the Nobel Memorial Prize in Economic Sciences, though it is not a Nobel Prize. Contemporary economics uses mathematics. Economists draw on the tools of calculus, linear algebra, statistics, game theory, and computer science. Professional economists are expected to be familiar with these tools, while a minority specialise in econometrics and mathematical methods. Women in economics Harriet Martineau (1802–1876) was a widely-read populariser of classical economic thought. Mary Paley Marshall (1850–1944), the first women lecturer at a British economics faculty, wrote The Economics of Industry with her husband Alfred Marshall. Joan Robinson (1903–1983) was an important post-Keynesian economist. The economic historian Anna Schwartz (1915–2012) coauthored A Monetary History of the United States, 1867–1960 with Milton Friedman. Three women have received the Nobel Prize in Economics: Elinor Ostrom (2009), Esther Duflo (2019) and Claudia Goldin (2023). Five have received the John Bates Clark Medal: Susan Athey (2007), Esther Duflo (2010), Amy Finkelstein (2012), Emi Nakamura (2019) and Melissa Dell (2020). Women's authorship share in prominent economic journals reduced from 1940 to the 1970s, but has subsequently risen, with different patterns of gendered coauthorship. Women remain globally under-represented in the profession (19% of authors in the RePEc database in 2018), with national variation. See also Notes References Sources Hoover, Kevin D.; Siegler, Mark V. (20 March 2008). "Sound and Fury: McCloskey and Significance Testing in Economics". Journal of Economic Methodology. 15 (1): 1–37. CiteSeerX 10.1.1.533.7658. doi:10.1080/13501780801913298. S2CID 216137286. Samuelson, Paul A; Nordhaus, William D. (2010). Economics. Boston: Irwin McGraw-Hill. ISBN 978-0073511290. OCLC 751033918. Further reading Anderson, David A. (2019). Survey of Economics. New York: Worth. ISBN 978-1-4292-5956-9. Blanchard, Olivier; Amighini, Alessia; Giavazzi, Francesco (2017). Macroeconomics: a European perspective (3rd ed.). Pearson. ISBN 978-1-292-08567-8. Blaug, Mark (1985). Economic Theory in Retrospect (4th ed.). Cambridge: Cambridge University Press. ISBN 978-0521316446. McCann, Charles Robert Jr. (2003). The Elgar Dictionary of Economic Quotations. Edward Elgar. ISBN 978-1840648201. Post, Louis F. (1927), The Basic Facts of Economics: A Common-Sense Primer for Advanced Students. United States: Columbian Printing Company, Incorporated. Economics public domain audiobook at LibriVox. External links General information Institutions and organizations === Study resources ===

Psychology is the scientific study of behavior and mind. Its subject matter includes the behavior of humans and nonhumans, both conscious and unconscious phenomena, and mental processes such as thoughts, feelings, and motives. Psychology is an academic discipline of immense scope, crossing the boundaries between the natural and social sciences. Biological psychologists seek an understanding of the emergent properties of brains, linking the discipline to neuroscience. As social scientists, psychologists aim to understand the behavior of individuals and groups. A professional practitioner or researcher involved in the discipline is called a psychologist. Some psychologists can also be classified as behavioral or cognitive scientists. Some psychologists attempt to understand the role of mental functions in individual and social behavior. Others explore the physiological and neurobiological processes that underlie cognitive functions and behaviors. As part of an interdisciplinary field, psychologists are involved in research on perception, cognition, attention, emotion, intelligence, subjective experiences, motivation, brain functioning, and personality. Psychologists' interests extend to interpersonal relationships, psychological resilience, family resilience, and other areas within social psychology. They also consider the unconscious mind. Research psychologists employ empirical methods to infer causal and correlational relationships between psychosocial variables. Some, but not all, clinical and counseling psychologists rely on symbolic interpretation. While psychological knowledge is often applied to the assessment and treatment of mental health problems, it is also directed towards understanding and solving problems in several spheres of human activity. By many accounts, psychology ultimately aims to benefit society. Many psychologists are involved in some kind of therapeutic role, practicing psychotherapy in clinical, counseling, or school settings. Other psychologists conduct scientific research on a wide range of topics related to mental processes and behavior. Typically the latter group of psychologists work in academic settings (e.g., universities, medical schools, or hospitals). Another group of psychologists is employed in industrial and organizational settings. Yet others are involved in work on human development, aging, sports, health, forensic science, education, and the media. Etymology and definitions The word psychology derives from the Greek word psyche, for spirit or soul. The latter part of the word psychology derives from -λογία -logia, which means "study" or "research". The word psychology was first used in the Renaissance. In its Latin form psychiologia, it was first employed by the Croatian humanist and Latinist Marko Marulić in his book Psichiologia de ratione animae humanae (Psychology, on the Nature of the Human Soul) in the decade 1510–1520 The earliest known reference to the word psychology in English was by Steven Blankaart in 1694 in The Physical Dictionary. The dictionary refers to "Anatomy, which treats the Body, and Psychology, which treats of the Soul." Ψ (psi), the first letter of the Greek word psyche from which the term psychology is derived, is commonly associated with the field of psychology. In 1890, William James defined psychology as "the science of mental life, both of its phenomena and their conditions." This definition enjoyed widespread currency for decades. However, this meaning was contested, notably by John B. Watson, who in 1913 asserted the methodological behaviorist view of psychology as a purely objective experimental branch of natural science, the theoretical goal of which "is the prediction and control of behavior." Since James defined "psychology", the term more strongly implicates scientific experimentation. Folk psychology is the understanding of the mental states and behaviors of people held by ordinary people, as contrasted with psychology professionals' understanding. History The ancient civilizations of Egypt, Greece, China, India, and Persia all engaged in the philosophical study of psychology. In Ancient Egypt the Ebers Papyrus mentioned depression and thought disorders. Historians note that Greek philosophers, including Thales, Plato, and Aristotle (especially in his De Anima treatise), addressed the workings of the mind. As early as the 4th century BC, the Greek physician Hippocrates theorized that mental disorders had physical rather than supernatural causes. In 387 BCE, Plato suggested that the brain is where mental processes take place, and in 335 BCE Aristotle suggested that it was the heart. In China, the foundations of psychological thought emerged from the philosophical works of ancient thinkers like Laozi and Confucius, as well as the teachings of Buddhism. This body of knowledge drew insights from introspection, observation, and techniques for focused thinking and behavior. It viewed the universe as comprising physical and mental realms, along with the interplay between the two. Chinese philosophy also emphasized purifying the mind in order to increase virtue and power. An ancient text known as The Yellow Emperor's Classic of Internal Medicine identifies the brain as the nexus of wisdom and sensation, includes theories of personality based on yin–yang balance, and analyzes mental disorder in terms of physiological and social disequilibria. Chinese scholarship that focused on the brain advanced during the Qing dynasty with the work of Western-educated Fang Yizhi (1611–1671), Liu Zhi (1660–1730), and Wang Qingren (1768–1831). Wang Qingren emphasized the importance of the brain as the center of the nervous system, linked mental disorder with brain diseases, investigated the causes of dreams and insomnia, and advanced a theory of hemispheric lateralization in brain function. Influenced by Hinduism, Indian philosophy explored distinctions in types of awareness. A central idea of the Upanishads and other Vedic texts that formed the foundations of Hinduism was the distinction between a person's transient mundane self and their eternal, unchanging soul. Divergent Hindu doctrines and Buddhism have challenged this hierarchy of selves, but have all emphasized the importance of reaching higher awareness. Yoga encompasses a range of techniques used in pursuit of this goal. Theosophy, a religion established by Russian-American philosopher Helena Blavatsky, drew inspiration from these doctrines during her time in British India. Psychology was of interest to Enlightenment thinkers in Europe. In Germany, Gottfried Wilhelm Leibniz (1646–1716) applied his principles of calculus to the mind, arguing that mental activity took place on an indivisible continuum. He suggested that the difference between conscious and unconscious awareness is only a matter of degree. Christian Wolff identified psychology as its own science, writing Psychologia Empirica in 1732 and Psychologia Rationalis in 1734. Immanuel Kant advanced the idea of anthropology as a discipline, with psychology an important subdivision. Kant, however, explicitly rejected the idea of an experimental psychology, writing that "the empirical doctrine of the soul can also never approach chemistry even as a systematic art of analysis or experimental doctrine, for in it the manifold of inner observation can be separated only by mere division in thought, and cannot then be held separate and recombined at will (but still less does another thinking subject suffer himself to be experimented upon to suit our purpose), and even observation by itself already changes and displaces the state of the observed object." In 1783, Ferdinand Ueberwasser (1752–1812) designated himself Professor of Empirical Psychology and Logic and gave lectures on scientific psychology, though these developments were soon overshadowed by the Napoleonic Wars. At the end of the Napoleonic era, Prussian authorities discontinued the Old University of Münster. Having consulted philosophers Hegel and Herbart, however, in 1825 the Prussian state established psychology as a mandatory discipline in its rapidly expanding and highly influential educational system. However, this discipline did not yet embrace experimentation. In England, early psychology involved phrenology and the response to social problems including alcoholism, violence, and the country's crowded "lunatic" asylums. Beginning of experimental psychology Philosopher John Stuart Mill believed that the human mind was open to scientific investigation, even if the science is in some ways inexact. Mill proposed a "mental chemistry" in which elementary thoughts could combine into ideas of greater complexity. Gustav Fechner began conducting psychophysics research in Leipzig in the 1830s. He articulated the principle that human perception of a stimulus varies logarithmically according to its intensity. The principle became known as the Weber–Fechner law. Fechner's 1860 Elements of Psychophysics challenged Kant's negative view with regard to conducting quantitative research on the mind. Fechner's achievement was to show that "mental processes could not only be given numerical magnitudes, but also that these could be measured by experimental methods." In Heidelberg, Hermann von Helmholtz conducted parallel research on sensory perception, and trained physiologist Wilhelm Wundt. Wundt, in turn, came to Leipzig University, where he established the psychological laboratory that brought experimental psychology to the world. Wundt focused on breaking down mental processes into the most basic components, motivated in part by an analogy to recent advances in chemistry, and its successful investigation of the elements and structure of materials. Paul Flechsig and Emil Kraepelin soon created another influential laboratory at Leipzig, a psychology-related lab, that focused more on experimental psychiatry. James McKeen Cattell, a professor of psychology at the University of Pennsylvania and Columbia University and the co-founder of Psychological Review, was the first professor of psychology in the United States. The German psychologist Hermann Ebbinghaus, a researcher at the University of Berlin, was a 19th-century contributor to the field. He pioneered the experimental study of memory and developed quantitative models of learning and forgetting. In the early 20th century, Wolfgang Kohler, Max Wertheimer, and Kurt Koffka co-founded the school of Gestalt psychology of Fritz Perls. The approach of Gestalt psychology is based upon the idea that individuals experience things as unified wholes. Rather than reducing thoughts and behavior into smaller component elements, as in structuralism, the Gestaltists maintain that whole of experience is important, "and is something else than the sum of its parts, because summing is a meaningless procedure, whereas the whole-part relationship is meaningful." Psychologists in Germany, Denmark, Austria, England, and the United States soon followed Wundt in setting up laboratories. G. Stanley Hall, an American who studied with Wundt, founded a psychology lab that became internationally influential. The lab was located at Johns Hopkins University. Hall, in turn, trained Yujiro Motora, who brought experimental psychology, emphasizing psychophysics, to the Imperial University of Tokyo. Wundt's assistant, Hugo Münsterberg, taught psychology at Harvard to students such as Narendra Nath Sen Gupta—who, in 1905, founded a psychology department and laboratory at the University of Calcutta. Wundt's students Walter Dill Scott, Lightner Witmer, and James McKeen Cattell worked on developing tests of mental ability. Cattell, who also studied with eugenicist Francis Galton, went on to found the Psychological Corporation. Witmer focused on the mental testing of children; Scott, on employee selection. Another student of Wundt, the Englishman Edward Titchener, created the psychology program at Cornell University and advanced "structuralist" psychology. The idea behind structuralism was to analyze and classify different aspects of the mind, primarily through the method of introspection. William James, John Dewey, and Harvey Carr advanced the idea of functionalism, an expansive approach to psychology that underlined the Darwinian idea of a behavior's usefulness to the individual. In 1890, James wrote an influential book, The Principles of Psychology, which expanded on the structuralism. He memorably described "stream of consciousness." James's ideas interested many American students in the emerging discipline. Dewey integrated psychology with societal concerns, most notably by promoting progressive education, inculcating moral values in children, and assimilating immigrants. A different strain of experimentalism, with a greater connection to physiology, emerged in South America, under the leadership of Horacio G. Piñero at the University of Buenos Aires. In Russia, too, researchers placed greater emphasis on the biological basis for psychology, beginning with Ivan Sechenov's 1873 essay, "Who Is to Develop Psychology and How?" Sechenov advanced the idea of brain reflexes and aggressively promoted a deterministic view of human behavior. The Russian-Soviet physiologist Ivan Pavlov discovered in dogs a learning process that was later termed "classical conditioning" and applied the process to human beings. Consolidation and funding One of the earliest psychology societies was La Société de Psychologie Physiologique in France, which lasted from 1885 to 1893. The first meeting of the International Congress of Psychology sponsored by the International Union of Psychological Science took place in Paris, in August 1889, amidst the World's Fair celebrating the centennial of the French Revolution. William James was one of three Americans among the 400 attendees. The American Psychological Association (APA) was founded soon after, in 1892. The International Congress continued to be held at different locations in Europe and with wide international participation. The Sixth Congress, held in Geneva in 1909, included presentations in Russian, Chinese, and Japanese, as well as Esperanto. After a hiatus for World War I, the Seventh Congress met in Oxford, with substantially greater participation from the war-victorious Anglo-Americans. In 1929, the Congress took place at Yale University in New Haven, Connecticut, attended by hundreds of members of the APA. Tokyo Imperial University led the way in bringing new psychology to the East. New ideas about psychology diffused from Japan into China. American psychology gained status upon the U.S.'s entry into World War I. A standing committee headed by Robert Yerkes administered mental tests ("Army Alpha" and "Army Beta") to almost 1.8 million soldiers. Subsequently, the Rockefeller family, via the Social Science Research Council, began to provide funding for behavioral research. Rockefeller charities funded the National Committee on Mental Hygiene, which disseminated the concept of mental illness and lobbied for applying ideas from psychology to child rearing. Through the Bureau of Social Hygiene and later funding of Alfred Kinsey, Rockefeller foundations helped establish research on sexuality in the U.S. Under the influence of the Carnegie-funded Eugenics Record Office, the Draper-funded Pioneer Fund, and other institutions, the eugenics movement also influenced American psychology. In the 1910s and 1920s, eugenics became a standard topic in psychology classes. In contrast to the US, in the UK psychology was met with antagonism by the scientific and medical establishments, and up until 1939, there were only six psychology chairs in universities in England. During World War II and the Cold War, the U.S. military and intelligence agencies established themselves as leading funders of psychology by way of the armed forces and in the new Office of Strategic Services intelligence agency. University of Michigan psychologist Dorwin Cartwright reported that university researchers began large-scale propaganda research in 1939–1941. He observed that "the last few months of the war saw a social psychologist become chiefly responsible for determining the week-by-week-propaganda policy for the United States Government." Cartwright also wrote that psychologists had significant roles in managing the domestic economy. The Army rolled out its new General Classification Test to assess the ability of millions of soldiers. The Army also engaged in large-scale psychological research of troop morale and mental health. In the 1950s, the Rockefeller Foundation and Ford Foundation collaborated with the Central Intelligence Agency (CIA) to fund research on psychological warfare. In 1965, public controversy called attention to the Army's Project Camelot, the "Manhattan Project" of social science, an effort which enlisted psychologists and anthropologists to analyze the plans and policies of foreign countries for strategic purposes. In Germany after World War I, psychology held institutional power through the military, which was subsequently expanded along with the rest of the military during Nazi Germany. Under the direction of Hermann Göring's cousin Matthias Göring, the Berlin Psychoanalytic Institute was renamed the Göring Institute. Freudian psychoanalysts were expelled and persecuted under the anti-Jewish policies of the Nazi Party, and all psychologists had to distance themselves from Freud and Adler, founders of psychoanalysis who were also Jewish. The Göring Institute was well-financed throughout the war with a mandate to create a "New German Psychotherapy." This psychotherapy aimed to align suitable Germans with the overall goals of the Reich. As described by one physician, "Despite the importance of analysis, spiritual guidance and the active cooperation of the patient represent the best way to overcome individual mental problems and to subordinate them to the requirements of the Volk and the Gemeinschaft." Psychologists were to provide Seelenführung [lit., soul guidance], the leadership of the mind, to integrate people into the new vision of a German community. Harald Schultz-Hencke melded psychology with the Nazi theory of biology and racial origins, criticizing psychoanalysis as a study of the weak and deformed. Johannes Heinrich Schultz, a German psychologist recognized for developing the technique of autogenic training, prominently advocated sterilization and euthanasia of men considered genetically undesirable, and devised techniques for facilitating this process. After the war, new institutions were created although some psychologists, because of their Nazi affiliation, were discredited. Alexander Mitscherlich founded a prominent applied psychoanalysis journal called Psyche. With funding from the Rockefeller Foundation, Mitscherlich established the first clinical psychosomatic medicine division at Heidelberg University. In 1970, psychology was integrated into the required studies of medical students. After the Russian Revolution, the Bolsheviks promoted psychology as a way to engineer the "New Man" of socialism. Consequently, university psychology departments trained large numbers of students in psychology. At the completion of training, positions were made available for those students at schools, workplaces, cultural institutions, and in the military. The Russian state emphasized pedology and the study of child development. Lev Vygotsky became prominent in the field of child development. The Bolsheviks also promoted free love and embraced the doctrine of psychoanalysis as an antidote to sexual repression. Although pedology and intelligence testing fell out of favor in 1936, psychology maintained its privileged position as an instrument of the Soviet Union. Stalinist purges took a heavy toll and instilled a climate of fear in the profession, as elsewhere in Soviet society. Following World War II, Jewish psychologists past and present, including Lev Vygotsky, A.R. Luria, and Aron Zalkind, were denounced; Ivan Pavlov (posthumously) and Stalin himself were celebrated as heroes of Soviet psychology. Soviet academics experienced a degree of liberalization during the Khrushchev Thaw. The topics of cybernetics, linguistics, and genetics became acceptable again. The new field of engineering psychology emerged. The field involved the study of the mental aspects of complex jobs (such as pilot and cosmonaut). Interdisciplinary studies became popular and scholars such as Georgy Shchedrovitsky developed systems theory approaches to human behavior. Twentieth-century Chinese psychology originally modeled itself on U.S. psychology, with translations from American authors like William James, the establishment of university psychology departments and journals, and the establishment of groups including the Chinese Association of Psychological Testing (1930) and the Chinese Psychological Society (1937). Chinese psychologists were encouraged to focus on education and language learning. Chinese psychologists were drawn to the idea that education would enable modernization. John Dewey, who lectured to Chinese audiences between 1919 and 1921, had a significant influence on psychology in China. Chancellor T'sai Yuan-p'ei introduced him at Peking University as a greater thinker than Confucius. Kuo Zing-yang who received a PhD at the University of California, Berkeley, became President of Zhejiang University and popularized behaviorism. After the Chinese Communist Party gained control of the country, the Stalinist Soviet Union became the major influence, with Marxism–Leninism the leading social doctrine and Pavlovian conditioning the approved means of behavior change. Chinese psychologists elaborated on Lenin's model of a "reflective" consciousness, envisioning an "active consciousness" (pinyin: tzu-chueh neng-tung-li) able to transcend material conditions through hard work and ideological struggle. They developed a concept of "recognition" (pinyin: jen-shih) which referred to the interface between individual perceptions and the socially accepted worldview; failure to correspond with party doctrine was "incorrect recognition." Psychology education was centralized under the Chinese Academy of Sciences, supervised by the State Council. In 1951, the academy created a Psychology Research Office, which in 1956 became the Institute of Psychology. Because most leading psychologists were educated in the United States, the first concern of the academy was the re-education of these psychologists in the Soviet doctrines. Child psychology and pedagogy for the purpose of a nationally cohesive education remained a central goal of the discipline. Women in psychology 1900–1949 Women in the early 1900s started to make key findings within the world of psychology. In 1923, Anna Freud, the daughter of Sigmund Freud, built on her father's work using different defense mechanisms (denial, repression, and suppression) to psychoanalyze children. She believed that once a child reached the latency period, child analysis could be used as a mode of therapy. She stated it is important focus on the child's environment, support their development, and prevent neurosis. She believed a child should be recognized as their own person with their own right and have each session catered to the child's specific needs. She encouraged drawing, moving freely, and expressing themselves in any way. This helped build a strong therapeutic alliance with child patients, which allows psychologists to observe their normal behavior. She continued her research on the impact of children after family separation, children with socio-economically disadvantaged backgrounds, and all stages of child development from infancy to adolescence. Functional periodicity, the belief women are mentally and physically impaired during menstruation, impacted women's rights because employers were less likely to hire them due to the belief they would be incapable of working for 1 week a month. Leta Stetter Hollingworth wanted to prove this hypothesis and Edward L. Thorndike's theory, that women have lesser psychological and physical traits than men and were simply mediocre, incorrect. Hollingworth worked to prove differences were not from male genetic superiority, but from culture. She also included the concept of women's impairment during menstruation in her research. She recorded both women and men performances on tasks (cognitive, perceptual, and motor) for three months. No evidence was found of decreased performance due to a woman's menstrual cycle. She also challenged the belief intelligence is inherited and women here are intellectually inferior to men. She stated that women do not reach positions of power due to the societal norms and roles they are assigned. As she states in her article, "Variability as related to sex differences in achievement: A Critique", the largest problem women have is the social order that was built due to the assumption women have less interests and abilities than men. To further prove her point, she completed another experiment with infants who have not been influenced by the environment of social norms, like the adult male getting more opportunities than women. She found no difference between infants besides size. After this research proved the original hypothesis wrong, Hollingworth was able to show there is no difference between the physiological and psychological traits of men and women, and women are not impaired during menstruation. The first half of the 1900s was filled with new theories and it was a turning point for women's recognition within the field of psychology. In addition to the contributions made by Leta Stetter Hollingworth and Anna Freud, Mary Whiton Calkins invented the paired associates technique of studying memory and developed self-psychology. Karen Horney developed the concept of "womb envy" and neurotic needs. Psychoanalyst Melanie Klein impacted developmental psychology with her research of play therapy. These great discoveries and contributions were made during struggles of sexism, discrimination, and little recognition for their work. 1950–1999 Women in the second half of the 20th century continued to do research that had large-scale impacts on the field of psychology. Mary Ainsworth's work centered around attachment theory. Building off fellow psychologist John Bowlby, Ainsworth spent years doing fieldwork to understand the development of mother-infant relationships. In doing this field research, Ainsworth developed the Strange Situation Procedure, a laboratory procedure meant to study attachment style by separating and uniting a child with their mother several different times under different circumstances. These field studies are also where she developed her attachment theory and the order of attachment styles, which was a landmark for developmental psychology. Because of her work, Ainsworth became one of the most cited psychologists of all time. Mamie Phipps Clark was another woman in psychology that changed the field with her research. She was one of the first African-Americans to receive a doctoral degree in psychology from Columbia University, along with her husband, Kenneth Clark. Her master's thesis, "The Development of Consciousness in Negro Pre-School Children," argued that black children's self-esteem was negatively impacted by racial discrimination. She and her husband conduced research building off her thesis throughout the 1940s. These tests, called the doll tests, asked young children to choose between identical dolls whose only difference was race, and they found that the majority of the children preferred the white dolls and attributed positive traits to them. Repeated over and over again, these tests helped to determine the negative effects of racial discrimination and segregation on black children's self-image and development. In 1954, this research would help decide the landmark Brown v. Board of Education decision, leading to the end of legal segregation across the nation. Clark went on to be an influential figure in psychology, her work continuing to focus on minority youth. As the field of psychology developed throughout the latter half of the 20th century, women in the field advocated for their voices to be heard and their perspectives to be valued. Second-wave feminism did not miss psychology. An outspoken feminist in psychology was Naomi Weisstein, who was an accomplished researcher in psychology and neuroscience, and is perhaps best known for her paper, "Kirche, Kuche, Kinder as Scientific Law: Psychology Constructs the Female." Psychology Constructs the Female criticized the field of psychology for centering men and using biology too much to explain gender differences without taking into account social factors. Her work set the stage for further research to be done in social psychology, especially in gender construction. Other women in the field also continued advocating for women in psychology, creating the Association for Women in Psychology to criticize how the field treated women. E. Kitsch Child, Phyllis Chesler, and Dorothy Riddle were some of the founding members of the organization in 1969. The latter half of the 20th century further diversified the field of psychology, with women of color reaching new milestones. In 1962, Martha Bernal became the first Latina woman to get a Ph.D. in psychology. In 1969, Marigold Linton, the first Native American woman to get a Ph.D. in psychology, founded the National Indian Education Association. She was also a founding member of the Society for Advancement of Chicanos and Native Americans in Science. In 1971, The Network of Indian Psychologists was established by Carolyn Attneave. Harriet McAdoo was appointed to the White House Conference on Families in 1979. 21st century In the 21st century, women have gained greater prominence in psychology, contributing significantly to a wide range of subfields. Many have taken on leadership roles, directed influential research labs, and guided the next generation of psychologists. However, gender disparities remain, especially when it comes to equal pay and representation in senior academic positions. The number of women pursuing education and training in psychological science has reached a record high. In the United States, estimates suggest that women make up about 78% of undergraduate students and 71% of graduate students in psychology. Disciplinary organizations Institutions In 1920, Édouard Claparède and Pierre Bovet created a new applied psychology organization called the International Congress of Psychotechnics Applied to Vocational Guidance, later called the International Congress of Psychotechnics and then the International Association of Applied Psychology. The IAAP is considered the oldest international psychology association. Today, at least 65 international groups deal with specialized aspects of psychology. In response to male predominance in the field, female psychologists in the U.S. formed the National Council of Women Psychologists in 1941. This organization became the International Council of Women Psychologists after World War II and the International Council of Psychologists in 1959. Several associations including the Association of Black Psychologists and the Asian American Psychological Association have arisen to promote the inclusion of non-European racial groups in the profession. The International Union of Psychological Science (IUPsyS) is the world federation of national psychological societies. The IUPsyS was founded in 1951 under the auspices of the United Nations Educational, Cultural and Scientific Organization (UNESCO). Psychology departments have since proliferated around the world, based primarily on the Euro-American model. Since 1966, the Union has published the International Journal of Psychology. IAAP and IUPsyS agreed in 1976 each to hold a congress every four years, on a staggered basis. IUPsyS recognizes 66 national psychology associations and at least 15 others exist. The American Psychological Association is the oldest and largest. Its membership has increased from 5,000 in 1945 to 100,000 in the present day. The APA includes 54 divisions, which since 1960 have steadily proliferated to include more specialties. Some of these divisions, such as the Society for the Psychological Study of Social Issues and the American Psychology–Law Society, began as autonomous groups. The Interamerican Psychological Society, founded in 1951, aspires to promote psychology across the Western Hemisphere. It holds the Interamerican Congress of Psychology and had 1,000 members in year 2000. The European Federation of Professional Psychology Associations, founded in 1981, represents 30 national associations with a total of 100,000 individual members. At least 30 other international organizations represent psychologists in different regions. In some places, governments legally regulate who can provide psychological services or represent themselves as a "psychologist." The APA defines a psychologist as someone with a doctoral degree in psychology. Boundaries Early practitioners of experimental psychology distinguished themselves from parapsychology, which in the late nineteenth century enjoyed popularity (including the interest of scholars such as William James). Some people considered parapsychology to be part of "psychology". Parapsychology, hypnotism, and psychism were major topics at the early International Congresses. But students of these fields were eventually ostracized, and more or less banished from the Congress in 1900–1905. Parapsychology persisted for a time at Imperial University in Japan, with publications such as Clairvoyance and Thoughtography by Tomokichi Fukurai, but it was mostly shunned by 1913. As a discipline, psychology has long sought to fend off accusations that it is a "soft" science. Philosopher of science Thomas Kuhn's 1962 critique implied psychology overall was in a pre-paradigm state, lacking agreement on the type of overarching theory found in mature hard sciences such as chemistry and physics. Because some areas of psychology rely on research methods such as self-reports in surveys and questionnaires, critics asserted that psychology is not an objective science. Skeptics have suggested that personality, thinking, and emotion cannot be directly measured and are often inferred from subjective self-reports, which may be problematic. Experimental psychologists have devised a variety of ways to indirectly measure these elusive phenomenological entities. Divisions still exist within the field, with some psychologists more oriented towards the unique experiences of individual humans, which cannot be understood only as data points within a larger population. Critics inside and outside the field have argued that mainstream psychology has become increasingly dominated by a "cult of empiricism", which limits the scope of research because investigators restrict themselves to methods derived from the physical sciences. Feminist critiques have argued that claims to scientific objectivity obscure the values and agenda of (historically) mostly male researchers. Jean Grimshaw, for example, argues that mainstream psychological research has advanced a patriarchal agenda through its efforts to control behavior. Major schools of thought Biological Psychologists generally consider biology the substrate of thought and feeling, and therefore an important area of study. Behaviorial neuroscience, also known as biological psychology, involves the application of biological principles to the study of physiological and genetic mechanisms underlying behavior in humans and other animals. The allied field of comparative psychology is the scientific study of the behavior and mental processes of non-human animals. A leading question in behavioral neuroscience has been whether and how mental functions are localized in the brain. From Phineas Gage to H.M. and Clive Wearing, individual people with mental deficits traceable to physical brain damage have inspired new discoveries in this area. Modern behavioral neuroscience could be said to originate in the 1870s, when in France Paul Broca traced production of speech to the left frontal gyrus, thereby also demonstrating hemispheric lateralization of brain function. Soon after, Carl Wernicke identified a related area necessary for the understanding of speech. The contemporary field of behavioral neuroscience focuses on the physical basis of behavior. Behaviorial neuroscientists use animal models, often relying on rats, to study the neural, genetic, and cellular mechanisms that underlie behaviors involved in learning, memory, and fear responses. Cognitive neuroscientists, by using neural imaging tools, investigate the neural correlates of psychological processes in humans. Neuropsychologists conduct psychological assessments to determine how an individual's behavior and cognition are related to the brain. The biopsychosocial model is a cross-disciplinary, holistic model that concerns the ways in which interrelationships of biological, psychological, and socio-environmental factors affect health and behavior. Evolutionary psychology approaches thought and behavior from a modern evolutionary perspective. This perspective suggests that psychological adaptations evolved to solve recurrent problems in human ancestral environments. Evolutionary psychologists attempt to find out how human psychological traits are evolved adaptations, the results of natural selection or sexual selection over the course of human evolution. The history of the biological foundations of psychology includes evidence of racism. The idea of white supremacy and indeed the modern concept of race itself arose during the process of world conquest by Europeans. Carl von Linnaeus's four-fold classification of humans classifies Europeans as intelligent and severe, Americans as contented and free, Asians as ritualistic, and Africans as lazy and capricious. Race was also used to justify the construction of socially specific mental disorders such as drapetomania and dysaesthesia aethiopica—the behavior of uncooperative African slaves. After the creation of experimental psychology, "ethnical psychology" emerged as a subdiscipline, based on the assumption that studying primitive races would provide an important link between animal behavior and the psychology of more evolved humans. Behaviorist A tenet of behavioral research is that a large part of both human and lower-animal behavior is learned. A principle associated with behavioral research is that the mechanisms involved in learning apply to humans and non-human animals. Behavioral researchers have developed a treatment known as behavior modification, which is used to help individuals replace undesirable behaviors with desirable ones. Early behavioral researchers studied stimulus–response pairings, now known as classical conditioning. They demonstrated that when a biologically potent stimulus (e.g., food that elicits salivation) is paired with a previously neutral stimulus (e.g., a bell) over several learning trials, the neutral stimulus by itself can come to elicit the response the biologically potent stimulus elicits. Ivan Pavlov—known best for inducing dogs to salivate in the presence of a stimulus previously linked with food—became a leading figure in the Soviet Union and inspired followers to use his methods on humans. In the United States, Edward Lee Thorndike initiated "connectionist" studies by trapping animals in "puzzle boxes" and rewarding them for escaping. Thorndike wrote in 1911, "There can be no moral warrant for studying man's nature unless the study will enable us to control his acts." From 1910 to 1913 the American Psychological Association went through a sea change of opinion, away from mentalism and towards "behavioralism." In 1913, John B. Watson coined the term behaviorism for this school of thought. Watson's famous Little Albert experiment in 1920 was at first thought to demonstrate that repeated use of upsetting loud noises could instill phobias (aversions to other stimuli) in an infant human, although such a conclusion was likely an exaggeration. Karl Lashley, a close collaborator with Watson, examined biological manifestations of learning in the brain. Clark L. Hull, Edwin Guthrie, and others did much to help behaviorism become a widely used paradigm. A new method of "instrumental" or "operant" conditioning added the concepts of reinforcement and punishment to the model of behavior change. Radical behaviorists avoided discussing the inner workings of the mind, especially the unconscious mind, which they considered impossible to assess scientifically. Operant conditioning was first described by Miller and Kanorski and popularized in the U.S. by B.F. Skinner, who emerged as a leading intellectual of the behaviorist movement. Noam Chomsky published an influential critique of radical behaviorism on the grounds that behaviorist principles could not adequately explain the complex mental process of language acquisition and language use. The review, which was scathing, did much to reduce the status of behaviorism within psychology. Martin Seligman and his colleagues discovered that they could condition in dogs a state of "learned helplessness", which was not predicted by the behaviorist approach to psychology. Edward C. Tolman advanced a hybrid "cognitive behavioral" model, most notably with his 1948 publication discussing the cognitive maps used by rats to guess at the location of food at the end of a maze. Skinner's behaviorism did not die, in part because it generated successful practical applications. The Association for Behavior Analysis International was founded in 1974 and by 2003 had members from 42 countries. The field has gained a foothold in Latin America and Japan. Applied behavior analysis is the term used for the application of the principles of operant conditioning to change socially significant behavior (it supersedes the term, "behavior modification"). Cognitive Cognitive psychology involves the study of mental processes, including perception, attention, language comprehension and production, memory, and problem solving. Researchers in the field of cognitive psychology are sometimes called cognitivists. They rely on an information processing model of mental functioning. Cognitivist research is informed by functionalism and experimental psychology. Starting in the 1950s, the experimental techniques developed by Wundt, James, Ebbinghaus, and others re-emerged as experimental psychology became increasingly cognitivist and, eventually, constituted a part of the wider, interdisciplinary cognitive science. Some called this development the cognitive revolution because it rejected the anti-mentalist dogma of behaviorism as well as the strictures of psychoanalysis. Albert Bandura helped along the transition in psychology from behaviorism to cognitive psychology. Bandura and other social learning theorists advanced the idea of vicarious learning. In other words, they advanced the view that a child can learn by observing the immediate social environment and not necessarily from having been reinforced for enacting a behavior, although they did not rule out the influence of reinforcement on learning a behavior. Technological advances also renewed interest in mental states and mental representations. English neuroscientist Charles Sherrington and Canadian psychologist Donald O. Hebb used experimental methods to link psychological phenomena to the structure and function of the brain. The rise of computer science, cybernetics, and artificial intelligence underlined the value of comparing information processing in humans and machines. A popular and representative topic in this area is cognitive bias, or irrational thought. Psychologists (and economists) have classified and described a sizeable catalog of biases which recur frequently in human thought. The availability heuristic, for example, is the tendency to overestimate the importance of something which happens to come readily to mind. Elements of behaviorism and cognitive psychology were synthesized to form cognitive behavioral therapy, a form of psychotherapy modified from techniques developed by American psychologist Albert Ellis and American psychiatrist Aaron T. Beck. On a broader level, cognitive science is an interdisciplinary enterprise involving cognitive psychologists, cognitive neuroscientists, linguists, and researchers in artificial intelligence, human–computer interaction, and computational neuroscience. The discipline of cognitive science covers cognitive psychology as well as philosophy of mind, computer science, and neuroscience. Computer simulations are sometimes used to model phenomena of interest. Social Social psychology is concerned with how behaviors, thoughts, feelings, and the social environment influence human interactions. Social psychologists study such topics as the influence of others on an individual's behavior (e.g. conformity, persuasion) and the formation of beliefs, attitudes, and stereotypes about other people. Social cognition fuses elements of social and cognitive psychology for the purpose of understanding how people process, remember, or distort social information. The study of group dynamics involves research on the nature of leadership, organizational communication, and related phenomena. In recent years, social psychologists have become interested in implicit measures, mediational models, and the interaction of person and social factors in accounting for behavior. Some concepts that sociologists have applied to the study of psychiatric disorders, concepts such as the social role, sick role, social class, life events, culture, migration, and total institution, have influenced social psychologists. Psychoanalytic Psychoanalysis is a collection of theories and therapeutic techniques intended to analyze the unconscious mind and its impact on everyday life. These theories and techniques inform treatments for mental disorders. Psychoanalysis originated in the 1890s, most prominently with the work of Sigmund Freud. Freud's psychoanalytic theory was largely based on interpretive methods, introspection, and clinical observation. It became very well known, largely because it tackled subjects such as sexuality, repression, and the unconscious. Freud pioneered the methods of free association and dream interpretation. Psychoanalytic theory is not monolithic. Other well-known psychoanalytic thinkers who diverged from Freud include Alfred Adler, Carl Jung, Erik Erikson, Melanie Klein, D. W. Winnicott, Karen Horney, Erich Fromm, John Bowlby, Freud's daughter Anna Freud, and Harry Stack Sullivan. These individuals ensured that psychoanalysis would evolve into diverse schools of thought. Among these schools are ego psychology, object relations, and interpersonal, Lacanian, and relational psychoanalysis. Psychologists such as Hans Eysenck and philosophers including Karl Popper sharply criticized psychoanalysis. Popper argued that psychoanalysis was not falsifiable (no claim it made could be proven wrong) and therefore inherently not a scientific discipline, whereas Eysenck advanced the view that psychoanalytic tenets had been contradicted by experimental data. By the end of the 20th century, psychology departments in American universities mostly had marginalized Freudian theory, dismissing it as a "desiccated and dead" historical artifact. Researchers such as António Damásio, Oliver Sacks, and Joseph LeDoux; and individuals in the emerging field of neuro-psychoanalysis have defended some of Freud's ideas on scientific grounds. Existential-humanistic Humanistic psychology, which has been influenced by existentialism and phenomenology, stresses free will and self-actualization. It emerged in the 1950s as a movement within academic psychology, in reaction to both behaviorism and psychoanalysis. The humanistic approach seeks to view the whole person, not just fragmented parts of the personality or isolated cognitions. Humanistic psychology also focuses on personal growth, self-identity, death, aloneness, and freedom. It emphasizes subjective meaning, the rejection of determinism, and concern for positive growth rather than pathology. Some founders of the humanistic school of thought were American psychologists Abraham Maslow, who formulated a hierarchy of human needs, and Carl Rogers, who created and developed client-centered therapy. Later, positive psychology opened up humanistic themes to scientific study. Positive psychology is the study of factors which contribute to human happiness and well-being, focusing more on people who are currently healthy. In 2010, Clinical Psychological Review published a special issue devoted to positive psychological interventions, such as gratitude journaling and the physical expression of gratitude. It is, however, far from clear that positive psychology is effective in making people happier. Positive psychological interventions have been limited in scope, but their effects are thought to be somewhat better than placebo effects. The American Association for Humanistic Psychology, formed in 1963, declared: Humanistic psychology is primarily an orientation toward the whole of psychology rather than a distinct area or school. It stands for respect for the worth of persons, respect for differences of approach, open-mindedness as to acceptable methods, and interest in exploration of new aspects of human behavior. As a "third force" in contemporary psychology, it is concerned with topics having little place in existing theories and systems: e.g., love, creativity, self, growth, organism, basic need-gratification, self-actualization, higher values, being, becoming, spontaneity, play, humor, affection, naturalness, warmth, ego-transcendence, objectivity, autonomy, responsibility, meaning, fair-play, transcendental experience, peak experience, courage, and related concepts. Existential psychology emphasizes the need to understand a client's total orientation towards the world. Existential psychology is opposed to reductionism, behaviorism, and other methods that objectify the individual. In the 1950s and 1960s, influenced by philosophers Søren Kierkegaard and Martin Heidegger, psychoanalytically trained American psychologist Rollo May helped to develop existential psychology. Existential psychotherapy, which follows from existential psychology, is a therapeutic approach that is based on the idea that a person's inner conflict arises from that individual's confrontation with the givens of existence. Swiss psychoanalyst Ludwig Binswanger and American psychologist George Kelly may also be said to belong to the existential school. Existential psychologists tend to differ from more "humanistic" psychologists in the former's relatively neutral view of human nature and relatively positive assessment of anxiety. Existential psychologists emphasized the humanistic themes of death, free will, and meaning, suggesting that meaning can be shaped by myths and narratives; meaning can be deepened by the acceptance of free will, which is requisite to living an authentic life, albeit often with anxiety with regard to death. Austrian existential psychiatrist and Holocaust survivor Viktor Frankl drew evidence of meaning's therapeutic power from reflections upon his own internment. He created a variation of existential psychotherapy called logotherapy, a type of existentialist analysis that focuses on a will to meaning (in one's life), as opposed to Adler's Nietzschean doctrine of will to power or Freud's will to pleasure. Themes Personality Personality psychology is concerned with enduring patterns of behavior, thought, and emotion. Theories of personality vary across different psychological schools of thought. Each theory carries different assumptions about such features as the role of the unconscious and the importance of childhood experience. According to Freud, personality is based on the dynamic interactions of the id, ego, and super-ego. By contrast, trait theorists have developed taxonomies of personality constructs in describing personality in terms of key traits. Trait theorists have often employed statistical data-reduction methods, such as factor analysis. Although the number of proposed traits has varied widely, Hans Eysenck's early biologically based model suggests at least three major trait constructs are necessary to describe human personality, extraversion–introversion, neuroticism-stability, and psychoticism-normality. Raymond Cattell empirically derived a theory of 16 personality factors at the primary-factor level and up to eight broader second-stratum factors. Since the 1980s, the Big Five (openness to experience, conscientiousness, extraversion, agreeableness, and neuroticism) emerged as an important trait theory of personality. Dimensional models of personality disorders are receiving increasing support, and a version of dimensional assessment, namely the Alternative DSM-5 Model for Personality Disorders, has been included in the DSM-5. However, despite a plethora of research into the various versions of the "Big Five" personality dimensions, it appears necessary to move on from static conceptualizations of personality structure to a more dynamic orientation, acknowledging that personality constructs are subject to learning and change over the lifespan. An early example of personality assessment was the Woodworth Personal Data Sheet, constructed during World War I. The popular, although psychometrically inadequate, Myers–Briggs Type Indicator was developed to assess individuals' "personality types" according to the personality theories of Carl Jung. The Minnesota Multiphasic Personality Inventory (MMPI), despite its name, is more a dimensional measure of psychopathology than a personality measure. California Psychological Inventory contains 20 personality scales (e.g., independence, tolerance). The International Personality Item Pool, which is in the public domain, has become a source of scales that can be used personality assessment. Unconscious mind Study of the unconscious mind, a part of the psyche outside the individual's awareness but that is believed to influence conscious thought and behavior, was a hallmark of early psychology. In one of the first psychology experiments conducted in the United States, C.S. Peirce and Joseph Jastrow found in 1884 that research subjects could choose the minutely heavier of two weights even if consciously uncertain of the difference. Freud popularized the concept of the unconscious mind, particularly when he referred to an uncensored intrusion of unconscious thought into one's speech (a Freudian slip) or to his efforts to interpret dreams. His 1901 book The Psychopathology of Everyday Life catalogs hundreds of everyday events that Freud explains in terms of unconscious influence. Pierre Janet advanced the idea of a subconscious mind, which could contain autonomous mental elements unavailable to the direct scrutiny of the subject. The concept of unconscious processes has remained important in psychology. Cognitive psychologists have used a "filter" model of attention. According to the model, much information processing takes place below the threshold of consciousness, and only certain stimuli, limited by their nature and number, make their way through the filter. Much research has shown that subconscious priming of certain ideas can covertly influence thoughts and behavior. Because of the unreliability of self-reporting, a major hurdle in this type of research involves demonstrating that a subject's conscious mind has not perceived a target stimulus. For this reason, some psychologists prefer to distinguish between implicit and explicit memory. In another approach, one can also describe a subliminal stimulus as meeting an objective but not a subjective threshold. The automaticity model of John Bargh and others involves the ideas of automaticity and unconscious processing in our understanding of social behavior, although there has been dispute with regard to replication. Some experimental data suggest that the brain begins to consider taking actions before the mind becomes aware of them. The influence of unconscious forces on people's choices bears on the philosophical question of free will. John Bargh, Daniel Wegner, and Ellen Langer describe free will as an illusion. Motivation Some psychologists study motivation or the subject of why people or lower animals initiate a behavior at a particular time. It also involves the study of why humans and lower animals continue or terminate a behavior. Psychologists such as William James initially used the term motivation to refer to intention, in a sense similar to the concept of will in European philosophy. With the steady rise of Darwinian and Freudian thinking, instinct also came to be seen as a primary source of motivation. According to drive theory, the forces of instinct combine into a single source of energy which exerts a constant influence. Psychoanalysis, like biology, regarded these forces as demands originating in the nervous system. Psychoanalysts believed that these forces, especially the sexual instincts, could become entangled and transmuted within the psyche. Classical psychoanalysis conceives of a struggle between the pleasure principle and the reality principle, roughly corresponding to id and ego. Later, in Beyond the Pleasure Principle, Freud introduced the concept of the death drive, a compulsion towards aggression, destruction, and psychic repetition of traumatic events. Meanwhile, behaviorist researchers used simple dichotomous models (pleasure/pain, reward/punishment) and well-established principles such as the idea that a thirsty creature will take pleasure in drinking. Clark Hull formalized the latter idea with his drive reduction model. Hunger, thirst, fear, sexual desire, and thermoregulation constitute fundamental motivations in animals. Humans seem to exhibit a more complex set of motivations—though theoretically these could be explained as resulting from desires for belonging, positive self-image, self-consistency, truth, love, and control. Motivation can be modulated or manipulated in many different ways. Researchers have found that eating, for example, depends not only on the organism's fundamental need for homeostasis—an important factor causing the experience of hunger—but also on circadian rhythms, food availability, food palatability, and cost. Abstract motivations are also malleable, as evidenced by such phenomena as goal contagion: the adoption of goals, sometimes unconsciously, based on inferences about the goals of others. Vohs and Baumeister suggest that contrary to the need-desire-fulfillment cycle of animal instincts, human motivations sometimes obey a "getting begets wanting" rule: the more you get a reward such as self-esteem, love, drugs, or money, the more you want it. They suggest that this principle can even apply to food, drink, sex, and sleep. Development psychology Developmental psychology is the scientific study of how and why the thought processes, emotions, and behaviors of humans change over the course of their lives. Some credit Charles Darwin with conducting the first systematic study within the rubric of developmental psychology, having published in 1877 a short paper detailing the development of innate forms of communication based on his observations of his infant son. The main origins of the discipline, however, are found in the work of Jean Piaget. Like Piaget, developmental psychologists originally focused primarily on the development of cognition from infancy to adolescence. Later, developmental psychology extended itself to the study cognition over the life span. In addition to studying cognition, developmental psychologists have also come to focus on affective, behavioral, moral, social, and neural development. Developmental psychologists who study children use a number of research methods. For example, they make observations of children in natural settings such as preschools and engage them in experimental tasks. Such tasks often resemble specially designed games and activities that are both enjoyable for the child and scientifically useful. Developmental researchers have even devised clever methods to study the mental processes of infants. In addition to studying children, developmental psychologists also study aging and processes throughout the life span, including old age. These psychologists draw on the full range of psychological theories to inform their research. Genes and environment All researched psychological traits are influenced by both genes and environment, to varying degrees. These two sources of influence are often confounded in observational research of individuals and families. An example of this confounding can be shown in the transmission of depression from a depressed mother to her offspring. A theory based on environmental transmission would hold that an offspring, by virtue of their having a problematic rearing environment managed by a depressed mother, is at risk for developing depression. On the other hand, a hereditarian theory would hold that depression risk in an offspring is influenced to some extent by genes passed to the child from the mother. Genes and environment in these simple transmission models are completely confounded. A depressed mother may both carry genes that contribute to depression in her offspring and also create a rearing environment that increases the risk of depression in her child. Behavioral genetics researchers have employed methodologies that help to disentangle this confound and understand the nature and origins of individual differences in behavior. Traditionally the research has involved twin studies and adoption studies, two designs where genetic and environmental influences can be partially un-confounded. More recently, gene-focused research has contributed to understanding genetic contributions to the development of psychological traits. The availability of microarray molecular genetic or genome sequencing technologies allows researchers to measure participant DNA variation directly, and test whether individual genetic variants within genes are associated with psychological traits and psychopathology through methods including genome-wide association studies. One goal of such research is similar to that in positional cloning and its success in Huntington's: once a causal gene is discovered biological research can be conducted to understand how that gene influences the phenotype. One major result of genetic association studies is the general finding that psychological traits and psychopathology, as well as complex medical diseases, are highly polygenic, where a large number (on the order of hundreds to thousands) of genetic variants, each of small effect, contribute to individual differences in the behavioral trait or propensity to the disorder. Active research continues to work toward understanding the genetic and environmental bases of behavior and their interaction. Applications Psychology encompasses many subfields and includes different approaches to the study of mental processes and behavior. Psychological testing Psychological testing has ancient origins, dating as far back as 2200 BC, in the examinations for the Chinese civil service. Written exams began during the Han dynasty (202 BC – AD 220). By 1370, the Chinese system required a stratified series of tests, involving essay writing and knowledge of diverse topics. The system was ended in 1906. In Europe, mental assessment took a different approach, with theories of physiognomy—judgment of character based on the face—described by Aristotle in 4th century BC Greece. Physiognomy remained current through the Enlightenment, and added the doctrine of phrenology: a study of mind and intelligence based on simple assessment of neuroanatomy. When experimental psychology came to Britain, Francis Galton was a leading practitioner. By virtue of his procedures for measuring reaction time and sensation, he is considered an inventor of modern mental testing (also known as psychometrics). James McKeen Cattell, a student of Wundt and Galton, brought the idea of psychological testing to the United States, and in fact coined the term "mental test". In 1901, Cattell's student Clark Wissler published discouraging results, suggesting that mental testing of Columbia and Barnard students failed to predict academic performance. In response to 1904 orders from the Minister of Public Instruction, One example of an observational study was run by Arthur Bandura. This observational study focused on children who were exposed to an adult exhibiting aggressive behaviors and their reaction to toys versus other children who were not exposed to these stimuli. The result shows that children who had seen the adult acting aggressively towards a toy, in turn, were aggressive towards their own toy when put in a situation that frustrated them. psychologists Alfred Binet and Théodore Simon developed and elaborated a new test of intelligence in 1905–1911. They used a range of questions diverse in their nature and difficulty. Binet and Simon introduced the concept of mental age and referred to the lowest scorers on their test as idiots. Henry H. Goddard put the Binet-Simon scale to work and introduced classifications of mental level such as imbecile and feebleminded. In 1916, (after Binet's death), Stanford professor Lewis M. Terman modified the Binet-Simon scale (renamed the Stanford–Binet scale) and introduced the intelligence quotient as a score report. Based on his test findings, and reflecting the racism common to that era, Terman concluded that intellectual disability "represents the level of intelligence which is very, very common among Spanish-Indians and Mexican families of the Southwest and also among negroes. Their dullness seems to be racial." Following the Army Alpha and Army Beta tests, which was developed by psychologist Robert Yerkes in 1917 and then used in World War 1 by industrial and organizational psychologists for large-scale employee testing and selection of military personnel. Mental testing also became popular in the U.S., where it was applied to schoolchildren. The federally created National Intelligence Test was administered to 7 million children in the 1920s. In 1926, the College Entrance Examination Board created the Scholastic Aptitude Test to standardize college admissions. The results of intelligence tests were used to argue for segregated schools and economic functions, including the preferential training of Black Americans for manual labor. These practices were criticized by Black intellectuals such a Horace Mann Bond and Allison Davis. Eugenicists used mental testing to justify and organize compulsory sterilization of individuals classified as mentally retarded (now referred to as intellectual disability). In the United States, tens of thousands of men and women were sterilized. Setting a precedent that has never been overturned, the U.S. Supreme Court affirmed the constitutionality of this practice in the 1927 case Buck v. Bell. Today mental testing is a routine phenomenon for people of all ages in Western societies. Modern testing aspires to criteria including standardization of procedure, consistency of results, output of an interpretable score, statistical norms describing population outcomes, and, ideally, effective prediction of behavior and life outcomes outside of testing situations. Psychological testing is regularly used in forensic contexts to aid legal judgments and decisions. Developments in psychometrics include work on test and scale reliability and validity. Developments in item-response theory, structural equation modeling, and bifactor analysis have helped in strengthening test and scale construction. Mental health care The provision of psychological health services is generally called clinical psychology in the U.S. Sometimes, however, members of the school psychology and counseling psychology professions engage in practices that resemble that of clinical psychologists. Clinical psychologists typically include people who have graduated from doctoral programs in clinical psychology. In Canada, some of the members of the abovementioned groups usually fall within the larger category of professional psychology. In Canada and the U.S., practitioners get bachelor's degrees and doctorates; doctoral students in clinical psychology usually spend one year in a predoctoral internship and one year in postdoctoral internship. In Mexico and most other Latin American and European countries, psychologists do not get bachelor's and doctoral degrees; instead, they take a three-year professional course following high school. Clinical psychology is at present the largest specialization within psychology. It includes the study and application of psychology for the purpose of understanding, preventing, and relieving psychological distress, dysfunction, and/or mental illness. Clinical psychologists also try to promote subjective well-being and personal growth. Central to the practice of clinical psychology are psychological assessment and psychotherapy although clinical psychologists may also engage in research, teaching, consultation, forensic testimony, and program development and administration. Credit for the first psychology clinic in the United States typically goes to Lightner Witmer, who established his practice in Philadelphia in 1896. Another modern psychotherapist was Morton Prince, an early advocate for the establishment of psychology as a clinical and academic discipline. In the first part of the twentieth century, most mental health care in the United States was performed by psychiatrists, who are medical doctors. Psychology entered the field with its refinements of mental testing, which promised to improve the diagnosis of mental problems. For their part, some psychiatrists became interested in using psychoanalysis and other forms of psychodynamic psychotherapy to understand and treat the mentally ill. Psychotherapy as conducted by psychiatrists blurred the distinction between psychiatry and psychology, and this trend continued with the rise of community mental health facilities. Some in the clinical psychology community adopted behavioral therapy, a thoroughly non-psychodynamic model that used behaviorist learning theory to change the actions of patients. A key aspect of behavior therapy is empirical evaluation of the treatment's effectiveness. In the 1970s, cognitive-behavior therapy emerged with the work of Albert Ellis and Aaron Beck. Although there are similarities between behavior therapy and cognitive-behavior therapy, cognitive-behavior therapy required the application of cognitive constructs. Since the 1970s, the popularity of cognitive-behavior therapy among clinical psychologists increased. A key practice in behavioral and cognitive-behavioral therapy is exposing patients to things they fear, based on the premise that their responses (fear, panic, anxiety) can be deconditioned. Mental health care today involves psychologists and social workers in increasing numbers. In 1977, National Institute of Mental Health director Bertram Brown described this shift as a source of "intense competition and role confusion." Graduate programs issuing doctorates in clinical psychology emerged in the 1950s and underwent rapid increase through the 1980s. The PhD degree is intended to train practitioners who could also conduct scientific research. The PsyD degree is more exclusively designed to train practitioners. Some clinical psychologists focus on the clinical management of patients with brain injury. This subspecialty is known as clinical neuropsychology. In many countries, clinical psychology is a regulated mental health profession. The emerging field of disaster psychology (see crisis intervention) involves professionals who respond to large-scale traumatic events. The work performed by clinical psychologists tends to be influenced by various therapeutic approaches, all of which involve a formal relationship between professional and client (usually an individual, couple, family, or small group). Typically, these approaches encourage new ways of thinking, feeling, or behaving. Four major theoretical perspectives are psychodynamic, cognitive behavioral, existential–humanistic, and systems or family therapy. There has been a growing movement to integrate the various therapeutic approaches, especially with an increased understanding of issues regarding culture, gender, spirituality, and sexual orientation. With the advent of more robust research findings regarding psychotherapy, there is evidence that most of the major therapies have equal effectiveness, with the key common element being a strong therapeutic alliance. Because of this, more training programs and psychologists are now adopting an eclectic therapeutic orientation. Diagnosis in clinical psychology usually follows the Diagnostic and Statistical Manual of Mental Disorders (DSM). The study of mental illnesses is called abnormal psychology. Education Educational psychology is the study of how humans learn in educational settings, the effectiveness of educational interventions, the psychology of teaching, and the social psychology of schools as organizations. Educational psychologists can be found in preschools, schools of all levels including post secondary institutions, community organizations and learning centers, Government or private research firms, and independent or private consultant. The work of developmental psychologists such as Lev Vygotsky, Jean Piaget, and Jerome Bruner has been influential in creating teaching methods and educational practices. Educational psychology is often included in teacher education programs in places such as North America, Australia, and New Zealand. School psychology combines principles from educational psychology and clinical psychology to understand and treat students with learning disabilities; to foster the intellectual growth of gifted students; to facilitate prosocial behaviors in adolescents; and otherwise to promote safe, supportive, and effective learning environments. School psychologists are trained in educational and behavioral assessment, intervention, prevention, and consultation, and many have extensive training in research. Work Industrial and organizational (I/O) psychology involves research and practices that apply psychological theories and principles to organizations and individuals' work-lives. In the field's beginnings, industrialists brought the nascent field of psychology to bear on the study of scientific management techniques for improving workplace efficiency. The field was at first called economic psychology or business psychology; later, industrial psychology, employment psychology, or psychotechnology. An influential early study examined workers at Western Electric's Hawthorne plant in Cicero, Illinois from 1924 to 1932. Western Electric experimented on factory workers to assess their responses to changes in illumination, breaks, food, and wages. The researchers came to focus on workers' responses to observation itself, and the term Hawthorne effect is now used to describe the fact that people's behavior can change when they think they are being observed. Although the Hawthorne research can be found in psychology textbooks, the research and its findings were weak at best. The name industrial and organizational psychology emerged in the 1960s. In 1973, it became enshrined in the name of the Society for Industrial and Organizational Psychology, Division 14 of the American Psychological Association. One goal of the discipline is to optimize human potential in the workplace. Personnel psychology is a subfield of I/O psychology. Personnel psychologists apply the methods and principles of psychology in selecting and evaluating workers. Another subfield, organizational psychology, examines the effects of work environments and management styles on worker motivation, job satisfaction, and productivity. Most I/O psychologists work outside of academia, for private and public organizations and as consultants. A psychology consultant working in business today might expect to provide executives with information and ideas about their industry, their target markets, and the organization of their company. Organizational behavior (OB) is an allied field involved in the study of human behavior within organizations. One way to differentiate I/O psychology from OB is that I/O psychologists train in university psychology departments and OB specialists, in business schools. Military and intelligence One role for psychologists in the military has been to evaluate and counsel soldiers and other personnel. In the U.S., this function began during World War I, when Robert Yerkes established the School of Military Psychology at Fort Oglethorpe in Georgia. The school provided psychological training for military staff. Today, U.S. Army psychologists perform psychological screening, clinical psychotherapy, suicide prevention, and treatment for post-traumatic stress, as well as provide prevention-related services, for example, smoking cessation. The United States Army's Mental Health Advisory Teams implement psychological interventions to help combat troops experiencing mental problems. Psychologists may also work on a diverse set of campaigns known broadly as psychological warfare. Psychological warfare chiefly involves the use of propaganda to influence enemy soldiers and civilians. This so-called black propaganda is designed to seem as if it originates from a source other than the Army. The CIA's MKULTRA program involved more individualized efforts at mind control, involving techniques such as hypnosis, torture, and covert involuntary administration of LSD. The U.S. military used the name Psychological Operations (PSYOP) until 2010, when these activities were reclassified as Military Information Support Operations (MISO), part of Information Operations (IO). Psychologists have sometimes been involved in assisting the interrogation and torture of suspects, staining the records of the psychologists involved. Health, well-being, and social change Social change An example of the contribution of psychologists to social change involves the research of Kenneth and Mamie Phipps Clark. These two African American psychologists studied segregation's adverse psychological impact on Black children. Their research findings played a role in the desegregation case Brown v. Board of Education (1954). The impact of psychology on social change includes the discipline's broad influence on teaching and learning. Research has shown that compared to the "whole word" or "whole language" approach, the phonics approach to reading instruction is more efficacious. Medical applications Medical facilities increasingly employ psychologists to perform various roles. One aspect of health psychology is the psychoeducation of patients: instructing them in how to follow a medical regimen. Health psychologists can also educate doctors and conduct research on patient compliance. Psychologists in the field of public health use a wide variety of interventions to influence human behavior. These range from public relations campaigns and outreach to governmental laws and policies. Psychologists study the composite influence of all these different tools in an effort to influence whole populations of people. Worker health, safety and wellbeing Psychologists work with organizations to apply findings from psychological research to improve the health and well-being of employees. Some work as external consultants hired by organizations to solve specific problems, whereas others are full-time employees of the organization. Applications include conducting surveys to identify issues and designing interventions to make work healthier. Some of the specific health areas include: Accidents and injuries: A major contribution is the concept of safety climate, which is employee shared perceptions of the behaviors that are encouraged (e.g., wearing safety gear) and discouraged (not following safety rules) at work. Organizations with strong safety climates have fewer work accidents and injuries. Cardiovascular disease: Cardiovascular disease has been related to lack of job control. Mental health: Exposure to occupational stress is associated with mental health disorder. Musculoskeletal disorder: These are injuries in bones, nerves and tendons due to overexertion and repetitive strain. They have been linked to job satisfaction and workplace stress. Physical health symptoms: Occupational stress has been linked to physical symptoms such as digestive distress and headache. Workplace violence: Violence prevention climate is related to being physically assaulted and psychologically mistreated at work. Interventions that improve climates are a way to address accidents and violence. Interventions that reduce stress at work or provide employees with tools to better manage it can help in areas where stress is an important component. Industrial psychology became interested in worker fatigue during World War I, when government ministers in Britain were concerned about the impact of fatigue on workers in munitions factories but not other types of factories. In the U. K. some interest in worker well-being emerged with the efforts of Charles Samuel Myers and his National Institute of Industrial Psychology (NIIP) during the inter-War years. In the U. S. during the mid-twentieth century industrial psychologist Arthur Kornhauser pioneered the study of occupational mental health, linking industrial working conditions to mental health as well as the spillover of an unsatisfying job into a worker's personal life. Zickar accumulated evidence to show that "no other industrial psychologist of his era was as devoted to advocating management and labor practices that would improve the lives of working people." Occupational health psychology As interest in the worker health expanded toward the end of the twentieth century, the field of occupational health psychology (OHP) emerged. OHP is a branch of psychology that is interdisciplinary. OHP is concerned with the health and safety of workers. OHP addresses topic areas such as the impact of occupational stressors on physical and mental health, mistreatment of workers (e.g., bullying and violence), work-family balance, the impact of involuntary unemployment on physical and mental health, the influence of psychosocial factors on safety and accidents, and interventions designed to improve/protect worker health. OHP grew out of health psychology, industrial and organizational psychology, and occupational medicine. OHP has also been informed by disciplines outside psychology, including industrial engineering, sociology, and economics. Research methods Quantitative psychological research lends itself to the statistical testing of hypotheses. Although the field makes abundant use of randomized and controlled experiments in laboratory settings, such research can only assess a limited range of short-term phenomena. Some psychologists rely on less rigorously controlled, but more ecologically valid, field experiments as well. Other research psychologists rely on statistical methods to glean knowledge from population data. The statistical methods research psychologists employ include the Pearson product–moment correlation coefficient, the analysis of variance, multiple linear regression, logistic regression, structural equation modeling, and hierarchical linear modeling. The measurement and operationalization of important constructs is an essential part of these research designs. Although this type of psychological research is much less abundant than quantitative research, some psychologists conduct qualitative research. This type of research can involve interviews, questionnaires, and first-hand observation. While hypothesis testing is rare, virtually impossible, in qualitative research, qualitative studies can be helpful in theory and hypothesis generation, interpreting seemingly contradictory quantitative findings, and understanding why some interventions fail and others succeed. Controlled experiments A true experiment with random assignment of research participants (sometimes called subjects) to rival conditions allows researchers to make strong inferences about causal relationships. When there are large numbers of research participants, the random assignment (also called random allocation) of those participants to rival conditions ensures that the individuals in those conditions will, on average, be similar on most characteristics, including characteristics that went unmeasured. In an experiment, the researcher alters one or more variables of influence, called independent variables, and measures resulting changes in the factors of interest, called dependent variables. Prototypical experimental research is conducted in a laboratory with a carefully controlled environment. A quasi-experiment is a situation in which different conditions are being studied, but random assignment to the different conditions is not possible. Investigators must work with preexisting groups of people. Researchers can use common sense to consider how much the nonrandom assignment threatens the study's validity. For example, in research on the best way to affect reading achievement in the first three grades of school, school administrators may not permit educational psychologists to randomly assign children to phonics and whole language classrooms, in which case the psychologists must work with preexisting classroom assignments. Psychologists will compare the achievement of children attending phonics and whole language classes and, perhaps, statistically adjust for any initial differences in reading level. Experimental researchers typically use a statistical hypothesis testing model which involves making predictions before conducting the experiment, then assessing how well the data collected are consistent with the predictions. These predictions are likely to originate from one or more abstract scientific hypotheses about how the phenomenon under study actually works. Other types of studies Surveys are used in psychology for the purpose of measuring attitudes and traits, monitoring changes in mood, and checking the validity of experimental manipulations (checking research participants' perception of the condition they were assigned to). Psychologists have commonly used paper-and-pencil surveys. However, surveys are also conducted over the phone or through e-mail. Web-based surveys are increasingly used to conveniently reach many subjects. Observational studies are commonly conducted in psychology. In cross-sectional observational studies, psychologists collect data at a single point in time. The goal of many cross-sectional studies is the assess the extent factors are correlated with each other. By contrast, in longitudinal studies psychologists collect data on the same sample at two or more points in time. Sometimes the purpose of longitudinal research is to study trends across time such as the stability of traits or age-related changes in behavior. Because some studies involve endpoints that psychologists cannot ethically study from an experimental standpoint, such as identifying the causes of depression, they conduct longitudinal studies a large group of depression-free people, periodically assessing what is happening in the individuals' lives. In this way psychologists have an opportunity to test causal hypotheses regarding conditions that commonly arise in people's lives that put them at risk for depression. Problems that affect longitudinal studies include selective attrition, the type of problem in which bias is introduced when a certain type of research participant disproportionately leaves a study. One example of an observational study was run by Arthur Bandura. This observational study focused on children who were exposed to an adult exhibiting aggressive behaviors and their reaction to toys versus other children who were not exposed to these stimuli. The result shows that children who had seen the adult acting aggressively towards a toy, in turn, were aggressive towards their own toy when put in a situation that frustrated them. Exploratory data analysis includes a variety of practices that researchers use to reduce a great many variables to a small number overarching factors. In Peirce's three modes of inference, exploratory data analysis corresponds to abduction. Meta-analysis is the technique research psychologists use to integrate results from many studies of the same variables and arriving at a grand average of the findings. Direct brain observation/manipulation A classic and popular tool used to relate mental and neural activity is the electroencephalogram (EEG), a technique using amplified electrodes on a person's scalp to measure voltage changes in different parts of the brain. Hans Berger, the first researcher to use EEG on an unopened skull, quickly found that brains exhibit signature "brain waves": electric oscillations which correspond to different states of consciousness. Researchers subsequently refined statistical methods for synthesizing the electrode data, and identified unique brain wave patterns such as the delta wave observed during non-REM sleep. Newer functional neuroimaging techniques include functional magnetic resonance imaging and positron emission tomography, both of which track the flow of blood through the brain. These technologies provide more localized information about activity in the brain and create representations of the brain with widespread appeal. They also provide insight which avoids the classic problems of subjective self-reporting. It remains challenging to draw hard conclusions about where in the brain specific thoughts originate—or even how usefully such localization corresponds with reality. However, neuroimaging has delivered unmistakable results showing the existence of correlations between mind and brain. Some of these draw on a systemic neural network model rather than a localized function model. Interventions such as transcranial magnetic stimulation and drugs also provide information about brain–mind interactions. Psychopharmacology is the study of drug-induced mental effects. Computer simulation Computational modeling is a tool used in mathematical psychology and cognitive psychology to simulate behavior. This method has several advantages. Since modern computers process information quickly, simulations can be run in a short time, allowing for high statistical power. Modeling also allows psychologists to visualize hypotheses about the functional organization of mental events that could not be directly observed in a human. Computational neuroscience uses mathematical models to simulate the brain. Another method is symbolic modeling, which represents many mental objects using variables and rules. Other types of modeling include dynamic systems and stochastic modeling. Animal studies Animal experiments aid in investigating many aspects of human psychology, including perception, emotion, learning, memory, and thought, to name a few. In the 1890s, Russian physiologist Ivan Pavlov famously used dogs to demonstrate classical conditioning. Non-human primates, cats, dogs, pigeons, and rats and other rodents are often used in psychological experiments. Ideally, controlled experiments introduce only one independent variable at a time, in order to ascertain its unique effects upon dependent variables. These conditions are approximated best in laboratory settings. In contrast, human environments and genetic backgrounds vary so widely, and depend upon so many factors, that it is difficult to control important variables for human subjects. There are pitfalls, however, in generalizing findings from animal studies to humans through animal models. Comparative psychology is the scientific study of the behavior and mental processes of non-human animals, especially as these relate to the phylogenetic history, adaptive significance, and development of behavior. Research in this area explores the behavior of many species, from insects to primates. It is closely related to other disciplines that study animal behavior such as ethology. Research in comparative psychology sometimes appears to shed light on human behavior, but some attempts to connect the two have been quite controversial, for example the Sociobiology of E.O. Wilson. Animal models are often used to study neural processes related to human behavior, e.g. in cognitive neuroscience. Qualitative research Qualitative research is often designed to answer questions about the thoughts, feelings, and behaviors of individuals. Qualitative research involving first-hand observation can help describe events as they occur, with the goal of capturing the richness of everyday behavior and with the hope of discovering and understanding phenomena that might have been missed if only more cursory examinations are made. Qualitative psychological research methods include interviews, first-hand observation, and participant observation. Creswell (2003) identified five main possibilities for qualitative research, including narrative, phenomenology, ethnography, case study, and grounded theory. Qualitative researchers sometimes aim to enrich our understanding of symbols, subjective experiences, or social structures. Sometimes hermeneutic and critical aims can give rise to quantitative research, as in Erich Fromm's application of psychological and sociological theories, in his book Escape from Freedom, to understanding why many ordinary Germans supported Hitler. Just as Jane Goodall studied chimpanzee social and family life by careful observation of chimpanzee behavior in the field, psychologists conduct naturalistic observation of ongoing human social, professional, and family life. Sometimes the participants are aware they are being observed, and other times the participants do not know they are being observed. Strict ethical guidelines must be followed when covert observation is being carried out. Program evaluation Program evaluation involves the systematic collection, analysis, and application of information to answer questions about projects, policies and programs, particularly about their effectiveness. In both the public and private sectors, stakeholders often want to know the extent which the programs they are funding, implementing, voting for, receiving, or objecting to are producing the intended effects. While program evaluation first focuses on effectiveness, important considerations often include how much the program costs per participant, how the program could be improved, whether the program is worthwhile, whether there are better alternatives, if there are unintended outcomes, and whether the program goals are appropriate and useful. Contemporary issues Metascience Metascience involves the application of scientific methodology to study science itself. The field of metascience has revealed problems in psychological research. Some psychological research has suffered from bias, problematic reproducibility, and misuse of statistics. These findings have led to calls for reform from within and from outside the scientific community. Confirmation bias In 1959, statistician Theodore Sterling examined the results of psychological studies and discovered that 97% of them supported their initial hypotheses, implying possible publication bias. Similarly, Fanelli (2010) found that 91.5% of psychiatry/psychology studies confirmed the effects they were looking for, and concluded that the odds of this happening (a positive result) was around five times higher than in fields such as space science or geosciences. Fanelli argued that this is because researchers in "softer" sciences have fewer constraints to their conscious and unconscious biases. Replication A replication crisis in psychology has emerged. Many notable findings in the field have not been replicated. Some researchers were even accused of publishing fraudulent results. Systematic efforts, including efforts by the Reproducibility Project of the Center for Open Science, to assess the extent of the problem found that as many as two-thirds of highly publicized findings in psychology failed to be replicated. Reproducibility has generally been stronger in cognitive psychology (in studies and journals) than social psychology and subfields of differential psychology. Other subfields of psychology have also been implicated in the replication crisis, including clinical psychology, developmental psychology, and a field closely related to psychology, educational research. Focus on the replication crisis has led to other renewed efforts in the discipline to re-test important findings. In response to concerns about publication bias and data dredging (conducting a large number of statistical tests on a great many variables but restricting reporting to the results that were statistically significant), 295 psychology and medical journals have adopted result-blind peer review where studies are accepted not on the basis of their findings and after the studies are completed, but before the studies are conducted and upon the basis of the methodological rigor of their experimental designs and the theoretical justifications for their proposed statistical analysis before data collection or analysis is conducted. In addition, large-scale collaborations among researchers working in multiple labs in different countries have taken place. The collaborators regularly make their data openly available for different researchers to assess. Allen and Mehler estimated that 61 per cent of result-blind studies have yielded null results, in contrast to an estimated 5 to 20 per cent in traditional research. Misuse of statistics Some critics view statistical hypothesis testing as misplaced. Psychologist and statistician Jacob Cohen wrote in 1994 that psychologists routinely confuse statistical significance with practical importance, enthusiastically reporting great certainty in unimportant facts. Some psychologists have responded with an increased use of effect size statistics, rather than sole reliance on p-values. WEIRD bias In 2008, Arnett pointed out that most articles in American Psychological Association journals were about U.S. populations when U.S. citizens are only 5% of the world's population. He complained that psychologists had no basis for assuming psychological processes to be universal and generalizing research findings to the rest of the global population. In 2010, Henrich, Heine, and Norenzayan reported a bias in conducting psychology studies with participants from "WEIRD" ("Western, Educated, Industrialized, Rich, and Democratic") societies. Henrich et al. found that "96% of psychological samples come from countries with only 12% of the world's population" (p. 63). The article gave examples of results that differ significantly between people from WEIRD and tribal cultures, including the Müller-Lyer illusion. Arnett (2008), Altmaier and Hall (2008) and Morgan-Consoli et al. (2018) view the Western bias in research and theory as a serious problem considering psychologists are increasingly applying psychological principles developed in WEIRD regions in their research, clinical work, and consultation with populations around the world. In 2018, Rad, Martingano, and Ginges showed that nearly a decade after Henrich et al.'s paper, over 80% of the samples used in studies published in the journal Psychological Science employed WEIRD samples. Moreover, their analysis showed that several studies did not fully disclose the origin of their samples; the authors offered a set of recommendations to editors and reviewers to reduce WEIRD bias. STRANGE bias Similar to the WEIRD bias, starting in 2020, researchers of non-human behavior have started to emphasize the need to document the possibility of the STRANGE (Social background, Trappability and self-selection, Rearing history, Acclimation and habituation, Natural changes in responsiveness, Genetic makeup, and Experience) bias in study conclusions. Unscientific mental health training Some observers perceive a gap between scientific theory and its application—in particular, the application of unsupported or unsound clinical practices. Critics say there has been an increase in the number of mental health training programs that do not instill scientific competence. Practices such as "facilitated communication for infantile autism"; memory-recovery techniques including body work; and other therapies, such as rebirthing and reparenting, may be dubious or even dangerous, despite their popularity. These practices, however, are outside the mainstream practices taught in clinical psychology doctoral programs. Ethics Ethical standards in the discipline have changed over time. Some famous past studies are today considered unethical and in violation of established codes (e.g., the Canadian Code of Conduct for Research Involving Humans, and the Belmont Report). The American Psychological Association has advanced a set of ethical principles and a code of conduct for the profession. The most important contemporary standards include informed and voluntary consent. After World War II, the Nuremberg Code was established because of Nazi abuses of experimental subjects. Later, most countries (and scientific journals) adopted the Declaration of Helsinki. In the U.S., the National Institutes of Health established the Institutional Review Board in 1966, and in 1974 adopted the National Research Act (HR 7724). All of these measures encouraged researchers to obtain informed consent from human participants in experimental studies. A number of influential but ethically dubious studies led to the establishment of this rule; such studies included the MIT-Harvard Fernald School radioisotope studies, the Thalidomide tragedy, the Willowbrook hepatitis study, Stanley Milgram's studies of obedience to authority, and the Stanford Prison Experiment. Ethics with Humans The ethics code of the American Psychological Association originated in 1951 as "Ethical Standards of Psychologists." This code has guided the formation of licensing laws in most American states. It has changed multiple times over the decades since its adoption, and contains both aspirational principles and binding ethical standards. The APA's Ethical Principles of Psychologists and Code of Conduct consists of five General Principles, which are meant to guide psychologists to higher ethical practice where a particular standard does not apply. Those principles are: A. Beneficence and Nonmaleficence - meaning the psychologists must work to benefit those they work with and "do no harm." This includes awareness of indirect benefits and harms their work might have on others due to personal, social, political, or other factors. B. Fidelity and Responsibility - an awareness of public trust in the profession and adherence to ethical standards and clarification of roles to preserve that trust. This includes managing conflicts of interest, as well as committing some portion of a psychologist's professional time to low-cost or pro bono work. C. Integrity - upholding honesty and accuracy in all psychological practices, including avoiding misrepresentations and fraud. In situations where psychologists would use deception (i.e., certain research), psychologists must consider the necessity, benefits, and harms, and mitigate any harms where possible. D. Justice - an understanding that psychology must be for everyone's benefit, and that psychologists take special care to avoid unjust practices as a result of biases or limitations of expertise. E. Respect for People's Rights and Dignity - the preservation of people's rights when working with psychologists, including confidentially, privacy, and autonomy. Psychologists should consider a multitude of factors, including a need for special safeguards for protected populations (e.g., minors, incarcerated individuals) and awareness of differences based on numerous factors, including culture, race, age, gender, and socioeconomic status. In 1989, the APA revised its policies on advertising and referral fees to negotiate the end of an investigation by the Federal Trade Commission. The 1992 incarnation was the first to distinguish between "aspirational" ethical standards and "enforceable" ones. The APA code was further revised in 2010 to prevent the use of the code to justify violating human rights, which was in response to the participation of APA members in interrogations under the administration of United States President George W. Bush. Members of the public have a five-year window to file ethics complaints about APA members with the APA ethics committee; members of the APA have a three-year window. The Canadian Psychological Association used the APA code until 1986, when it developed its own code drawing from four similar principles: 1) Respect for the Dignity of Persons and Peoples, 2) Responsible Caring, 3) Integrity in Relationships, 4) Responsibility to Society. The European Federation of Psychologist's Associations, have adopted a model code using the principles of the Canadian Code, while also drawing from the APA code. Universities have ethics committees dedicated to protecting the rights (e.g., voluntary nature of participation in the research, privacy) and well-being (e.g., minimizing distress) of research participants. University ethics committees evaluate proposed research to ensure that researchers protect the rights and well-being of participants; an investigator's research project cannot be conducted unless approved by such an ethics committee. The field of psychology also identifies certain categories of people that require additional or special protection due to particular vulnerabilities, unequal power dynamics, or diminished capacity for informed consent. This list often includes, but is not limited to, children, incarcerated individuals, pregnant women, human fetuses and neonates, institutionalized persons, those with physical or mental disabilities, and the educationally or economically disadvantaged. Some of the ethical issues considered most important are the requirement to practice only within the area of competence, to maintain confidentiality with the patients, and to avoid sexual relations with them. Another important principle is informed consent, the idea that a patient or research subject must understand and freely choose a procedure they are undergoing. Some of the most common complaints against clinical psychologists include sexual misconduct and breaches in confidentiality or privacy. Psychology ethics apply to all types of human contact in a psychologist's professional capacity, including therapy, assessment, teaching, training, work with research subjects, testimony in courts and before government bodies, consulting, and statements to the public or media pertaining to matters of psychology. Ethics with other animals Research on other animals is governed by university ethics committees. Research on nonhuman animals cannot proceed without permission of the ethics committee, of the researcher's home institution. Ethical guidelines state that using non-human animals for scientific purposes is only acceptable when the harm (physical or psychological) done to animals is outweighed by the benefits of the research. Psychologists can use certain research techniques on animals that could not be used on humans. Comparative psychologist Harry Harlow drew moral condemnation for isolation experiments on rhesus macaque monkeys at the University of Wisconsin–Madison in the 1970s. The aim of the research was to produce an animal model of clinical depression. Harlow also devised what he called a "rape rack", to which the female isolates were tied in normal monkey mating posture. In 1974, American literary critic Wayne C. Booth wrote that, "Harry Harlow and his colleagues go on torturing their nonhuman primates decade after decade, invariably proving what we all knew in advance—that social creatures can be destroyed by destroying their social ties." He writes that Harlow made no mention of the criticism of the morality of his work. Animal research is influential in psychology, while still being debated among academics. The testing of animals for research has led to medical breakthroughs in human medicine. Many psychologists argue animal experimentation is essential for human advancement, but must be regulated by the government to ensure ethicality. References Sources Further reading External links American Psychological Association Association for Psychological Science

Sociology is the scientific study of human society that focuses on society, human social behavior, patterns of social relationships, social interaction, and aspects of culture associated with everyday life. The term sociology was coined in the late 18th century to describe the scientific study of society. Regarded as a part of both the social sciences and humanities, sociology uses various methods of empirical investigation and critical analysis to develop a body of knowledge about social order and social change. Sociological subject matter ranges from micro-level analyses of individual interaction and agency to macro-level analyses of social systems and social structure. Applied sociological research may be applied directly to social policy and welfare, whereas theoretical approaches may focus on the understanding of social processes and phenomenological method. Traditional focuses of sociology include social stratification, social class, social mobility, religion, secularization, law, sexuality, gender, and deviance. Recent studies have added socio-technical aspects of the digital divide as a new focus. Digital sociology examines the impact of digital technologies on social behavior and institutions, encompassing professional, analytical, critical, and public dimensions. The internet has reshaped social networks and power relations, illustrating the growing importance of digital sociology. As all spheres of human activity are affected by the interplay between social structure and individual agency, sociology has gradually expanded its focus to other subjects and institutions, such as health and the institution of medicine; economy; military; punishment and systems of control; the Internet; sociology of education; social capital; and the role of social activity in the development of scientific knowledge. The range of social scientific methods has also expanded, as social researchers draw upon a variety of qualitative and quantitative techniques. The linguistic and cultural turns of the mid-20th century, especially, have led to increasingly interpretative, hermeneutic, and philosophical approaches towards the analysis of society. Conversely, the turn of the 21st century has seen the rise of new analytically, mathematically, and computationally rigorous techniques, such as agent-based modelling and social network analysis. Social research has influence throughout various industries and sectors of life, such as among politicians, policy makers, and legislators; educators; planners; administrators; developers; business magnates and managers; social workers; non-governmental organizations; and non-profit organizations, as well as individuals interested in resolving social issues in general. History Sociological reasoning predates the foundation of the discipline itself. Social analysis has origins in the common stock of universal, global knowledge and philosophy, having been carried out as far back as the time of old comic poetry which features social and political criticism, and ancient Greek philosophers Socrates, Plato, and Aristotle. For instance, the origin of the survey can be traced back to at least the Domesday Book in 1086, while ancient philosophers such as Confucius wrote about the importance of social roles. Medieval Arabic writings encompass a rich tradition that unveils early insights into the field of sociology. Some sources consider Ibn Khaldun, a 14th-century Muslim scholar from Tunisia, to have been the father of sociology, although there is no reference to his work in the writings of European contributors to modern sociology. Khaldun's Muqaddimah was considered to be amongst the first works to advance social-scientific reasoning on social cohesion and social conflict. Etymology The word sociology derives part of its name from the Latin word socius ('companion' or 'fellowship'). The suffix -logy ('the study of') comes from that of the Greek -λογία, derived from λόγος (lógos, 'word' or 'knowledge'). The term sociology was first coined in 1780 by the French essayist Emmanuel-Joseph Sieyès in an unpublished manuscript. Sociology was later defined independently by French philosopher of science Auguste Comte (1798–1857) in 1838 as a new way of looking at society. Comte had earlier used the term social physics, but it had been subsequently appropriated by others, most notably the Belgian statistician Adolphe Quetelet. Comte endeavored to unify history, psychology, and economics through the scientific understanding of social life. Writing shortly after the malaise of the French Revolution, he proposed that social ills could be remedied through sociological positivism, an epistemological approach outlined in the Course in Positive Philosophy (1830–1842), later included in A General View of Positivism (1848). Comte believed a positivist stage would mark the final era in the progression of human understanding, after conjectural theological and metaphysical phases. In observing the circular dependence of theory and observation in science, and having classified the sciences, Comte may be regarded as the first philosopher of science in the modern sense of the term. Comte gave a powerful impetus to the development of sociology, an impetus that bore fruit in the later decades of the nineteenth century. To say this is certainly not to claim that French sociologists such as Durkheim were devoted disciples of the high priest of positivism. But by insisting on the irreducibility of each of his basic sciences to the particular science of sciences which it presupposed in the hierarchy and by emphasizing the nature of sociology as the scientific study of social phenomena Comte put sociology on the map. To be sure, [its] beginnings can be traced back well beyond Montesquieu, for example, and to Condorcet, not to speak of Saint-Simon, Comte's immediate predecessor. But Comte's clear recognition of sociology as a particular science, with a character of its own, justified Durkheim in regarding him as the father or founder of this science, even though Durkheim did not accept the idea of the three states and criticized Comte's approach to sociology. Marx Both Comte and Karl Marx set out to develop scientifically justified systems in the wake of European industrialization and secularization, informed by various key movements in the philosophies of history and science. Marx rejected Comtean positivism but in attempting to develop a "science of society" nevertheless came to be recognized as a founder of sociology as the word gained wider meaning. For Isaiah Berlin, even though Marx did not consider himself to be a sociologist, he may be regarded as the "true father" of modern sociology, "in so far as anyone can claim the title."To have given clear and unified answers in familiar empirical terms to those theoretical questions which most occupied men's minds at the time, and to have deduced from them clear practical directives without creating obviously artificial links between the two, was the principal achievement of Marx's theory. The sociological treatment of historical and moral problems, which Comte and after him, Spencer and Taine, had discussed and mapped, became a precise and concrete study only when the attack of militant Marxism made its conclusions a burning issue, and so made the search for evidence more zealous and the attention to method more intense. Spencer Herbert Spencer was one of the most popular and influential 19th-century sociologists. It is estimated that he sold one million books in his lifetime, far more than any other sociologist at the time. So strong was his influence that many other 19th-century thinkers, including Émile Durkheim, defined their ideas in relation to his. Durkheim's Division of Labour in Society is to a large extent an extended debate with Spencer from whose sociology Durkheim borrowed extensively. Also a notable biologist, Spencer coined the term survival of the fittest. While Marxian ideas defined one strand of sociology, Spencer was a critic of socialism, as well as a strong advocate for a laissez-faire style of government. His ideas were closely observed by conservative political circles, especially in the United States and England. Foundations of the academic discipline The first formal Department of Sociology in the world was established in 1892 by Albion Small—from the invitation of William Rainey Harper—at the University of Chicago. The American Journal of Sociology was founded shortly thereafter in 1895 by Small as well. The institutionalization of sociology as an academic discipline, however, was chiefly led by Émile Durkheim, who developed positivism as a foundation for practical social research. While Durkheim rejected much of the detail of Comte's philosophy, he retained and refined its method, maintaining that the social sciences are a logical continuation of the natural ones into the realm of human activity, and insisting that they may retain the same objectivity, rationalism, and approach to causality. Durkheim set up the first European department of sociology at the University of Bordeaux in 1895, publishing his Rules of the Sociological Method (1895). For Durkheim, sociology could be described as the "science of institutions, their genesis and their functioning." Durkheim's monograph Suicide (1897) is considered a seminal work in statistical analysis by contemporary sociologists. Suicide is a case study of variations in suicide rates among Catholic and Protestant populations, and served to distinguish sociological analysis from psychology or philosophy. It also marked a major contribution to the theoretical concept of structural functionalism. By carefully examining suicide statistics in different police districts, he attempted to demonstrate that Catholic communities have a lower suicide rate than that of Protestants, something he attributed to social (as opposed to individual or psychological) causes. He developed the notion of objective social facts to delineate a unique empirical object for the science of sociology to study. Through such studies he posited that sociology would be able to determine whether any given society is healthy or pathological, and seek social reform to negate organic breakdown, or "social anomie". Sociology quickly evolved as an academic response to the perceived challenges of modernity, such as industrialization, urbanization, secularization, and the process of rationalization. The field predominated in continental Europe, with British anthropology and statistics generally following on a separate trajectory. By the turn of the 20th century, however, many theorists were active in the English-speaking world. Few early sociologists were confined strictly to the subject, interacting also with economics, jurisprudence, psychology and philosophy, with theories being appropriated in a variety of different fields. Since its inception, sociological epistemology, methods, and frames of inquiry, have significantly expanded and diverged. Durkheim, Marx, and the German theorist Max Weber are typically cited as the three principal architects of sociology. Herbert Spencer, William Graham Sumner, Lester F. Ward, W. E. B. Du Bois, Vilfredo Pareto, Alexis de Tocqueville, Werner Sombart, Thorstein Veblen, Ferdinand Tönnies, Georg Simmel, Jane Addams and Karl Mannheim are often included on academic curricula as founding theorists. Curricula also may include Charlotte Perkins Gilman, Marianne Weber, Harriet Martineau, and Friedrich Engels as founders of the feminist tradition in sociology. Each key figure is associated with a particular theoretical perspective and orientation. Marx and Engels associated the emergence of modern society above all with the development of capitalism; for Durkheim it was connected in particular with industrialization and the new social division of labor which this brought about; for Weber it had to do with the emergence of a distinctive way of thinking, the rational calculation which he associated with the Protestant Ethic (more or less what Marx and Engels speak of in terms of those 'icy waves of egotistical calculation'). Together the works of these great classical sociologists suggest what Giddens has recently described as 'a multidimensional view of institutions of modernity' and which emphasises not only capitalism and industrialism as key institutions of modernity, but also 'surveillance' (meaning 'control of information and social supervision') and 'military power' (control of the means of violence in the context of the industrialisation of war). Further developments The first college course entitled "Sociology" was taught in the United States at Yale in 1875 by William Graham Sumner. In 1883, Lester F. Ward, who later became the first president of the American Sociological Association (ASA), published Dynamic Sociology—Or Applied social science as based upon statical sociology and the less complex sciences, attacking the laissez-faire sociology of Herbert Spencer and Sumner. Ward's 1,200-page book was used as core material in many early American sociology courses. In 1890, the oldest continuing American course in the modern tradition began at the University of Kansas, lectured by Frank W. Blackmar. The Department of Sociology at the University of Chicago was established in 1892 by Albion Small, who also published the first sociology textbook: An introduction to the study of society. George Herbert Mead and Charles Cooley, who had met at the University of Michigan in 1891 (along with John Dewey), moved to Chicago in 1894. Their influence gave rise to social psychology and the symbolic interactionism of the modern Chicago School. The American Journal of Sociology was founded in 1895, followed by the ASA in 1905. The sociological canon of classics with Durkheim and Max Weber at the top owes its existence in part to Talcott Parsons, who is largely credited with introducing both to American audiences. Parsons consolidated the sociological tradition and set the agenda for American sociology at the point of its fastest disciplinary growth. Sociology in the United States was less historically influenced by Marxism than its European counterpart, and to this day broadly remains more statistical in its approach. The first sociology department established in the United Kingdom was at the London School of Economics and Political Science (home of the British Journal of Sociology) in 1904. Leonard Trelawny Hobhouse and Edvard Westermarck became the lecturers in the discipline at the University of London in 1907. Harriet Martineau, an English translator of Comte, has been cited as the first female sociologist. In 1909, the German Sociological Association was founded by Ferdinand Tönnies, Max Weber, and Georg Simmel, among others. Weber established the first department in Germany at the Ludwig Maximilian University of Munich in 1919, having presented an influential new antipositivist sociology. In 1920, Florian Znaniecki set up the first department in Poland. The Institute for Social Research at the University of Frankfurt (later to become the Frankfurt School of critical theory) was founded in 1923. International co-operation in sociology began in 1893, when René Worms founded the Institut International de Sociologie, an institution later eclipsed by the much larger International Sociological Association (ISA), founded in 1949. Theoretical traditions Positivism and anti-positivism Positivism The overarching methodological principle of positivism is to conduct sociology in broadly the same manner as natural science. An emphasis on empiricism and the scientific method is sought to provide a tested foundation for sociological research based on the assumption that the only authentic knowledge is scientific knowledge, and that such knowledge can only arrive by positive affirmation through scientific methodology. Our main goal is to extend scientific rationalism to human conduct.... What has been called our positivism is but a consequence of this rationalism. The term has long since ceased to carry this meaning; there are no fewer than twelve distinct epistemologies that are referred to as positivism. Many of these approaches do not self-identify as "positivist", some because they themselves arose in opposition to older forms of positivism, and some because the label has over time become a pejorative term by being mistakenly linked with a theoretical empiricism. The extent of antipositivist criticism has also diverged, with many rejecting the scientific method and others only seeking to amend it to reflect 20th-century developments in the philosophy of science. However, positivism (broadly understood as a scientific approach to the study of society) remains dominant in contemporary sociology, especially in the United States. Loïc Wacquant distinguishes three major strains of positivism: Durkheimian, Logical, and Instrumental. None of these are the same as that set forth by Comte, who was unique in advocating such a rigid (and perhaps optimistic) version. While Émile Durkheim rejected much of the detail of Comte's philosophy, he retained and refined its method. Durkheim maintained that the social sciences are a logical continuation of the natural ones into the realm of human activity, and insisted that they should retain the same objectivity, rationalism, and approach to causality. He developed the notion of objective sui generis "social facts" to serve as unique empirical objects for the science of sociology to study. The variety of positivism that remains dominant today is termed instrumental positivism. This approach eschews epistemological and metaphysical concerns (such as the nature of social facts) in favour of methodological clarity, replicability, reliability and validity. This positivism is more or less synonymous with quantitative research, and so only resembles older positivism in practice. Since it carries no explicit philosophical commitment, its practitioners may not belong to any particular school of thought. Modern sociology of this type is often credited to Paul Lazarsfeld, who pioneered large-scale survey studies and developed statistical techniques for analysing them. This approach lends itself to what Robert K. Merton called middle-range theory: abstract statements that generalize from segregated hypotheses and empirical regularities rather than starting with an abstract idea of a social whole. Antipositivism The German philosopher Hegel criticised traditional empiricist epistemology, which he rejected as uncritical, and determinism, which he viewed as overly mechanistic. Karl Marx's methodology borrowed from Hegelian dialecticism but also a rejection of positivism in favour of critical analysis, seeking to supplement the empirical acquisition of "facts" with the elimination of illusions. He maintained that appearances need to be critiqued rather than simply documented. Early hermeneuticians such as Wilhelm Dilthey pioneered the distinction between natural and social science ('Geisteswissenschaft'). Various neo-Kantian philosophers, phenomenologists and human scientists further theorized how the analysis of the social world differs to that of the natural world due to the irreducibly complex aspects of human society, culture, and being. In the Italian context of development of social sciences and of sociology in particular, there are oppositions to the first foundation of the discipline, sustained by speculative philosophy in accordance with the antiscientific tendencies matured by critique of positivism and evolutionism, so a tradition Progressist struggles to establish itself. At the turn of the 20th century, the first generation of German sociologists formally introduced methodological anti-positivism, proposing that research should concentrate on human cultural norms, values, symbols, and social processes viewed from a resolutely subjective perspective. Max Weber argued that sociology may be loosely described as a science as it is able to identify causal relationships of human "social action"—especially among "ideal types", or hypothetical simplifications of complex social phenomena. As a non-positivist, however, Weber sought relationships that are not as "historical, invariant, or generalisable" as those pursued by natural scientists. Fellow German sociologist, Ferdinand Tönnies, theorised on two crucial abstract concepts with his work on "gemeinschaft and gesellschaft" (lit. 'community' and 'society'). Tönnies marked a sharp line between the realm of concepts and the reality of social action: the first must be treated axiomatically and in a deductive way ("pure sociology"), whereas the second empirically and inductively ("applied sociology"). [Sociology is] ... the science whose object is to interpret the meaning of social action and thereby give a causal explanation of the way in which the action proceeds and the effects which it produces. By 'action' in this definition is meant the human behaviour when and to the extent that the agent or agents see it as subjectively meaningful ... the meaning to which we refer may be either (a) the meaning actually intended either by an individual agent on a particular historical occasion or by a number of agents on an approximate average in a given set of cases, or (b) the meaning attributed to the agent or agents, as types, in a pure type constructed in the abstract. In neither case is the 'meaning' to be thought of as somehow objectively 'correct' or 'true' by some metaphysical criterion. This is the difference between the empirical sciences of action, such as sociology and history, and any kind of prior discipline, such as jurisprudence, logic, ethics, or aesthetics whose aim is to extract from their subject-matter 'correct' or 'valid' meaning. Both Weber and Georg Simmel pioneered the "Verstehen" (or 'interpretative') method in social science; a systematic process by which an outside observer attempts to relate to a particular cultural group, or indigenous people, on their own terms and from their own point of view. Through the work of Simmel, in particular, sociology acquired a possible character beyond positivist data-collection or grand, deterministic systems of structural law. Relatively isolated from the sociological academy throughout his lifetime, Simmel presented idiosyncratic analyses of modernity more reminiscent of the phenomenological and existential writers than of Comte or Durkheim, paying particular concern to the forms of, and possibilities for, social individuality. His sociology engaged in a neo-Kantian inquiry into the limits of perception, asking 'What is society?' in a direct allusion to Kant's question 'What is nature?' The deepest problems of modern life flow from the attempt of the individual to maintain the independence and individuality of his existence against the sovereign powers of society, against the weight of the historical heritage and the external culture and technique of life. The antagonism represents the most modern form of the conflict which primitive man must carry on with nature for his bodily existence. The eighteenth century may have called for liberation from all the ties which grew up historically in politics, in religion, in morality, and in economics to permit the original natural virtue of man, which is equal in everyone, to develop without inhibition; the nineteenth century may have sought to promote, in addition to man's freedom, his individuality (which is connected with the division of labor) and his achievements which make him unique and indispensable but which at the same time make him so much the more dependent on the complementary activity of others; Nietzsche may have seen the relentless struggle of the individual as the prerequisite for his full development, while socialism found the same thing in the suppression of all competition – but in each of these the same fundamental motive was at work, namely the resistance of the individual to being leveled, swallowed up in the social-technological mechanism. Classical theory The contemporary discipline of sociology is theoretically multi-paradigmatic in line with the contentions of classical social theory. Randall Collins' well-cited survey of sociological theory retroactively labels various theorists as belonging to four theoretical traditions: Functionalism, Conflict, Symbolic Interactionism, and Utilitarianism. Accordingly, modern sociological theory predominantly descends from functionalist (Durkheim) and conflict (Marx and Weber) approaches to social structure, as well as from symbolic-interactionist approaches to social interaction, such as micro-level structural (Simmel) and pragmatist (Mead, Cooley) perspectives. Utilitarianism (also known as rational choice or social exchange), although often associated with economics, is an established tradition within sociological theory. Lastly, as argued by Raewyn Connell, a tradition that is often forgotten is that of Social Darwinism, which applies the logic of Darwinian biological evolution to people and societies. This tradition often aligns with classical functionalism, and was once the dominant theoretical stance in American sociology, from c. 1881 – c. 1915, associated with several founders of sociology, primarily Herbert Spencer, Lester F. Ward, and William Graham Sumner. Contemporary sociological theory retains traces of each of these traditions and they are by no means mutually exclusive. Functionalism A broad historical paradigm in both sociology and anthropology, functionalism addresses the social structure—referred to as "social organization" by the classical theorists—with respect to the whole as well as the necessary function of the whole's constituent elements. A common analogy (popularized by Herbert Spencer) is to regard norms and institutions as 'organs' that work towards the proper functioning of the entire 'body' of society. The perspective was implicit in the original sociological positivism of Comte but was theorized in full by Durkheim, again with respect to observable, structural laws. Functionalism also has an anthropological basis in the work of theorists such as Marcel Mauss, Bronisław Malinowski, and Radcliffe-Brown. It is in the latter's specific usage that the prefix "structural" emerged. Classical functionalist theory is generally united by its tendency towards biological analogy and notions of social evolutionism, in that the basic form of society would increase in complexity and those forms of social organization that promoted solidarity would eventually overcome social disorganization. As Giddens states:Functionalist thought, from Comte onwards, has looked particularly towards biology as the science providing the closest and most compatible model for social science. Biology has been taken to provide a guide to conceptualizing the structure and the function of social systems and to analyzing processes of evolution via mechanisms of adaptation. Functionalism strongly emphasizes the pre-eminence of the social world over its individual parts (i.e. its constituent actors, human subjects). Conflict theory Functionalist theories emphasize "cohesive systems" and are often contrasted with "conflict theories", which critique the overarching socio-political system or emphasize the inequality between particular groups. The following quotes from Durkheim and Marx epitomize the political, as well as theoretical, disparities, between functionalist and conflict thought respectively: To aim for a civilization beyond that made possible by the nexus of the surrounding environment will result in unloosing sickness into the very society we live in. Collective activity cannot be encouraged beyond the point set by the condition of the social organism without undermining health. The history of all hitherto existing society is the history of class struggles. Freeman and slave, patrician and plebeian, lord and serf, guild-master and journeyman, in a word, oppressor and oppressed, stood in constant opposition to one another, carried on an uninterrupted, now hidden, now open fight, a fight that each time ended, either in a revolutionary re-constitution of society at large, or in the common ruin of the contending classes. Symbolic interactionism Symbolic interaction—often associated with interactionism, phenomenology, dramaturgy, interpretivism—is a sociological approach that places emphasis on subjective meanings and the empirical unfolding of social processes, generally accessed through micro-analysis. This tradition emerged in the Chicago School of the 1920s and 1930s, which, prior to World War II, "had been the center of sociological research and graduate study." The approach focuses on creating a framework for building a theory that sees society as the product of the everyday interactions of individuals. Society is nothing more than the shared reality that people construct as they interact with one another. This approach sees people interacting in countless settings using symbolic communications to accomplish the tasks at hand. Therefore, society is a complex, ever-changing mosaic of subjective meanings. Some critics of this approach argue that it only looks at what is happening in a particular social situation, and disregards the effects that culture, race or gender (i.e. social-historical structures) may have in that situation. Some important sociologists associated with this approach include Max Weber, George Herbert Mead, Erving Goffman, George Homans, and Peter Blau. It is also in this tradition that the radical-empirical approach of ethnomethodology emerges from the work of Harold Garfinkel. Utilitarianism Utilitarianism is often referred to as exchange theory or rational choice theory in the context of sociology. This tradition tends to privilege the agency of individual rational actors and assumes that within interactions individuals always seek to maximize their own self-interest. As argued by Josh Whitford, rational actors are assumed to have four basic elements: "a knowledge of alternatives;" "a knowledge of, or beliefs about the consequences of the various alternatives;" "an ordering of preferences over outcomes;" and "a decision rule, to select among the possible alternatives" Exchange theory is specifically attributed to the work of George C. Homans, Peter Blau and Richard Emerson. Organizational sociologists James G. March and Herbert A. Simon noted that an individual's rationality is bounded by the context or organizational setting. The utilitarian perspective in sociology was, most notably, revitalized in the late 20th century by the work of former ASA president James Coleman. 20th-century social theory Following the decline of theories of sociocultural evolution in the United States, the interactionist thought of the Chicago School dominated American sociology. As Anselm Strauss describes, "we didn't think symbolic interaction was a perspective in sociology; we thought it was sociology." Moreover, philosophical and psychological pragmatism grounded this tradition. After World War II, mainstream sociology shifted to the survey-research of Paul Lazarsfeld at Columbia University and the general theorizing of Pitirim Sorokin, followed by Talcott Parsons at Harvard University. Ultimately, "the failure of the Chicago, Columbia, and Wisconsin [sociology] departments to produce a significant number of graduate students interested in and committed to general theory in the years 1936–45 was to the advantage of the Harvard department." As Parsons began to dominate general theory, his work primarily referenced European sociology—almost entirely omitting citations of both the American tradition of sociocultural-evolution as well as pragmatism. In addition to Parsons' revision of the sociological canon (which included Marshall, Pareto, Weber and Durkheim), the lack of theoretical challenges from other departments nurtured the rise of the Parsonian structural-functionalist movement, which reached its crescendo in the 1950s, but by the 1960s was in rapid decline. By the 1980s, most functionalist perspectives in Europe had broadly been replaced by conflict-oriented approaches, and to many in the discipline, functionalism was considered "as dead as a dodo:" According to Giddens:The orthodox consensus terminated in the late 1960s and 1970s as the middle ground shared by otherwise competing perspectives gave way and was replaced by a baffling variety of competing perspectives. This third 'generation' of social theory includes phenomenologically inspired approaches, critical theory, ethnomethodology, symbolic interactionism, structuralism, post-structuralism, and theories written in the tradition of hermeneutics and ordinary language philosophy. Pax Wisconsana While some conflict approaches also gained popularity in the United States, the mainstream of the discipline instead shifted to a variety of empirically oriented middle-range theories with no single overarching, or "grand", theoretical orientation. John Levi Martin refers to this "golden age of methodological unity and theoretical calm" as the Pax Wisconsana, as it reflected the composition of the sociology department at the University of Wisconsin–Madison: numerous scholars working on separate projects with little contention. Omar Lizardo describes the pax wisconsana as "a Midwestern flavored, Mertonian resolution of the theory/method wars in which [sociologists] all agreed on at least two working hypotheses: (1) grand theory is a waste of time; [and] (2) good theory has to be good to think with or goes in the trash bin." Despite the aversion to grand theory in the latter half of the 20th century, several new traditions have emerged that propose various syntheses: structuralism, post-structuralism, cultural sociology and systems theory. Some sociologists have called for a return to 'grand theory' to combat the rise of scientific and pragmatist influences within the tradition of sociological thought (see Duane Rousselle). Structuralism The structuralist movement originated primarily from the work of Durkheim as interpreted by two European scholars: Anthony Giddens, a sociologist, whose theory of structuration draws on the linguistic theory of Ferdinand de Saussure; and Claude Lévi-Strauss, an anthropologist. In this context, 'structure' does not refer to 'social structure', but to the semiotic understanding of human culture as a system of signs. One may delineate four central tenets of structuralism: Structure is what determines the structure of a whole. Structuralists believe that every system has a structure. Structuralists are interested in 'structural' laws that deal with coexistence rather than changes. Structures are the 'real things' beneath the surface or the appearance of meaning. The second tradition of structuralist thought, contemporaneous with Giddens, emerges from the American School of social network analysis in the 1970s and 1980s, spearheaded by the Harvard Department of Social Relations led by Harrison White and his students. This tradition of structuralist thought argues that, rather than semiotics, social structure is networks of patterned social relations. And, rather than Levi-Strauss, this school of thought draws on the notions of structure as theorized by Levi-Strauss' contemporary anthropologist, Radcliffe-Brown. Some refer to this as "network structuralism", and equate it to "British structuralism" as opposed to the "French structuralism" of Levi-Strauss. Post-structuralism Post-structuralist thought has tended to reject 'humanist' assumptions in the construction of social theory. Michel Foucault provides an important critique in his Archaeology of the Human Sciences, though Habermas (1986) and Rorty (1986) have both argued that Foucault merely replaces one such system of thought with another. The dialogue between these intellectuals highlights a trend in recent years for certain schools of sociology and philosophy to intersect. The anti-humanist position has been associated with "postmodernism", a term used in specific contexts to describe an era or phenomena, but occasionally construed as a method. Central theoretical problems Overall, there is a strong consensus regarding the central problems of sociological theory, which are largely inherited from the classical theoretical traditions. This consensus is: how to link, transcend or cope with the following "big three" dichotomies: subjectivity and objectivity, which deal with knowledge; structure and agency, which deal with action; and synchrony and diachrony, which deal with time. Lastly, sociological theory often grapples with the problem of integrating or transcending the divide between micro, meso, and macro-scale social phenomena, which is a subset of all three central problems. Subjectivity and objectivity The problem of subjectivity and objectivity can be divided into two parts: a concern over the general possibilities of social actions, and the specific problem of social scientific knowledge. In the former, the subjective is often equated (though not necessarily) with the individual, and the individual's intentions and interpretations of the objective. The objective is often considered any public or external action or outcome, on up to society writ large. A primary question for social theorists, then, is how knowledge reproduces along the chain of subjective-objective-subjective, that is to say: how is intersubjectivity achieved? While, historically, qualitative methods have attempted to tease out subjective interpretations, quantitative survey methods also attempt to capture individual subjectivities. Qualitative methods take an approach to objective description known as in situ, meaning that descriptions must have appropriate contextual information to understand the information. The latter concern with scientific knowledge results from the fact that a sociologist is part of the very object they seek to explain, as Bourdieu explains: How can the sociologist effect in practice this radical doubting which is indispensable for bracketing all the presuppositions inherent in the fact that she is a social being, that she is therefore socialised and led to feel "like a fish in water" within that social world whose structures she has internalised? How can she prevent the social world itself from carrying out the construction of the object, in a sense, through her, through these unself-conscious operations or operations unaware of themselves of which she is the apparent subject? Structure and agency Structure and agency, sometimes referred to as determinism versus voluntarism, form an enduring ontological debate in social theory: "Do social structures determine an individual's behaviour or does human agency?" In this context, agency refers to the capacity of individuals to act independently and make free choices, whereas structure relates to factors that limit or affect the choices and actions of individuals (e.g. social class, religion, gender, ethnicity, etc.). Discussions over the primacy of either structure or agency relate to the core of sociological epistemology (i.e. "what is the social world made of?", "what is a cause in the social world, and what is an effect?"). A perennial question within this debate is that of "social reproduction": how are structures (specifically, structures producing inequality) reproduced through the choices of individuals? Synchrony and diachrony Synchrony and diachrony (or statics and dynamics) within social theory are terms that refer to a distinction that emerged through the work of Levi-Strauss who inherited it from the linguistics of Ferdinand de Saussure. Synchrony slices moments of time for analysis, thus it is an analysis of static social reality. Diachrony, on the other hand, attempts to analyse dynamic sequences. Following Saussure, synchrony would refer to social phenomena as a static concept like a language, while diachrony would refer to unfolding processes like actual speech. In Anthony Giddens' introduction to Central Problems in Social Theory, he states that, "in order to show the interdependence of action and structure…we must grasp the time space relations inherent in the constitution of all social interaction." And like structure and agency, time is integral to discussion of social reproduction. In terms of sociology, historical sociology is often better positioned to analyse social life as diachronic, while survey research takes a snapshot of social life and is thus better equipped to understand social life as synchronized. Some argue that the synchrony of social structure is a methodological perspective rather than an ontological claim. Nonetheless, the problem for theory is how to integrate the two manners of recording and thinking about social data. Research methodology Sociological research methods may be divided into two broad, though often supplementary, categories: Qualitative designs emphasize understanding of social phenomena through direct observation, communication with participants, or analysis of texts, and may stress contextual and subjective accuracy over generality. Quantitative designs approach social phenomena through quantifiable evidence, and often rely on statistical analysis of many cases (or across intentionally designed treatments in an experiment) to establish valid and reliable general claims. Sociologists are often divided into camps of support for particular research techniques. These disputes relate to the epistemological debates at the historical core of social theory. While very different in many aspects, both qualitative and quantitative approaches involve a systematic interaction between theory and data. Quantitative methodologies hold the dominant position in sociology, especially in the United States. In the discipline's two most cited journals, quantitative articles have historically outnumbered qualitative ones by a factor of two. (Most articles published in the largest British journal, on the other hand, are qualitative.) Most textbooks on the methodology of social research are written from the quantitative perspective, and the very term "methodology" is often used synonymously with "statistics". Practically all sociology PhD programmes in the United States require training in statistical methods. The work produced by quantitative researchers is also deemed more 'trustworthy' and 'unbiased' by the general public, though this judgment continues to be challenged by antipositivists. The choice of method often depends largely on what the researcher intends to investigate. For example, a researcher concerned with drawing a statistical generalization across an entire population may administer a survey questionnaire to a representative sample population. By contrast, a researcher who seeks full contextual understanding of an individual's social actions may choose ethnographic participant observation or open-ended interviews. Studies will commonly combine, or 'triangulate', quantitative and qualitative methods as part of a 'multi-strategy' design. For instance, a quantitative study may be performed to obtain statistical patterns on a target sample, and then combined with a qualitative interview to determine the play of agency. Sampling Quantitative methods are often used to ask questions about a population that is very large, making a census or a complete enumeration of all the members in that population infeasible. A 'sample' then forms a manageable subset of a population. In quantitative research, statistics are used to draw inferences from this sample regarding the population as a whole. The process of selecting a sample is referred to as 'sampling'. While it is usually best to sample randomly, concern with differences between specific subpopulations sometimes calls for stratified sampling. Conversely, the impossibility of random sampling sometimes necessitates nonprobability sampling, such as convenience sampling or snowball sampling. Methods The following list of research methods is neither exclusive nor exhaustive: Archival research (or the Historical method): Draws upon the secondary data located in historical archives and records, such as biographies, memoirs, journals, and so on. Content analysis: The content of interviews and other texts is systematically analysed. Often data is 'coded' as a part of the 'grounded theory' approach using qualitative data analysis (QDA) software, such as Atlas.ti, MAXQDA, NVivo, or QDA Miner. Experimental research: The researcher isolates a single social process and reproduces it in a laboratory (for example, by creating a situation where unconscious sexist judgements are possible), seeking to determine whether or not certain social variables can cause, or depend upon, other variables (for instance, seeing if people's feelings about traditional gender roles can be manipulated by the activation of contrasting gender stereotypes). Participants are randomly assigned to different groups that either serve as controls—acting as reference points because they are tested with regard to the dependent variable, albeit without having been exposed to any independent variables of interest—or receive one or more treatments. Randomization allows the researcher to be sure that any resulting differences between groups are the result of the treatment. Longitudinal study: An extensive examination of a specific person or group over a long period of time. Observation: Using data from the senses, the researcher records information about social phenomenon or behaviour. Observation techniques may or may not feature participation. In participant observation, the researcher goes into the field (e.g. a community or a place of work), and participates in the activities of the field for a prolonged period of time in order to acquire a deep understanding of it. Data acquired through these techniques may be analysed either quantitatively or qualitatively. In the observation research, a sociologist might study global warming in some part of the world that is less populated. Program Evaluation is a systematic method for collecting, analyzing, and using information to answer questions about projects, policies and programs, particularly about their effectiveness and efficiency. In both the public and private sectors, stakeholders often want to know whether the programs they are funding, implementing, voting for, or objecting to are producing the intended effect. While program evaluation first focuses on this definition, important considerations often include how much the program costs per participant, how the program could be improved, whether the program is worthwhile, whether there are better alternatives, if there are unintended outcomes, and whether the program goals are appropriate and useful. Survey research: The researcher gathers data using interviews, questionnaires, or similar feedback from a set of people sampled from a particular population of interest. Survey items from an interview or questionnaire may be open-ended or closed-ended. Data from surveys is usually analysed statistically on a computer. Computational sociology Sociologists increasingly draw upon computationally intensive methods to analyse and model social phenomena. Using computer simulations, artificial intelligence, text mining, complex statistical methods, and new analytic approaches like social network analysis and social sequence analysis, computational sociology develops and tests theories of complex social processes through bottom-up modelling of social interactions. Although the subject matter and methodologies in social science differ from those in natural science or computer science, several of the approaches used in contemporary social simulation originated from fields such as physics and artificial intelligence. By the same token, some of the approaches that originated in computational sociology have been imported into the natural sciences, such as measures of network centrality from the fields of social network analysis and network science. In relevant literature, computational sociology is often related to the study of social complexity. Social complexity concepts such as complex systems, non-linear interconnection among macro and micro process, and emergence, have entered the vocabulary of computational sociology. A practical and well-known example is the construction of a computational model in the form of an "artificial society", by which researchers can analyse the structure of a social system. Subfields Culture Sociologists' approach to culture can be divided into "sociology of culture" and "cultural sociology"—terms which are similar, though not entirely interchangeable. Sociology of culture is an older term, and considers some topics and objects as more or less "cultural" than others. Conversely, cultural sociology sees all social phenomena as inherently cultural. Sociology of culture often attempts to explain certain cultural phenomena as a product of social processes, while cultural sociology sees culture as a potential explanation of social phenomena. For Simmel, culture referred to "the cultivation of individuals through the agency of external forms which have been objectified in the course of history." While early theorists such as Durkheim and Mauss were influential in cultural anthropology, sociologists of culture are generally distinguished by their concern for modern (rather than primitive or ancient) society. Cultural sociology often involves the hermeneutic analysis of words, artefacts and symbols, or ethnographic interviews. However, some sociologists employ historical-comparative or quantitative techniques in the analysis of culture, Weber and Bourdieu for instance. The subfield is sometimes allied with critical theory in the vein of Theodor W. Adorno, Walter Benjamin, and other members of the Frankfurt School. Loosely distinct from the sociology of culture is the field of cultural studies. Birmingham School theorists such as Richard Hoggart and Stuart Hall questioned the division between "producers" and "consumers" evident in earlier theory, emphasizing the reciprocity in the production of texts. Cultural Studies aims to examine its subject matter in terms of cultural practices and their relation to power. For example, a study of a subculture (e.g. white working class youth in London) would consider the social practices of the group as they relate to the dominant class. The "cultural turn" of the 1960s ultimately placed culture much higher on the sociological agenda. Art, music and literature Sociology of literature, film, and art is a subset of the sociology of culture. This field studies the social production of artistic objects and its social implications. A notable example is Pierre Bourdieu's Les Règles de L'Art: Genèse et Structure du Champ Littéraire (1992). None of the founding fathers of sociology produced a detailed study of art, but they did develop ideas that were subsequently applied to literature by others. Marx's theory of ideology was directed at literature by Pierre Macherey, Terry Eagleton and Fredric Jameson. Weber's theory of modernity as cultural rationalization, which he applied to music, was later applied to all the arts, literature included, by Frankfurt School writers such as Theodor Adorno and Jürgen Habermas. Durkheim's view of sociology as the study of externally defined social facts was redirected towards literature by Robert Escarpit. Bourdieu's own work is clearly indebted to Marx, Weber and Durkheim. Criminality, deviance, law and punishment Criminologists analyse the nature, causes, and control of criminal activity, drawing upon methods across sociology, psychology, and the behavioural sciences. The sociology of deviance focuses on actions or behaviours that violate norms, including both infringements of formally enacted rules (e.g., crime) and informal violations of cultural norms. It is the remit of sociologists to study why these norms exist; how they change over time; and how they are enforced. The concept of social disorganization is when the broader social systems leads to violations of norms. For instance, Robert K. Merton produced a typology of deviance, which includes both individual and system level causal explanations of deviance. Sociology of law The study of law played a significant role in the formation of classical sociology. Durkheim famously described law as the "visible symbol" of social solidarity. The sociology of law refers to both a sub-discipline of sociology and an approach within the field of legal studies. Sociology of law is a diverse field of study that examines the interaction of law with other aspects of society, such as the development of legal institutions and the effect of laws on social change and vice versa. For example, an influential recent work in the field relies on statistical analyses to argue that the increase in incarceration in the US over the last 30 years is due to changes in law and policing and not to an increase in crime; and that this increase has significantly contributed to the persistence of racial stratification. Communications and information technologies The sociology of communications and information technologies includes "the social aspects of computing, the Internet, new media, computer networks, and other communication and information technologies." Internet and digital media The Internet is of interest to sociologists in various ways, most practically as a tool for research and as a discussion platform. The sociology of the Internet in the broad sense concerns the analysis of online communities (e.g. newsgroups, social networking sites) and virtual worlds, meaning that there is often overlap with community sociology. Online communities may be studied statistically through network analysis or interpreted qualitatively through virtual ethnography. Moreover, organizational change is catalysed through new media, thereby influencing social change at-large, perhaps forming the framework for a transformation from an industrial to an informational society. One notable text is Manuel Castells' The Internet Galaxy—the title of which forms an inter-textual reference to Marshall McLuhan's The Gutenberg Galaxy. Closely related to the sociology of the Internet is digital sociology, which expands the scope of study to address not only the internet but also the impact of the other digital media and devices that have emerged since the first decade of the twenty-first century. Media As with cultural studies, media study is a distinct discipline that owes to the convergence of sociology and other social sciences and humanities, in particular, literary criticism and critical theory. Though neither the production process nor the critique of aesthetic forms is in the remit of sociologists, analyses of socializing factors, such as ideological effects and audience reception, stem from sociological theory and method. Thus the 'sociology of the media' is not a subdiscipline per se, but the media is a common and often indispensable topic. Economic sociology The term "economic sociology" was first used by William Stanley Jevons in 1879, later to be coined in the works of Durkheim, Weber, and Simmel between 1890 and 1920. Economic sociology arose as a new approach to the analysis of economic phenomena, emphasizing class relations and modernity as a philosophical concept. The relationship between capitalism and modernity is a salient issue, perhaps best demonstrated in Weber's The Protestant Ethic and the Spirit of Capitalism (1905) and Simmel's The Philosophy of Money (1900). The contemporary period of economic sociology, also known as new economic sociology, was consolidated by the 1985 work of Mark Granovetter titled "Economic Action and Social Structure: The Problem of Embeddedness". This work elaborated the concept of embeddedness, which states that economic relations between individuals or firms take place within existing social relations (and are thus structured by these relations as well as the greater social structures of which those relations are a part). Social network analysis has been the primary methodology for studying this phenomenon. Granovetter's theory of the strength of weak ties and Ronald Burt's concept of structural holes are two of the best known theoretical contributions of this field. Work, employment, and industry The sociology of work, or industrial sociology, examines "the direction and implications of trends in technological change, globalization, labour markets, work organization, managerial practices and employment relations to the extent to which these trends are intimately related to changing patterns of inequality in modern societies and to the changing experiences of individuals and families the ways in which workers challenge, resist and make their own contributions to the patterning of work and shaping of work institutions." Education The sociology of education is the study of how educational institutions determine social structures, experiences, and other outcomes. It is particularly concerned with the schooling systems of modern industrial societies. A classic 1966 study in this field by James Coleman, known as the "Coleman Report", analysed the performance of over 150,000 students and found that student background and socioeconomic status are much more important in determining educational outcomes than are measured differences in school resources (i.e. per pupil spending). The controversy over "school effects" ignited by that study has continued to this day. The study also found that socially disadvantaged black students profited from schooling in racially mixed classrooms, and thus served as a catalyst for desegregation busing in American public schools. Environment Environmental sociology is the study of human interactions with the natural environment, typically emphasizing human dimensions of environmental problems, social impacts of those problems, and efforts to resolve them. As with other sub-fields of sociology, scholarship in environmental sociology may be at one or multiple levels of analysis, from global (e.g. world-systems) to local, societal to individual. Attention is paid also to the processes by which environmental problems become defined and known to humans. As argued by notable environmental sociologist John Bellamy Foster, the predecessor to modern environmental sociology is Marx's analysis of the metabolic rift, which influenced contemporary thought on sustainability. Environmental sociology is often interdisciplinary and overlaps with the sociology of risk, rural sociology and the sociology of disaster. Human ecology Human ecology deals with interdisciplinary study of the relationship between humans and their natural, social, and built environments. In addition to Environmental sociology, this field overlaps with architectural sociology, urban sociology, and to some extent visual sociology. In turn, visual sociology—which is concerned with all visual dimensions of social life—overlaps with media studies in that it uses photography, film and other technologies of media. Social pre-wiring Social pre-wiring deals with the study of fetal social behavior and social interactions in a multi-fetal environment. Specifically, social pre-wiring refers to the ontogeny of social interaction. Also informally referred to as, "wired to be social". The theory questions whether there is a propensity to socially oriented action already present before birth. Research in the theory concludes that newborns are born into the world with a unique genetic wiring to be social. Circumstantial evidence supporting the social pre-wiring hypothesis can be revealed when examining newborns' behavior. Newborns, not even hours after birth, have been found to display a preparedness for social interaction. This preparedness is expressed in ways such as their imitation of facial gestures. This observed behavior cannot be attributed to any current form of socialization or social construction. Rather, newborns most likely inherit to some extent social behavior and identity through genetics. Principal evidence of this theory is uncovered by examining Twin pregnancies. The main argument is, if there are social behaviors that are inherited and developed before birth, then one should expect twin foetuses to engage in some form of social interaction before they are born. Thus, ten foetuses were analyzed over a period of time using ultrasound techniques. Using kinematic analysis, the results of the experiment were that the twin foetuses would interact with each other for longer periods and more often as the pregnancies went on. Researchers were able to conclude that the performance of movements between the co-twins were not accidental but specifically aimed. The social pre-wiring hypothesis was proved correct: The central advance of this study is the demonstration that 'social actions' are already performed in the second trimester of gestation. Starting from the 14th week of gestation twin foetuses plan and execute movements specifically aimed at the co-twin. These findings force us to predate the emergence of social behavior: when the context enables it, as in the case of twin foetuses, other-directed actions are not only possible but predominant over self-directed actions. Family, gender, and sexuality Family, gender and sexuality form a broad area of inquiry studied in many sub-fields of sociology. A family is a group of people who are related by kinship ties :- Relations of blood / marriage / civil partnership or adoption. The family unit is one of the most important social institutions found in some form in nearly all known societies. It is the basic unit of social organization and plays a key role in socializing children into the culture of their society. The sociology of the family examines the family, as an institution and unit of socialization, with special concern for the comparatively modern historical emergence of the nuclear family and its distinct gender roles. The notion of "childhood" is also significant. As one of the more basic institutions to which one may apply sociological perspectives, the sociology of the family is a common component on introductory academic curricula. Feminist sociology, on the other hand, is a normative sub-field that observes and critiques the cultural categories of gender and sexuality, particularly with respect to power and inequality. The primary concern of feminist theory is the patriarchy and the systematic oppression of women apparent in many societies, both at the level of small-scale interaction and in terms of the broader social structure. Feminist sociology also analyses how gender interlocks with race and class to produce and perpetuate social inequalities. "How to account for the differences in definitions of femininity and masculinity and in sex role across different societies and historical periods" is also a concern. Health, illness, and the body The sociology of health and illness focuses on the social effects of, and public attitudes toward, illnesses, diseases, mental health and disabilities. This sub-field also overlaps with gerontology and the study of the ageing process. Medical sociology, by contrast, focuses on the inner-workings of the medical profession, its organizations, its institutions and how these can shape knowledge and interactions. In Britain, sociology was introduced into the medical curriculum following the Goodenough Report (1944). The sociology of the body and embodiment takes a broad perspective on the idea of "the body" and includes "a wide range of embodied dynamics including human and non-human bodies, morphology, human reproduction, anatomy, body fluids, biotechnology, genetics". This often intersects with health and illness, but also theories of bodies as political, social, cultural, economic and ideological productions. The ISA maintains a Research Committee devoted to "the Body in the Social Sciences". Death, dying, bereavement A subfield of the sociology of health and illness that overlaps with cultural sociology is the study of death, dying and bereavement, sometimes referred to broadly as the sociology of death. This topic is exemplified by the work of Douglas Davies and Michael C. Kearl. Knowledge and science The sociology of knowledge is the study of the relationship between human thought and the social context within which it arises, and of the effects prevailing ideas have on societies. The term first came into widespread use in the 1920s, when a number of German-speaking theorists, most notably Max Scheler, and Karl Mannheim, wrote extensively on it. With the dominance of functionalism through the middle years of the 20th century, the sociology of knowledge tended to remain on the periphery of mainstream sociological thought. It was largely reinvented and applied much more closely to everyday life in the 1960s, particularly by Peter L. Berger and Thomas Luckmann in The Social Construction of Reality (1966) and is still central for methods dealing with qualitative understanding of human society (compare socially constructed reality). The "archaeological" and "genealogical" studies of Michel Foucault are of considerable contemporary influence. The sociology of science involves the study of science as a social activity, especially dealing "with the social conditions and effects of science, and with the social structures and processes of scientific activity." Important theorists in the sociology of science include Robert K. Merton and Bruno Latour. These branches of sociology have contributed to the formation of science and technology studies. Both the ASA and the BSA have sections devoted to the subfield of Science, Knowledge and Technology. The ISA maintains a Research Committee on Science and Technology. Leisure Sociology of leisure is the study of how humans organize their free time. Leisure includes a broad array of activities, such as sport, tourism, and the playing of games. The sociology of leisure is closely tied to the sociology of work, as each explores a different side of the work–leisure relationship. More recent studies in the field move away from the work–leisure relationship and focus on the relation between leisure and culture. This area of sociology began with Thorstein Veblen's Theory of the Leisure Class. Peace, war, and conflict This subfield of sociology studies, broadly, the dynamics of war, conflict resolution, peace movements, war refugees, conflict resolution and military institutions. As a subset of this subfield, military sociology aims towards the systematic study of the military as a social group rather than as an organization. It is a highly specialized sub-field which examines issues related to service personnel as a distinct group with coerced collective action based on shared interests linked to survival in vocation and combat, with purposes and values that are more defined and narrower than within civil society. Military sociology also concerns civilian-military relations and interactions between other groups or governmental agencies. Topics include the dominant assumptions held by those in the military, changes in military members' willingness to fight, military unionization, military professionalism, the increased utilization of women, the military industrial-academic complex, the military's dependence on research, and the institutional and organizational structure of military. Political sociology Historically, political sociology concerned the relations between political organization and society. A typical research question in this area might be: "Why do so few American citizens choose to vote?" In this respect questions of political opinion formation brought about some of the pioneering uses of statistical survey research by Paul Lazarsfeld. A major subfield of political sociology developed in relation to such questions, which draws on comparative history to analyse socio-political trends. The field developed from the work of Max Weber and Moisey Ostrogorsky. Contemporary political sociology includes these areas of research, but it has been opened up to wider questions of power and politics. Today political sociologists are as likely to be concerned with how identities are formed that contribute to structural domination by one group over another; the politics of who knows how and with what authority; and questions of how power is contested in social interactions in such a way as to bring about widespread cultural and social change. Such questions are more likely to be studied qualitatively. The study of social movements and their effects has been especially important in relation to these wider definitions of politics and power. Political sociology has also moved beyond methodological nationalism and analysed the role of non-governmental organizations, the diffusion of the nation-state throughout the Earth as a social construct, and the role of stateless entities in the modern world society. Contemporary political sociologists also study inter-state interactions and human rights. Population and demography Demographers or sociologists of population study the size, composition and change over time of a given population. Demographers study how these characteristics impact, or are impacted by, various social, economic or political systems. The study of population is also closely related to human ecology and environmental sociology, which studies a population's relationship with the surrounding environment and often overlaps with urban or rural sociology. Researchers in this field may study the movement of populations: transportation, migrations, diaspora, etc., which falls into the subfield known as mobilities studies and is closely related to human geography. Demographers may also study spread of disease within a given population or epidemiology. Public sociology Public sociology refers to an approach to the discipline which seeks to transcend the academy in order to engage with wider audiences. It is perhaps best understood as a style of sociology rather than a particular method, theory, or set of political values. This approach is primarily associated with Michael Burawoy who contrasted it with professional sociology, a form of academic sociology that is concerned primarily with addressing other professional sociologists. Public sociology is also part of the broader field of science communication or science journalism. Race and ethnic relations The sociology of race and of ethnic relations is the area of the discipline that studies the social, political, and economic relations between races and ethnicities at all levels of society. This area encompasses the study of racism, residential segregation, and other complex social processes between different racial and ethnic groups. This research frequently interacts with other areas of sociology such as stratification and social psychology, as well as with postcolonial theory. At the level of political policy, ethnic relations are discussed in terms of either assimilationism or multiculturalism. Anti-racism forms another style of policy, particularly popular in the 1960s and 1970s. Religion The sociology of religion concerns the practices, historical backgrounds, developments, universal themes and roles of religion in society. There is particular emphasis on the recurring role of religion in all societies and throughout recorded history. The sociology of religion is distinguished from the philosophy of religion in that sociologists do not set out to assess the validity of religious truth-claims, instead assuming what Peter L. Berger has described as a position of "methodological atheism". It may be said that the modern formal discipline of sociology began with the analysis of religion in Durkheim's 1897 study of suicide rates among Roman Catholic and Protestant populations. Max Weber published four major texts on religion in a context of economic sociology and social stratification: The Protestant Ethic and the Spirit of Capitalism (1905), The Religion of China: Confucianism and Taoism (1915), The Religion of India: The Sociology of Hinduism and Buddhism (1915), and Ancient Judaism (1920). Contemporary debates often centre on topics such as secularization, civil religion, the intersection of religion and economics and the role of religion in a context of globalization and multiculturalism. Social change and development The sociology of change and development attempts to understand how societies develop and how they can be changed. This includes studying many different aspects of society, for example demographic trends, political or technological trends, or changes in culture. Within this field, sociologists often use macrosociological methods or historical-comparative methods. In contemporary studies of social change, there are overlaps with international development or community development. However, most of the founders of sociology had theories of social change based on their study of history. For instance, Marx contended that the material circumstances of society ultimately caused the ideal or cultural aspects of society, while Weber argued that it was in fact the cultural mores of Protestantism that ushered in a transformation of material circumstances. In contrast to both, Durkheim argued that societies moved from simple to complex through a process of sociocultural evolution. Sociologists in this field also study processes of globalization and imperialism. Most notably, Immanuel Wallerstein extends Marx's theoretical frame to include large spans of time and the entire globe in what is known as world systems theory. Development sociology is also heavily influenced by post-colonialism. In recent years, Raewyn Connell issued a critique of the bias in sociological research towards countries in the Global North. She argues that this bias blinds sociologists to the lived experiences of the Global South, specifically, so-called, "Northern Theory" lacks an adequate theory of imperialism and colonialism. There are many organizations studying social change, including the Fernand Braudel Center for the Study of Economies, Historical Systems, and Civilizations, and the Global Social Change Research Project. Social networks A social network is a social structure composed of individuals (or organizations) called "nodes", which are tied (connected) by one or more specific types of interdependency, such as friendship, kinship, financial exchange, dislike, sexual relationships, or relationships of beliefs, knowledge or prestige. Social networks operate on many levels, from families up to the level of nations, and play a critical role in determining the way problems are solved, organizations are run, and the degree to which individuals succeed in achieving their goals. An underlying theoretical assumption of social network analysis is that groups are not necessarily the building blocks of society: the approach is open to studying less-bounded social systems, from non-local communities to networks of exchange. Drawing theoretically from relational sociology, social network analysis avoids treating individuals (persons, organizations, states) as discrete units of analysis, it focuses instead on how the structure of ties affects and constitutes individuals and their relationships. In contrast to analyses that assume that socialization into norms determines behaviour, network analysis looks to see the extent to which the structure and composition of ties affect norms. On the other hand, recent research by Omar Lizardo also demonstrates that network ties are shaped and created by previously existing cultural tastes. Social network theory is usually defined in formal mathematics and may include integration of geographical data into sociomapping. Social psychology Sociological social psychology focuses on micro-scale social actions. This area may be described as adhering to "sociological miniaturism", examining whole societies through the study of individual thoughts and emotions as well as behaviour of small groups. One special concern to psychological sociologists is how to explain a variety of demographic, social, and cultural facts in terms of human social interaction. Some of the major topics in this field are social inequality, group dynamics, prejudice, aggression, social perception, group behaviour, social change, non-verbal behaviour, socialization, conformity, leadership, and social identity. Social psychology may be taught with psychological emphasis. In sociology, researchers in this field are the most prominent users of the experimental method (however, unlike their psychological counterparts, they also frequently employ other methodologies). Social psychology looks at social influences, as well as social perception and social interaction. Stratification, poverty and inequality Social stratification is the hierarchical arrangement of individuals into social classes, castes, and divisions within a society. Modern Western societies stratification traditionally relates to cultural and economic classes arranged in three main layers: upper class, middle class, and lower class, but each class may be further subdivided into smaller classes (e.g. occupational). Social stratification is interpreted in radically different ways within sociology. Proponents of structural functionalism suggest that, since the stratification of classes and castes is evident in all societies, hierarchy must be beneficial in stabilizing their existence. Conflict theorists, by contrast, critique the inaccessibility of resources and lack of social mobility in stratified societies. Karl Marx distinguished social classes by their connection to the means of production in the capitalist system: the bourgeoisie own the means, but this effectively includes the proletariat itself as the workers can only sell their own labour power (forming the material base of the cultural superstructure). Max Weber critiqued Marxist economic determinism, arguing that social stratification is not based purely on economic inequalities, but on other status and power differentials (e.g. patriarchy). According to Weber, stratification may occur among at least three complex variables: Property (class): A person's economic position in a society, based on birth and individual achievement. Weber differs from Marx in that he does not see this as the supreme factor in stratification. Weber noted how managers of corporations or industries control firms they do not own; Marx would have placed such a person in the proletariat. Prestige (status): A person's prestige, or popularity in a society. This could be determined by the kind of job this person does or wealth. Power (political party): A person's ability to get their way despite the resistance of others. For example, individuals in state jobs, such as an employee of the Federal Bureau of Investigation, or a member of the United States Congress, may hold little property or status but they still hold immense power. Pierre Bourdieu provides a modern example in the concepts of cultural and symbolic capital. Theorists such as Ralf Dahrendorf have noted the tendency towards an enlarged middle-class in modern Western societies, particularly in relation to the necessity of an educated work force in technological or service-based economies. Perspectives concerning globalization, such as dependency theory, suggest this effect owes to the shift of workers to the developing countries. Urban and rural sociology Urban sociology involves the analysis of social life and human interaction in metropolitan areas. It is a discipline seeking to provide advice for planning and policy making. After the Industrial Revolution, works such as Georg Simmel's The Metropolis and Mental Life (1903) focused on urbanization and the effect it had on alienation and anonymity. In the 1920s and 1930s The Chicago School produced a major body of theory on the nature of the city, important to both urban sociology and criminology, utilizing symbolic interactionism as a method of field research. Contemporary research is commonly placed in a context of globalization, for instance, in Saskia Sassen's study of the "global city". Rural sociology, by contrast, is the analysis of non-metropolitan areas. As agriculture and wilderness tend to be a more prominent social fact in rural regions, rural sociologists often overlap with environmental sociologists. Community sociology Often grouped with urban and rural sociology is that of community sociology or the sociology of community. Taking various communities—including online communities—as the unit of analysis, community sociologists study the origin and effects of different associations of people. For instance, German sociologist Ferdinand Tönnies distinguished between two types of human association: gemeinschaft (usually translated as "community") and gesellschaft ("society" or "association"). In his 1887 work, Gemeinschaft und Gesellschaft, Tönnies argued that Gemeinschaft is perceived to be a tighter and more cohesive social entity, due to the presence of a "unity of will". The 'development' or 'health' of a community is also a central concern of community sociologists also engage in development sociology, exemplified by the literature surrounding the concept of social capital. Other academic disciplines Sociology overlaps with a variety of disciplines that study society, in particular social anthropology, political science, economics, social work and social philosophy. Many comparatively new fields such as communication studies, cultural studies, demography and literary theory, draw upon methods that originated in sociology. The terms "social science" and "social research" have both gained a degree of autonomy since their origination in classical sociology. The distinct field of social anthropology or anthroposociology is the dominant constituent of anthropology throughout the United Kingdom and Commonwealth and much of Europe (France in particular), where it is distinguished from cultural anthropology. In the United States, social anthropology is commonly subsumed within cultural anthropology (or under the relatively new designation of sociocultural anthropology). Sociology and applied sociology are connected to the professional and academic discipline of social work. Both disciplines study social interactions, community and the effect of various systems (i.e. family, school, community, laws, political sphere) on the individual. However, social work is generally more focused on practical strategies to alleviate social dysfunctions; sociology in general provides a thorough examination of the root causes of these problems. For example, a sociologist might study why a community is plagued with poverty. The applied sociologist would be more focused on practical strategies on what needs to be done to alleviate this burden. The social worker would be focused on action; implementing theses strategies "directly" or "indirectly" by means of mental health therapy, counselling, advocacy, community organization or community mobilization. Social anthropology is the branch of anthropology that studies how contemporary living human beings behave in social groups. Practitioners of social anthropology, like sociologists, investigate various facets of social organization. Traditionally, social anthropologists analyzed non-industrial and non-Western societies, whereas sociologists focused on industrialized societies in the Western world. In recent years, however, social anthropology has expanded its focus to modern Western societies, meaning that the two disciplines increasingly converge. Sociocultural anthropology, which includes linguistic anthropology, is concerned with the problems of difference and similarity within and between human populations. The discipline arose concomitantly with the expansion of European colonial empires, and its practices and theories have been questioned and reformulated along with processes of decolonization. Such issues have re-emerged as transnational processes have challenged the centrality of the nation-state to theorizations about culture and power. New challenges have emerged as public debates about multiculturalism, and the increasing use of the culture concept outside of the academy and among peoples studied by anthropology. Irving Louis Horowitz, in his The Decomposition of Sociology (1994), has argued that the discipline, while arriving from a "distinguished lineage and tradition", is in decline due to deeply ideological theory and a lack of relevance to policy making: "The decomposition of sociology began when this great tradition became subject to ideological thinking, and an inferior tradition surfaced in the wake of totalitarian triumphs." Furthermore: "A problem yet unmentioned is that sociology's malaise has left all the social sciences vulnerable to pure positivism—to an empiricism lacking any theoretical basis. Talented individuals who might, in an earlier time, have gone into sociology are seeking intellectual stimulation in business, law, the natural sciences, and even creative writing; this drains sociology of much needed potential." Horowitz cites the lack of a 'core discipline' as exacerbating the problem. Randall Collins, the Dorothy Swaine Thomas Professor in Sociology at the University of Pennsylvania and a member of the Advisory Editors Council of the Social Evolution & History journal, has voiced similar sentiments: "we have lost all coherence as a discipline, we are breaking up into a conglomerate of specialities, each going on its own way and with none too high regard for each other." In 2007, The Times Higher Education Guide published a list of 'The most cited authors of books in the Humanities' (including philosophy and psychology). Seven of the top ten are listed as sociologists: Michel Foucault (1), Pierre Bourdieu (2), Anthony Giddens (5), Erving Goffman (6), Jürgen Habermas (7), Max Weber (8), and Bruno Latour (10). Journals The most highly ranked general journals which publish original research in the field of sociology are the American Journal of Sociology and the American Sociological Review. The Annual Review of Sociology, which publishes original review essays, is also highly ranked. Many other generalist and specialized journals exist. See also Bibliography of sociology Critical juncture theory Cultural theory Engaged theory Historic recurrence History of the social sciences List of sociologists Outline of sociology Political sociology Post-industrial society Social theory Social psychology Sociological Francoism Notes References Citations Sources External links American Sociological Association (ASA) Eastern Sociological Society (ESS) Australian Sociological Association (TASA) Bangladesh Sociological Society (BSS) British Sociological Association (BSA) Canadian Association of French-speaking Sociologists and Anthropologists Canadian Sociological Association (CSA) European Sociological Association (ESA) French Sociological Association German Sociological Association (DGS) Guide to the University of Chicago Department of Sociology Interviews 1972 at the University of Chicago Special Collections Research Center Guide to the University of Chicago Department of Sociology Records 1924-2001 at the University of Chicago Special Collections Research Center Indian Sociological Society (ISS) International Institute of Sociology (IIS) International Sociological Association (ISA) Latin American Sociological Association (ALAS) Observatory of International Research (OOIR): Latest Papers and Trends in Sociology Portuguese Sociological Association (APS) Sociological Association of Ireland (SAI) The Nordic Sociological Association (NSA) The Swedish Sociological Association(in swedish)

Religion is a range of social-cultural systems, including designated behaviors and practices, morals, beliefs, worldviews, texts, sanctified places, prophecies, ethics, or organizations, that generally relate humanity to supernatural, transcendental, and spiritual elements—although there is no scholarly consensus over what precisely constitutes a religion. It is an essentially contested concept. Different religions may or may not contain various elements ranging from the divine, sacredness, faith, and a supernatural being or beings. The origin of religious belief is an open question, with possible explanations including awareness of individual death, a sense of community, and dreams. Religions have sacred histories, narratives, and mythologies, preserved in oral traditions, sacred texts, symbols, and holy places, that may attempt to explain the origin of life, the universe, and other phenomena. Religious practice may include rituals, sermons, commemoration or veneration (of deities or saints), sacrifices, festivals, feasts, trances, initiations, matrimonial and funerary services, meditation, prayer, music, art, dance, or public service. There are an estimated 10,000 distinct religions worldwide, though nearly all of them have regionally based, relatively small followings. Four religions—Christianity, Islam, Hinduism, and Buddhism—account for over 77% of the world's population, and 92% of the world either follows one of those four religions or identifies as nonreligious, meaning that the vast majority of remaining religions account for only 8% of the population combined. The religiously unaffiliated demographic includes those who do not identify with any particular religion, atheists, and agnostics, although many in the demographic still have various religious beliefs. Many world religions are also organized religions, most definitively including the Abrahamic religions Christianity, Islam, Judaism, and the Baháʼí Faith, while others are arguably less so, in particular folk religions, indigenous religions, and some Eastern religions. A portion of the world's population are members of new religious movements. Scholars have indicated that global religiosity may be increasing due to religious countries having generally higher birth rates. The study of religion comprises a wide variety of academic disciplines, including theology, philosophy of religion, comparative religion, and social scientific studies. Theories about religion offer various explanations for its origins and workings, including the ontological foundations of religious being and belief. Etymology and history of the concept of "religion" Etymology The term religion comes from both Old French and Anglo-Norman (1200s CE) and means respect for sense of right, moral obligation, sanctity, what is sacred, reverence for the gods. It is ultimately derived from the Latin word religiō. According to Roman philosopher Cicero, religiō comes from relegere: re (meaning 'again') + lego (meaning 'read'), where lego is in the sense of 'go over', 'choose', or 'consider carefully'. Contrarily, some modern scholars such as Tom Harpur and Joseph Campbell have argued that religiō is derived from religare: re (meaning 'again') + ligare ('bind' or 'connect'), which was made prominent by St. Augustine following the interpretation given by Lactantius in Divinae institutiones, IV, 28. The medieval usage alternates with order in designating bonded communities like those of monastic orders: "we hear of the 'religion' of the Golden Fleece, of a knight 'of the religion of Avys'." Religiō In classic antiquity, religiō broadly meant conscientiousness, sense of right, moral obligation, or duty to anything. In the ancient and medieval world, the etymological Latin root religiō was understood as an individual virtue of worship in mundane contexts; never as doctrine, practice, or actual source of knowledge. In general, religiō referred to broad social obligations towards anything including family, neighbors, rulers, and even towards God. Religiō was most often used by the ancient Romans not in the context of a relation towards gods, but as a range of general emotions which arose from heightened attention in any mundane context such as hesitation, caution, anxiety, or fear, as well as feelings of being bound, restricted, or inhibited. The term was also closely related to other terms like scrupulus (which meant "very precisely"), and some Roman authors related the term superstitio (which meant too much fear or anxiety or shame) to religiō at times. When religiō came into English around the 1200s as religion, it took the meaning of "life bound by monastic vows" or monastic orders. The compartmentalized concept of religion, where religious and worldly things were separated, was not used before the 1500s. The concept of religion was first used in the 1500s to distinguish the domain of the church and the domain of civil authorities; the Peace of Augsburg marks such an instance, which has been described by Christian Reus-Smit as "the first step on the road toward a European system of sovereign states." Roman general Julius Caesar used religiō to mean 'obligation of an oath' when discussing captured soldiers making an oath to their captors. Roman naturalist Pliny the Elder used the term religiō to describe the apparent respect given by elephants to the night sky. Cicero used religiō as being related to cultum deorum (worship of the gods). Threskeia In Ancient Greece, the Greek term threskeia (θρησκεία) was loosely translated into Latin as religiō in late antiquity. Threskeia was sparsely used in classical Greece but became more frequently used in the writings of Josephus in the 1st century CE. It was used in mundane contexts and could mean multiple things from respectful fear to excessive or harmfully distracting practices of others, to cultic practices. It was often contrasted with the Greek word deisidaimonia, which meant too much fear. History of the concept of "religion" Religion is a modern concept. And is not a universal concept across history, cultures or languages. The concept was invented recently in the English language and is found in texts from the 17th century due to events such as the splitting of Christendom during the Protestant Reformation and globalization in the Age of Exploration, which involved contact with numerous foreign cultures with non-European languages. Some argue that regardless of its definition, it is not appropriate to apply the term religion to non-Western cultures, while some followers of various faiths rebuke using the word to describe their own belief system. The concept of "ancient religion" stems from modern interpretations of a range of practices that conform to a modern concept of religion, influenced by early modern and 19th century Christian discourse. The concept of religion was formed in the 16th and 17th centuries, despite the fact that ancient sacred texts like the Bible, the Quran, and others did not have a word or even a concept of religion in the original languages and neither did the people or the cultures in which these sacred texts were written. For example, there is no precise equivalent of religion in Hebrew, and Judaism does not distinguish clearly between religious, national, racial, or ethnic identities. One of its central concepts is halakha, meaning the walk or path sometimes translated as law, which guides religious practice and belief and many aspects of daily life. Even though the beliefs and traditions of Judaism are found in the ancient world, ancient Jews saw Jewish identity as being about an ethnic or national identity and did not entail a compulsory belief system or regulated rituals. In the 1st century CE, Josephus had used the Greek term ioudaismos (Judaism) as an ethnic term and was not linked to modern abstract concepts of religion or a set of beliefs. The very concept of "Judaism" was invented by the Christian Church, and it was in the 19th century that Jews began to see their ancestral culture as a religion analogous to Christianity. The Greek word threskeia, which was used by Greek writers such as Herodotus and Josephus, is found in the New Testament. Threskeia is sometimes translated as "religion" in today's translations, but the term was understood as generic "worship" well into the medieval period. In the Quran, the Arabic word din is often translated as religion in modern translations, but up to the mid-1600s translators expressed din as "law." The Sanskrit word dharma, sometimes translated as religion, also means law. Throughout classical South Asia, the study of law consisted of concepts such as penance through piety and ceremonial as well as practical traditions. Medieval Japan at first had a similar union between imperial law and universal or Buddha law, but these later became independent sources of power. Though traditions, sacred texts, and practices have existed throughout time, most cultures did not align with Western conceptions of religion since they did not separate everyday life from the sacred. In the 18th and 19th centuries, the terms Buddhism, Hinduism, Taoism, Confucianism, and world religions first entered the English language. Native Americans were also thought of as not having religions and also had no word for religion in their languages either. No one self-identified as a Hindu or Buddhist or other similar terms before the 1800s. "Hindu" has historically been used as a geographical, cultural, and later religious identifier for people indigenous to the Indian subcontinent. Throughout its long history, Japan had no concept of religion since there was no corresponding Japanese word, nor anything close to its meaning, but when American warships appeared off the coast of Japan in 1853 and forced the Japanese government to sign treaties demanding, among other things, freedom of religion, the country had to contend with this idea. According to the philologist Max Müller in the 19th century, the root of the English word religion, the Latin religiō, was originally used to mean only reverence for God or the gods, careful pondering of divine things, piety (which Cicero further derived to mean diligence). Müller characterized many other cultures around the world, including Egypt, Persia, and India, as having a similar power structure at this point in history. What is called ancient religion today, they would have only called law. History of "nonreligion" and "secular" Accordingly, other concepts like the "secular", "nonreligion", and "atheism" have been called into question since they are, like religion, not universal concepts as they are not found in many other cultures. Anthropoligically, this is also the case where Western terms and concepts like "religion" and "secular" do not exist in other cultures. Sociologists and demographers have noted that outside the West, concepts of "religion" or "the secular" are not always rooted in local culture and may not even be present. Other studies indicate that religion and nonreligion are not necessarily mutually exclusive experiences since there is overlap in individuals regular lives. Definition Scholars have failed to agree on a definition of religion. There are, however, two general definition systems: the sociological/functional and the phenomenological/philosophical. Modern Western The concept of religion originated in the modern era in the West. Parallel concepts are not found in many current and past cultures; there is no equivalent term for religion in many languages. Scholars have found it difficult to develop a consistent definition, with some giving up on the possibility of a definition. Others argue that regardless of its definition, it is not appropriate to apply it to non-Western cultures. An increasing number of scholars have expressed reservations about ever defining the essence of religion. They observe that the way the concept today is used is a particularly modern construct that would not have been understood through much of history and in many cultures outside the West (or even in the West until after the Peace of Westphalia). The MacMillan Encyclopedia of Religions states: The very attempt to define religion, to find some distinctive or possibly unique essence or set of qualities that distinguish the religious from the remainder of human life, is primarily a Western concern. The attempt is a natural consequence of the Western speculative, intellectualistic, and scientific disposition. It is also the product of the dominant Western religious mode, what is called the Judeo-Christian climate or, more accurately, the theistic inheritance from Judaism, Christianity, and Islam. The theistic form of belief in this tradition, even when downgraded culturally, is formative of the dichotomous Western view of religion. That is, the basic structure of theism is essentially a distinction between a transcendent deity and all else, between the creator and his creation, between God and man. The anthropologist Clifford Geertz defined religion as a: ... system of symbols which acts to establish powerful, pervasive, and long-lasting moods and motivations in men by formulating conceptions of a general order of existence and clothing these conceptions with such an aura of factuality that the moods and motivations seem uniquely realistic. Alluding perhaps to Tylor's "deeper motive", Geertz remarked that: ... we have very little idea of how, in empirical terms, this particular miracle is accomplished. We just know that it is done, annually, weekly, daily, for some people almost hourly; and we have an enormous ethnographic literature to demonstrate it. The theologian Antoine Vergote took the term supernatural simply to mean whatever transcends the powers of nature or human agency. He also emphasized the cultural reality of religion, which he defined as: ... the entirety of the linguistic expressions, emotions and, actions and signs that refer to a supernatural being or supernatural beings. Peter Mandaville and Paul James intended to get away from the modernist dualisms or dichotomous understandings of immanence/transcendence, spirituality/materialism, and sacredness/secularity. They define religion as: ... a relatively-bounded system of beliefs, symbols and practices that addresses the nature of existence, and in which communion with others and Otherness is lived as if it both takes in and spiritually transcends socially-grounded ontologies of time, space, embodiment and knowing. According to the MacMillan Encyclopedia of Religions, there is an experiential aspect to religion which can be found in almost every culture: ... almost every known culture [has] a depth dimension in cultural experiences ... toward some sort of ultimacy and transcendence that will provide norms and power for the rest of life. When more or less distinct patterns of behavior are built around this depth dimension in a culture, this structure constitutes religion in its historically recognizable form. Religion is the organization of life around the depth dimensions of experience—varied in form, completeness, and clarity in accordance with the environing culture. Anthropologists Lyle Steadman and Craig T. Palmer emphasized the communication of supernatural beliefs, defining religion as: ... the communicated acceptance by individuals of another individual's "supernatural" claim, a claim whose accuracy is not verifiable by the senses. Classical Friedrich Schleiermacher in the late 18th century defined religion as das schlechthinnige Abhängigkeitsgefühl, commonly translated as "the feeling of absolute dependence". His contemporary Georg Wilhelm Friedrich Hegel disagreed thoroughly, defining religion as "the Divine Spirit becoming conscious of Himself through the finite spirit." Edward Burnett Tylor defined religion in 1871 as "the belief in spiritual beings". He argued that narrowing the definition to mean the belief in a supreme deity or judgment after death or idolatry and so on, would exclude many peoples from the category of religious, and thus "has the fault of identifying religion rather with particular developments than with the deeper motive which underlies them." He also argued that the belief in spiritual beings exists in all known societies. In his book The Varieties of Religious Experience, the psychologist William James defined religion as "the feelings, acts, and experiences of individual men in their solitude, so far as they apprehend themselves to stand in relation to whatever they may consider the divine." By the term divine James meant "any object that is godlike, whether it be a concrete deity or not" to which the individual feels impelled to respond with solemnity and gravity. Sociologist Émile Durkheim, in his seminal book The Elementary Forms of the Religious Life, defined religion as a "unified system of beliefs and practices relative to sacred things". By sacred things he meant things "set apart and forbidden—beliefs and practices which unite into one single moral community called a Church, all those who adhere to them." Sacred things are not, however, limited to gods or spirits. On the contrary, a sacred thing can be "a rock, a tree, a spring, a pebble, a piece of wood, a house, in a word, anything can be sacred." Religious beliefs, myths, dogmas and legends are the representations that express the nature of these sacred things, and the virtues and powers which are attributed to them. Echoes of James' and Durkheim's definitions are to be found in the writings of, for example, Frederick Ferré who defined religion as "one's way of valuing most comprehensively and intensively". Similarly, for the theologian Paul Tillich, faith is "the state of being ultimately concerned", which "is itself religion. Religion is the substance, the ground, and the depth of man's spiritual life." When religion is seen in terms of sacred, divine, intensive valuing, or ultimate concern, then it is possible to understand why scientific findings and philosophical criticisms (e.g., those made by Richard Dawkins) do not necessarily disturb its adherents. Aspects Beliefs The origin of religious belief is an open question, with possible explanations including awareness of individual death, a sense of community, and dreams. Traditionally, faith, in addition to reason, has been considered a source of religious beliefs. The interplay between faith and reason, and their use as perceived support for religious beliefs, have been a subject of interest to philosophers and theologians. Mythology The word myth has several meanings: A traditional story of ostensibly historical events that serves to unfold part of the world view of a people or explain a practice, belief, or natural phenomenon; A person or thing having only an imaginary or unverifiable existence; or A metaphor for the spiritual potentiality in the human being. Ancient polytheistic religions, such as those of Greece, Rome, and Scandinavia, are usually categorized under the heading of mythology. Religions of pre-industrial peoples, or cultures in development, are similarly called myths in the anthropology of religion. The term myth can be used pejoratively by both religious and non-religious people. By defining another person's religious stories and beliefs as mythology, one implies that they are less real or true than one's own religious stories and beliefs. Joseph Campbell remarked, "Mythology is often thought of as other people's religions, and religion can be defined as misinterpreted mythology." In sociology, however, the term myth has a non-pejorative meaning. There, myth is defined as a story that is important for the group, whether or not it is objectively or provably true. Examples include the resurrection of their real-life founder Jesus, which, to Christians, explains the means by which they are freed from sin, is symbolic of the power of life over death, and is also said to be a historical event. But from a mythological outlook, whether or not the event actually occurred is unimportant. Instead, the symbolism of the death of an old life and the start of a new life is most significant. Religious believers may or may not accept such symbolic interpretations. Practices The practices of a religion may include rituals, sermons, commemoration or veneration of a deity (god or goddess), sacrifices, festivals, feasts, trances, initiations, funerary services, matrimonial services, meditation, prayer, religious music, religious art, sacred dance, public service, or other aspects of human culture. Social organisation Religions have a societal basis, either as a living tradition which is carried by lay participants, or with an organized clergy, and a definition of what constitutes adherence or membership. Academic study A number of disciplines study the phenomenon of religion: theology, comparative religion, history of religion, evolutionary origin of religions, anthropology of religion, psychology of religion (including neuroscience of religion and evolutionary psychology of religion), law and religion, and sociology of religion. Daniel L. Pals mentions eight classical theories of religion, focusing on various aspects of religion: animism and magic, by E.B. Tylor and J.G. Frazer; the psycho-analytic approach of Sigmund Freud; and further Émile Durkheim, Karl Marx, Max Weber, Mircea Eliade, E.E. Evans-Pritchard, and Clifford Geertz. Michael Stausberg gives an overview of contemporary theories of religion, including cognitive and biological approaches. Theories Sociological and anthropological theories of religion generally attempt to explain the origin and function of religion. These theories define what they present as universal characteristics of religious belief and practice. Origins and development The origin of religion is uncertain. There are a number of theories regarding the subsequent origins of religious practices. According to anthropologists John Monaghan and Peter Just, "Many of the great world religions appear to have begun as revitalization movements of some sort, as the vision of a charismatic prophet fires the imaginations of people seeking a more comprehensive answer to their problems than they feel is provided by everyday beliefs. Charismatic individuals have emerged at many times and places in the world. It seems that the key to long-term success—and many movements come and go with little long-term effect—has relatively little to do with the prophets, who appear with surprising regularity, but more to do with the development of a group of supporters who are able to institutionalize the movement." The development of religion has taken different forms in different cultures. Some religions place an emphasis on belief, while others emphasize practice. Some religions focus on the subjective experience of the religious individual, while others consider the activities of the religious community to be most important. Some religions claim to be universal, believing their laws and cosmology to be binding for everyone, while others are intended to be practiced only by a closely defined or localized group. In many places, religion has been associated with public institutions such as education, hospitals, the family, government, and political hierarchies. Anthropologists John Monoghan and Peter Just state that, "it seems apparent that one thing religion or belief helps us do is deal with problems of human life that are significant, persistent, and intolerable. One important way in which religious beliefs accomplish this is by providing a set of ideas about how and why the world is put together that allows people to accommodate anxieties and deal with misfortune." Cultural system While religion is difficult to define, one standard model of religion, used in religious studies courses, was proposed by Clifford Geertz, who simply called it a "cultural system". A critique of Geertz's model by Talal Asad categorized religion as "an anthropological category". Richard Niebuhr's (1894–1962) five-fold classification of the relationship between Christ and culture, however, indicates that religion and culture can be seen as two separate systems, though with some interplay. Social constructionism One modern academic theory of religion, social constructionism, says that religion is a modern concept that suggests all spiritual practice and worship follows a model similar to the Abrahamic religions as an orientation system that helps to interpret reality and define human beings. Among the main proponents of this theory of religion are Daniel Dubuisson, Timothy Fitzgerald, Talal Asad, and Jason Ānanda Josephson. The social constructionists argue that religion is a modern concept that developed from Christianity and was then applied inappropriately to non-Western cultures. Cognitive science Cognitive science of religion is the study of religious thought and behavior from the perspective of the cognitive and evolutionary sciences. The field employs methods and theories from a very broad range of disciplines, including: cognitive psychology, evolutionary psychology, cognitive anthropology, artificial intelligence, cognitive neuroscience, neurobiology, zoology, and ethology. Scholars in this field seek to explain how human minds acquire, generate, and transmit religious thoughts, practices, and schemas by means of ordinary cognitive capacities. Hallucinations and delusions related to religious content occurs in about 60% of people with schizophrenia. While this number varies across cultures, this had led to theories about a number of influential religious phenomena and possible relation to psychotic disorders. A number of prophetic experiences are consistent with psychotic symptoms, although retrospective diagnoses are practically impossible. Schizophrenic episodes are also experienced by people who do not have belief in gods. Religious content is also common in temporal lobe epilepsy, and obsessive-compulsive disorder. Atheistic content is also found to be common with temporal lobe epilepsy. Comparativism Comparative religion is the branch of the study of religions concerned with the systematic comparison of the doctrines and practices of the world's religions. In general, the comparative study of religion yields a deeper understanding of the fundamental philosophical concerns of religion such as ethics, metaphysics, and the nature and form of salvation. Studying such material is meant to give one a richer and more sophisticated understanding of human beliefs and practices regarding the sacred, numinous, spiritual and divine. In the field of comparative religion, a common geographical classification of the main world religions includes Middle Eastern religions (including Zoroastrianism and Iranian religions), Indian religions, East Asian religions, African religions, American religions, Oceanic religions, and classical Hellenistic religions. Classification In the 19th and 20th centuries, the academic practice of comparative religion divided religious belief into philosophically defined categories called world religions. Some academics studying the subject have divided religions into three broad categories: World religions, a term which refers to transcultural, international religions; Indigenous religions, which refers to smaller, culture-specific or nation-specific religious groups; and New religious movements, which refers to recently developed religions. Some recent scholarship has argued that not all types of religion are necessarily separated by mutually exclusive philosophies, and furthermore that the utility of ascribing a practice to a certain philosophy, or even calling a given practice religious, rather than cultural, political, or social in nature, is limited. The current state of psychological study about the nature of religiousness suggests that it is better to refer to religion as a largely invariant phenomenon that should be distinguished from cultural norms (i.e. religions). Morphological classification Some religion scholars classify religions as either universal religions that seek worldwide acceptance and actively look for new converts, such as the Baháʼí Faith, Buddhism, Christianity, Islam, and Jainism, while ethnic religions are identified with a particular ethnic group and do not seek converts. Others reject the distinction, pointing out that all religious practices, whatever their philosophical origin, are ethnic because they come from a particular culture. Demographic classification The five largest religious groups by world population, estimated to account for 5.8 billion people and 84% of the population, are Christianity, Islam, Buddhism, Hinduism (with the relative numbers for Buddhism and Hinduism dependent on the extent of syncretism), and traditional folk religions. A global poll in 2012 surveyed 57 countries and reported that 59% of the world's population identified as religious, 23% as not religious, 13% as convinced atheists, and also a 9% decrease in identification as religious when compared to the 2005 average from 39 countries. A follow-up poll in 2015 found that 63% of the globe identified as religious, 22% as not religious, and 11% as convinced atheists. On average, women are more religious than men. Some people follow multiple religions or multiple religious principles at the same time, regardless of whether or not the religious principles they follow traditionally allow for syncretism. Unaffiliated populations are projected to drop, even when taking disaffiliation rates into account, due to differences in birth rates. Scholars have indicated that global religiosity may be increasing due to religious countries having higher birth rates in general. Specific religions Abrahamic Abrahamic religions are monotheistic religions which believe they descend from Abraham. Judaism Judaism is the oldest Abrahamic religion, originating in the people of ancient Israel and Judah. The Torah is its foundational text, and is part of the larger text known as the Tanakh or Hebrew Bible. It is supplemented by oral tradition, set down in written form in later texts such as the Midrash and the Talmud. Judaism includes a wide corpus of texts, practices, theological positions, and forms of organization. Within Judaism there are a variety of movements, most of which emerged from Rabbinic Judaism, which holds that God revealed his laws and commandments to Moses on Mount Sinai in the form of both the Written and Oral Torah; historically, this assertion was challenged by various groups. The Jewish people were scattered after the destruction of the Temple in Jerusalem in 70 CE. Today there are about 13 million Jews, about 40 per cent living in Israel and 40 per cent in the United States. The largest Jewish religious movements are Orthodox Judaism (Haredi Judaism and Modern Orthodox Judaism), Conservative Judaism and Reform Judaism. Christianity Christianity is based on the life and teachings of Jesus of Nazareth (1st century) as presented in the New Testament. The Christian faith is essentially faith in Jesus as the Christ, the Son of God, and as Savior and Lord. Almost all Christians believe in the Trinity, which teaches the unity of Father, Son (Jesus Christ), and Holy Spirit as three persons in one Godhead. Most Christians can describe their faith with the Nicene Creed. As the religion of Byzantine Empire in the first millennium and of Western Europe during the time of colonization, Christianity has been propagated throughout the world via missionary work. It is the world's largest religion, with about 2.3 billion followers as of 2015. The main divisions of Christianity are, according to the number of adherents: The Catholic Church, led by the Bishop of Rome and the bishops worldwide in communion with him, is a communion of 24 Churches sui iuris, including the Latin Church and 23 Eastern Catholic churches, such as the Maronite Catholic Church. Eastern Christianity, which include Eastern Orthodoxy, Oriental Orthodoxy, and the Church of the East. Protestantism, separated from the Catholic Church in the 16th-century Protestant Reformation and is split into thousands of denominations. Major branches of Protestantism include Anglicanism, Baptists, Calvinism, Lutheranism, and Methodism, though each of these contain many different denominations or groups. There are also smaller groups, including: Restorationism, the belief that Christianity should be restored (as opposed to reformed) along the lines of what is known about the apostolic early church. Latter-day Saint movement, founded by Joseph Smith in the late 1820s. Jehovah's Witnesses, founded in the late 1870s by Charles Taze Russell. Islam Islam is a monotheistic religion based on the Quran, one of the holy books considered by Muslims to be revealed by God, and on the teachings (hadith) of the Islamic prophet Muhammad, a major political and religious figure of the 7th century CE. Islam is based on the unity of all religious philosophies and accepts all of the Abrahamic prophets of Judaism, Christianity and other Abrahamic religions before Muhammad. It is the most widely practiced religion of Southeast Asia, North Africa, Western Asia, and Central Asia, while Muslim-majority countries also exist in parts of South Asia, Sub-Saharan Africa, and Southeast Europe. There are also several Islamic republics, including Iran, Pakistan, Mauritania, and Afghanistan. With about 1.8 billion followers (2015), almost a quarter of earth's population are Muslims. Sunni Islam is the largest denomination within Islam and follows the Qur'an, the ahadith (plural of Hadith) which record the sunnah, whilst placing emphasis on the sahabah. Shia Islam is the second largest denomination of Islam and its adherents believe that Ali succeeded Muhammad and further places emphasis on Muhammad's family. There are also Muslim revivalist movements such as Muwahhidism and Salafism. Other denominations of Islam include Nation of Islam, Ibadi, Sufism, Quranism, Mahdavia, Ahmadiyya and non-denominational Muslims. Wahhabism is the dominant Muslim schools of thought in the Kingdom of Saudi Arabia. Other Judaism, Christianity, and Islam are the three most popular Abrahamic faiths, however there are smaller and newer traditions that lay claim to the designation of Abrahamic as well. For example, the Baháʼí Faith is a new religious movement that has links to the major Abrahamic religions as well as other religions (e.g., of Eastern philosophy). Founded in 19th-century Iran, it teaches the unity of all religious philosophies and accepts all of the prophets of Judaism, Christianity, and Islam as well as additional prophets (Buddha, Mahavira), including its founder Bahá'u'lláh. It is an offshoot of Bábism. One of its divisions is the Orthodox Baháʼí Faith. Even smaller regional Abrahamic groups also exist, including Samaritanism (primarily in Israel and the State of Palestine), the Rastafari movement (primarily in Jamaica), and Druze (primarily in Syria, Lebanon, and Israel). The Druze faith originally developed out of Isma'ilism, and it has sometimes been considered an Islamic school by some Islamic authorities, but Druze themselves do not identify as Muslims. Scholars classify the Druze faith as an independent Abrahamic religion because it developed its own unique doctrines and eventually separated from both Isma'ilism and Islam altogether. One of these doctrines includes the belief that Al-Ḥākim bi-Amr Allāh was an incarnation of God. Mandaeism, sometimes also known as Sabianism (after the mysterious Sabians mentioned in the Quran, a name historically claimed by several religious groups), is a Gnostic, monotheistic and ethnic religion. Its adherents, the Mandaeans, consider John the Baptist to be their chief prophet. Mandaeans are the last surviving Gnostics from antiquity. East Asian East Asian religions (also known as Far Eastern religions or Taoic religions) consist of several religions of East Asia which make use of the concept of Tao (in Chinese), Dō (in Japanese or Korean) or Đạo (in Vietnamese). They include: Taoism and Confucianism Taoism and Confucianism, as well as Korean, Vietnamese, and Japanese religion influenced by Chinese thought. Folk religions Chinese folk religion: the indigenous religions of the Han Chinese, or, by metonymy, of all the populations of the Chinese cultural sphere. It includes the syncretism of Confucianism, Taoism and Buddhism, Wuism, as well as many new religious movements such as Chen Tao, Falun Gong and Yiguandao. Other folk and new religions of East Asia and Southeast Asia such as Korean shamanism, Chondogyo, and Jeung San Do in Korea; indigenous Philippine folk religions in the Philippines; Shinto, Shugendo, Ryukyuan religion, and Japanese new religions in Japan; Satsana Phi in Laos; Vietnamese folk religion, and Cao Đài, Hòa Hảo in Vietnam. Indian religions Indian religions are practiced or were founded in the Indian subcontinent. They are sometimes classified as the dharmic religions, as they all feature dharma, the specific law of reality and duties expected according to the religion. Hinduism Hinduism is also called Vaidika Dharma, the dharma of the Vedas, although many practitioners refer to their religion as Sanātana Dharma ("the Eternal Dharma") which refers to the idea that its origins lie beyond human history. Vaidika Dharma is a synecdoche describing the similar philosophies of Vaishnavism, Shaivism, and related groups practiced or founded in the Indian subcontinent. Concepts most of them share in common include karma, caste, reincarnation, mantras, yantras, and darśana. Deities in Hinduism are referred to as Deva (masculine) and Devi (feminine). Major deities include Vishnu, Lakshmi, Shiva, Parvati, Brahma and Saraswati. These deities have distinct and complex personalities yet are often viewed as aspects of the same Ultimate Reality called Brahman. Hinduism is one of the most ancient of still-active religious belief systems, with origins perhaps as far back as prehistoric times. Therefore, Hinduism has been called the oldest religion in the world. Jainism Jainism, taught primarily by Rishabhanatha (the founder of ahimsa) is an ancient Indian religion that prescribes a path of non-violence, truth and anekantavada for all forms of living beings in this universe; which helps them to eliminate all the Karmas, and hence to attain freedom from the cycle of birth and death (saṃsāra), that is, achieving nirvana. Jains are found mostly in India. According to Dundas, outside of the Jain tradition, historians date the Mahavira as about contemporaneous with the Buddha in the 5th-century BCE, and accordingly the historical Parshvanatha, based on the c. 250-year gap, is placed in 8th or 7th century BCE. Digambara Jainism (or sky-clad) is mainly practiced in South India. Their holy books are Pravachanasara and Samayasara written by their Prophets Kundakunda and Amritchandra as their original canon is lost. Shwetambara Jainism (or white-clad) is mainly practiced in Western India. Their holy books are Jain Agamas, written by their Prophet Sthulibhadra. Buddhism Buddhism was founded by Siddhartha Gautama in the 5th century BCE. Buddhists generally agree that Gotama aimed to help sentient beings end their suffering (dukkha) by understanding the true nature of phenomena, thereby escaping the cycle of suffering and rebirth (saṃsāra), that is, achieving nirvana. Theravada Buddhism, which is practiced mainly in Sri Lanka and Southeast Asia alongside folk religion, shares some characteristics of Indian religions. It is based in a large collection of texts called the Pali Canon. Mahayana Buddhism (or the Great Vehicle) under which are a multitude of doctrines that became prominent in China and are still relevant in Vietnam, Korea, Japan and to a lesser extent in Europe and the United States. Mahayana Buddhism includes such disparate teachings as Zen or Pure Land. Vajrayana Buddhism first appeared in India in the 3rd century CE. It is currently most prominent in the Himalaya regions and extends across all of Asia (cf. Mikkyō). Two notable new Buddhist sects are Hòa Hảo and the Navayana (Dalit Buddhist movement), which were developed separately in the 20th century. Sikhism Sikhism is a panentheistic religion founded on the teachings of Guru Nanak and ten successive Sikh gurus in 15th-century Punjab. It is the fifth-largest organized religion in the world, with approximately 30 million Sikhs. Sikhs are expected to embody the qualities of a Sant-Sipāhī—a saint-soldier, have control over one's internal vices and be able to be constantly immersed in virtues clarified in the Guru Granth Sahib. The principal beliefs of Sikhi are faith in Waheguru—represented by the phrase ik ōaṅkār, one cosmic divine actioner (God), who prevails in everything, along with a praxis in which the Sikh is enjoined to engage in social reform through the pursuit of justice for all human beings. Indigenous and folk Indigenous religions or folk religions refers to a broad category of traditional religions that can be characterised by shamanism, animism and ancestor worship, where traditional means "indigenous, that which is aboriginal or foundational, handed down from generation to generation...". These are religions that are closely associated with a particular group of people, ethnicity or tribe; they often have no formal creeds or sacred texts. Some religions are syncretic, fusing diverse religious beliefs and practices. Australian Aboriginal religions. Folk religions of the Americas: Native American religions Folk religions are often omitted as a category in surveys even in countries where they are widely practiced, e.g., in China. Traditional African African traditional religion encompasses the traditional religious beliefs of people in Africa. In West Africa, these religions include the Akan religion, Dahomey (Fon) mythology, Efik mythology, Odinani, Serer religion (A ƭat Roog), and Yoruba religion, while Bushongo mythology, Mbuti (Pygmy) mythology, Lugbara mythology, Dinka religion, and Lotuko mythology come from central Africa. Southern African traditions include Akamba mythology, Masai mythology, Malagasy mythology, San religion, Lozi mythology, Tumbuka mythology, and Zulu mythology. Bantu mythology is found throughout central, southeast, and southern Africa. In north Africa, these traditions include Berber and ancient Egyptian. There are also notable African diasporic religions practiced in the Americas, such as Santeria, Candomble, Vodun, Lucumi, Umbanda, and Macumba. Iranian Iranian religions are ancient religions whose roots predate the Islamization of Greater Iran. Nowadays these religions are practiced only by minorities. Zoroastrianism is based on the teachings of prophet Zoroaster in the 6th century BCE. Zoroastrians worship the creator Ahura Mazda. In Zoroastrianism, good and evil have distinct sources, with evil trying to destroy the creation of Mazda, and good trying to sustain it. Kurdish religions include the traditional beliefs of the Yazidi, Alevi, and Ahl-e Haqq. Sometimes these are labeled Yazdânism. New religious movements The Baháʼí Faith teaches the unity of all religious philosophies. Cao Đài is a syncretistic, monotheistic religion, established in Vietnam in 1926. Eckankar is a pantheistic religion with the purpose of making God an everyday reality in one's life. Epicureanism is a Hellenistic philosophy that is considered by many of its practitioners as a type of (sometimes non-theistic) religious identity. It has its own scriptures, a monthly "feast of reason" on the Twentieth and considers friendship to be holy. Hindu reform movements, such as Ayyavazhi, Swaminarayan Faith and Ananda Marga, are examples of new religious movements within Indian religions. Japanese new religions (shinshukyo) is a general category for a wide variety of religious movements founded in Japan since the 19th century. These movements share almost nothing in common except the place of their founding. The largest religious movements centered in Japan include Soka Gakkai, Tenrikyo, and Seicho-No-Ie among hundreds of smaller groups. Jehovah's Witnesses, a non-trinitarian Christian Reformist movement sometimes described as millenarian. Neo-Druidism is a religion promoting harmony with nature, named after but not necessarily connected to the Iron Age druids. Modern pagan movements attempting to reconstruct or revive ancient pagan practices, such as Heathenry, Hellenism, Roman Traditionalism, and Kemeticism. Noahidism is a monotheistic ideology based on the Seven Laws of Noah, and on their traditional interpretations within Rabbinic Judaism. Some forms of parody religion or fiction-based religion like Jediism, Pastafarianism, Dudeism, "Tolkien religion", and others often develop their own writings, traditions, and cultural expressions, and end up behaving like traditional religions. Satanism is a broad category of religions that, for example, worship Satan as a deity (Theistic Satanism) or use Satan as a symbol of carnality and earthly values (LaVeyan Satanism and The Satanic Temple). Scientology is defined as a cult, a scam, a commercial business, or a new religious movement. Its mythological framework is similar to a UFO cult and includes references to aliens, but it is kept secret from most followers. It charges a fee for its central activity, on the basis of which it has been characterised as a commercial enterprise. UFO Religions in which extraterrestrial entities are an element of belief, such as Raëlism, Aetherius Society, and Marshall Vian Summers's New Message from God. Unitarian Universalism is a religion characterized by support for a free and responsible search for truth and meaning, and has no accepted creed or theology. Wicca is a neo-pagan religion first popularised in 1954 by British civil servant Gerald Gardner, involving the worship of a God and Goddess. Related aspects Law The study of law and religion is a relatively new field, with several thousand scholars involved in law schools, and academic departments including political science, religion, and history since 1980. Scholars in the field are not only focused on strictly legal issues about religious freedom or non-establishment, but also study religions as they are qualified through judicial discourses or legal understanding of religious phenomena. Exponents look at canon law, natural law, and state law, often in a comparative perspective. Specialists have explored themes in Western history regarding Christianity and justice and mercy, rule and equity, and discipline and love. Common topics of interest include marriage and the family and human rights. Outside of Christianity, scholars have looked at law and religion links in the Muslim Middle East and pagan Rome. Studies have focused on secularization. In particular, the issue of wearing religious symbols in public, such as headscarves that are banned in French schools, have received scholarly attention in the context of human rights and feminism. Science Science acknowledges reason and empirical evidence; and religions include revelation, faith and sacredness whilst also acknowledging philosophical and metaphysical explanations with regard to the study of the universe. Both science and religion are not monolithic, timeless, or static because both are complex social and cultural endeavors that have changed through time across languages and cultures. The concepts of science and religion are a recent invention: the term religion emerged in the 17th century in the midst of colonization and globalization and the Protestant Reformation. The term science emerged in the 19th century out of natural philosophy in the midst of attempts to narrowly define those who studied nature (natural science), and the phrase religion and science emerged in the 19th century due to the reification of both concepts. It was in the 19th century that the terms Buddhism, Hinduism, Taoism, and Confucianism first emerged. In the ancient and medieval world, the etymological Latin roots of both science (scientia) and religion (religio) were understood as inner qualities of the individual or virtues, never as doctrines, practices, or actual sources of knowledge. In general, the scientific method gains knowledge by testing hypotheses to develop theories through elucidation of facts or evaluation by experiments and thus only answers cosmological questions about the universe that can be observed and measured. It develops theories of the world which best fit physically observed evidence. All scientific knowledge is subject to later refinement, or even rejection, in the face of additional evidence. Scientific theories that have an overwhelming preponderance of favorable evidence are often treated as de facto verities in general parlance, such as the theories of general relativity and natural selection to explain respectively the mechanisms of gravity and evolution. Religion does not have a method per se partly because religions emerge through time from diverse cultures and it is an attempt to find meaning in the world, and to explain humanity's place in it and relationship to it and to any posited entities. In terms of Christian theology and ultimate truths, people rely on reason, experience, scripture, and tradition to test and gauge what they experience and what they should believe. Furthermore, religious models, understanding, and metaphors are also revisable, as are scientific models. Regarding religion and science, Albert Einstein states (1940): "For science can only ascertain what is, but not what should be, and outside of its domain value judgments of all kinds remain necessary. Religion, on the other hand, deals only with evaluations of human thought and action; it cannot justifiably speak of facts and relationships between facts...Now, even though the realms of religion and science in themselves are clearly marked off from each other, nevertheless there exist between the two strong reciprocal relationships and dependencies. Though religion may be that which determine the goals, it has, nevertheless, learned from science, in the broadest sense, what means will contribute to the attainment of the goals it has set up." Morality Many religions have value frameworks regarding personal behavior meant to guide adherents in determining between right and wrong. These include the Five Vows of Jainism, Judaism's halakha, Islam's sharia, Catholicism's canon law, Buddhism's Noble Eightfold Path, and Zoroastrianism's good thoughts, good words, and good deeds concept, among others. Religion and morality are not synonymous. While it is often assumed in Christian thought that morality is ultimately based in religion, it can also have a secular basis. The study of religion and morality can be contentious due to ethnocentric views on morality, failure to distinguish between in group and out group altruism, and inconsistent definitions of religiosity. Politics Impact Religion has had a significant impact on the political system in many countries. Notably, most Muslim-majority countries adopt various aspects of sharia, the Islamic law. Some countries even define themselves in religious terms, such as The Islamic Republic of Iran. The sharia thus affects up to 23% of the global population, or 1.57 billion people who are Muslims. However, religion also affects political decisions in many western countries. For instance, in the United States, 51% of voters would be less likely to vote for a presidential candidate who did not believe in God, and only 6% more likely. Christians made up 92% of members of the US Congress, compared with 71% of the general public (in 2014). At the same time, while 23% of US adults are religiously unaffiliated, only one former member of Congress (Kyrsten Sinema, Arizona), or 0.2% of that body, claims no religious affiliation. In most European countries, however, religion has a much smaller influence on politics although it used to be much more important. For instance, same-sex marriage and abortion were illegal in many European countries until recently, following Christian (usually Catholic) doctrine. Several European leaders are atheists (e.g., France's former president Francois Hollande or Greece's prime minister Alexis Tsipras). In Asia, the role of religion differs widely between countries. For instance, India is still one of the most religious countries and religion still has a strong impact on politics, given that Hindu nationalists have been targeting minorities like the Muslims and the Christians, who historically belonged to the lower castes. By contrast, countries such as China or Japan are largely secular and thus religion has a much smaller impact on politics. Secularism Secularization is the transformation of the politics of a society from close identification with a particular religion's values and institutions toward nonreligious values and secular institutions. The purpose of this is frequently modernization or protection of the population's religious diversity. Economics One study has found there is a negative correlation between self-defined religiosity and the wealth of nations. In other words, the richer a nation is, the less likely its inhabitants to call themselves religious, whatever this word means to them (Many people identify themselves as part of a religion (not irreligion) but do not self-identify as religious). Sociologist and political economist Max Weber has argued that Protestant Christian countries are wealthier because of their Protestant work ethic. According to a study from 2015, Christians hold the largest amount of wealth (55% of the total world wealth), followed by Muslims (5.8%), Hindus (3.3%) and Jews (1.1%). According to the same study it was found that adherents under the classification Irreligion or other religions hold about 34.8% of the total global wealth (while making up only about 20% of the world population, see section on classification). Health Mayo Clinic researchers examined the association between religious involvement and spirituality, and physical health, mental health, health-related quality of life, and other health outcomes. The authors reported that: "Most studies have shown that religious involvement and spirituality are associated with better health outcomes, including greater longevity, coping skills, and health-related quality of life (even during terminal illness) and less anxiety, depression, and suicide." The authors of a subsequent study concluded that the influence of religion on health is largely beneficial, based on a review of related literature. According to academic James W. Jones, several studies have discovered "positive correlations between religious belief and practice and mental and physical health and longevity." An analysis of data from the 1998 US General Social Survey, whilst broadly confirming that religious activity was associated with better health and well-being, also suggested that the role of different dimensions of spirituality/religiosity in health is rather more complicated. The results suggested "that it may not be appropriate to generalize findings about the relationship between spirituality/religiosity and health from one form of spirituality/religiosity to another, across denominations, or to assume effects are uniform for men and women. Violence Critics such as Hector Avalos, Regina Schwartz, Christopher Hitchens, and Richard Dawkins have argued that religions are inherently violent and harmful to society by using violence to promote their goals, in ways that are endorsed and exploited by their leaders. Anthropologist Jack David Eller asserts that religion is not inherently violent, arguing "religion and violence are clearly compatible, but they are not identical." He asserts that "violence is neither essential to nor exclusive to religion" and that "virtually every form of religious violence has its nonreligious corollary." Animal sacrifice Some (but not all) religions practise animal sacrifice, the ritual killing and offering of an animal to appease or maintain favour with a deity. It has been banned in India. Superstition Greek and Roman pagans, who saw their relations with the gods in political and social terms, scorned the man who constantly trembled with fear at the thought of the gods (deisidaimonia), as a slave might fear a cruel and capricious master. The Romans called such fear of the gods superstitio. Ancient Greek historian Polybius described superstition in ancient Rome as an instrumentum regni, an instrument of maintaining the cohesion of the Empire. Superstition has been described as the non-rational establishment of cause and effect. Religion is more complex and is often composed of social institutions and has a moral aspect. Some religions may include superstitions or make use of magical thinking. Adherents of one religion sometimes think of other religions as superstition. Some atheists, deists, and skeptics regard religious belief as superstition. The Roman Catholic Church considers superstition to be sinful in the sense that it denotes a lack of trust in the divine providence of God and, as such, is a violation of the first of the Ten Commandments. The Catechism of the Catholic Church states that superstition "in some sense represents a perverse excess of religion" (para. #2110). "Superstition", it says, "is a deviation of religious feeling and of the practices this feeling imposes. It can even affect the worship we offer the true God, e.g., when one attributes an importance in some way magical to certain practices otherwise lawful or necessary. To attribute the efficacy of prayers or of sacramental signs to their mere external performance, apart from the interior dispositions that they demand is to fall into superstition. Cf. Matthew 23:16–22" (para. #2111) Agnosticism and atheism The terms atheist (lack of belief in gods) and agnostic (belief in the unknowability of the existence of gods), though specifically contrary to theistic (e.g., Christian, Jewish, and Muslim) religious teachings, do not by definition mean the opposite of religious. The true opposite of religious is the word irreligious. Irreligion describes an absence of any religion; antireligion describes an active opposition or aversion toward religions in general. There are religions (including Buddhism and Taoism) that classify some of their followers as agnostic, atheistic, or nontheistic. For example, in ancient India, there were large atheistic movements and traditions (Nirīśvaravāda) that rejected the Vedas, such as the atheistic Ājīvika and the Ajñana which taught agnosticism. Interfaith cooperation Because religion continues to be recognized in Western thought as a universal impulse, many religious practitioners have aimed to band together in interfaith dialogue, cooperation, and religious peacebuilding. The first major dialogue was the Parliament of the World's Religions at the 1893 Chicago World's Fair, which affirmed universal values and recognition of the diversity of practices among different cultures. The 20th century has been especially fruitful in use of interfaith dialogue as a means of solving ethnic, political, or even religious conflict, with Christian–Jewish reconciliation representing a complete reverse in the attitudes of many Christian communities towards Jews. Recent interfaith initiatives include A Common Word, launched in 2007 and focused on bringing Muslim and Christian leaders together, the "C1 World Dialogue", the Common Ground initiative between Islam and Buddhism, and a United Nations sponsored "World Interfaith Harmony Week". Culture Culture and religion have usually been seen as closely related. Paul Tillich looked at religion as the soul of culture and culture as the form or framework of religion. In his own words: Religion as ultimate concern is the meaning-giving substance of culture, and culture is the totality of forms in which the basic concern of religion expresses itself. In abbreviation: religion is the substance of culture, culture is the form of religion. Such a consideration definitely prevents the establishment of a dualism of religion and culture. Every religious act, not only in organized religion, but also in the most intimate movement of the soul, is culturally formed. Ernst Troeltsch, similarly, looked at culture as the soil of religion and thought that, therefore, transplanting a religion from its original culture to a foreign culture would kill it in the same manner that transplanting a plant from its natural soil to an alien soil would kill it. However, there have been many attempts in the modern pluralistic situation to distinguish culture from religion. Domenic Marbaniang has argued that elements grounded on beliefs of a metaphysical nature (religious) are distinct from elements grounded on nature and the natural (cultural). For instance, language (with its grammar) is a cultural element while sacralization of language in which a particular religious scripture is written is more often a religious practice. The same applies to music and the arts. Criticism Criticism of religion is criticism of the ideas, the truth, or the practice of religion, including its political and social implications. See also Notes References Sources Further reading External links Kevin Schilbrack. "The Concept of Religion". In Zalta, Edward N. (ed.). Stanford Encyclopedia of Philosophy. Religion Statistics from UCB Libraries GovPubs Major Religions of the World Ranked by Number of Adherents by Adherents.com August 2005 Studying Religion Archived 29 February 2012 at the Wayback Machine – Introduction to the methods and scholars of the academic study of religion A Contribution to the Critique of Hegel's Philosophy of Right – Marx's original reference to religion as the opium of the people. The Complexity of Religion and the Definition of "Religion" in International Law – Harvard Human Rights Journal article from the President and Fellows of Harvard College (2003) Sociology of Religion Resources Video: 5 Religions spreading across the world

Myth is a genre of folklore consisting primarily of narratives that play a fundamental role in a society. For scholars, this is totally different from the ordinary sense of the term myth, meaning a belief that is not true, as the veracity of a piece of folklore is entirely irrelevant to determining whether it constitutes a myth. Myths are often endorsed by religious and secular authorities, and may be natural or supernatural in character. Many societies group their myths, legends, and history together, considering myths and legends to be factual accounts of their remote past. In particular, creation myths take place in a primordial age when the world had not achieved its later form. Origin myths explain how a society's customs, institutions, and taboos were established and sanctified. National myths are narratives about a nation's past that symbolize the nation's values. There is a complex relationship between recital of myths and the enactment of rituals. Etymology The word myth comes from Ancient Greek μῦθος (mȳthos), meaning 'speech', 'narrative', or 'fiction'. In turn, Ancient Greek μυθολογία (mythología 'story', 'legends', or 'story-telling') combines the word mȳthos with the suffix -λογία (-logia 'study'). Accordingly, Plato used mythología as a general term for fiction or story-telling of any kind. This word began was adapted into other European languages in the early 19th century, in a much narrower sense, as a scholarly term for "[a] traditional story, especially one concerning the early history of a people or explaining a natural or social phenomenon, and typically involving supernatural beings or events." The Greek term mythología was then borrowed into Late Latin, occurring in the title of Latin author Fabius Planciades Fulgentius' 5th-century Mythologiæ to denote what is now referred to as classical mythology—i.e., Greco-Roman etiological stories involving their gods. Fulgentius's Mythologiæ explicitly treated its subject matter as allegories requiring interpretation and not as true events. The Latin term was then adopted in Middle French as mythologie. Whether from French or Latin usage, English adopted the word mythology in the 15th century, initially meaning 'the exposition of a myth or myths', 'the interpretation of fables', or 'a book of such expositions'. The word is first attested in John Lydgate's Troy Book (c. 1425). From Lydgate until the 17th or 18th century, mythology meant a moral, fable, allegory or a parable, or collection of traditional stories, understood to be false. It came eventually to be applied to similar bodies of traditional stories among other polytheistic cultures around the world. Thus mythology entered the English language before myth. Johnson's Dictionary, for example, has an entry for mythology, but not for myth. Indeed, the Greek loanword mythos (pl. mythoi) and Latinate mythus (pl. mythi) both appeared in English before the first example of myth in 1830. Protagonists and structure The main characters in myths are usually non-humans, such as gods, demigods, and other supernatural figures. Others include humans, animals, or combinations in their classification of myth. Stories of everyday humans, although often of leaders of some type, are usually contained in legends, as opposed to myths. Myths are sometimes distinguished from legends in that myths deal with gods, usually have no historical basis, and are set in a world of the remote past, very different from that of the present. Definitions Definitions of myth vary to some extent among scholars, though Finnish folklorist Lauri Honko offers a widely-cited definition: Myth, a story of the gods, a religious account of the beginning of the world, the creation, fundamental events, the exemplary deeds of the gods as a result of which the world, nature and culture were created together with all parts thereof and given their order, which still obtains. A myth expresses and confirms society's religious values and norms, it provides a pattern of behavior to be imitated, testifies to the efficacy of ritual with its practical ends and establishes the sanctity of cult. Another definition of myth comes from myth criticism theorist and professor José Manuel Losada. According to Cultural Myth Criticism, the studies of myth must explain and understand "myth from inside", that is, only "as a myth". Losada defines myth as "a functional, symbolic and thematic narrative of one or several extraordinary events with a transcendent, sacred and supernatural referent; that lacks, in principle, historical testimony; and that refers to an individual or collective, but always absolute, cosmogony or eschatology". According to the hylistic myth research by assyriologist Annette Zgoll and classic philologist Christian Zgoll, "A myth can be defined as an Erzählstoff [narrative material] which is polymorphic through its variants and – depending on the variant – polystratic; an Erzählstoff in which transcending interpretations of what can be experienced are combined into a hyleme sequence with an implicit claim to relevance for the interpretation and mastering of the human condition." Scholars in other fields use the term myth in varied ways. In a broad sense, the word can refer to any traditional story, popular misconception or imaginary entity. Though myth and other folklore genres may overlap, myth is often thought to differ from genres such as legend and folktale in that neither are considered to be sacred narratives. Some kinds of folktales, such as fairy stories, are not considered true by anyone, and may be seen as distinct from myths for this reason. Main characters in myths are usually gods, demigods or supernatural humans, while legends generally feature humans as their main characters. Many exceptions and combinations exist, as in the Iliad, Odyssey and Aeneid. Moreover, as stories spread between cultures or as faiths change, myths can come to be considered folktales, their divine characters recast as either as humans or demihumans such as giants, elves and faeries. Conversely, historical and literary material may acquire mythological qualities over time. For example, the Matter of Britain (the legendary history of Great Britain, especially those focused on King Arthur and the knights of the Round Table) and the Matter of France, seem distantly to originate in historical events of the 5th and 8th centuries, respectively, and became mythologised over the following centuries. In colloquial use, myth can also be used of a collectively held belief that has no basis in fact, or any false story. This usage, which is often pejorative, arose from labelling the religious myths and beliefs of other cultures as incorrect, but it has spread to cover non-religious beliefs as well. As commonly used by folklorists and academics in other relevant fields, such as anthropology, myth has no implication whether the narrative may be understood as true or otherwise. Among biblical scholars of both the Old and New Testament, the word myth has a technical meaning, in that it usually refers to "describe the actions of the other‐worldly in terms of this world" such as the Creation and the Fall. Since myth is popularly used to describe stories that are not objectively true, the identification of a narrative as a myth can be highly controversial. Many religious adherents believe that the narratives told in their respective religious traditions are historical without question, and so object to their identification as myths while labelling traditional narratives from other religions as such. Hence, some scholars may label all religious narratives as "myths" for practical reasons, such as to avoid depreciating any one tradition because cultures interpret each other differently relative to one another. Other scholars may abstain from using the term myth altogether for purposes of avoiding placing pejorative overtones on sacred narratives. Related terms Mythology In present use, mythology usually refers to the collection of myths of a group of people. For example, Greek mythology, Roman mythology, Celtic mythology and Hittite mythology all describe the body of myths retold among those cultures. "Mythology" can also refer to the study of myths and mythologies. Mythography The compilation or description of myths is sometimes known as "mythography", a term also used for a scholarly anthology of myths or of the study of myths generally. Key mythographers in the Classical tradition include: Ovid (43 BCE – 17/18 CE), whose tellings of myths have been profoundly influential; Fabius Planciades Fulgentius, a Latin writer of the late-5th to early-6th centuries, whose Mythologies (Latin: Mitologiarum libri III) gathered and gave moralistic interpretations of a wide range of myths; the anonymous medieval Vatican Mythographers, who developed anthologies of Classical myths that remained influential to the end of the Middle Ages; and Renaissance scholar Natalis Comes, whose ten-book Mythologiae became a standard source for classical mythology in later Renaissance Europe. Other prominent mythographies include the thirteenth-century Prose Edda attributed to the Icelander Snorri Sturluson, which is the main surviving survey of Norse Mythology from the Middle Ages. Jeffrey G. Snodgrass (professor of anthropology at the Colorado State University) has termed India's Bhats as mythographers. Myth criticism Myth criticism is a system of anthropological interpretation of culture created by French philosopher Gilbert Durand. Scholars have used myth criticism to explain the mythical roots of contemporary fiction, which means that modern myth criticism needs to be interdisciplinary. Losada offers his own methodological, hermeneutic and epistemological approach to myth. While assuming mythopoetical perspectives, Losada's cultural myth criticism takes a step further, incorporating the study of the transcendent dimension (its function, its disappearance) to evaluate the role of myth as a mirror of contemporary culture. Cultural myth criticism, without abandoning the analysis of the symbolic, invades all cultural manifestations and delves into the difficulties in understanding myth today. This cultural myth criticism studies mythical manifestations in fields as wide as literature, film and television, theater, sculpture, painting, video games, music, dancing, the Internet and other artistic fields. Myth criticism, a discipline that studies myths (mythology contains them, like a pantheon its statues), is by nature interdisciplinary: it combines the contributions of literary theory, the history of literature, the fine arts and the new ways of dissemination in the age of communication. Likewise, it undertakes its object of study from its interrelation with other human and social sciences, in particular sociology, anthropology and economics. The need for an approach, for a methodology that allows us to understand the complexity of the myth and its manifestations in contemporary times, is justified. Mythos Because myth is sometimes used in a pejorative sense, some scholars have opted for mythos instead. "Mythos" now more commonly refers to its Aristotelian sense as a "plot point" or to a body of interconnected myths or stories, especially those belonging to a particular religious or cultural tradition. It is sometimes used specifically for modern, fictional mythologies, such as the world building of H. P. Lovecraft. Mythopoeia Mythopoeia (mytho- + -poeia, 'I make myth') was termed by J. R. R. Tolkien, amongst others, to refer to the "conscious generation" of mythology. It was notoriously also suggested, separately, by Nazi ideologist Alfred Rosenberg. Interpretations Comparative mythology Comparative mythology is a systematic comparison of myths from different cultures. It seeks to discover underlying themes that are common to the myths of multiple cultures. In some cases, comparative mythologists use the similarities between separate mythologies to argue that those mythologies have a common source. This source may inspire myths or provide a common "protomythology" that diverged into the mythologies of each culture. Functionalism A number of commentators have argued that myths function to form and shape society and social behaviour. Eliade argued that one of the foremost functions of myth is to establish models for behavior and that myths may provide a religious experience. By telling or reenacting myths, members of traditional societies detach themselves from the present, returning to the mythical age, thereby coming closer to the divine. Honko asserted that, in some cases, a society reenacts a myth in an attempt to reproduce the conditions of the mythical age. For example, it might reenact the healing performed by a god at the beginning of time in order to heal someone in the present. Similarly, Barthes argued that modern culture explores religious experience. Since it is not the job of science to define human morality, a religious experience is an attempt to connect with a perceived moral past, which is in contrast with the technological present. Pattanaik defines mythology as "the subjective truth of people communicated through stories, symbols and rituals." He says, "Facts are everybody's truth. Fiction is nobody's truth. Myths are somebody's truth." Euhemerism One theory claims that myths are distorted accounts of historical events. According to this theory, storytellers repeatedly elaborate upon historical accounts until the figures in those accounts gain the status of gods. For example, the myth of the wind-god Aeolus may have evolved from a historical account of a king who taught his people to use sails and interpret the winds. Herodotus (fifth-century BCE) and Prodicus made claims of this kind. This theory is named euhemerism after mythologist Euhemerus (c. 320 BCE), who suggested that Greek gods developed from legends about humans. Allegory Some theories propose that myths began as allegories for natural phenomena: Apollo represents the sun, Poseidon represents water, and so on. According to another theory, myths began as allegories for philosophical or spiritual concepts: Athena represents wise judgment, Aphrodite romantic desire, and so on. Müller supported an allegorical theory of myth. He believed myths began as allegorical descriptions of nature and gradually came to be interpreted literally. For example, a poetic description of the sea as "raging" was eventually taken literally and the sea was then thought of as a raging god. Personification Some thinkers claimed that myths result from the personification of objects and forces. According to these thinkers, the ancients worshiped natural phenomena, such as fire and air, gradually deifying them. For example, according to this theory, ancients tended to view things as gods, not as mere objects. Thus, they described natural events as acts of personal gods, giving rise to myths. Ritualism According to the myth-ritual theory, myth is tied to ritual. In its most extreme form, this theory claims myths arose to explain rituals. This claim was first put forward by Smith, who argued that people begin performing rituals for reasons not related to myth. Forgetting the original reason for a ritual, they account for it by inventing a myth and claiming the ritual commemorates the events described in that myth. James George Frazer—author of The Golden Bough, a book on the comparative study of mythology and religion—argued that humans started out with a belief in magical rituals; later, they began to lose faith in magic and invented myths about gods, reinterpreting their rituals as religious rituals intended to appease the gods. Academic discipline history Historically, important approaches to the study of mythology have included those of Vico, Schelling, Schiller, Jung, Freud, Lévy-Bruhl, Lévi-Strauss, Frye, the Soviet school, and the Myth and Ritual School. Ancient Greece The critical interpretation of myth began with the Presocratics. Euhemerus was one of the most important pre-modern mythologists. He interpreted myths as accounts of actual historical events, though distorted over many retellings. Sallustius divided myths into five categories: theological; physical (or concerning natural law); animistic (or concerning soul); material; and mixed, which concerns myths that show the interaction between two or more of the previous categories and are particularly used in initiations. Plato condemned poetic myth when discussing education in the Republic. His critique was primarily on the grounds that the uneducated might take the stories of gods and heroes literally. Nevertheless, he constantly referred to myths throughout his writings. As Platonism developed in the phases commonly called Middle Platonism and neoplatonism, writers such as Plutarch, Porphyry, Proclus, Olympiodorus, and Damascius wrote explicitly about the symbolic interpretation of traditional and Orphic myths. Mythological themes were consciously employed in literature, beginning with Homer. The resulting work may expressly refer to a mythological background without itself becoming part of a body of myths (Cupid and Psyche). Medieval romance in particular plays with this process of turning myth into literature. Euhemerism, as stated earlier, refers to the rationalization of myths, putting themes formerly imbued with mythological qualities into pragmatic contexts. An example of this would be following a cultural or religious paradigm shift (notably the re-interpretation of pagan mythology following Christianization). European Renaissance Interest in polytheistic mythology revived during the Renaissance, with early works of mythography appearing in the sixteenth century, among them the Theologia Mythologica (1532). 19th century The first modern, Western scholarly theories of myth appeared during the second half of the 19th century—at the same time as "myth" was adopted as a scholarly term in European languages. They were driven partly by a new interest in Europe's ancient past and vernacular culture, associated with Romantic Nationalism and epitomised by the research of Jacob Grimm (1785–1863). This movement drew European scholars' attention not only to Classical myths, but also material now associated with Norse mythology, Finnish mythology, and so forth. Western theories were also partly driven by Europeans' efforts to comprehend and control the cultures, stories and religions they were encountering through colonialism. These encounters included both extremely old texts such as the Sanskrit Rigveda and the Sumerian Epic of Gilgamesh, and current oral narratives such as mythologies of the indigenous peoples of the Americas or stories told in traditional African religions. The intellectual context for nineteenth-century scholars was profoundly shaped by emerging ideas about evolution. These ideas included the recognition that many Eurasian languages—and therefore, conceivably, stories—were all descended from a lost common ancestor (the Indo-European language) which could rationally be reconstructed through the comparison of its descendant languages. They also included the idea that cultures might evolve in ways comparable to species. In general, 19th-century theories framed myth as a failed or obsolete mode of thought, often by interpreting myth as the primitive counterpart of modern science within a unilineal framework that imagined that human cultures are travelling, at different speeds, along a linear path of cultural development. Nature One of the dominant mythological theories of the latter 19th century was nature mythology, the foremost exponents of which included Max Müller and Edward Burnett Tylor. This theory posited that "primitive man" was primarily concerned with the natural world. It tended to interpret myths that seemed distasteful to European Victorians—such as tales about sex, incest, or cannibalism—as metaphors for natural phenomena like agricultural fertility. Unable to conceive impersonal natural laws, early humans tried to explain natural phenomena by attributing souls to inanimate objects, thus giving rise to animism. According to Tylor, human thought evolved through stages, starting with mythological ideas and gradually progressing to scientific ideas. Müller also saw myth as originating from language, even calling myth a "disease of language". He speculated that myths arose due to the lack of abstract nouns and neuter gender in ancient languages. Anthropomorphic figures of speech, necessary in such languages, were eventually taken literally, leading to the idea that natural phenomena were in actuality conscious or divine. Not all scholars, not even all 19th-century scholars, accepted this view. Lucien Lévy-Bruhl claimed that "the primitive mentality is a condition of the human mind and not a stage in its historical development." Recent scholarship, noting the fundamental lack of evidence for "nature mythology" interpretations among people who actually circulated myths, has likewise abandoned the key ideas of "nature mythology". Ritual Frazer saw myths as a misinterpretation of magical rituals, which were themselves based on a mistaken idea of natural law. This idea was central to the "myth and ritual" school of thought. According to Frazer, humans begin with an unfounded belief in impersonal magical laws. When they realize applications of these laws do not work, they give up their belief in natural law in favor of a belief in personal gods controlling nature, thus giving rise to religious myths. Meanwhile, humans continue practicing formerly magical rituals through force of habit, reinterpreting them as reenactments of mythical events. Finally, humans come to realize nature follows natural laws, and they discover their true nature through science. Here again, science makes myth obsolete as humans progress "from magic through religion to science." Segal asserted that by pitting mythical thought against modern scientific thought, such theories imply modern humans must abandon myth. 20th century The earlier 20th century saw major work developing psychoanalytical approaches to interpreting myth, led by Sigmund Freud, who, drawing inspiration from Classical myth, began developing the concept of the Oedipus complex in his 1899 The Interpretation of Dreams. Jung likewise tried to understand the psychology behind world myths. Jung asserted that all humans share certain innate unconscious psychological forces, which he called archetypes. He believed similarities between the myths of different cultures reveals the existence of these universal archetypes. The mid-20th century saw the influential development of a structuralist theory of mythology, led by Lévi-Strauss. Strauss argued that myths reflect patterns in the mind and interpreted those patterns more as fixed mental structures, specifically pairs of opposites (good/evil, compassionate/callous), rather than unconscious feelings or urges. Meanwhile, Bronislaw Malinowski developed analyses of myths focusing on their social functions in the real world. He is associated with the idea that myths such as origin stories might provide a "mythic charter"—a legitimisation—for cultural norms and social institutions. Thus, following the Structuralist Era (c. 1960s–1980s), the predominant anthropological and sociological approaches to myth increasingly treated myth as a form of narrative that can be studied, interpreted, and analyzed like ideology, history, and culture. In other words, myth is a form of understanding and telling stories that are connected to power, political structures, and political and economic interests. These approaches contrast with approaches, such as those of Joseph Campbell and Eliade, which hold that myth has some type of essential connection to ultimate sacred meanings that transcend cultural specifics. In particular, myth was studied in relation to history from diverse social sciences. Most of these studies share the assumption that history and myth are not distinct in the sense that history is factual, real, accurate, and truth, while myth is the opposite. In the 1950s, Barthes published a series of essays examining modern myths and the process of their creation in his book Mythologies, which stood as an early work in the emerging post-structuralist approach to mythology, which recognised myths' existence in the modern world and in popular culture. The 20th century saw rapid secularization in Western culture. This made Western scholars more willing to analyse narratives in the Abrahamic religions as myths; theologians such as Rudolf Bultmann argued that a modern Christianity needed to demythologize; and other religious scholars embraced the idea that the mythical status of Abrahamic narratives was a legitimate feature of their importance. This, in his appendix to Myths, Dreams and Mysteries, and in The Myth of the Eternal Return, Eliade attributed modern humans' anxieties to their rejection of myths and the sense of the sacred. The Christian theologian Conrad Hyers wrote: [M]yth today has come to have negative connotations which are the complete opposite of its meaning in a religious context... In a religious context, myths are storied vehicles of supreme truth, the most basic and important truths of all. By them, people regulate and interpret their lives and find worth and purpose in their existence. Myths put one in touch with sacred realities, the fundamental sources of being, power, and truth. They are seen not only as being the opposite of error but also as being clearly distinguishable from stories told for entertainment and from the workaday, domestic, practical language of a people. They provide answers to the mysteries of being and becoming, mysteries which, as mysteries, are hidden, yet mysteries which are revealed through story and ritual. Myths deal not only with truth but with ultimate truth. 21st century Both in 19th-century research, which tended to see existing records of stories and folklore as imperfect fragments of partially lost myths, and in 20th-century structuralist work, which sought to identify underlying patterns and structures in often diverse versions of a given myth, there had been a tendency to synthesise sources to attempt to reconstruct what scholars supposed to be more perfect or underlying forms of myths. From the late 20th century, researchers influenced by postmodernism tended instead to argue that each account of a given myth has its own cultural significance and meaning, and argued that rather than representing degradation from a once more perfect form, myths are inherently plastic and variable. There is, consequently, no such thing as the 'original version' or 'original form' of a myth. One prominent example of this movement was A. K. Ramanujan's essay "Three Hundred Ramayanas". Correspondingly, scholars challenged the precedence that had once been given to texts as a medium for mythology, arguing that other media, such as the visual arts or even landscape and place-naming, could be as or more important. Myths are not texts, but narrative materials (Erzählstoffe) that can be adapted in various media (such as epics, hymns, handbooks, movies, dances, etc.). In contrast to other academic approaches, which primarily focus on the (social) function of myths, hylistic myth research aims to understand myths and their nature out of themselves. As part of the Göttingen myth research, Annette and Christian Zgoll developed the method of hylistics (narrative material research) to extract mythical materials from their media and make possible a transmedial comparison. The content of the medium is broken down into the smallest possible plot components (hylemes), which are listed in standardized form (so-called hyleme analysis). Inconsistencies in content can indicate stratification, i.e. the overlapping of several materials, narrative variants and edition layers within the same medial concretion. To a certain extent, this can also be used to reconstruct earlier and alternative variants of the same material that were in competition and/or were combined with each other. The juxtaposition of hyleme sequences enables the systematic comparison of different variants of the same material or several different materials that are related or structurally similar to each other. In his overall presentation of the hundred-year history of myth research, the classical philologist and myth researcher Udo Reinhardt mentions Christian Zgoll's basic work Tractatus mythologicus as "the latest handbook on myth theory" with "outstanding significance" for modern myth research. Modernity Scholars in the field of cultural studies research how myth has worked itself into modern discourses. Mythological discourse can reach greater audiences than ever before via digital media. Various mythic elements appear in popular culture, as well as television, cinema and video games. Although myth was traditionally transmitted through the oral tradition on a small scale, the film industry has enabled filmmakers to transmit myths to large audiences via film. In Jungian psychology, myths are the expression of a culture or society's goals, fears, ambitions and dreams. The basis of modern visual storytelling is rooted in the mythological tradition. Many contemporary films rely on ancient myths to construct narratives. The Walt Disney Company is well known among cultural study scholars for "reinventing" traditional childhood myths. While few films are as obvious as Disney fairy tales, the plots of many films are based on the rough structure of myths. Mythological archetypes, such as the cautionary tale regarding the abuse of technology, battles between gods and creation stories, are often the subject of major film productions. These films are often created under the guise of cyberpunk action films, fantasy, dramas and apocalyptic tales. 21st-century films such as Clash of the Titans, Immortals and Thor continue the trend of using traditional mythology to frame modern plots. Authors use mythology as a basis for their books, such as Rick Riordan, whose Percy Jackson and the Olympians series is situated in a modern-day world where the Greek deities are manifest. Scholars, particularly those within the field of fan studies, and fans of popular culture have also noted a connection between fan fiction and myth. Ika Willis identified three models of this: fan fiction as a reclaiming of popular stories from corporations, myth as a means of critiquing or dismantling hegemonic power, and myth as "a commons of story and a universal story world". Willis supports the third model, a universal story world, and argues that fanfiction can be seen as mythic due to its hyperseriality—a term invented by Sarah Iles Johnston to describe a hyperconnected universe in which characters and stories are interwoven. In an interview for the New York Times, Henry Jenkins stated that fanfiction 'is a way of the culture repairing the damage done in a system where contemporary myths are owned by corporations instead of owned by the folk.' See also List of mythologies List of mythological objects List of mythology books and sources Magic and mythology Mythopoeia, artificially constructed mythology, mainly for the purpose of storytelling Myth: Its Meaning and Functions in Ancient and Other Cultures by G. S. Kirk Notes Sources External links "Mythology.net".

Anthropology is the scientific study of humanity that crosses biology and sociology, concerned with human behavior, human biology, cultures, societies, and linguistics, in both the present and past, including archaic humans. Social anthropology studies patterns of behaviour, while cultural anthropology studies cultural meaning, including norms and values. The term sociocultural anthropology is commonly used today. Linguistic anthropology studies how language influences social life. Biological (or physical) anthropology studies the biology and evolution of humans and their close primate relatives. Archaeology, often referred to as the "anthropology of the past," explores human activity by examining physical remains. In North America and Asia, it is generally regarded as a branch of anthropology, whereas in Europe, it is considered either an independent discipline or classified under related fields like history and palaeontology. Etymology The abstract noun anthropology is first attested in reference to history. Its present use first appeared in Renaissance Germany in the works of Magnus Hundt and Otto Casmann. Their Neo-Latin anthropologia derived from the combining forms of the Greek words ánthrōpos (ἄνθρωπος, "human") and lógos (λόγος, "study"). Its adjectival form appeared in the works of Aristotle. It began to be used in English, possibly via French Anthropologie, by the early 18th century. Origin and development of the term Through the 19th century In 1647, the Bartholins, early scholars of the University of Copenhagen, defined l'anthropologie as follows: Anthropology, that is to say the science that treats of man, is divided ordinarily and with reason into Anatomy, which considers the body and the parts, and Psychology, which speaks of the soul. Sporadic use of the term for some of the subject matter occurred subsequently, including its use by Étienne Serres in 1839 to describe the natural history, or paleontology, of man, based on comparative anatomy, and the creation of a chair in anthropology and ethnography in 1850 at the French National Museum of Natural History by Jean Louis Armand de Quatrefages de Bréau. Various short-lived organizations of anthropologists had already been formed. The Société Ethnologique de Paris, the first to use the term ethnology, was formed in 1839 and focused on methodically studying human races. After the death of its founder, William Frédéric Edwards, in 1842, it gradually declined in activity until it eventually dissolved in 1862. Meanwhile, the Ethnological Society of New York, now the American Ethnological Society, was founded on its model in 1842, as well as the Ethnological Society of London in 1843, a break-away group of the Aborigines' Protection Society. These anthropologists were liberal, anti-slavery, and pro-human rights. They maintained international connections. Anthropology and many other current fields are the intellectual results of the comparative methods developed in the earlier 19th century. Theorists in diverse fields such as anatomy, linguistics, and ethnology, started making feature-by-feature comparisons of their subject matters, and were beginning to suspect that similarities between animals, languages, and folkways were the result of processes or laws unknown to them then. For them, the publication of Charles Darwin's On the Origin of Species was the epiphany of everything they had begun to suspect. Darwin himself arrived at his conclusions through comparison of species he had seen in agronomy and in the wild. Darwin and Wallace unveiled evolution in the late 1850s. There was an immediate rush to bring it into the social sciences. Paul Broca in Paris was in the process of breaking away from the Société de biologie to form the first of the explicitly anthropological societies, the Société d'Anthropologie de Paris, meeting for the first time in Paris in 1859. When he read Darwin, he became an immediate convert to Transformisme, as the French called evolutionism. His definition now became "the study of the human group, considered as a whole, in its details, and in relation to the rest of nature". Broca, being what today would be called a neurosurgeon, had gained an interest in the pathology of speech. He wanted to localize the difference between man and the other animals, which appeared to reside in speech. He discovered the speech center of the human brain, today called Broca's area after him. His interest was mainly in biological anthropology, but a German philosopher specializing in psychology, Theodor Waitz, took up the theme of general and social anthropology in his six-volume work, entitled Die Anthropologie der Naturvölker, 1859–1864. The title was soon translated as "The Anthropology of Primitive Peoples". The last two volumes were published posthumously. Waitz defined anthropology as "the science of the nature of man". Following Broca's lead, Waitz points out that anthropology is a new field, which would gather material from other fields, but would differ from them in the use of comparative anatomy, physiology, and psychology to differentiate man from "the animals nearest to him". He stresses that the data of comparison must be empirical, gathered by experimentation. The history of civilization, as well as ethnology, are to be brought into the comparison. It is to be presumed fundamentally that the species, man, is a unity, and that "the same laws of thought are applicable to all men". Waitz was influential among British ethnologists. In 1863, the explorer Richard Francis Burton and the speech therapist James Hunt broke away from the Ethnological Society of London to form the Anthropological Society of London, which henceforward would follow the path of the new anthropology rather than just ethnology. It was the 2nd society dedicated to general anthropology in existence. Representatives from the French Société were present, though not Broca. In his keynote address, printed in the first volume of its new publication, The Anthropological Review, Hunt stressed the work of Waitz, adopting his definitions as a standard. Among the first associates were the young Edward Burnett Tylor, inventor of cultural anthropology, and his brother Alfred Tylor, a geologist. Previously Edward had referred to himself as an ethnologist; subsequently, an anthropologist. Similar organizations in other countries followed: The Anthropological Society of Madrid (1865), the American Anthropological Association in 1902, the Anthropological Society of Vienna (1870), the Italian Society of Anthropology and Ethnology (1871), and many others subsequently. The majority of these were evolutionists. One notable exception was the Berlin Society for Anthropology, Ethnology, and Prehistory (1869) founded by Rudolph Virchow, known for his vituperative attacks on the evolutionists. Not religious himself, he insisted that Darwin's conclusions lacked empirical foundation. During the last three decades of the 19th century, a proliferation of anthropological societies and associations occurred, most independent, most publishing their own journals, and all international in membership and association. The major theorists belonged to these organizations. They supported the gradual osmosis of anthropology curricula into the major institutions of higher learning. By 1898, 48 educational institutions in 13 countries had some curriculum in anthropology. None of the 75 faculty members were under a department named anthropology. Anthropology is considered by some to have become a tool for colonisers studying their subjects to gain a better understanding and control. 20th and 21st centuries Anthropology as a specialized field of academic study developed much through the end of the 19th century. Then it rapidly expanded beginning in the early 20th century to the point where many of the world's higher educational institutions typically included anthropology departments. Thousands of anthropology departments have come into existence, and anthropology has also diversified from a few major subdivisions to dozens more. Practical anthropology, the use of anthropological knowledge and technique to solve specific problems, has arrived; for example, the presence of buried victims might stimulate the use of a forensic archaeologist to recreate the final scene. The organization has also reached a global level. For example, the World Council of Anthropological Associations (WCAA), "a network of national, regional and international associations that aims to promote worldwide communication and cooperation in anthropology", currently contains members from about three dozen nations. Since the work of Franz Boas and Bronisław Malinowski in the late 19th and early 20th centuries, social anthropology in Great Britain and cultural anthropology in the US have been distinguished from other social sciences by their emphasis on cross-cultural comparisons, long-term in-depth examination of context, and the importance they place on participant-observation or experiential immersion in the area of research. Cultural anthropology, in particular, has emphasized cultural relativism, holism, and the use of findings to frame cultural critiques. This has been particularly prominent in the United States, from Boas' arguments against 19th-century racial ideology, through Margaret Mead's advocacy for gender equality and sexual liberation, to current criticisms of post-colonial oppression and promotion of multiculturalism. Ethnography is one of its primary research designs as well as the text that is generated from anthropological fieldwork. In Great Britain and the Commonwealth countries, the British tradition of social anthropology tends to dominate. In the United States, anthropology has traditionally been divided into the four field approach developed by Franz Boas in the early 20th century: biological or physical anthropology; social, cultural, or sociocultural anthropology; archaeological anthropology; and linguistic anthropology. These fields frequently overlap but tend to use different methodologies and techniques. European countries with overseas colonies tended to practice more ethnology (a term coined and defined by Adam F. Kollár in 1783). It is sometimes referred to as sociocultural anthropology in the parts of the world that were influenced by the European tradition. Fields Anthropology is a global discipline involving humanities, social sciences and natural sciences. Anthropology builds upon knowledge from natural sciences, including the discoveries about the origin and evolution of Homo sapiens, human physical traits, human behavior, the variations among different groups of humans, how the evolutionary past of Homo sapiens has influenced its social organization and culture, and from social sciences, including the organization of human social and cultural relations, institutions, social conflicts, etc. Early anthropology originated in Classical Greece and Persia and studied and tried to understand observable cultural diversity. As such, anthropology has been central in the development of several new (late 20th century) interdisciplinary fields such as cognitive science, global studies, and various ethnic studies. According to Clifford Geertz, ...anthropology is perhaps the last of the great nineteenth-century conglomerate disciplines still for the most part organizationally intact. Long after natural history, moral philosophy, philology, and political economy have dissolved into their specialized successors, it has remained a diffuse assemblage of ethnology, human biology, comparative linguistics, and prehistory, held together mainly by the vested interests, sunk costs, and administrative habits of academia, and by a romantic image of comprehensive scholarship. Sociocultural anthropology has been heavily influenced by structuralist and postmodern theories, as well as a shift toward the analysis of modern societies. During the 1970s and 1990s, there was an epistemological shift away from the positivist traditions that had largely informed the discipline. During this shift, enduring questions about the nature and production of knowledge came to occupy a central place in cultural and social anthropology. In contrast, archaeology and biological anthropology remained largely positivist. Sociocultural Sociocultural anthropology draws together the principal axes of cultural anthropology and social anthropology. Cultural anthropology is the comparative study of the manifold ways in which people make sense of the world around them, while social anthropology is the study of the relationships among individuals and groups. Cultural anthropology is more related to philosophy, literature and the arts (how one's culture affects the experience for self and group, contributing to a more complete understanding of the people's knowledge, customs, and institutions), while social anthropology is more related to sociology and history. In that, it helps develop an understanding of social structures, typically of others and other populations (such as minorities, subgroups, dissidents, etc.). There is no hard-and-fast distinction between them, and these categories overlap to a considerable degree. Inquiry in sociocultural anthropology is guided in part by cultural relativism, the attempt to understand other societies in terms of their own cultural symbols and values. Accepting other cultures in their own terms moderates reductionism in cross-cultural comparison. This project is often accommodated in the field of ethnography. Ethnography can refer to both a methodology and the product of ethnographic research, i.e. an ethnographic monograph. As a methodology, ethnography is based upon long-term fieldwork within a community or other research site. Participant observation is one of the foundational methods of social and cultural anthropology. Ethnology involves the systematic comparison of different cultures. The process of participant-observation can be especially helpful to understanding a culture from an emic (conceptual, vs. etic, or technical) point of view. The study of kinship and social organization is a central focus of sociocultural anthropology, as kinship is a human universal. Sociocultural anthropology also covers economic and political organization, law and conflict resolution, patterns of consumption and exchange, material culture, technology, infrastructure, gender relations, ethnicity, childrearing and socialization, religion, myth, symbols, values, etiquette, worldview, sports, music, nutrition, recreation, games, food, festivals, and language- which is also the object of study in linguistic anthropology. Comparison across cultures is a key element of method in sociocultural anthropology, including the industrialized (and de-industrialized) West. The Standard Cross-Cultural Sample (SCCS) includes 186 such cultures. Biological Biological anthropology and physical anthropology are synonymous terms to describe anthropological research focused on the study of humans and other primates in their biological, evolutionary, and demographic dimensions. It examines the biological and social factors that have affected the evolution of humans and other primates, and that generate, maintain or change contemporary genetic and physiological variation. Archaeological Archaeology is the study of the human past through its material remains. Artifacts, faunal remains, and human altered landscapes are evidence of the cultural and material lives of past societies. Archaeologists examine material remains in order to deduce patterns of past human behavior and cultural practices. Ethnoarchaeology is a type of archaeology that studies the practices and material remains of living human groups in order to gain a better understanding of the evidence left behind by past human groups, who are presumed to have lived in similar ways. Linguistic Linguistic anthropology (not to be confused with anthropological linguistics) seeks to understand the processes of human communications, verbal and non-verbal, variation in language across time and space, the social uses of language, and the relationship between language and culture. It is the branch of anthropology that brings linguistic methods to bear on anthropological problems, linking the analysis of linguistic forms and processes to the interpretation of sociocultural processes. Linguistic anthropologists often draw on related fields including sociolinguistics, pragmatics, cognitive linguistics, semiotics, discourse analysis, and narrative analysis. Ethnography Ethnography is a method of analysing social or cultural interaction. It often involves participant observation though an ethnographer may also draw from texts written by participants of in social interactions. Ethnography views first-hand experience and social context as important. Tim Ingold distinguishes ethnography from anthropology arguing that anthropology tries to construct general theories of human experience, applicable in general and novel settings, while ethnography concerns itself with fidelity. He argues that the anthropologist must make his writing consistent with their understanding of literature and other theory but notes that ethnography may be of use to the anthropologists and the fields inform one another. Key topics by field: sociocultural Art, media, music, dance and film Art One of the central problems in the anthropology of art concerns the universality of 'art' as a cultural phenomenon. Several anthropologists have noted that the Western categories of 'painting', 'sculpture', or 'literature', conceived as independent artistic activities, do not exist, or exist in a significantly different form, in most non-Western contexts. To surmount this difficulty, anthropologists of art have focused on formal features in objects which, without exclusively being 'artistic', have certain evident 'aesthetic' qualities. Boas' Primitive Art, Claude Lévi-Strauss' The Way of the Masks (1982) or Geertz's 'Art as Cultural System' (1983) are examples in this trend of transforming the anthropology of 'art' into an anthropology of culturally specific 'aesthetics'. Media Media anthropology emphasizes ethnographic studies as a means of understanding producers, audiences, and other cultural and social aspects of mass media. The types of ethnographic contexts explored range from contexts of media production (e.g., ethnographies of newsrooms in newspapers, journalists in the field, film production) to contexts of media reception, following audiences in their everyday responses to media. Other types include cyber anthropology, a relatively new area of internet research, as well as ethnographies of other areas of research which happen to involve media, such as development work, social movements, or health education. This is in addition to many classic ethnographic contexts, where media such as radio, the press, new media, and television have started to make their presences felt since the early 1990s. Music Ethnomusicology is an academic field encompassing various approaches to the study of music (broadly defined), that emphasize its cultural, social, material, cognitive, biological, and other dimensions or contexts instead of or in addition to its isolated sound component or any particular repertoire. Ethnomusicology can be used in a wide variety of fields, such as teaching, politics, cultural anthropology etc. While the origins of ethnomusicology date back to the 18th and 19th centuries, it was formally termed "ethnomusicology" by Dutch scholar Jaap Kunst c. 1950. Later, the influence of study in this area spawned the creation of the periodical Ethnomusicology and the Society of Ethnomusicology. Visual Visual anthropology is concerned, in part, with the study and production of ethnographic photography, film and, since the mid-1990s, new media. While the term is sometimes used interchangeably with ethnographic film, visual anthropology also encompasses the anthropological study of visual representation, including areas such as performance, museums, art, and the production and reception of mass media. Visual representations from all cultures, such as sandpaintings, tattoos, sculptures and reliefs, cave paintings, scrimshaw, jewelry, hieroglyphs, paintings, and photographs are included in the focus of visual anthropology. Economic, political economic, applied and development Economic Economic anthropology attempts to explain human economic behavior in its widest historic, geographic and cultural scope. It has a complex relationship with the discipline of economics, of which it is highly critical. Its origins as a sub-field of anthropology begin with the Polish-British founder of anthropology, Bronisław Malinowski, and his French compatriot, Marcel Mauss, on the nature of gift-giving exchange (or reciprocity) as an alternative to market exchange. Economic anthropology remains, for the most part, focused upon exchange. The school of thought derived from Marx and known as political economy focuses on production, in contrast. Economic anthropologists have abandoned the primitivist niche they were relegated to by economists, and have now turned to examine corporations, banks, and the global financial system from an anthropological perspective. Political economy Political economy in anthropology is the application of the theories and methods of historical materialism to the traditional concerns of anthropology, including, but not limited to, non-capitalist societies. Political economy introduced questions of history and colonialism to ahistorical anthropological theories of social structure and culture. Three main areas of interest rapidly developed. The first of these areas was concerned with the "pre-capitalist" societies that were subject to evolutionary "tribal" stereotypes. Sahlin's work on hunter-gatherers as the "original affluent society" did much to dissipate that image. The second area was concerned with the vast majority of the world's population at the time, the peasantry, many of whom were involved in complex revolutionary wars such as in Vietnam. The third area was on colonialism, imperialism, and the creation of the capitalist world-system. More recently, these political economists have more directly addressed issues of industrial (and post-industrial) capitalism around the world. Applied Applied anthropology refers to the application of the method and theory of anthropology to the analysis and solution of practical problems. It is a "complex of related, research-based, instrumental methods which produce change or stability in specific cultural systems through the provision of data, initiation of direct action, and/or the formulation of policy". Applied anthropology is the practical side of anthropological research; it includes researcher involvement and activism within the participating community. It is closely related to development anthropology (distinct from the more critical anthropology of development). Development Anthropology of development tends to view development from a critical perspective. The kind of issues addressed and implications for the approach involve pondering why, if a key development goal is to alleviate poverty, is poverty increasing? Why is there such a gap between plans and outcomes? Why are those working in development so willing to disregard history and the lessons it might offer? Why is development so externally driven rather than having an internal basis? In short, why does so much planned development fail? Kinship, feminism, gender and sexuality Kinship Kinship can refer both to the study of the patterns of social relationships in one or more human cultures, or it can refer to the patterns of social relationships themselves. Over its history, anthropology has developed a number of related concepts and terms, such as "descent", "descent groups", "lineages", "affines", "cognates", and even "fictive kinship". Broadly, kinship patterns may be considered to include people related both by descent (one's social relations during development), and also relatives by marriage. Within kinship you have two different families. People have their biological families and it is the people they share DNA with. This is called consanguinity or "blood ties". People can also have a chosen family in which they chose who they want to be a part of their family. In some cases, people are closer with their chosen family more than with their biological families. Feminist Feminist anthropology is a four field approach to anthropology (archeological, biological, cultural, linguistic) that seeks to reduce male bias in research findings, anthropological hiring practices, and the scholarly production of knowledge. Anthropology engages often with feminists from non-Western traditions, whose perspectives and experiences can differ from those of white feminists of Europe, America, and elsewhere. From the perspective of the Western world, historically such 'peripheral' perspectives have been ignored, observed only from an outsider perspective, and regarded as less-valid or less-important than knowledge from the Western world. Exploring and addressing that double bias against women from marginalized racial or ethnic groups is of particular interest in intersectional feminist anthropology. Feminist anthropologists have stated that their publications have contributed to anthropology, along the way correcting against the systemic biases beginning with the "patriarchal origins of anthropology (and (academia)" and note that from 1891 to 1930 doctorates in anthropology went to males more than 85%, more than 81% were under 35, and only 7.2% to anyone over 40 years old, thus reflecting an age gap in the pursuit of anthropology by first-wave feminists until later in life. This correction of systemic bias may include mainstream feminist theory, history, linguistics, archaeology, and anthropology. Feminist anthropologists are often concerned with the construction of gender across societies. Gender constructs are of particular interest when studying sexism. According to St. Clair Drake, Vera Mae Green was, until "[w]ell into the 1960s", the only African American female anthropologist who was also a Caribbeanist. She studied ethnic and family relations in the Caribbean as well as the United States, and thereby tried to improve the way black life, experiences, and culture were studied. However, Zora Neale Hurston, although often primarily considered to be a literary author, was trained in anthropology by Franz Boas, and published Tell my Horse about her "anthropological observations" of voodoo in the Caribbean (1938). Feminist anthropology is inclusive of the anthropology of birth as a specialization, which is the anthropological study of pregnancy and childbirth within cultures and societies. Medical, nutritional, psychological, cognitive and transpersonal Medical Medical anthropology is an interdisciplinary field which studies "human health and disease, health care systems, and biocultural adaptation". It is believed that William Caudell was the first to discover the field of medical anthropology. Currently, research in medical anthropology is one of the main growth areas in the field of anthropology as a whole. It focuses on the following six basic fields: The development of systems of medical knowledge and medical care The patient-physician relationship The integration of alternative medical systems in culturally diverse environments The interaction of social, environmental and biological factors which influence health and illness both in the individual and the community as a whole The critical analysis of interaction between psychiatric services and migrant populations ("critical ethnopsychiatry": Beneduce 2004, 2007) The impact of biomedicine and biomedical technologies in non-Western settings Other subjects that have become central to medical anthropology worldwide are violence and social suffering (Farmer, 1999, 2003; Beneduce, 2010) as well as other issues that involve physical and psychological harm and suffering that are not a result of illness. On the other hand, there are fields that intersect with medical anthropology in terms of research methodology and theoretical production, such as cultural psychiatry and transcultural psychiatry or ethnopsychiatry. Nutritional Nutritional anthropology is a synthetic concept that deals with the interplay between economic systems, nutritional status and food security, and how changes in the former affect the latter. If economic and environmental changes in a community affect access to food, food security, and dietary health, then this interplay between culture and biology is in turn connected to broader historical and economic trends associated with globalization. Nutritional status affects overall health status, work performance potential, and the overall potential for economic development (either in terms of human development or traditional western models) for any given group of people. Psychological Psychological anthropology is an interdisciplinary subfield of anthropology that studies the interaction of cultural and mental processes. This subfield tends to focus on ways in which humans' development and enculturation within a particular cultural group – with its own history, language, practices, and conceptual categories – shape processes of human cognition, emotion, perception, motivation, and mental health. It also examines how the understanding of cognition, emotion, motivation, and similar psychological processes inform or constrain our models of cultural and social processes. Cognitive Cognitive anthropology seeks to explain patterns of shared knowledge, cultural innovation, and transmission over time and space using the methods and theories of the cognitive sciences (especially experimental psychology and evolutionary biology) often through close collaboration with historians, ethnographers, archaeologists, linguists, musicologists and other specialists engaged in the description and interpretation of cultural forms. Cognitive anthropology is concerned with what people from different groups know and how that implicit knowledge changes the way people perceive and relate to the world around them. Transpersonal Transpersonal anthropology studies the relationship between altered states of consciousness and culture. As with transpersonal psychology, the field is much concerned with altered states of consciousness (ASC) and transpersonal experience. However, the field differs from mainstream transpersonal psychology in taking more cognizance of cross-cultural issues – for instance, the roles of myth, ritual, diet, and text in evoking and interpreting extraordinary experiences. Political and legal Political Political anthropology concerns the structure of political systems, looked at from the basis of the structure of societies. Political anthropology developed as a discipline concerned primarily with politics in stateless societies, a new development started from the 1960s, and is still unfolding: anthropologists started increasingly to study more "complex" social settings in which the presence of states, bureaucracies and markets entered both ethnographic accounts and analysis of local phenomena. The turn towards complex societies meant that political themes were taken up at two main levels. Firstly, anthropologists continued to study political organization and political phenomena that lay outside the state-regulated sphere (as in patron-client relations or tribal political organization). Secondly, anthropologists slowly started to develop a disciplinary concern with states and their institutions (and on the relationship between formal and informal political institutions). An anthropology of the state developed, and it is a most thriving field today. Geertz's comparative work on "Negara", the Balinese state, is an early, famous example. Legal Legal anthropology or anthropology of law specializes in "the cross-cultural study of social ordering". Earlier legal anthropological research often focused more narrowly on conflict management, crime, sanctions, or formal regulation. More recent applications include issues such as human rights, legal pluralism, and political uprisings. Public Public anthropology was created by Robert Borofsky, a professor at Hawaii Pacific University, to "demonstrate the ability of anthropology and anthropologists to effectively address problems beyond the discipline – illuminating larger social issues of our times as well as encouraging broad, public conversations about them with the explicit goal of fostering social change". Nature, science, and technology Cyborg Cyborg anthropology originated as a sub-focus group within the American Anthropological Association's annual meeting in 1993. The sub-group was very closely related to STS and the Society for the Social Studies of Science. Donna Haraway's 1985 Cyborg Manifesto could be considered the founding document of cyborg anthropology by first exploring the philosophical and sociological ramifications of the term. Cyborg anthropology studies humankind and its relations with the technological systems it has built, specifically modern technological systems that have reflexively shaped notions of what it means to be human beings. Digital Digital anthropology is the study of the relationship between humans and digital-era technology and extends to various areas where anthropology and technology intersect. It is sometimes grouped with sociocultural anthropology, and sometimes considered part of material culture. The field is new, and thus has a variety of names with a variety of emphases. These include techno-anthropology, digital ethnography, cyberanthropology, and virtual anthropology. Ecological Ecological anthropology is defined as the "study of cultural adaptations to environments". The sub-field is also defined as, "the study of relationships between a population of humans and their biophysical environment". The focus of its research concerns "how cultural beliefs and practices helped human populations adapt to their environments, and how their environments change across space and time. The contemporary perspective of environmental anthropology, and arguably at least the backdrop, if not the focus of most of the ethnographies and cultural fieldworks of today, is political ecology. Many characterize this new perspective as more informed with culture, politics and power, globalization, localized issues, century anthropology and more. The focus and data interpretation is often used for arguments for/against or creation of policy, and to prevent corporate exploitation and damage of land. Often, the observer has become an active part of the struggle either directly (organizing, participation) or indirectly (articles, documentaries, books, ethnographies). Such is the case with environmental justice advocate Melissa Checker and her relationship with the people of Hyde Park. Environment Social sciences, like anthropology, can provide interdisciplinary approaches to the environment. Professor Kay Milton, Director of the Anthropology research network in the School of History and Anthropology, describes anthropology as distinctive, with its most distinguishing feature being its interest in non-industrial indigenous and traditional societies. Anthropological theory is distinct because of the consistent presence of the concept of culture; not an exclusive topic but a central position in the study and a deep concern with the human condition. Milton describes three trends that are causing a fundamental shift in what characterizes anthropology: dissatisfaction with the cultural relativist perspective, reaction against cartesian dualisms which obstructs progress in theory (nature culture divide), and finally an increased attention to globalization (transcending the barriers or time/space). Environmental discourse appears to be characterized by a high degree of globalization. (The troubling problem is borrowing non-indigenous practices and creating standards, concepts, philosophies and practices in western countries.) Anthropology and environmental discourse now have become a distinct position in anthropology as a discipline. Knowledge about diversities in human culture can be important in addressing environmental problems - anthropology is now a study of human ecology. Human activity is the most important agent in creating environmental change, a study commonly found in human ecology which can claim a central place in how environmental problems are examined and addressed. Other ways anthropology contributes to environmental discourse is by being theorists and analysts, or by refinement of definitions to become more neutral/universal, etc. In exploring environmentalism - the term typically refers to a concern that the environment should be protected, particularly from the harmful effects of human activities. Environmentalism itself can be expressed in many ways. Anthropologists can open the doors of environmentalism by looking beyond industrial society, understanding the opposition between industrial and non-industrial relationships, knowing what ecosystem people and biosphere people are and are affected by, dependent and independent variables, "primitive" ecological wisdom, diverse environments, resource management, diverse cultural traditions, and knowing that environmentalism is a part of culture. Historical Ethnohistory is the study of ethnographic cultures and indigenous customs by examining historical records. It is also the study of the history of various ethnic groups that may or may not exist today. Ethnohistory uses both historical and ethnographic data as its foundation. Its historical methods and materials go beyond the standard use of documents and manuscripts. Practitioners recognize the utility of such source material as maps, music, paintings, photography, folklore, oral tradition, site exploration, archaeological materials, museum collections, enduring customs, language, and place names. Religion The anthropology of religion involves the study of religious institutions in relation to other social institutions, and the comparison of religious beliefs and practices across cultures. Modern anthropology assumes that there is complete continuity between magical thinking and religion, and that every religion is a cultural product created by the human community that worships it. Urban Urban anthropology is concerned with issues of urbanization, poverty, and neoliberalism. Ulf Hannerz quotes a 1960s remark that traditional anthropologists were "a notoriously agoraphobic lot, anti-urban by definition". Various social processes in the Western World as well as in the "Third World" (the latter being the habitual focus of attention of anthropologists) brought the attention of "specialists in 'other cultures'" closer to their homes. There are two main approaches to urban anthropology: examining the types of cities or examining the social issues within the cities. These two methods are overlapping and dependent of each other. By defining different types of cities, one would use social factors as well as economic and political factors to categorize the cities. By directly looking at the different social issues, one would also be studying how they affect the dynamic of the city. Key topics by field: archaeological and biological Anthrozoology Anthrozoology (also known as "human–animal studies") is the study of interaction between living things. It is an interdisciplinary field that overlaps with a number of other disciplines, including anthropology, ethology, medicine, psychology, veterinary medicine and zoology. A major focus of anthrozoologic research is the quantifying of the positive effects of human-animal relationships on either party and the study of their interactions. It includes scholars from a diverse range of fields, including anthropology, sociology, biology, and philosophy. Biocultural Biocultural anthropology is the scientific exploration of the relationships between human biology and culture. Physical anthropologists throughout the first half of the 20th century viewed this relationship from a racial perspective; that is, from the assumption that typological human biological differences lead to cultural differences. After World War II the emphasis began to shift toward an effort to explore the role culture plays in shaping human biology. Evolutionary Evolutionary anthropology is the interdisciplinary study of the evolution of human physiology and human behaviour and the relation between hominins and non-hominin primates. Evolutionary anthropology is based in natural science and social science, combining the human development with socioeconomic factors. Evolutionary anthropology is concerned with both biological and cultural evolution of humans, past and present. It is based on a scientific approach, and brings together fields such as archaeology, behavioral ecology, psychology, primatology, and genetics. It is a dynamic and interdisciplinary field, drawing on many lines of evidence to understand the human experience, past and present. Forensic Forensic anthropology is the application of the science of physical anthropology and human osteology in a legal setting, most often in criminal cases where the victim's remains are in the advanced stages of decomposition. A forensic anthropologist can assist in the identification of deceased individuals whose remains are decomposed, burned, mutilated or otherwise unrecognizable. The adjective "forensic" refers to the application of this subfield of science to a court of law. Palaeoanthropology Paleoanthropology combines the disciplines of paleontology and physical anthropology. It is the study of ancient humans, as found in fossil hominid evidence such as petrifacted bones and footprints. Genetics and morphology of specimens are crucially important to this field. Markers on specimens, such as enamel fractures and dental decay on teeth, can also give insight into the behaviour and diet of past populations. Organizations Contemporary anthropology is an established science with academic departments at most universities and colleges. The single largest organization of anthropologists is the American Anthropological Association (AAA), which was founded in 1903. Its members are anthropologists from around the globe. In 1989, a group of European and American scholars in the field of anthropology established the European Association of Social Anthropologists (EASA) which serves as a major professional organization for anthropologists working in Europe. The EASA seeks to advance the status of anthropology in Europe and to increase visibility of marginalized anthropological traditions and thereby contribute to the project of a global anthropology or world anthropology. Hundreds of other organizations exist in the various sub-fields of anthropology, sometimes divided up by nation or region, and many anthropologists work with collaborators in other disciplines, such as geology, physics, zoology, paleontology, anatomy, music theory, art history, sociology and so on, belonging to professional societies in those disciplines as well. List of major organizations Ethics As the field has matured it has debated and arrived at ethical principles aimed at protecting both the subjects of anthropological research as well as the researchers themselves, and professional societies have generated codes of ethics. Anthropologists, like other researchers (especially historians and scientists engaged in field research), have over time assisted state policies and projects, especially colonialism. Some commentators have contended: That the discipline grew out of colonialism, perhaps was in league with it, and derives some of its key notions from it, consciously or not. (See, for example, Gough, Pels and Salemink, but cf. Lewis 2004). That ethnographic work is often ahistorical, writing about people as if they were "out of time" in an "ethnographic present" (Johannes Fabian, Time and Its Other). In his article "The Misrepresentation of Anthropology and Its Consequences," Herbert S. Lewis critiqued older anthropological works that presented other cultures as if they were strange and unusual. While the findings of those researchers should not be discarded, the field should learn from its mistakes. Cultural relativism As part of their quest for scientific objectivity, present-day anthropologists typically urge cultural relativism, which has an influence on all the sub-fields of anthropology. This is the notion that cultures should not be judged by another's values or viewpoints, but be examined dispassionately on their own terms. There should be no notions, in good anthropology, of one culture being better or worse than another culture. Ethical commitments in anthropology include noticing and documenting genocide, infanticide, racism, sexism, mutilation (including circumcision and subincision), and torture. Topics like racism, slavery, and human sacrifice attract anthropological attention and theories ranging from nutritional deficiencies, to genes, to acculturation, to colonialism, have been proposed to explain their origins and continued recurrences. To illustrate the depth of an anthropological approach, one can take just one of these topics, such as racism, and find thousands of anthropological references, stretching across all the major and minor sub-fields. Military involvement Anthropologists' involvement with the U.S. government, in particular, has caused bitter controversy within the discipline. Franz Boas publicly objected to US participation in World War I, and after the war, he published a brief exposé and condemnation of the participation of several American archaeologists in espionage in Mexico under their cover as scientists. But by the 1940s, many of Boas' anthropologist contemporaries were active in the allied war effort against the Axis Powers (Nazi Germany, Fascist Italy, and Imperial Japan). Many served in the armed forces, while others worked in intelligence (for example, the Office of Strategic Services and the Office of War Information). At the same time, David H. Price's work on American anthropology during the Cold War provides detailed accounts of the pursuit and dismissal of several anthropologists from their jobs for communist sympathies. Attempts to accuse anthropologists of complicity with the CIA and government intelligence activities during the Vietnam War years have turned up little. Many anthropologists (students and teachers) were active in the antiwar movement. Numerous resolutions condemning the war in all its aspects were passed overwhelmingly at the annual meetings of the American Anthropological Association (AAA). Professional anthropological bodies often object to the use of anthropology for the benefit of the state. Their codes of ethics or statements may proscribe anthropologists from giving secret briefings. The Association of Social Anthropologists of the UK and Commonwealth (ASA) has called certain scholarship ethically dangerous. The "Principles of Professional Responsibility" issued by the American Anthropological Association and amended through November 1986 stated that "in relation with their own government and with host governments ... no secret research, no secret reports or debriefings of any kind should be agreed to or given." The current "Principles of Professional Responsibility" does not make explicit mention of ethics surrounding state interactions. Anthropologists, along with other social scientists, were working with the US military as part of the US Army's strategy in Afghanistan. The Christian Science Monitor reports that "Counterinsurgency efforts focus on better grasping and meeting local needs" in Afghanistan, under the Human Terrain System (HTS) program; in addition, HTS teams are working with the US military in Iraq. In 2009, the American Anthropological Association's Commission on the Engagement of Anthropology with the US Security and Intelligence Communities (CEAUSSIC) released its final report concluding, in part, that: When ethnographic investigation is determined by military missions, not subject to external review, where data collection occurs in the context of war, integrated into the goals of counterinsurgency, and in a potentially coercive environment – all characteristic factors of the HTS concept and its application – it can no longer be considered a legitimate professional exercise of anthropology. In summary, while we stress that constructive engagement between anthropology and the military is possible, CEAUSSIC suggests that the AAA emphasize the incompatibility of HTS with disciplinary ethics and practice for job seekers and that it further recognize the problem of allowing HTS to define the meaning of 'anthropology' within DoD. Post-World War II developments Before WWII British 'social anthropology' and American 'cultural anthropology' were still distinct traditions. After the war, enough British and American anthropologists borrowed ideas and methodological approaches from one another that some began to speak of them collectively as 'sociocultural' anthropology. Basic trends There are several characteristics that tend to unite anthropological work. One of the central characteristics is that anthropology tends to provide a comparatively more holistic account of phenomena and tends to be highly empirical. The quest for holism leads most anthropologists to study a particular place, problem or phenomenon in detail, using a variety of methods, over a more extensive period than normal in many parts of academia. In the 1990s and 2000s, calls for clarification of what constitutes a culture, of how an observer knows where their own culture ends and another begins, and other crucial topics in writing anthropology were heard. These dynamic relationships, between what can be observed on the ground, as opposed to what can be observed by compiling many local observations remain fundamental in any kind of anthropology, whether cultural, biological, linguistic or archaeological. Biological anthropologists are interested in both human variation and in the possibility of human universals (behaviors, ideas or concepts shared by virtually all human cultures). They use many different methods of study, but modern population genetics, participant observation and other techniques often take anthropologists "into the field," which means traveling to a community in its own setting, to do something called "fieldwork." On the biological or physical side, human measurements, genetic samples, nutritional data may be gathered and published as articles or monographs. Along with dividing up their project by theoretical emphasis, anthropologists typically divide the world up into relevant time periods and geographic regions. Time periods are divided up into relevant cultural traditions based on material, such as the Paleolithic and the Neolithic, of particular use in archaeology. Further cultural subdivisions according to tool types, such as Oldowan, Mousterian or Levalloisian help archaeologists and other anthropologists in understanding major trends in the past. Anthropologists and geographers share approaches to culture regions as well, since mapping cultures is central to both sciences. By making comparisons across cultural traditions (time-based) and cultural regions (space-based), anthropologists have developed various kinds of comparative method, a central part of their science. Commonalities between fields Because anthropology developed from so many different enterprises (see History of anthropology), including but not limited to fossil-hunting, exploring, documentary film-making, paleontology, primatology, antiquity dealings and curatorship, philology, etymology, genetics, regional analysis, ethnology, history, philosophy, and religious studies, it is difficult to characterize the entire field in a brief article, although attempts to write histories of the entire field have been made. Some authors argue that anthropology originated and developed as the study of "other cultures", both in terms of time (past societies) and space (non-European/non-Western societies). For example, the classic of urban anthropology, Ulf Hannerz in the introduction to his seminal Exploring the City: Inquiries Toward an Urban Anthropology mentions that the "Third World" had habitually received most of attention; anthropologists who traditionally specialized in "other cultures" looked for them far away and started to look "across the tracks" only in late 1960s. Now there exist many works focusing on peoples and topics very close to the author's "home". It is also argued that other fields of study, like History and Sociology, on the contrary focus disproportionately on the West. In France, the study of Western societies has been traditionally left to sociologists, but this is increasingly changing, starting in the 1970s from scholars like Isac Chiva and journals like Terrain ("fieldwork") and developing with the center founded by Marc Augé (Le Centre d'anthropologie des mondes contemporains, the Anthropological Research Center of Contemporary Societies). Since the 1980s it has become common for social and cultural anthropologists to set ethnographic research in the North Atlantic region, frequently examining the connections between locations rather than limiting research to a single locale. There has also been a related shift toward broadening the focus beyond the daily life of ordinary people; increasingly, research is set in settings such as scientific laboratories, social movements, governmental and nongovernmental organizations and businesses. See also Lists Notes References Works cited Further reading Dictionaries and encyclopedias Fieldnotes and memoirs Histories Textbooks and key theoretical works External links Open Encyclopedia of Anthropology. Haller, Dieter. "Interviews with German Anthropologists: Video Portal for the History of German Anthropology post 1945". Ruhr-Universität Bochum. Retrieved 22 March 2015. (AIO)

Biology is the scientific study of life and living organisms. It is a broad natural science that encompasses a wide range of fields and unifying principles that explain the structure, function, growth, origin, evolution, and distribution of life. Central to biology are five fundamental themes: the cell as the basic unit of life, genes and heredity as the basis of inheritance, evolution as the driver of biological diversity, energy transformation for sustaining life processes, and the maintenance of internal stability (homeostasis). Biology examines life across multiple levels of organization, from molecules and cells to organisms, populations, and ecosystems. Subdisciplines include molecular biology, physiology, ecology, evolutionary biology, developmental biology, and systematics, among others. Each of these fields applies a range of methods to investigate biological phenomena, including observation, experimentation, and mathematical modeling. Modern biology is grounded in the theory of evolution by natural selection, first articulated by Charles Darwin, and in the molecular understanding of genes encoded in DNA. The discovery of the structure of DNA and advances in molecular genetics have transformed many areas of biology, leading to applications in medicine, agriculture, biotechnology, and environmental science. Life on Earth is believed to have originated over 3.7 billion years ago. Today, it includes a vast diversity of organisms—from single-celled archaea and bacteria to complex multicellular plants, fungi, and animals. Biologists classify organisms based on shared characteristics and evolutionary relationships, using taxonomic and phylogenetic frameworks. These organisms interact with each other and with their environments in ecosystems, where they play roles in energy flow and nutrient cycling. As a constantly evolving field, biology incorporates new discoveries and technologies that enhance the understanding of life and its processes, while contributing to solutions for challenges such as disease, climate change, and biodiversity loss. Etymology From Greek bios, life, (from Proto-Indo-European root *gwei-, to live) and logy, study of. The compound was coined in 1800 by Karl Friedrich Burdach and in 1802 used by both German naturalist Gottfried Reinhold Treviranus and Jean-Baptiste Lamarck. History The earliest of roots of science, which included medicine, can be traced to ancient Egypt and Mesopotamia in around 3000 to 1200 BCE. Their contributions shaped ancient Greek natural philosophy. Ancient Greek philosophers such as Aristotle (384–322 BCE) contributed extensively to the development of biological knowledge. He explored biological causation and the diversity of life. His successor, Theophrastus, began the scientific study of plants. Scholars of the medieval Islamic world who wrote on biology included al-Jahiz (781–869), Al-Dīnawarī (828–896), who wrote on botany, and Rhazes (865–925) who wrote on anatomy and physiology. Medicine was especially well studied by Islamic scholars working in Greek philosopher traditions, while natural history drew heavily on Aristotelian thought. Biology began to quickly develop with Anton van Leeuwenhoek's dramatic improvement of the microscope. It was then that scholars discovered spermatozoa, bacteria, infusoria and the diversity of microscopic life. Investigations by Jan Swammerdam led to new interest in entomology and helped to develop techniques of microscopic dissection and staining. Advances in microscopy had a profound impact on biological thinking. In the early 19th century, biologists pointed to the central importance of the cell. In 1838, Schleiden and Schwann began promoting the now universal ideas that (1) the basic unit of organisms is the cell and (2) that individual cells have all the characteristics of life, although they opposed the idea that (3) all cells come from the division of other cells, continuing to support spontaneous generation. However, Robert Remak and Rudolf Virchow were able to reify the third tenet, and by the 1860s most biologists accepted all three tenets which consolidated into cell theory. Meanwhile, taxonomy and classification became the focus of natural historians. Carl Linnaeus published a basic taxonomy for the natural world in 1735, and in the 1750s introduced scientific names for all his species. Georges-Louis Leclerc, Comte de Buffon, treated species as artificial categories and living forms as malleable—even suggesting the possibility of common descent. Serious evolutionary thinking originated with the works of Jean-Baptiste Lamarck, who presented a coherent theory of evolution. The British naturalist Charles Darwin, combining the biogeographical approach of Humboldt, the uniformitarian geology of Lyell, Malthus's writings on population growth, and his own morphological expertise and extensive natural observations, forged a more successful evolutionary theory based on natural selection; similar reasoning and evidence led Alfred Russel Wallace to independently reach the same conclusions. The basis for modern genetics began with the work of Gregor Mendel in 1865. This outlined the principles of biological inheritance. However, the significance of his work was not realized until the early 20th century when evolution became a unified theory as the modern synthesis reconciled Darwinian evolution with classical genetics. In the 1940s and early 1950s, a series of experiments by Alfred Hershey and Martha Chase pointed to DNA as the component of chromosomes that held the trait-carrying units that had become known as genes. A focus on new kinds of model organisms such as viruses and bacteria, along with the discovery of the double-helical structure of DNA by James Watson and Francis Crick in 1953, marked the transition to the era of molecular genetics. From the 1950s onwards, biology has been vastly extended in the molecular domain. The genetic code was cracked by Har Gobind Khorana, Robert W. Holley and Marshall Warren Nirenberg after DNA was understood to contain codons. The Human Genome Project was launched in 1990 to map the human genome. Chemical basis Atoms and molecules All organisms are made up of chemical elements; oxygen, carbon, hydrogen, and nitrogen account for most (96%) of the mass of all organisms, with calcium, phosphorus, sulfur, sodium, chlorine, and magnesium constituting essentially all the remainder. Different elements can combine to form compounds such as water, which is fundamental to life. Biochemistry is the study of chemical processes within and relating to living organisms. Molecular biology is the branch of biology that seeks to understand the molecular basis of biological activity in and between cells, including molecular synthesis, modification, mechanisms, and interactions. Water Life arose from the Earth's first ocean, which formed some 3.8 billion years ago. Since then, water continues to be the most abundant molecule in every organism. Water is important to life because it is an effective solvent, capable of dissolving solutes such as sodium and chloride ions or other small molecules to form an aqueous solution. Once dissolved in water, these solutes are more likely to come in contact with one another and therefore take part in chemical reactions that sustain life. In terms of its molecular structure, water is a small polar molecule with a bent shape formed by the polar covalent bonds of two hydrogen (H) atoms to one oxygen (O) atom (H2O). Because the O–H bonds are polar, the oxygen atom has a slight negative charge and the two hydrogen atoms have a slight positive charge. This polar property of water allows it to attract other water molecules via hydrogen bonds, which makes water cohesive. Surface tension results from the cohesive force due to the attraction between molecules at the surface of the liquid. Water is also adhesive as it is able to adhere to the surface of any polar or charged non-water molecules. Water is denser as a liquid than it is as a solid (or ice). This unique property of water allows ice to float above liquid water such as ponds, lakes, and oceans, thereby insulating the liquid below from the cold air above. Water has the capacity to absorb energy, giving it a higher specific heat capacity than other solvents such as ethanol. Thus, a large amount of energy is needed to break the hydrogen bonds between water molecules to convert liquid water into water vapor. As a molecule, water is not completely stable as each water molecule continuously dissociates into hydrogen and hydroxyl ions before reforming into a water molecule again. In pure water, the number of hydrogen ions balances (or equals) the number of hydroxyl ions, resulting in a pH that is neutral. Organic compounds Organic compounds are molecules that contain carbon bonded to another element such as hydrogen. With the exception of water, nearly all the molecules that make up each organism contain carbon. Carbon can form covalent bonds with up to four other atoms, enabling it to form diverse, large, and complex molecules. For example, a single carbon atom can form four single covalent bonds such as in methane, two double covalent bonds such as in carbon dioxide (CO2), or a triple covalent bond such as in carbon monoxide (CO). Moreover, carbon can form very long chains of interconnecting carbon–carbon bonds such as octane or ring-like structures such as glucose. The simplest form of an organic molecule is the hydrocarbon, which is a large family of organic compounds that are composed of hydrogen atoms bonded to a chain of carbon atoms. A hydrocarbon backbone can be substituted by other elements such as oxygen (O), hydrogen (H), phosphorus (P), and sulfur (S), which can change the chemical behavior of that compound. Groups of atoms that contain these elements (O-, H-, P-, and S-) and are bonded to a central carbon atom or skeleton are called functional groups. There are six prominent functional groups that can be found in organisms: amino group, carboxyl group, carbonyl group, hydroxyl group, phosphate group, and sulfhydryl group. In 1953, the Miller–Urey experiment showed that organic compounds could be synthesized abiotically within a closed system mimicking the conditions of early Earth, thus suggesting that complex organic molecules could have arisen spontaneously in early Earth (see abiogenesis). Macromolecules Macromolecules are large molecules made up of smaller subunits or monomers. Monomers include sugars, amino acids, and nucleotides. Carbohydrates include monomers and polymers of sugars. Lipids are the only class of macromolecules that are not made up of polymers. They include steroids, phospholipids, and fats, largely nonpolar and hydrophobic (water-repelling) substances. Proteins are the most diverse of the macromolecules. They include enzymes, transport proteins, large signaling molecules, antibodies, and structural proteins. The basic unit (or monomer) of a protein is an amino acid. Twenty amino acids are used in proteins. Nucleic acids are polymers of nucleotides. Their function is to store, transmit, and express hereditary information. Cells Cell theory states that cells are the fundamental units of life, that all living things are composed of one or more cells, and that all cells arise from preexisting cells through cell division. Most cells are very small, with diameters ranging from 1 to 100 micrometers and are therefore only visible under a light or electron microscope. There are generally two types of cells: eukaryotic cells, which contain a nucleus, and prokaryotic cells, which do not. Prokaryotes are single-celled organisms such as bacteria, whereas eukaryotes can be single-celled or multicellular. In multicellular organisms, every cell in the organism's body is derived ultimately from a single cell in a fertilized egg. Cell structure Every cell is enclosed within a cell membrane that separates its cytoplasm from the extracellular space. A cell membrane consists of a lipid bilayer, including cholesterols that sit between phospholipids to maintain their fluidity at various temperatures. Cell membranes are semipermeable, allowing small molecules such as oxygen, carbon dioxide, and water to pass through while restricting the movement of larger molecules and charged particles such as ions. Cell membranes also contain membrane proteins, including integral membrane proteins that go across the membrane serving as membrane transporters, and peripheral proteins that loosely attach to the outer side of the cell membrane, acting as enzymes shaping the cell. Cell membranes are involved in various cellular processes such as cell adhesion, storing electrical energy, and cell signalling and serve as the attachment surface for several extracellular structures such as a cell wall, glycocalyx, and cytoskeleton. Within the cytoplasm of a cell, there are many biomolecules such as proteins and nucleic acids. In addition to biomolecules, eukaryotic cells have specialized structures called organelles that have their own lipid bilayers or are spatially units. These organelles include the cell nucleus, which contains most of the cell's DNA, or mitochondria, which generate adenosine triphosphate (ATP) to power cellular processes. Other organelles such as endoplasmic reticulum and Golgi apparatus play a role in the synthesis and packaging of proteins, respectively. Biomolecules such as proteins can be engulfed by lysosomes, another specialized organelle. Plant cells have additional organelles that distinguish them from animal cells such as a cell wall that provides support for the plant cell, chloroplasts that harvest sunlight energy to produce sugar, and vacuoles that provide storage and structural support as well as being involved in reproduction and breakdown of plant seeds. Eukaryotic cells also have cytoskeleton that is made up of microtubules, intermediate filaments, and microfilaments, all of which provide support for the cell and are involved in the movement of the cell and its organelles. In terms of their structural composition, the microtubules are made up of tubulin (e.g., α-tubulin and β-tubulin) whereas intermediate filaments are made up of fibrous proteins. Microfilaments are made up of actin molecules that interact with other strands of proteins. Metabolism All cells require energy to sustain cellular processes. Metabolism is the set of chemical reactions in an organism. The three main purposes of metabolism are: the conversion of food to energy to run cellular processes; the conversion of food/fuel to monomer building blocks; and the elimination of metabolic wastes. These enzyme-catalyzed reactions allow organisms to grow and reproduce, maintain their structures, and respond to their environments. Metabolic reactions may be categorized as catabolic—the breaking down of compounds (for example, the breaking down of glucose to pyruvate by cellular respiration); or anabolic—the building up (synthesis) of compounds (such as proteins, carbohydrates, lipids, and nucleic acids). Usually, catabolism releases energy, and anabolism consumes energy. The chemical reactions of metabolism are organized into metabolic pathways, in which one chemical is transformed through a series of steps into another chemical, each step being facilitated by a specific enzyme. Enzymes are crucial to metabolism because they allow organisms to drive desirable reactions that require energy that will not occur by themselves, by coupling them to spontaneous reactions that release energy. Enzymes act as catalysts—they allow a reaction to proceed more rapidly without being consumed by it—by reducing the amount of activation energy needed to convert reactants into products. Enzymes also allow the regulation of the rate of a metabolic reaction, for example in response to changes in the cell's environment or to signals from other cells. Cellular respiration Cellular respiration is a set of metabolic reactions and processes that take place in cells to convert chemical energy from nutrients into adenosine triphosphate (ATP), and then release waste products. The reactions involved in respiration are catabolic reactions, which break large molecules into smaller ones, releasing energy. Respiration is one of the key ways a cell releases chemical energy to fuel cellular activity. The overall reaction occurs in a series of biochemical steps, some of which are redox reactions. Although cellular respiration is technically a combustion reaction, it clearly does not resemble one when it occurs in a cell because of the slow, controlled release of energy from the series of reactions. Sugar in the form of glucose is the main nutrient used by animal and plant cells in respiration. Cellular respiration involving oxygen is called aerobic respiration, which has four stages: glycolysis, citric acid cycle (or Krebs cycle), electron transport chain, and oxidative phosphorylation. Glycolysis is a metabolic process that occurs in the cytoplasm whereby glucose is converted into two pyruvates, with two net molecules of ATP being produced at the same time. Each pyruvate is then oxidized into acetyl-CoA by the pyruvate dehydrogenase complex, which also generates NADH and carbon dioxide. Acetyl-CoA enters the citric acid cycle, which takes places inside the mitochondrial matrix. At the end of the cycle, the total yield from 1 glucose (or 2 pyruvates) is 6 NADH, 2 FADH2, and 2 ATP molecules. Finally, the next stage is oxidative phosphorylation, which in eukaryotes, occurs in the mitochondrial cristae. Oxidative phosphorylation comprises the electron transport chain, which is a series of four protein complexes that transfer electrons from one complex to another, thereby releasing energy from NADH and FADH2 that is coupled to the pumping of protons (hydrogen ions) across the inner mitochondrial membrane (chemiosmosis), which generates a proton motive force. Energy from the proton motive force drives the enzyme ATP synthase to synthesize more ATPs by phosphorylating ADPs. The transfer of electrons terminates with molecular oxygen being the final electron acceptor. If oxygen were not present, pyruvate would not be metabolized by cellular respiration but undergoes a process of fermentation. The pyruvate is not transported into the mitochondrion but remains in the cytoplasm, where it is converted to waste products that may be removed from the cell. This serves the purpose of oxidizing the electron carriers so that they can perform glycolysis again and removing the excess pyruvate. Fermentation oxidizes NADH to NAD+ so it can be re-used in glycolysis. In the absence of oxygen, fermentation prevents the buildup of NADH in the cytoplasm and provides NAD+ for glycolysis. This waste product varies depending on the organism. In skeletal muscles, the waste product is lactic acid. This type of fermentation is called lactic acid fermentation. In strenuous exercise, when energy demands exceed energy supply, the respiratory chain cannot process all of the hydrogen atoms joined by NADH. During anaerobic glycolysis, NAD+ regenerates when pairs of hydrogen combine with pyruvate to form lactate. Lactate formation is catalyzed by lactate dehydrogenase in a reversible reaction. Lactate can also be used as an indirect precursor for liver glycogen. During recovery, when oxygen becomes available, NAD+ attaches to hydrogen from lactate to form ATP. In yeast, the waste products are ethanol and carbon dioxide. This type of fermentation is known as alcoholic or ethanol fermentation. The ATP generated in this process is made by substrate-level phosphorylation, which does not require oxygen. Photosynthesis Photosynthesis is a process used by plants and other organisms to convert light energy into chemical energy that can later be released to fuel the organism's metabolic activities via cellular respiration. This chemical energy is stored in carbohydrate molecules, such as sugars, which are synthesized from carbon dioxide and water. In most cases, oxygen is released as a waste product. Most plants, algae, and cyanobacteria perform photosynthesis, which is largely responsible for producing and maintaining the oxygen content of the Earth's atmosphere, and supplies most of the energy necessary for life on Earth. Photosynthesis has four stages: Light absorption, electron transport, ATP synthesis, and carbon fixation. Light absorption is the initial step of photosynthesis whereby light energy is absorbed by chlorophyll pigments attached to proteins in the thylakoid membranes. The absorbed light energy is used to remove electrons from a donor (water) to a primary electron acceptor, a quinone designated as Q. In the second stage, electrons move from the quinone primary electron acceptor through a series of electron carriers until they reach a final electron acceptor, which is usually the oxidized form of NADP+, which is reduced to NADPH, a process that takes place in a protein complex called photosystem I (PSI). The transport of electrons is coupled to the movement of protons (or hydrogen) from the stroma to the thylakoid membrane, which forms a pH gradient across the membrane as hydrogen becomes more concentrated in the lumen than in the stroma. This is analogous to the proton-motive force generated across the inner mitochondrial membrane in aerobic respiration. During the third stage of photosynthesis, the movement of protons down their concentration gradients from the thylakoid lumen to the stroma through the ATP synthase is coupled to the synthesis of ATP by that same ATP synthase. The NADPH and ATPs generated by the light-dependent reactions in the second and third stages, respectively, provide the energy and electrons to drive the synthesis of glucose by fixing atmospheric carbon dioxide into existing organic carbon compounds, such as ribulose bisphosphate (RuBP) in a sequence of light-independent (or dark) reactions called the Calvin cycle. Cell signaling Cell signaling (or communication) is the ability of cells to receive, process, and transmit signals with its environment and with itself. Signals can be non-chemical such as light, electrical impulses, and heat, or chemical signals (or ligands) that interact with receptors, which can be found embedded in the cell membrane of another cell or located deep inside a cell. There are generally four types of chemical signals: autocrine, paracrine, juxtacrine, and hormones. In autocrine signaling, the ligand affects the same cell that releases it. Tumor cells, for example, can reproduce uncontrollably because they release signals that initiate their own self-division. In paracrine signaling, the ligand diffuses to nearby cells and affects them. For example, brain cells called neurons release ligands called neurotransmitters that diffuse across a synaptic cleft to bind with a receptor on an adjacent cell such as another neuron or muscle cell. In juxtacrine signaling, there is direct contact between the signaling and responding cells. Finally, hormones are ligands that travel through the circulatory systems of animals or vascular systems of plants to reach their target cells. Once a ligand binds with a receptor, it can influence the behavior of another cell, depending on the type of receptor. For instance, neurotransmitters that bind with an inotropic receptor can alter the excitability of a target cell. Other types of receptors include protein kinase receptors (e.g., receptor for the hormone insulin) and G protein-coupled receptors. Activation of G protein-coupled receptors can initiate second messenger cascades. The process by which a chemical or physical signal is transmitted through a cell as a series of molecular events is called signal transduction. Cell cycle The cell cycle is a series of events that take place in a cell that cause it to divide into two daughter cells. These events include the duplication of its DNA and some of its organelles, and the subsequent partitioning of its cytoplasm into two daughter cells in a process called cell division. In eukaryotes (i.e., animal, plant, fungal, and protist cells), there are two distinct types of cell division: mitosis and meiosis. Mitosis is part of the cell cycle, in which replicated chromosomes are separated into two new nuclei. Cell division gives rise to genetically identical cells in which the total number of chromosomes is maintained. In general, mitosis (division of the nucleus) is preceded by the S stage of interphase (during which the DNA is replicated) and is often followed by telophase and cytokinesis; which divides the cytoplasm, organelles and cell membrane of one cell into two new cells containing roughly equal shares of these cellular components. The different stages of mitosis all together define the mitotic phase of an animal cell cycle—the division of the mother cell into two genetically identical daughter cells. The cell cycle is a vital process by which a single-celled fertilized egg develops into a mature organism, as well as the process by which hair, skin, blood cells, and some internal organs are renewed. After cell division, each of the daughter cells begin the interphase of a new cycle. In contrast to mitosis, meiosis results in four haploid daughter cells by undergoing one round of DNA replication followed by two divisions. Homologous chromosomes are separated in the first division (meiosis I), and sister chromatids are separated in the second division (meiosis II). Both of these cell division cycles are used in the process of sexual reproduction at some point in their life cycle. Both are believed to be present in the last eukaryotic common ancestor. Prokaryotes (i.e., archaea and bacteria) can also undergo cell division (or binary fission). Unlike the processes of mitosis and meiosis in eukaryotes, binary fission in prokaryotes takes place without the formation of a spindle apparatus on the cell. Before binary fission, DNA in the bacterium is tightly coiled. After it has uncoiled and duplicated, it is pulled to the separate poles of the bacterium as it increases the size to prepare for splitting. Growth of a new cell wall begins to separate the bacterium (triggered by FtsZ polymerization and "Z-ring" formation). The new cell wall (septum) fully develops, resulting in the complete split of the bacterium. The new daughter cells have tightly coiled DNA rods, ribosomes, and plasmids. Sexual reproduction and meiosis Meiosis is a central feature of sexual reproduction in eukaryotes, and the most fundamental function of meiosis appears to be conservation of the integrity of the genome that is passed on to progeny by parents. Two aspects of sexual reproduction, meiotic recombination and outcrossing, are likely maintained respectively by the adaptive advantages of recombinational repair of genomic DNA damage and genetic complementation which masks the expression of deleterious recessive mutations. The beneficial effect of genetic complementation, derived from outcrossing (cross-fertilization) is also referred to as hybrid vigor or heterosis. Charles Darwin in his 1878 book The Effects of Cross and Self-Fertilization in the Vegetable Kingdom at the start of chapter XII noted "The first and most important of the conclusions which may be drawn from the observations given in this volume, is that generally cross-fertilisation is beneficial and self-fertilisation often injurious, at least with the plants on which I experimented." Genetic variation, often produced as a byproduct of sexual reproduction, may provide long-term advantages to those sexual lineages that engage in outcrossing. Genetics Inheritance Genetics is the scientific study of inheritance. Mendelian inheritance, specifically, is the process by which genes and traits are passed on from parents to offspring. It has several principles. The first is that genetic characteristics, alleles, are discrete and have alternate forms (e.g., purple vs. white or tall vs. dwarf), each inherited from one of two parents. Based on the law of dominance and uniformity, which states that some alleles are dominant while others are recessive; an organism with at least one dominant allele will display the phenotype of that dominant allele. During gamete formation, the alleles for each gene segregate, so that each gamete carries only one allele for each gene. Heterozygotic individuals produce gametes with an equal frequency of two alleles. Finally, the law of independent assortment, states that genes of different traits can segregate independently during the formation of gametes, i.e., genes are unlinked. An exception to this rule would include traits that are sex-linked. Test crosses can be performed to experimentally determine the underlying genotype of an organism with a dominant phenotype. A Punnett square can be used to predict the results of a test cross. The chromosome theory of inheritance, which states that genes are found on chromosomes, was supported by Thomas Morgans's experiments with fruit flies, which established the sex linkage between eye color and sex in these insects. Genes and DNA A gene is a unit of heredity that corresponds to a region of deoxyribonucleic acid (DNA) that carries genetic information that controls form or function of an organism. DNA is composed of two polynucleotide chains that coil around each other to form a double helix. It is found as linear chromosomes in eukaryotes, and circular chromosomes in prokaryotes. The set of chromosomes in a cell is collectively known as its genome. In eukaryotes, DNA is mainly in the cell nucleus. In prokaryotes, the DNA is held within the nucleoid. The genetic information is held within genes, and the complete assemblage in an organism is called its genotype. DNA replication is a semiconservative process whereby each strand serves as a template for a new strand of DNA. Mutations are heritable changes in DNA. They can arise spontaneously as a result of replication errors that were not corrected by proofreading or can be induced by an environmental mutagen such as a chemical (e.g., nitrous acid, benzopyrene) or radiation (e.g., x-ray, gamma ray, ultraviolet radiation, particles emitted by unstable isotopes). Mutations can lead to phenotypic effects such as loss-of-function, gain-of-function, and conditional mutations. Some mutations are beneficial, as they are a source of genetic variation for evolution. Others are harmful if they were to result in a loss of function of genes needed for survival. Gene expression Gene expression is the molecular process by which a genotype encoded in DNA gives rise to an observable phenotype in the proteins of an organism's body. This process is summarized by the central dogma of molecular biology, which was formulated by Francis Crick in 1958. According to the Central Dogma, genetic information flows from DNA to RNA to protein. There are two gene expression processes: transcription (DNA to RNA) and translation (RNA to protein). Gene regulation The regulation of gene expression by environmental factors and during different stages of development can occur at each step of the process such as transcription, RNA splicing, translation, and post-translational modification of a protein. Gene expression can be influenced by positive or negative regulation, depending on which of the two types of regulatory proteins called transcription factors bind to the DNA sequence close to or at a promoter. A cluster of genes that share the same promoter is called an operon, found mainly in prokaryotes and some lower eukaryotes (e.g., Caenorhabditis elegans). In positive regulation of gene expression, the activator is the transcription factor that stimulates transcription when it binds to the sequence near or at the promoter. Negative regulation occurs when another transcription factor called a repressor binds to a DNA sequence called an operator, which is part of an operon, to prevent transcription. Repressors can be inhibited by compounds called inducers (e.g., allolactose), thereby allowing transcription to occur. Specific genes that can be activated by inducers are called inducible genes, in contrast to constitutive genes that are almost constantly active. In contrast to both, structural genes encode proteins that are not involved in gene regulation. In addition to regulatory events involving the promoter, gene expression can also be regulated by epigenetic changes to chromatin, which is a complex of DNA and protein found in eukaryotic cells. Genes, development, and evolution Development is the process by which a multicellular organism (plant or animal) goes through a series of changes, starting from a single cell, and taking on various forms that are characteristic of its life cycle. There are four key processes that underlie development: Determination, differentiation, morphogenesis, and growth. Determination sets the developmental fate of a cell, which becomes more restrictive during development. Differentiation is the process by which specialized cells arise from less specialized cells such as stem cells. Stem cells are undifferentiated or partially differentiated cells that can differentiate into various types of cells and proliferate indefinitely to produce more of the same stem cell. Cellular differentiation dramatically changes a cell's size, shape, membrane potential, metabolic activity, and responsiveness to signals, which are largely due to highly controlled modifications in gene expression and epigenetics. With a few exceptions, cellular differentiation almost never involves a change in the DNA sequence itself. Thus, different cells can have very different physical characteristics despite having the same genome. Morphogenesis, or the development of body form, is the result of spatial differences in gene expression. A small fraction of the genes in an organism's genome called the developmental-genetic toolkit control the development of that organism. These toolkit genes are highly conserved among phyla, meaning that they are ancient and very similar in widely separated groups of animals. Differences in deployment of toolkit genes affect the body plan and the number, identity, and pattern of body parts. Among the most important toolkit genes are the Hox genes. Hox genes determine where repeating parts, such as the many vertebrae of snakes, will grow in a developing embryo or larva. Evolution Evolutionary processes Evolution is a central organizing concept in biology. It is the change in heritable characteristics of populations over successive generations. In artificial selection, animals were selectively bred for specific traits. Given that traits are inherited, populations contain a varied mix of traits, and reproduction is able to increase any population, Darwin argued that in the natural world, it was nature that played the role of humans in selecting for specific traits. Darwin inferred that individuals who possessed heritable traits better adapted to their environments are more likely to survive and produce more offspring than other individuals. He further inferred that this would lead to the accumulation of favorable traits over successive generations, thereby increasing the match between the organisms and their environment. Speciation A species is a group of organisms that mate with one another and speciation is the process by which one lineage splits into two lineages as a result of having evolved independently from each other. For speciation to occur, there has to be reproductive isolation. Reproductive isolation can result from incompatibilities between genes as described by Bateson–Dobzhansky–Muller model. Reproductive isolation also tends to increase with genetic divergence. Speciation can occur when there are physical barriers that divide an ancestral species, a process known as allopatric speciation. Phylogeny A phylogeny is an evolutionary history of a specific group of organisms or their genes. It can be represented using a phylogenetic tree, a diagram showing lines of descent among organisms or their genes. Each line drawn on the time axis of a tree represents a lineage of descendants of a particular species or population. When a lineage divides into two, it is represented as a fork or split on the phylogenetic tree. Phylogenetic trees are the basis for comparing and grouping different species. Different species that share a feature inherited from a common ancestor are described as having homologous features (or synapomorphy). Phylogeny provides the basis of biological classification. This classification system is rank-based, with the highest rank being the domain followed by kingdom, phylum, class, order, family, genus, and species. All organisms can be classified as belonging to one of three domains: Archaea (originally Archaebacteria), Bacteria (originally eubacteria), or Eukarya (includes the fungi, plant, and animal kingdoms). History of life The history of life on Earth traces how organisms have evolved from the earliest emergence of life to present day. Earth formed about 4.5 billion years ago and all life on Earth, both living and extinct, descended from a last universal common ancestor that lived about 3.5 billion years ago. Geologists have developed a geologic time scale that divides the history of the Earth into major divisions, starting with four eons (Hadean, Archean, Proterozoic, and Phanerozoic), the first three of which are collectively known as the Precambrian, which lasted approximately 4 billion years. Each eon can be divided into eras, with the Phanerozoic eon that began 539 million years ago being subdivided into Paleozoic, Mesozoic, and Cenozoic eras. These three eras together comprise eleven periods (Cambrian, Ordovician, Silurian, Devonian, Carboniferous, Permian, Triassic, Jurassic, Cretaceous, Tertiary, and Quaternary). The similarities among all known present-day species indicate that they have diverged through the process of evolution from their common ancestor. Biologists regard the ubiquity of the genetic code as evidence of universal common descent for all bacteria, archaea, and eukaryotes. Microbial mats of coexisting bacteria and archaea were the dominant form of life in the early Archean eon and many of the major steps in early evolution are thought to have taken place in this environment. The earliest evidence of eukaryotes dates from 1.85 billion years ago, and while they may have been present earlier, their diversification accelerated when they started using oxygen in their metabolism. Later, around 1.7 billion years ago, multicellular organisms began to appear, with differentiated cells performing specialised functions. Algae-like multicellular land plants are dated back to about 1 billion years ago, although evidence suggests that microorganisms formed the earliest terrestrial ecosystems, at least 2.7 billion years ago. Microorganisms are thought to have paved the way for the inception of land plants in the Ordovician period. Land plants were so successful that they are thought to have contributed to the Late Devonian extinction event. Ediacara biota appear during the Ediacaran period, while vertebrates, along with most other modern phyla originated about 525 million years ago during the Cambrian explosion. During the Permian period, synapsids, including the ancestors of mammals, dominated the land, but most of this group became extinct in the Permian–Triassic extinction event 252 million years ago. During the recovery from this catastrophe, archosaurs became the most abundant land vertebrates; one archosaur group, the dinosaurs, dominated the Jurassic and Cretaceous periods. After the Cretaceous–Paleogene extinction event 66 million years ago killed off the non-avian dinosaurs, mammals increased rapidly in size and diversity. Such mass extinctions may have accelerated evolution by providing opportunities for new groups of organisms to diversify. Diversity Bacteria and Archaea Bacteria are a type of cell that constitute a large domain of prokaryotic microorganisms. Typically a few micrometers in length, bacteria have a number of shapes, ranging from spheres to rods and spirals. Bacteria were among the first life forms to appear on Earth, and are present in most of its habitats. Bacteria inhabit soil, water, acidic hot springs, radioactive waste, and the deep biosphere of the Earth's crust. Bacteria also live in symbiotic and parasitic relationships with plants and animals. Most bacteria have not been characterised, and only about 27 percent of the bacterial phyla have species that can be grown in the laboratory. Archaea constitute the other domain of prokaryotic cells and were initially classified as bacteria, receiving the name archaebacteria (in the Archaebacteria kingdom), a term that has fallen out of use. Archaeal cells have unique properties separating them from the other two domains, Bacteria and Eukaryota. Archaea are further divided into multiple recognized phyla. Archaea and bacteria are generally similar in size and shape, although a few archaea have very different shapes, such as the flat and square cells of Haloquadratum walsbyi. Despite this morphological similarity to bacteria, archaea possess genes and several metabolic pathways that are more closely related to those of eukaryotes, notably for the enzymes involved in transcription and translation. Other aspects of archaeal biochemistry are unique, such as their reliance on ether lipids in their cell membranes, including archaeols. Archaea use more energy sources than eukaryotes: these range from organic compounds, such as sugars, to ammonia, metal ions or even hydrogen gas. Salt-tolerant archaea (the Haloarchaea) use sunlight as an energy source, and other species of archaea fix carbon, but unlike plants and cyanobacteria, no known species of archaea does both. Archaea reproduce asexually by binary fission, fragmentation, or budding; unlike bacteria, no known species of Archaea form endospores. The first observed archaea were extremophiles, living in extreme environments, such as hot springs and salt lakes with no other organisms. Improved molecular detection tools led to the discovery of archaea in almost every habitat, including soil, oceans, and marshlands. Archaea are particularly numerous in the oceans, and the archaea in plankton may be one of the most abundant groups of organisms on the planet. Archaea are a major part of Earth's life. They are part of the microbiota of all organisms. In the human microbiome, they are important in the gut, mouth, and on the skin. Their morphological, metabolic, and geographical diversity permits them to play multiple ecological roles: carbon fixation; nitrogen cycling; organic compound turnover; and maintaining microbial symbiotic and syntrophic communities, for example. Eukaryotes Eukaryotes are hypothesized to have split from archaea, which was followed by their endosymbioses with bacteria (or symbiogenesis) that gave rise to mitochondria and chloroplasts, both of which are now part of modern-day eukaryotic cells. The major lineages of eukaryotes diversified in the Precambrian about 1.5 billion years ago and can be classified into eight major clades: alveolates, excavates, stramenopiles, plants, rhizarians, amoebozoans, fungi, and animals. Five of these clades are collectively known as protists, which are mostly microscopic eukaryotic organisms that are not plants, fungi, or animals. While it is likely that protists share a common ancestor (the last eukaryotic common ancestor), protists by themselves do not constitute a separate clade as some protists may be more closely related to plants, fungi, or animals than they are to other protists. Like groupings such as algae, invertebrates, or protozoans, the protist grouping is not a formal taxonomic group but is used for convenience. Most protists are unicellular; these are called microbial eukaryotes. Plants are mainly multicellular organisms, predominantly photosynthetic eukaryotes of the kingdom Plantae, which would exclude fungi and some algae. Plant cells were derived by endosymbiosis of a cyanobacterium into an early eukaryote about one billion years ago, which gave rise to chloroplasts. The first several clades that emerged following primary endosymbiosis were aquatic and most of the aquatic photosynthetic eukaryotic organisms are collectively described as algae, which is a term of convenience as not all algae are closely related. Algae comprise several distinct clades such as glaucophytes, which are microscopic freshwater algae that may have resembled in form to the early unicellular ancestor of Plantae. Unlike glaucophytes, the other algal clades such as red and green algae are multicellular. Green algae comprise three major clades: chlorophytes, coleochaetophytes, and stoneworts. Fungi are eukaryotes that digest foods outside their bodies, secreting digestive enzymes that break down large food molecules before absorbing them through their cell membranes. Many fungi are also saprobes, feeding on dead organic matter, making them important decomposers in ecological systems. Animals are multicellular eukaryotes. With few exceptions, animals consume organic material, breathe oxygen, are able to move, can reproduce sexually, and grow from a hollow sphere of cells, the blastula, during embryonic development. Over 1.5 million living animal species have been described—of which around 1 million are insects—but it has been estimated there are over 7 million animal species in total. They have complex interactions with each other and their environments, forming intricate food webs. Viruses Viruses are submicroscopic infectious agents that replicate inside the cells of organisms. Viruses infect all types of life forms, from animals and plants to microorganisms, including bacteria and archaea. More than 6,000 virus species have been described in detail. Viruses are found in almost every ecosystem on Earth and are the most numerous type of biological entity. The origins of viruses in the evolutionary history of life are unclear: some may have evolved from plasmids—pieces of DNA that can move between cells—while others may have evolved from bacteria. In evolution, viruses are an important means of horizontal gene transfer, which increases genetic diversity in a way analogous to sexual reproduction. Because viruses possess some but not all characteristics of life, they have been described as "organisms at the edge of life", and as self-replicators. Ecology Ecology is the study of the distribution and abundance of life, the interaction between organisms and their environment. Ecosystems The community of living (biotic) organisms in conjunction with the nonliving (abiotic) components (e.g., water, light, radiation, temperature, humidity, atmosphere, acidity, and soil) of their environment is called an ecosystem. These biotic and abiotic components are linked together through nutrient cycles and energy flows. Energy from the sun enters the system through photosynthesis and is incorporated into plant tissue. By feeding on plants and on one another, animals move matter and energy through the system. They also influence the quantity of plant and microbial biomass present. By breaking down dead organic matter, decomposers release carbon back to the atmosphere and facilitate nutrient cycling by converting nutrients stored in dead biomass back to a form that can be readily used by plants and other microbes. Populations A population is the group of organisms of the same species that occupies an area and reproduce from generation to generation. Population size can be estimated by multiplying population density by the area or volume. The carrying capacity of an environment is the maximum population size of a species that can be sustained by that specific environment, given the food, habitat, water, and other resources that are available. The carrying capacity of a population can be affected by changing environmental conditions such as changes in the availability of resources and the cost of maintaining them. In human populations, new technologies such as the Green revolution have helped increase the Earth's carrying capacity for humans over time, which has stymied the attempted predictions of impending population decline, the most famous of which was by Thomas Malthus in the 18th century. Communities A community is a group of populations of species occupying the same geographical area at the same time. A biological interaction is the effect that a pair of organisms living together in a community have on each other. They can be either of the same species (intraspecific interactions), or of different species (interspecific interactions). These effects may be short-term, like pollination and predation, or long-term; both often strongly influence the evolution of the species involved. A long-term interaction is called a symbiosis. Symbioses range from mutualism, beneficial to both partners, to competition, harmful to both partners. Every species participates as a consumer, resource, or both in consumer–resource interactions, which form the core of food chains or food webs. There are different trophic levels within any food web, with the lowest level being the primary producers (or autotrophs) such as plants and algae that convert energy and inorganic material into organic compounds, which can then be used by the rest of the community. At the next level are the heterotrophs, which are the species that obtain energy by breaking apart organic compounds from other organisms. Heterotrophs that consume plants are primary consumers (or herbivores) whereas heterotrophs that consume herbivores are secondary consumers (or carnivores). And those that eat secondary consumers are tertiary consumers and so on. Omnivorous heterotrophs are able to consume at multiple levels. Finally, there are decomposers that feed on the waste products or dead bodies of organisms. On average, the total amount of energy incorporated into the biomass of a trophic level per unit of time is about one-tenth of the energy of the trophic level that it consumes. Waste and dead material used by decomposers as well as heat lost from metabolism make up the other ninety percent of energy that is not consumed by the next trophic level. Biosphere In the global ecosystem or biosphere, matter exists as different interacting compartments, which can be biotic or abiotic as well as accessible or inaccessible, depending on their forms and locations. For example, matter from terrestrial autotrophs are both biotic and accessible to other organisms whereas the matter in rocks and minerals are abiotic and inaccessible. A biogeochemical cycle is a pathway by which specific elements of matter are turned over or moved through the biotic (biosphere) and the abiotic (lithosphere, atmosphere, and hydrosphere) compartments of Earth. There are biogeochemical cycles for nitrogen, carbon, and water. Conservation Conservation biology is the study of the conservation of Earth's biodiversity with the aim of protecting species, their habitats, and ecosystems from excessive rates of extinction and the erosion of biotic interactions. It is concerned with factors that influence the maintenance, loss, and restoration of biodiversity and the science of sustaining evolutionary processes that engender genetic, population, species, and ecosystem diversity. The concern stems from estimates suggesting that up to 50% of all species on the planet will disappear within the next 50 years, which has contributed to poverty, starvation, and will reset the course of evolution on this planet. Biodiversity affects the functioning of ecosystems, which provide a variety of services upon which people depend. Conservation biologists research and educate on the trends of biodiversity loss, species extinctions, and the negative effect these are having on our capabilities to sustain the well-being of human society. Organizations and citizens are responding to the current biodiversity crisis through conservation action plans that direct research, monitoring, and education programs that engage concerns at local through global scales. See also References Further reading External links OSU's Phylocode Biology Online – Wiki Dictionary MIT video lecture series on biology OneZoom Tree of Life Journal of the History of Biology (springer.com) Journal links PLOS ONE PLOS Biology A peer-reviewed, open-access journal published by the Public Library of Science Current Biology: General journal publishing original research from all areas of biology Biology Letters: A high-impact Royal Society journal publishing peer-reviewed biology papers of general interest Science: Internationally renowned AAAS science journal – see sections of the life sciences International Journal of Biological Sciences: A biological journal publishing significant peer-reviewed scientific papers Perspectives in Biology and Medicine: An interdisciplinary scholarly journal publishing essays of broad relevance

Physics is the scientific study of matter, its fundamental constituents, its motion and behavior through space and time, and the related entities of energy and force. It is one of the most fundamental scientific disciplines. A scientist who specializes in the field of physics is called a physicist. Physics is one of the oldest academic disciplines. Over much of the past two millennia, physics, chemistry, biology, and certain branches of mathematics were a part of natural philosophy, but during the Scientific Revolution in the 17th century, these natural sciences branched into separate research endeavors. Physics intersects with many interdisciplinary areas of research, such as biophysics and quantum chemistry, and the boundaries of physics are not rigidly defined. New ideas in physics often explain the fundamental mechanisms studied by other sciences and suggest new avenues of research in these and other academic disciplines such as mathematics and philosophy. Advances in physics often enable new technologies. For example, advances in the understanding of electromagnetism, solid-state physics, and nuclear physics led directly to the development of technologies that have transformed modern society, such as television, computers, domestic appliances, and nuclear weapons; advances in thermodynamics led to the development of industrialization; and advances in mechanics inspired the development of calculus. History The word physics comes from the Latin physica ('study of nature'), which itself is a borrowing of the Greek φυσική (phusikḗ 'natural science'), a term derived from φύσις (phúsis 'origin, nature, property'). Ancient astronomy Astronomy is one of the oldest natural sciences. Early civilizations dating before 3000 BCE, such as the Sumerians, ancient Egyptians, and the Indus Valley Civilization, had a predictive knowledge and a basic awareness of the motions of the Sun, Moon, and stars. The stars and planets, believed to represent gods, were often worshipped. While the explanations for the observed positions of the stars were often unscientific and lacking in evidence, these early observations laid the foundation for later astronomy, as the stars were found to traverse great circles across the sky, which could not explain the positions of the planets. According to Asger Aaboe, the origins of Western astronomy can be found in Mesopotamia, and all Western efforts in the exact sciences are descended from late Babylonian astronomy. Egyptian astronomers left monuments showing knowledge of the constellations and the motions of the celestial bodies, while Greek poet Homer wrote of various celestial objects in his Iliad and Odyssey; later Greek astronomers provided names, which are still used today, for most constellations visible from the Northern Hemisphere. Natural philosophy Natural philosophy has its origins in Greece during the Archaic period (650 BCE – 480 BCE), when pre-Socratic philosophers like Thales rejected non-naturalistic explanations for natural phenomena and proclaimed that every event had a natural cause. They proposed ideas verified by reason and observation, and many of their hypotheses proved successful in experiment; for example, atomism was found to be correct approximately 2000 years after it was proposed by Leucippus and his pupil Democritus. Aristotle and Hellenistic physics During the classical period in Greece (6th, 5th and 4th centuries BCE) and in Hellenistic times, natural philosophy developed along many lines of inquiry. Aristotle (Greek: Ἀριστοτέλης, Aristotélēs) (384–322 BCE), a student of Plato, wrote on many subjects, including a substantial treatise on "Physics" – in the 4th century BC. Aristotelian physics was influential for about two millennia. His approach mixed some limited observation with logical deductive arguments, but did not rely on experimental verification of deduced statements. Aristotle's foundational work in Physics, though very imperfect, formed a framework against which later thinkers further developed the field. His approach is entirely superseded today. He explained ideas such as motion (and gravity) with the theory of four elements. Aristotle believed that each of the four classical elements (air, fire, water, earth) had its own natural place. Because of their differing densities, each element will revert to its own specific place in the atmosphere. So, because of their weights, fire would be at the top, air underneath fire, then water, then lastly earth. He also stated that when a small amount of one element enters the natural place of another, the less abundant element will automatically go towards its own natural place. For example, if there is a fire on the ground, the flames go up into the air in an attempt to go back into its natural place where it belongs. His laws of motion included: that heavier objects will fall faster, the speed being proportional to the weight and the speed of the object that is falling depends inversely on the density object it is falling through (e.g. density of air). He also stated that, when it comes to violent motion (motion of an object when a force is applied to it by a second object) that the speed that object moves, will only be as fast or strong as the measure of force applied to it. The problem of motion and its causes was studied carefully, leading to the philosophical notion of a "prime mover" as the ultimate source of all motion in the world (Book 8 of his treatise Physics). Medieval European and Islamic The Western Roman Empire fell to invaders and internal decay in the fifth century, resulting in a decline in intellectual pursuits in western Europe. By contrast, the Eastern Roman Empire (usually known as the Byzantine Empire) resisted the attacks from invaders and continued to advance various fields of learning, including physics. In the sixth century, John Philoponus challenged the dominant Aristotelian approach to science although much of his work was focused on Christian theology. In the sixth century, Isidore of Miletus created an important compilation of Archimedes' works that are copied in the Archimedes Palimpsest. Islamic scholarship inherited Aristotelian physics from the Greeks and during the Islamic Golden Age developed it further, especially placing emphasis on observation and a priori reasoning, developing early forms of the scientific method. The most notable innovations under Islamic scholarship were in the field of optics and vision, which came from the works of many scientists like Ibn Sahl, Al-Kindi, Ibn al-Haytham, Al-Farisi and Avicenna. The most notable work was The Book of Optics (also known as Kitāb al-Manāẓir), written by Ibn al-Haytham, in which he presented the alternative to the ancient Greek idea about vision. His discussed his experiments with camera obscura, showing that light moved in a straight line; he encouraged readers to reproduce his experiments making him one of the originators of the scientific method Scientific Revolution Physics became a separate science when early modern Europeans used experimental and quantitative methods to discover what are now considered to be the laws of physics. Major developments in this period include the replacement of the geocentric model of the Solar System with the heliocentric Copernican model, the laws governing the motion of planetary bodies (determined by Johannes Kepler between 1609 and 1619), Galileo's pioneering work on telescopes and observational astronomy in the 16th and 17th centuries, and Isaac Newton's discovery and unification of the laws of motion and universal gravitation (that would come to bear his name). Newton, and separately Gottfried Wilhelm Leibniz, developed calculus, the mathematical study of continuous change, and Newton applied it to solve physical problems. 19th century The discovery of laws in thermodynamics, chemistry, and electromagnetics resulted from research efforts during the Industrial Revolution as energy needs increased. By the end of the 19th century, theories of thermodynamics, mechanics, and electromagnetics matched a wide variety of observations. Taken together these theories became the basis for what would later be called classical physics. A few experimental results remained inexplicable. Classical electromagnetism presumed a medium, an luminiferous aether to support the propagation of waves, but this medium could not be detected. The intensity of light from hot glowing blackbody objects did not match the predictions of thermodynamics and electromagnetism. The character of electron emission of illuminated metals differed from predictions. These failures, seemingly insignificant in the big picture would upset the physics world in first two decades of the 20th century. 20th century Modern physics began in the early 20th century with the work of Max Planck in quantum theory and Albert Einstein's theory of relativity. Both of these theories came about due to inaccuracies in classical mechanics in certain situations. Classical mechanics predicted that the speed of light depends on the motion of the observer, which could not be resolved with the constant speed predicted by Maxwell's equations of electromagnetism. This discrepancy was corrected by Einstein's theory of special relativity, which replaced classical mechanics for fast-moving bodies and allowed for a constant speed of light. Black-body radiation provided another problem for classical physics, which was corrected when Planck proposed that the excitation of material oscillators is possible only in discrete steps proportional to their frequency. This, along with the photoelectric effect and a complete theory predicting discrete energy levels of electron orbitals, led to the theory of quantum mechanics improving on classical physics at very small scales. Quantum mechanics would come to be pioneered by Werner Heisenberg, Erwin Schrödinger and Paul Dirac. From this early work, and work in related fields, the Standard Model of particle physics was derived. Following the discovery of a particle with properties consistent with the Higgs boson at CERN in 2012, all fundamental particles predicted by the standard model, and no others, appear to exist; however, physics beyond the Standard Model, with theories such as supersymmetry, is an active area of research. Areas of mathematics in general are important to this field, such as the study of probabilities and groups. Core theories Physics deals with a wide variety of systems, although certain theories are used by all physicists. Each of these theories was experimentally tested numerous times and found to be an adequate approximation of nature. These central theories are important tools for research into more specialized topics, and any physicist, regardless of their specialization, is expected to be literate in them. These include classical mechanics, quantum mechanics, thermodynamics and statistical mechanics, electromagnetism, and special relativity. Distinction between classical and modern physics In the first decades of the 20th century physics was revolutionized by the discoveries of quantum mechanics and relativity. The changes were so fundamental that these new concepts became the foundation of "modern physics", with other topics becoming "classical physics". The majority of applications of physics are essentially classical. The laws of classical physics accurately describe systems whose important length scales are greater than the atomic scale and whose motions are much slower than the speed of light. Outside of this domain, observations do not match predictions provided by classical mechanics. Classical theory Classical physics includes the traditional branches and topics that were recognized and well-developed before the beginning of the 20th century—classical mechanics, thermodynamics, and electromagnetism. Classical mechanics is concerned with bodies acted on by forces and bodies in motion and may be divided into statics (study of the forces on a body or bodies not subject to an acceleration), kinematics (study of motion without regard to its causes), and dynamics (study of motion and the forces that affect it); mechanics may also be divided into solid mechanics and fluid mechanics (known together as continuum mechanics), the latter include such branches as hydrostatics, hydrodynamics and pneumatics. Acoustics is the study of how sound is produced, controlled, transmitted and received. Important modern branches of acoustics include ultrasonics, the study of sound waves of very high frequency beyond the range of human hearing; bioacoustics, the physics of animal calls and hearing, and electroacoustics, the manipulation of audible sound waves using electronics. Optics, the study of light, is concerned not only with visible light but also with infrared and ultraviolet radiation, which exhibit all of the phenomena of visible light except visibility, e.g., reflection, refraction, interference, diffraction, dispersion, and polarization of light. Heat is a form of energy, the internal energy possessed by the particles of which a substance is composed; thermodynamics deals with the relationships between heat and other forms of energy. Electricity and magnetism have been studied as a single branch of physics since the intimate connection between them was discovered in the early 19th century; an electric current gives rise to a magnetic field, and a changing magnetic field induces an electric current. Electrostatics deals with electric charges at rest, electrodynamics with moving charges, and magnetostatics with magnetic poles at rest. Modern theory The discovery of relativity and of quantum mechanics in the first decades of the 20th century transformed the conceptual basis of physics without reducing the practical value of most of the physical theories developed up to that time. Consequently the topics of physics have come to be divided into "classical physics" and "modern physics", with the latter category including effects related to quantum mechanics and relativity. Classical physics is generally concerned with matter and energy on the normal scale of observation, while much of modern physics is concerned with the behavior of matter and energy under extreme conditions or on a very large or very small scale. For example, atomic and nuclear physics study matter on the smallest scale at which chemical elements can be identified. The physics of elementary particles is on an even smaller scale since it is concerned with the most basic units of matter; this branch of physics is also known as high-energy physics because of the extremely high energies necessary to produce many types of particles in particle accelerators. On this scale, ordinary, commonsensical notions of space, time, matter, and energy are no longer valid. The two chief theories of modern physics present a different picture of the concepts of space, time, and matter from that presented by classical physics. Classical mechanics approximates nature as continuous, while quantum theory is concerned with the discrete nature of many phenomena at the atomic and subatomic level and with the complementary aspects of particles and waves in the description of such phenomena. The theory of relativity is concerned with the description of phenomena that take place in a frame of reference that is in motion with respect to an observer; the special theory of relativity is concerned with motion in the absence of gravitational fields and the general theory of relativity with motion and its connection with gravitation. Both quantum theory and the theory of relativity find applications in many areas of modern physics. Fundamental concepts in modern physics include: Action Causality Covariance Particle Physical field Physical interaction Quantum Statistical ensemble Symmetry Wave Research Scientific method Physicists use the scientific method to test the validity of a physical theory. By using a methodical approach to compare the implications of a theory with the conclusions drawn from its related experiments and observations, physicists are better able to test the validity of a theory in a logical, unbiased, and repeatable way. To that end, experiments are performed and observations are made in order to determine the validity or invalidity of a theory. A scientific law is a concise verbal or mathematical statement of a relation that expresses a fundamental principle of some theory, such as Newton's law of universal gravitation. Theory and experiment Theorists seek to develop mathematical models that both agree with existing experiments and successfully predict future experimental results, while experimentalists devise and perform experiments to test theoretical predictions and explore new phenomena. Although theory and experiment are developed separately, they strongly affect and depend upon each other. Progress in physics frequently comes about when experimental results defy explanation by existing theories, prompting intense focus on applicable modeling, and when new theories generate experimentally testable predictions, which inspire the development of new experiments (and often related equipment). Physicists who work at the interplay of theory and experiment are called phenomenologists, who study complex phenomena observed in experiment and work to relate them to a fundamental theory. Theoretical physics has historically taken inspiration from philosophy; electromagnetism was unified this way. Beyond the known universe, the field of theoretical physics also deals with hypothetical issues, such as parallel universes, a multiverse, and higher dimensions. Theorists invoke these ideas in hopes of solving particular problems with existing theories; they then explore the consequences of these ideas and work toward making testable predictions. Experimental physics expands, and is expanded by, engineering and technology. Experimental physicists who are involved in basic research design and perform experiments with equipment such as particle accelerators and lasers, whereas those involved in applied research often work in industry, developing technologies such as magnetic resonance imaging (MRI) and transistors. Feynman has noted that experimentalists may seek areas that have not been explored well by theorists. Scope and aims Physics covers a wide range of phenomena, from elementary particles (such as quarks, neutrinos, and electrons) to the largest superclusters of galaxies. Included in these phenomena are the most basic objects composing all other things. Therefore, physics is sometimes called the "fundamental science". Physics aims to describe the various phenomena that occur in nature in terms of simpler phenomena. Thus, physics aims to both connect the things observable to humans to root causes, and then connect these causes together. For example, the ancient Chinese observed that certain rocks (lodestone and magnetite) were attracted to one another by an invisible force. This effect was later called magnetism, which was first rigorously studied in the 17th century. But even before the Chinese discovered magnetism, the ancient Greeks knew of other objects such as amber, that when rubbed with fur would cause a similar invisible attraction between the two. This was also first studied rigorously in the 17th century and came to be called electricity. Thus, physics had come to understand two observations of nature in terms of some root cause (electricity and magnetism). However, further work in the 19th century revealed that these two forces were just two different aspects of one force—electromagnetism. This process of "unifying" forces continues today, and electromagnetism and the weak nuclear force are now considered to be two aspects of the electroweak interaction. Physics hopes to find an ultimate reason (theory of everything) for why nature is as it is (see section Current research below for more information). Current research Research in physics is continually progressing on a large number of fronts. In condensed matter physics, an important unsolved theoretical problem is that of high-temperature superconductivity. Many condensed matter experiments are aiming to fabricate workable spintronics and quantum computers. In particle physics, the first pieces of experimental evidence for physics beyond the Standard Model have begun to appear. Foremost among these are indications that neutrinos have non-zero mass. These experimental results appear to have solved the long-standing solar neutrino problem, and the physics of massive neutrinos remains an area of active theoretical and experimental research. The Large Hadron Collider has already found the Higgs boson, but future research aims to prove or disprove the supersymmetry, which extends the Standard Model of particle physics. Research on the nature of the major mysteries of dark matter and dark energy is also currently ongoing. Although much progress has been made in high-energy, quantum, and astronomical physics, many everyday phenomena involving complexity, chaos, or turbulence are still poorly understood. Complex problems that seem like they could be solved by a clever application of dynamics and mechanics remain unsolved; examples include the formation of sandpiles, nodes in trickling water, the shape of water droplets, mechanisms of surface tension catastrophes, and self-sorting in shaken heterogeneous collections. These complex phenomena have received growing attention since the 1970s for several reasons, including the availability of modern mathematical methods and computers, which enabled complex systems to be modeled in new ways. Complex physics has become part of increasingly interdisciplinary research, as exemplified by the study of turbulence in aerodynamics and the observation of pattern formation in biological systems. In the 1932 Annual Review of Fluid Mechanics, Horace Lamb said: I am an old man now, and when I die and go to heaven there are two matters on which I hope for enlightenment. One is quantum electrodynamics, and the other is the turbulent motion of fluids. And about the former I am rather optimistic. Branches and fields Fields The major fields of physics, along with their subfields and the theories and concepts they employ, are shown in the following table. Since the 20th century, the individual fields of physics have become increasingly specialized, and today most physicists work in a single field for their entire careers. "Universalists" such as Einstein (1879–1955) and Lev Landau (1908–1968), who worked in multiple fields of physics, are now very rare. Contemporary research in physics can be broadly divided into nuclear and particle physics; condensed matter physics; atomic, molecular, and optical physics; astrophysics; and applied physics. Some physics departments also support physics education research and physics outreach. Nuclear and particle Particle physics is the study of the elementary constituents of matter and energy and the interactions between them. In addition, particle physicists design and develop the high-energy accelerators, detectors, and computer programs necessary for this research. The field is also called "high-energy physics" because many elementary particles do not occur naturally but are created only during high-energy collisions of other particles. Currently, the interactions of elementary particles and fields are described by the Standard Model. The model accounts for the 12 known particles of matter (quarks and leptons) that interact via the strong, weak, and electromagnetic fundamental forces. Dynamics are described in terms of matter particles exchanging gauge bosons (gluons, W and Z bosons, and photons, respectively). The Standard Model also predicts a particle known as the Higgs boson. In July 2012 CERN, the European laboratory for particle physics, announced the detection of a particle consistent with the Higgs boson, an integral part of the Higgs mechanism. Nuclear physics is the field of physics that studies the constituents and interactions of atomic nuclei. The most commonly known applications of nuclear physics are nuclear power generation and nuclear weapons technology, but the research has provided application in many fields, including those in nuclear medicine and magnetic resonance imaging, ion implantation in materials engineering, and radiocarbon dating in geology and archaeology. Atomic, molecular, and optical Atomic, molecular, and optical physics (AMO) is the study of matter—matter and light—matter interactions on the scale of single atoms and molecules. The three areas are grouped together because of their interrelationships, the similarity of methods used, and the commonality of their relevant energy scales. All three areas include both classical, semi-classical and quantum treatments; they can treat their subject from a microscopic view (in contrast to a macroscopic view). Atomic physics studies the electron shells of atoms. Current research focuses on activities in quantum control, cooling and trapping of atoms and ions, low-temperature collision dynamics and the effects of electron correlation on structure and dynamics. Atomic physics is influenced by the nucleus (see hyperfine splitting), but intra-nuclear phenomena such as fission and fusion are considered part of nuclear physics. Molecular physics focuses on multi-atomic structures and their internal and external interactions with matter and light. Optical physics is distinct from optics in that it tends to focus not on the control of classical light fields by macroscopic objects but on the fundamental properties of optical fields and their interactions with matter in the microscopic realm. Condensed matter Condensed matter physics is the field of physics that deals with the macroscopic physical properties of matter. In particular, it is concerned with the "condensed" phases that appear whenever the number of particles in a system is extremely large and the interactions between them are strong. The most familiar examples of condensed phases are solids and liquids, which arise from the bonding by way of the electromagnetic force between atoms. More exotic condensed phases include the superfluid and the Bose–Einstein condensate found in certain atomic systems at very low temperature, the superconducting phase exhibited by conduction electrons in certain materials, and the ferromagnetic and antiferromagnetic phases of spins on atomic lattices. Condensed matter physics is the largest field of contemporary physics. Historically, condensed matter physics grew out of solid-state physics, which is now considered one of its main subfields. The term condensed matter physics was apparently coined by Philip Anderson when he renamed his research group—previously solid-state theory—in 1967. In 1978, the Division of Solid State Physics of the American Physical Society was renamed as the Division of Condensed Matter Physics. Condensed matter physics has a large overlap with chemistry, materials science, nanotechnology and engineering. Astrophysics Astrophysics and astronomy are the application of the theories and methods of physics to the study of stellar structure, stellar evolution, the origin of the Solar System, and related problems of cosmology. Because astrophysics is a broad subject, astrophysicists typically apply many disciplines of physics, including mechanics, electromagnetism, statistical mechanics, thermodynamics, quantum mechanics, relativity, nuclear and particle physics, and atomic and molecular physics. The discovery by Karl Jansky in 1931 that radio signals were emitted by celestial bodies initiated the science of radio astronomy. Most recently, the frontiers of astronomy have been expanded by space exploration. Perturbations and interference from the Earth's atmosphere make space-based observations necessary for infrared, ultraviolet, gamma-ray, and X-ray astronomy. Physical cosmology is the study of the formation and evolution of the universe on its largest scales. Albert Einstein's theory of relativity plays a central role in all modern cosmological theories. In the early 20th century, Hubble's discovery that the universe is expanding, as shown by the Hubble diagram, prompted rival explanations known as the steady state universe and the Big Bang. The Big Bang was confirmed by the success of Big Bang nucleosynthesis and the discovery of the cosmic microwave background in 1964. The Big Bang model rests on two theoretical pillars: Albert Einstein's general relativity and the cosmological principle. Cosmologists have recently established the ΛCDM model of the evolution of the universe, which includes cosmic inflation, dark energy, and dark matter. Other aspects Education Careers Philosophy Physics, as with the rest of science, relies on the philosophy of science and its "scientific method" to advance knowledge of the physical world. The scientific method employs a priori and a posteriori reasoning as well as the use of Bayesian inference to measure the validity of a given theory. Study of the philosophical issues surrounding physics, the philosophy of physics, involves issues such as the nature of space and time, determinism, and metaphysical outlooks such as empiricism, naturalism, and realism. Many physicists have written about the philosophical implications of their work, for instance Laplace, who championed causal determinism, and Erwin Schrödinger, who wrote on quantum mechanics. The mathematical physicist Roger Penrose has been called a Platonist by Stephen Hawking, a view Penrose discusses in his book, The Road to Reality. Hawking referred to himself as an "unashamed reductionist" and took issue with Penrose's views. Mathematics provides a compact and exact language used to describe the order in nature. This was noted and advocated by Pythagoras, Plato, Galileo, and Newton. Some theorists, like Hilary Putnam and Penelope Maddy, hold that logical truths, and therefore mathematical reasoning, depend on the empirical world. This is usually combined with the claim that the laws of logic express universal regularities found in the structural features of the world, which may explain the peculiar relation between these fields. Physics uses mathematics to organize and formulate experimental results. From those results, precise or estimated solutions are obtained, or quantitative results, from which new predictions can be made and experimentally confirmed or negated. The results from physics experiments are numerical data, with their units of measure and estimates of the errors in the measurements. Technologies based on mathematics, like computation have made computational physics an active area of research. Ontology is a prerequisite for physics, but not for mathematics. It means physics is ultimately concerned with descriptions of the real world, while mathematics is concerned with abstract patterns, even beyond the real world. Thus physics statements are synthetic, while mathematical statements are analytic. Mathematics contains hypotheses, while physics contains theories. Mathematics statements have to be only logically true, while predictions of physics statements must match observed and experimental data. The distinction is clear-cut, but not always obvious. For example, mathematical physics is the application of mathematics in physics. Its methods are mathematical, but its subject is physical. The problems in this field start with a "mathematical model of a physical situation" (system) and a "mathematical description of a physical law" that will be applied to that system. Every mathematical statement used for solving has a hard-to-find physical meaning. The final mathematical solution has an easier-to-find meaning, because it is what the solver is looking for. Fundamental vs. applied physics Physics is a branch of fundamental science (also called basic science). Physics is also called "the fundamental science" because all branches of natural science including chemistry, astronomy, geology, and biology are constrained by laws of physics. Similarly, chemistry is often called the central science because of its role in linking the physical sciences. For example, chemistry studies properties, structures, and reactions of matter (chemistry's focus on the molecular and atomic scale distinguishes it from physics). Structures are formed because particles exert electrical forces on each other, properties include physical characteristics of given substances, and reactions are bound by laws of physics, like conservation of energy, mass, and charge. Fundamental physics seeks to better explain and understand phenomena in all spheres, without a specific practical application as a goal, other than the deeper insight into the phenomema themselves. Applied physics is a general term for physics research and development that is intended for a particular use. An applied physics curriculum usually contains a few classes in an applied discipline, like geology or electrical engineering. It usually differs from engineering in that an applied physicist may not be designing something in particular, but rather is using physics or conducting physics research with the aim of developing new technologies or solving a problem. The approach is similar to that of applied mathematics. Applied physicists use physics in scientific research. For instance, people working on accelerator physics might seek to build better particle detectors for research in theoretical physics. Physics is used heavily in engineering. For example, statics, a subfield of mechanics, is used in the building of bridges and other static structures. The understanding and use of acoustics results in sound control and better concert halls; similarly, the use of optics creates better optical devices. An understanding of physics makes for more realistic flight simulators, video games, and movies, and is often critical in forensic investigations. With the standard consensus that the laws of physics are universal and do not change with time, physics can be used to study things that would ordinarily be mired in uncertainty. For example, in the study of the origin of the Earth, a physicist can reasonably model Earth's mass, temperature, and rate of rotation, as a function of time allowing the extrapolation forward or backward in time and so predict future or prior events. It also allows for simulations in engineering that speed up the development of a new technology. There is also considerable interdisciplinarity, so many other important fields are influenced by physics (e.g., the fields of econophysics and sociophysics). See also Earth science – Fields of natural science related to Earth Neurophysics – Study of the nervous system with physics Psychophysics – Branch of knowledge relating physical stimuli and psychological perception Relationship between mathematics and physics Science tourism – Travel to notable science locations Lists List of important publications in physics List of physicists Lists of physics equations Notes References Sources External links Physics at Quanta Magazine Usenet Physics FAQ – FAQ compiled by sci.physics and other physics newsgroups Website of the Nobel Prize in physics Archived 7 December 2021 at the Wayback Machine – Award for outstanding contributions to the subject World of Physics Archived 25 June 2025 at the Wayback Machine – Online encyclopedic dictionary of physics Nature Physics – Academic journal Physics Archived 28 June 2025 at the Wayback Machine – Online magazine by the American Physical Society The Vega Science Trust Archived 7 June 2023 at the Wayback Machine – Science videos, including physics HyperPhysics website Archived 8 April 2011 at the Wayback Machine – Physics and astronomy mind-map from Georgia State University Physics at MIT OpenCourseWare Archived 15 March 2022 at the Wayback Machine – Online course material from Massachusetts Institute of Technology The Feynman Lectures on Physics Archived 4 March 2022 at the Wayback Machine

Chemistry is the scientific study of the properties and behavior of matter. It is a physical science within the natural sciences that studies the chemical elements that make up matter and compounds made of atoms, molecules and ions: their composition, structure, properties, behavior and the changes they undergo during reactions with other substances. Chemistry also addresses the nature of chemical bonds in chemical compounds. In the scope of its subject, chemistry occupies an intermediate position between physics and biology. It is sometimes called the central science because it provides a foundation for understanding both basic and applied scientific disciplines at a fundamental level. For example, chemistry explains aspects of plant growth (botany), the formation of igneous rocks (geology), how atmospheric ozone is formed and how environmental pollutants are degraded (ecology), the properties of the soil on the Moon (cosmochemistry), how medications work (pharmacology), and how to collect DNA evidence at a crime scene (forensics). Chemistry has existed under various names since ancient times. It has evolved, and now chemistry encompasses various areas of specialisation, or subdisciplines, that continue to increase in number and interrelate to create further interdisciplinary fields of study. The applications of various fields of chemistry are used frequently for economic purposes in the chemical industry. Etymology The word chemistry comes from a modification during the Renaissance of the word alchemy, which referred to an earlier set of practices that encompassed elements of chemistry, metallurgy, philosophy, astrology, astronomy, mysticism, and medicine. Alchemy is often associated with the quest to turn lead or other base metals into gold, though alchemists were also interested in many of the questions of modern chemistry. The modern word alchemy in turn is derived from the Arabic word al-kīmīā (الكیمیاء). This may have Egyptian origins since al-kīmīā is derived from the Ancient Greek χημία, which is in turn derived from the word Kemet, which is the ancient name of Egypt in the Egyptian language. Alternately, al-kīmīā may derive from χημεία 'cast together'. Modern principles The current model of atomic structure is the quantum mechanical model. Traditional chemistry starts with the study of elementary particles, atoms, molecules, substances, metals, crystals and other aggregates of matter. Matter can be studied in solid, liquid, gas and plasma states, in isolation or in combination. The interactions, reactions and transformations that are studied in chemistry are usually the result of interactions between atoms, leading to rearrangements of the chemical bonds which hold atoms together. Such behaviors are studied in a chemistry laboratory. The chemistry laboratory stereotypically uses various forms of laboratory glassware. However glassware is not central to chemistry, and a great deal of experimental (as well as applied/industrial) chemistry is done without it. A chemical reaction is a transformation of some substances into one or more different substances. The basis of such a chemical transformation is the rearrangement of electrons in the chemical bonds between atoms. It can be symbolically depicted through a chemical equation, which usually involves atoms as subjects. The number of atoms on the left and the right in the equation for a chemical transformation is equal. (When the number of atoms on either side is unequal, the transformation is referred to as a nuclear reaction or radioactive decay.) The type of chemical reactions a substance may undergo and the energy changes that may accompany it are constrained by certain basic rules, known as chemical laws. Energy and entropy considerations are invariably important in almost all chemical studies. Chemical substances are classified in terms of their structure, phase, as well as their chemical compositions. They can be analyzed using the tools of chemical analysis, e.g. spectroscopy and chromatography. Scientists engaged in chemical research are known as chemists. Most chemists specialize in one or more sub-disciplines. Several concepts are essential for the study of chemistry; some of them are: Matter In chemistry, matter is defined as anything that has rest mass and volume (it takes up space) and is made up of particles. The particles that make up matter have rest mass as well – not all particles have rest mass, such as the photon. Matter can be a pure chemical substance or a mixture of substances. Atom The atom is the basic unit of chemistry. It consists of a dense core called the atomic nucleus surrounded by a space occupied by an electron cloud. The nucleus is made up of positively charged protons and uncharged neutrons (together called nucleons), while the electron cloud consists of negatively charged electrons which orbit the nucleus. In a neutral atom, the negatively charged electrons balance out the positive charge of the protons. The nucleus is dense; the mass of a nucleon is approximately 1,836 times that of an electron, yet the radius of an atom is about 10,000 times that of its nucleus. The atom is also the smallest entity that can be envisaged to retain the chemical properties of the element, such as electronegativity, ionization potential, preferred oxidation state(s), coordination number, and preferred types of bonds to form (e.g., metallic, ionic, covalent). Element A chemical element is a pure substance which is composed of a single type of atom, characterized by its particular number of protons in the nuclei of its atoms, known as the atomic number and represented by the symbol Z. The mass number is the sum of the number of protons and neutrons in a nucleus. Although all the nuclei of all atoms belonging to one element will have the same atomic number, they may not necessarily have the same mass number; atoms of an element which have different mass numbers are known as isotopes. For example, all atoms with 6 protons in their nuclei are atoms of the chemical element carbon, but atoms of carbon may have mass numbers of 12 or 13. The standard presentation of the chemical elements is in the periodic table, which orders elements by atomic number. The periodic table is arranged in groups, or columns, and periods, or rows. The periodic table is useful in identifying periodic trends. Compound A compound is a pure chemical substance composed of more than one element. The properties of a compound bear little similarity to those of its elements. The standard nomenclature of compounds is set by the International Union of Pure and Applied Chemistry (IUPAC). Organic compounds are named according to the organic nomenclature system. The names for inorganic compounds are created according to the inorganic nomenclature system. When a compound has more than one component, then they are divided into two classes, the electropositive and the electronegative components. In addition the Chemical Abstracts Service (CAS) has devised a method to index chemical substances. In this scheme each chemical substance is identifiable by a number known as its CAS registry number. Molecule A molecule is the smallest indivisible portion of a pure chemical substance that has its unique set of chemical properties, that is, its potential to undergo a certain set of chemical reactions with other substances. However, this definition only works well for substances that are composed of molecules, which is not true of many substances (see below). Molecules are typically a set of atoms bound together by covalent bonds, such that the structure is electrically neutral and all valence electrons are paired with other electrons either in bonds or in lone pairs. Thus, molecules exist as electrically neutral units, unlike ions. When this rule is broken, giving the "molecule" a charge, the result is sometimes named a molecular ion or a polyatomic ion. However, the discrete and separate nature of the molecular concept usually requires that molecular ions be present only in well-separated form, such as a directed beam in a vacuum in a mass spectrometer. Charged polyatomic collections residing in solids (for example, common sulfate or nitrate ions) are generally not considered "molecules" in chemistry. Some molecules contain one or more unpaired electrons, creating radicals. Most radicals are comparatively reactive, but some, such as nitric oxide (NO) can be stable. The "inert" or noble gas elements (helium, neon, argon, krypton, xenon and radon) are composed of lone atoms as their smallest discrete unit, but the other isolated chemical elements consist of either molecules or networks of atoms bonded to each other in some way. Identifiable molecules compose familiar substances such as water, air, and many organic compounds like alcohol, sugar, gasoline, and the various pharmaceuticals. However, not all substances or chemical compounds consist of discrete molecules, and indeed most of the solid substances that make up the solid crust, mantle, and core of the Earth are chemical compounds without molecules. These other types of substances, such as ionic compounds and network solids, are organized in such a way as to lack the existence of identifiable molecules per se. Instead, these substances are discussed in terms of formula units or unit cells as the smallest repeating structure within the substance. Examples of such substances are mineral salts (such as table salt), solids like carbon and diamond, metals, and familiar silica and silicate minerals such as quartz and granite. One of the main characteristics of a molecule is its geometry often called its structure. While the structure of diatomic, triatomic or tetra-atomic molecules may be trivial, (linear, angular pyramidal etc.) the structure of polyatomic molecules, that are constituted of more than six atoms (of several elements) can be crucial for its chemical nature. Substance and mixture A chemical substance is a kind of matter with a definite composition and set of properties. A collection of substances is called a mixture. Examples of mixtures are air and alloys. Mole and amount of substance The mole is a unit of measurement that denotes an amount of substance (also called chemical amount). One mole is defined to contain exactly 6.02214076×1023 particles (atoms, molecules, ions, or electrons), where the number of particles per mole is known as the Avogadro constant. Molar concentration is the amount of a particular substance per volume of solution, and is commonly reported in mol/dm3. Phase In addition to the specific chemical properties that distinguish different chemical classifications, chemicals can exist in several phases. For the most part, the chemical classifications are independent of these bulk phase classifications; however, some more exotic phases are incompatible with certain chemical properties. A phase is a set of states of a chemical system that have similar bulk structural properties, over a range of conditions, such as pressure or temperature. Physical properties, such as density and refractive index tend to fall within values characteristic of the phase. The phase of matter is defined by the phase transition, which is when energy put into or taken out of the system goes into rearranging the structure of the system, instead of changing the bulk conditions. Sometimes the distinction between phases can be continuous instead of having a discrete boundary; in this case the matter is considered to be in a supercritical state. When three states meet based on the conditions, it is known as a triple point and since this is invariant, it is a convenient way to define a set of conditions. The most familiar examples of phases are solids, liquids, and gases. Many substances exhibit multiple solid phases. For example, there are three phases of solid iron (alpha, gamma, and delta) that vary based on temperature and pressure. A principal difference between solid phases is the crystal structure, or arrangement, of the atoms. Another phase commonly encountered in the study of chemistry is the aqueous phase, which is the state of substances dissolved in aqueous solution (that is, in water). Less familiar phases include plasmas, Bose–Einstein condensates and fermionic condensates and the paramagnetic and ferromagnetic phases of magnetic materials. While most familiar phases deal with three-dimensional systems, it is also possible to define analogs in two-dimensional systems, which has received attention for its relevance to systems in biology. Bonding Atoms sticking together in molecules or crystals are said to be bonded with one another. A chemical bond may be visualized as the multipole balance between the positive charges in the nuclei and the negative charges oscillating about them. More than simple attraction and repulsion, the energies and distributions characterize the availability of an electron to bond to another atom. The chemical bond can be a covalent bond, an ionic bond, a hydrogen bond or just because of Van der Waals force. Each of these kinds of bonds is ascribed to some potential. These potentials create the interactions which hold atoms together in molecules or crystals. In many simple compounds, valence bond theory, the Valence Shell Electron Pair Repulsion model (VSEPR), and the concept of oxidation number can be used to explain molecular structure and composition. An ionic bond is formed when a metal loses one or more of its electrons, becoming a positively charged cation, and the electrons are then gained by the non-metal atom, becoming a negatively charged anion. The two oppositely charged ions attract one another, and the ionic bond is the electrostatic force of attraction between them. For example, sodium (Na), a metal, loses one electron to become an Na+ cation while chlorine (Cl), a non-metal, gains this electron to become Cl−. The ions are held together due to electrostatic attraction, and that compound sodium chloride (NaCl), or common table salt, is formed. In a covalent bond, one or more pairs of valence electrons are shared by two atoms: the resulting electrically neutral group of bonded atoms is termed a molecule. Atoms will share valence electrons in such a way as to create a noble gas electron configuration (eight electrons in their outermost shell) for each atom. Atoms that tend to combine in such a way that they each have eight electrons in their valence shell are said to follow the octet rule. However, some elements like hydrogen and lithium need only two electrons in their outermost shell to attain this stable configuration; these atoms are said to follow the duet rule, and in this way they are reaching the electron configuration of the noble gas helium, which has two electrons in its outer shell. Similarly, theories from classical physics can be used to predict many ionic structures. With more complicated compounds, such as metal complexes, valence bond theory is less applicable and alternative approaches, such as the molecular orbital theory, are generally used. Energy In the context of chemistry, energy is an attribute of a substance as a consequence of its atomic, molecular or aggregate structure. Since a chemical transformation is accompanied by a change in one or more of these kinds of structures, it is invariably accompanied by an increase or decrease of energy of the substances involved. Some energy is transferred between the surroundings and the reactants of the reaction in the form of heat or light; thus the products of a reaction may have more or less energy than the reactants. A reaction is said to be exergonic if the final state is lower on the energy scale than the initial state; in the case of endergonic reactions the situation is the reverse. A reaction is said to be exothermic if the reaction releases heat to the surroundings; in the case of endothermic reactions, the reaction absorbs heat from the surroundings. Chemical reactions are invariably not possible unless the reactants surmount an energy barrier known as the activation energy. The speed of a chemical reaction (at given temperature T) is related to the activation energy E, by the Boltzmann's population factor e − E / k T {\displaystyle e^{-E/kT}} – that is the probability of a molecule to have energy greater than or equal to E at the given temperature T. This exponential dependence of a reaction rate on temperature is known as the Arrhenius equation. The activation energy necessary for a chemical reaction to occur can be in the form of heat, light, electricity or mechanical force in the form of ultrasound. A related concept free energy, which also incorporates entropy considerations, is a very useful means for predicting the feasibility of a reaction and determining the state of equilibrium of a chemical reaction, in chemical thermodynamics. A reaction is feasible only if the total change in the Gibbs free energy is negative, Δ G ≤ 0 {\displaystyle \Delta G\leq 0\,} ; if it is equal to zero the chemical reaction is said to be at equilibrium. There exist only limited possible states of energy for electrons, atoms and molecules. These are determined by the rules of quantum mechanics, which require quantization of energy of a bound system. The atoms/molecules in a higher energy state are said to be excited. The molecules/atoms of substance in an excited energy state are often much more reactive; that is, more amenable to chemical reactions. The phase of a substance is invariably determined by its energy and the energy of its surroundings. When the intermolecular forces of a substance are such that the energy of the surroundings is not sufficient to overcome them, it occurs in a more ordered phase like liquid or solid as is the case with water (H2O); a liquid at room temperature because its molecules are bound by hydrogen bonds. Whereas hydrogen sulfide (H2S) is a gas at room temperature and standard pressure, as its molecules are bound by weaker dipole–dipole interactions. The transfer of energy from one chemical substance to another depends on the size of energy quanta emitted from one substance. However, heat energy is often transferred more easily from almost any substance to another because the phonons responsible for vibrational and rotational energy levels in a substance have much less energy than photons invoked for the electronic energy transfer. Thus, because vibrational and rotational energy levels are more closely spaced than electronic energy levels, heat is more easily transferred between substances relative to light or other forms of electronic energy. For example, ultraviolet electromagnetic radiation is not transferred with as much efficacy from one substance to another as thermal or electrical energy. The existence of characteristic energy levels for different chemical substances is useful for their identification by the analysis of spectral lines. Different kinds of spectra are often used in chemical spectroscopy, e.g. IR, microwave, NMR, ESR, etc. Spectroscopy is also used to identify the composition of remote objects – like stars and distant galaxies – by analyzing their radiation spectra. The term chemical energy is often used to indicate the potential of a chemical substance to undergo a transformation through a chemical reaction or to transform other chemical substances. Reaction When a chemical substance is transformed as a result of its interaction with another substance or with energy, a chemical reaction is said to have occurred. A chemical reaction is therefore a concept related to the "reaction" of a substance when it comes in close contact with another, whether as a mixture or a solution; exposure to some form of energy, or both. It results in some energy exchange between the constituents of the reaction as well as with the system environment, which may be designed vessels—often laboratory glassware. Chemical reactions can result in the formation or dissociation of molecules, that is, molecules breaking apart to form two or more molecules or rearrangement of atoms within or across molecules. Chemical reactions usually involve the making or breaking of chemical bonds. Oxidation, reduction, dissociation, acid–base neutralization and molecular rearrangement are some examples of common chemical reactions. A chemical reaction can be symbolically depicted through a chemical equation. While in a non-nuclear chemical reaction the number and kind of atoms on both sides of the equation are equal, for a nuclear reaction this holds true only for the nuclear particles viz. protons and neutrons. The sequence of steps in which the reorganization of chemical bonds may be taking place in the course of a chemical reaction is called its mechanism. A chemical reaction can be envisioned to take place in a number of steps, each of which may have a different speed. Many reaction intermediates with variable stability can thus be envisaged during the course of a reaction. Reaction mechanisms are proposed to explain the kinetics and the relative product mix of a reaction. Many physical chemists specialize in exploring and proposing the mechanisms of various chemical reactions. Several empirical rules, like the Woodward–Hoffmann rules often come in handy while proposing a mechanism for a chemical reaction. According to the IUPAC gold book, a chemical reaction is "a process that results in the interconversion of chemical species." Accordingly, a chemical reaction may be an elementary reaction or a stepwise reaction. An additional caveat is made, in that this definition includes cases where the interconversion of conformers is experimentally observable. Such detectable chemical reactions normally involve sets of molecular entities as indicated by this definition, but it is often conceptually convenient to use the term also for changes involving single molecular entities (i.e. 'microscopic chemical events'). Ions and salts An ion is a charged species, an atom or a molecule, that has lost or gained one or more electrons. When an atom loses an electron and thus has more protons than electrons, the atom is a positively charged ion or cation. When an atom gains an electron and thus has more electrons than protons, the atom is a negatively charged ion or anion. Cations and anions can form a crystalline lattice of neutral salts, such as the Na+ and Cl− ions forming sodium chloride, or NaCl. Examples of polyatomic ions that do not split up during acid–base reactions are hydroxide (OH−) and phosphate (PO43−). Plasma is composed of gaseous matter that has been completely ionized, usually through high temperature. Acidity and basicity A substance can often be classified as an acid or a base. There are several different theories which explain acid–base behavior. The simplest is Arrhenius theory, which states that an acid is a substance that produces hydronium ions when it is dissolved in water, and a base is one that produces hydroxide ions when dissolved in water. According to Brønsted–Lowry acid–base theory, acids are substances that donate a positive hydrogen ion to another substance in a chemical reaction; by extension, a base is the substance which receives that hydrogen ion. A third common theory is Lewis acid–base theory, which is based on the formation of new chemical bonds. Lewis theory explains that an acid is a substance which is capable of accepting a pair of electrons from another substance during the process of bond formation, while a base is a substance which can provide a pair of electrons to form a new bond. There are several other ways in which a substance may be classified as an acid or a base, as is evident in the history of this concept. Acid strength is commonly measured by two methods. One measurement, based on the Arrhenius definition of acidity, is pH, which is a measurement of the hydronium ion concentration in a solution, as expressed on a negative logarithmic scale. Thus, solutions that have a low pH have a high hydronium ion concentration and can be said to be more acidic. The other measurement, based on the Brønsted–Lowry definition, is the acid dissociation constant (Ka), which measures the relative ability of a substance to act as an acid under the Brønsted–Lowry definition of an acid. That is, substances with a higher Ka are more likely to donate hydrogen ions in chemical reactions than those with lower Ka values. Redox Redox (reduction-oxidation) reactions include all chemical reactions in which atoms have their oxidation state changed by either gaining electrons (reduction) or losing electrons (oxidation). Substances that have the ability to oxidize other substances are said to be oxidative and are known as oxidizing agents, oxidants or oxidizers. An oxidant removes electrons from another substance. Similarly, substances that have the ability to reduce other substances are said to be reductive and are known as reducing agents, reductants, or reducers. A reductant transfers electrons to another substance and is thus oxidized itself. And because it "donates" electrons it is also called an electron donor. Oxidation and reduction properly refer to a change in oxidation number—the actual transfer of electrons may never occur. Thus, oxidation is better defined as an increase in oxidation number, and reduction as a decrease in oxidation number. Equilibrium Although the concept of equilibrium is widely used across sciences, in the context of chemistry, it arises whenever a number of different states of the chemical composition are possible, as for example, in a mixture of several chemical compounds that can react with one another, or when a substance can be present in more than one kind of phase. A system of chemical substances at equilibrium, even though having an unchanging composition, is most often not static; molecules of the substances continue to react with one another thus giving rise to a dynamic equilibrium. Thus the concept describes the state in which the parameters such as chemical composition remain unchanged over time. Chemical laws Chemical reactions are governed by certain laws, which have become fundamental concepts in chemistry. Some of them are: History The history of chemistry spans a period from the ancient past to the present. Since several millennia BC, civilizations were using technologies that would eventually form the basis of the various branches of chemistry. Examples include extracting metals from ores, making pottery and glazes, fermenting beer and wine, extracting chemicals from plants for medicine and perfume, rendering fat into soap, making glass, and making alloys like bronze. Chemistry was preceded by its protoscience, alchemy, which operated a non-scientific approach to understanding the constituents of matter and their interactions. Despite being unsuccessful in explaining the nature of matter and its transformations, alchemists set the stage for modern chemistry by performing experiments and recording the results. Robert Boyle, although skeptical of elements and convinced of alchemy, played a key part in elevating the "sacred art" as an independent, fundamental and philosophical discipline in his work The Sceptical Chymist (1661). While both alchemy and chemistry are concerned with matter and its transformations, the crucial difference was given by the scientific method that chemists employed in their work. Chemistry, as a body of knowledge distinct from alchemy, became an established science with the work of Antoine Lavoisier, who developed a law of conservation of mass that demanded careful measurement and quantitative observations of chemical phenomena. The history of chemistry afterwards is intertwined with the history of thermodynamics, especially through the work of Willard Gibbs. Definition The definition of chemistry has changed over time, as new discoveries and theories add to the functionality of the science. The term "chymistry", in the view of noted scientist Robert Boyle in 1661, meant the subject of the material principles of mixed bodies. In 1663, the chemist Christopher Glaser described "chymistry" as a scientific art, by which one learns to dissolve bodies, and draw from them the different substances on their composition, and how to unite them again, and exalt them to a higher perfection. The 1730 definition of the word "chemistry", as used by Georg Ernst Stahl, meant the art of resolving mixed, compound, or aggregate bodies into their principles; and of composing such bodies from those principles. In 1837, Jean-Baptiste Dumas considered the word "chemistry" to refer to the science concerned with the laws and effects of molecular forces. This definition further evolved until, in 1947, it came to mean the science of substances: their structure, their properties, and the reactions that change them into other substances—a characterization accepted by Linus Pauling. More recently, in 1998, Professor Raymond Chang broadened the definition of "chemistry" to mean the study of matter and the changes it undergoes. Background Early civilizations, such as the Egyptians, Babylonians, and Indians, amassed practical knowledge concerning the arts of metallurgy, pottery and dyes, but did not develop a systematic theory. A basic chemical hypothesis first emerged in Classical Greece with the theory of four elements as propounded definitively by Aristotle stating that fire, air, earth and water were the fundamental elements from which everything is formed as a combination. Greek atomism dates back to 440 BC, arising in works by philosophers such as Democritus and Epicurus. In 50 BCE, the Roman philosopher Lucretius expanded upon the theory in his poem De rerum natura (On The Nature of Things). Unlike modern concepts of science, Greek atomism was purely philosophical in nature, with little concern for empirical observations and no concern for chemical experiments. An early form of the idea of conservation of mass is the notion that "Nothing comes from nothing" in Ancient Greek philosophy, which can be found in Empedocles (approx. 4th century BC): "For it is impossible for anything to come to be from what is not, and it cannot be brought about or heard of that what is should be utterly destroyed." and Epicurus (3rd century BC), who, describing the nature of the Universe, wrote that "the totality of things was always such as it is now, and always will be". In the Hellenistic world the art of alchemy first proliferated, mingling magic and occultism into the study of natural substances with the ultimate goal of transmuting elements into gold and discovering the elixir of eternal life. Work, particularly the development of distillation, continued in the early Byzantine period with the most famous practitioner being the 4th century Greek-Egyptian Zosimos of Panopolis. Alchemy continued to be developed and practised throughout the Arab world after the Muslim conquests, and from there, and from the Byzantine remnants, diffused into medieval and Renaissance Europe through Latin translations. The Arabic works attributed to Jabir ibn Hayyan introduced a systematic classification of chemical substances, and provided instructions for deriving an inorganic compound (sal ammoniac or ammonium chloride) from organic substances (such as plants, blood, and hair) by chemical means. Some Arabic Jabirian works (e.g., the "Book of Mercy", and the "Book of Seventy") were later translated into Latin under the Latinized name "Geber", and in 13th-century Europe an anonymous writer, usually referred to as pseudo-Geber, started to produce alchemical and metallurgical writings under this name. Later influential Muslim philosophers, such as Abū al-Rayhān al-Bīrūnī and Avicenna disputed the theories of alchemy, particularly the theory of the transmutation of metals. Improvements of the refining of ores and their extractions to smelt metals was widely used source of information for early chemists in the 16th century, among them Georg Agricola (1494–1555), who published his major work De re metallica in 1556. His work, describing highly developed and complex processes of mining metal ores and metal extraction, were the pinnacle of metallurgy during that time. His approach removed all mysticism associated with the subject, creating the practical base upon which others could and would build. The work describes the many kinds of furnaces used to smelt ore, and stimulated interest in minerals and their composition. Agricola has been described as the "father of metallurgy" and the founder of geology as a scientific discipline. Under the influence of the Scientific Revolution and its new empirical methods propounded by Sir Francis Bacon and others, a group of chemists at Oxford, Robert Boyle, Robert Hooke and John Mayow began to reshape the old alchemical traditions into a scientific discipline. Boyle in particular questioned some commonly held chemical theories and argued for chemical practitioners to be more "philosophical" and less commercially focused in The Sceptical Chemyst. He formulated Boyle's law, rejected the classical "four elements" and proposed a mechanistic alternative of atoms and chemical reactions that could be subject to rigorous experiment. In the following decades, many important discoveries were made, such as the nature of 'air' which was discovered to be composed of many different gases. The Scottish chemist Joseph Black and the Flemish Jan Baptist van Helmont discovered carbon dioxide, or what Black called 'fixed air' in 1754; Henry Cavendish discovered hydrogen and elucidated its properties and Joseph Priestley and, independently, Carl Wilhelm Scheele isolated pure oxygen. The theory of phlogiston (a substance at the root of all combustion) was propounded by the German Georg Ernst Stahl in the early 18th century and was only overturned by the end of the century by the French chemist Antoine Lavoisier, the chemical analogue of Newton in physics. Lavoisier did more than any other to establish the new science on proper theoretical footing, by elucidating the principle of conservation of mass and developing a new system of chemical nomenclature used to this day. English scientist John Dalton proposed the modern theory of atoms; that all substances are composed of indivisible 'atoms' of matter and that different atoms have varying atomic weights. The development of the electrochemical theory of chemical combinations occurred in the early 19th century as the result of the work of two scientists in particular, Jöns Jacob Berzelius and Humphry Davy, made possible by the prior invention of the voltaic pile by Alessandro Volta. Davy discovered nine new elements including the alkali metals by extracting them from their oxides with electric current. British William Prout first proposed ordering all the elements by their atomic weight as all atoms had a weight that was an exact multiple of the atomic weight of hydrogen. J.A.R. Newlands devised an early table of elements, which was then developed into the modern periodic table of elements in the 1860s by Dmitri Mendeleev and independently by several other scientists including Julius Lothar Meyer. The inert gases, later called the noble gases were discovered by William Ramsay in collaboration with Lord Rayleigh at the end of the century, thereby filling in the basic structure of the table. Organic chemistry was developed by Justus von Liebig and others, following Friedrich Wöhler's synthesis of urea. Other crucial 19th century advances were; an understanding of valence bonding (Edward Frankland in 1852) and the application of thermodynamics to chemistry (J. W. Gibbs and Svante Arrhenius in the 1870s). At the turn of the twentieth century the theoretical underpinnings of chemistry were finally understood due to a series of remarkable discoveries that succeeded in probing and discovering the very nature of the internal structure of atoms. In 1897, J.J. Thomson of the University of Cambridge discovered the electron and soon after the French scientist Becquerel as well as the couple Pierre and Marie Curie investigated the phenomenon of radioactivity. In a series of pioneering scattering experiments Ernest Rutherford at the University of Manchester discovered the internal structure of the atom and the existence of the proton, classified and explained the different types of radioactivity and successfully transmuted the first element by bombarding nitrogen with alpha particles. His work on atomic structure was improved on by his students, the Danish physicist Niels Bohr, the Englishman Henry Moseley and the German Otto Hahn, who went on to father the emerging nuclear chemistry and discovered nuclear fission. The electronic theory of chemical bonds and molecular orbitals was developed by the American scientists Linus Pauling and Gilbert N. Lewis. The year 2011 was declared by the United Nations as the International Year of Chemistry. It was an initiative of the International Union of Pure and Applied Chemistry, and of the United Nations Educational, Scientific, and Cultural Organization and involves chemical societies, academics, and institutions worldwide and relied on individual initiatives to organize local and regional activities. Practice In the practice of chemistry, pure chemistry is the study of the fundamental principles of chemistry, while applied chemistry applies that knowledge to develop technology and solve real-world problems. Subdisciplines Chemistry is typically divided into several major sub-disciplines. There are also several main cross-disciplinary and more specialized fields of chemistry. Analytical chemistry is the analysis of material samples to gain an understanding of their chemical composition and structure. Analytical chemistry incorporates standardized experimental methods in chemistry. These methods may be used in all subdisciplines of chemistry, excluding purely theoretical chemistry. Biochemistry is the study of the chemicals, chemical reactions and interactions that take place at a molecular level in living organisms. Biochemistry is highly interdisciplinary, covering medicinal chemistry, neurochemistry, molecular biology, forensics, plant science and genetics. Inorganic chemistry is the study of the properties and reactions of inorganic compounds, such as metals and minerals. The distinction between organic and inorganic disciplines is not absolute and there is much overlap, most importantly in the sub-discipline of organometallic chemistry. Materials chemistry is the preparation, characterization, and understanding of solid state components or devices with a useful current or future function. The field is a new breadth of study in graduate programs, and it integrates elements from all classical areas of chemistry like organic chemistry, inorganic chemistry, and crystallography with a focus on fundamental issues that are unique to materials. Primary systems of study include the chemistry of condensed phases (solids, liquids, polymers) and interfaces between different phases. Neurochemistry is the study of neurochemicals; including transmitters, peptides, proteins, lipids, sugars, and nucleic acids; their interactions, and the roles they play in forming, maintaining, and modifying the nervous system. Nuclear chemistry is the study of how subatomic particles come together and make nuclei. Modern transmutation is a large component of nuclear chemistry, and the table of nuclides is an important result and tool for this field. In addition to medical applications, nuclear chemistry encompasses nuclear engineering which explores the topic of using nuclear power sources for generating energy. Organic chemistry is the study of the structure, properties, composition, mechanisms, and reactions of organic compounds. An organic compound is defined as any compound based on a carbon skeleton. Organic compounds can be classified, organized and understood in reactions by their functional groups, unit atoms or molecules that show characteristic chemical properties in a compound. Physical chemistry is the study of the physical and fundamental basis of chemical systems and processes. In particular, the energetics and dynamics of such systems and processes are of interest to physical chemists. Important areas of study include chemical thermodynamics, chemical kinetics, electrochemistry, statistical mechanics, spectroscopy, and more recently, astrochemistry. Physical chemistry has large overlap with molecular physics. Physical chemistry involves the use of infinitesimal calculus in deriving equations. It is usually associated with quantum chemistry and theoretical chemistry. Physical chemistry is a distinct discipline from chemical physics, but again, there is very strong overlap. Theoretical chemistry is the study of chemistry via fundamental theoretical reasoning (usually within mathematics or physics). In particular the application of quantum mechanics to chemistry is called quantum chemistry. Since the end of the Second World War, the development of computers has allowed a systematic development of computational chemistry, which is the art of developing and applying computer programs for solving chemical problems. Theoretical chemistry has large overlap with (theoretical and experimental) condensed matter physics and molecular physics. Other subdivisions include electrochemistry, femtochemistry, flavor chemistry, flow chemistry, immunohistochemistry, hydrogenation chemistry, mathematical chemistry, molecular mechanics, natural product chemistry, organometallic chemistry, petrochemistry, photochemistry, physical organic chemistry, polymer chemistry, radiochemistry, sonochemistry, supramolecular chemistry, synthetic chemistry, and many others. Interdisciplinary Interdisciplinary fields include agrochemistry, astrochemistry (and cosmochemistry), atmospheric chemistry, chemical engineering, chemical biology, chemo-informatics, environmental chemistry, geochemistry, green chemistry, immunochemistry, marine chemistry, materials science, mechanochemistry, medicinal chemistry, molecular biology, nanotechnology, oenology, pharmacology, phytochemistry, solid-state chemistry, surface science, thermochemistry, and many others. Industry The chemical industry represents an important economic activity worldwide. The global top 50 chemical producers in 2013 had sales of US$980.5 billion with a profit margin of 10.3%. Professional societies See also References Bibliography Further reading Popular reading Atkins, P. W. Galileo's Finger (Oxford University Press) ISBN 0-19-860941-8 Atkins, P. W. Atkins' Molecules (Cambridge University Press) ISBN 0-521-82397-8 Kean, Sam. The Disappearing Spoon – and Other True Tales from the Periodic Table (Black Swan) London, England, 2010 ISBN 978-0-552-77750-6 Levi, Primo The Periodic Table (Penguin Books) [1975] translated from the Italian by Raymond Rosenthal (1984) ISBN 978-0-14-139944-7 Stwertka, A. A Guide to the Elements (Oxford University Press) ISBN 0-19-515027-9 "Dictionary of the History of Ideas". Archived from the original on 10 March 2008. "Chemistry" . Encyclopædia Britannica. Vol. 6 (11th ed.). 1911. pp. 33–76. Introductory undergraduate textbooks Atkins, P.W., Overton, T., Rourke, J., Weller, M. and Armstrong, F. Shriver and Atkins Inorganic Chemistry (4th ed.) 2006 (Oxford University Press) ISBN 0-19-926463-5 Chang, Raymond. Chemistry 6th ed. Boston, Massachusetts: James M. Smith, 1998. ISBN 0-07-115221-0 Clayden, Jonathan; Greeves, Nick; Warren, Stuart; Wothers, Peter (2001). Organic Chemistry (1st ed.). Oxford University Press. ISBN 978-0-19-850346-0. Voet and Voet. Biochemistry (Wiley) ISBN 0-471-58651-X Advanced undergraduate-level or graduate textbooks Atkins, P. W. Physical Chemistry (Oxford University Press) ISBN 0-19-879285-9 Atkins, P. W. et al. Molecular Quantum Mechanics (Oxford University Press) McWeeny, R. Coulson's Valence (Oxford Science Publications) ISBN 0-19-855144-4 Pauling, L. The Nature of the chemical bond (Cornell University Press) ISBN 0-8014-0333-2 Pauling, L., and Wilson, E. B. Introduction to Quantum Mechanics with Applications to Chemistry (Dover Publications) ISBN 0-486-64871-0 Smart and Moore. Solid State Chemistry: An Introduction (Chapman and Hall) ISBN 0-412-40040-5 Stephenson, G. Mathematical Methods for Science Students (Longman) ISBN 0-582-44416-0 External links General Chemistry principles, patterns and applications.

Computer science is the study of computation, information, and automation. Computer science spans theoretical disciplines (such as algorithms, theory of computation, and information theory) to applied disciplines (including the design and implementation of hardware and software). Algorithms and data structures are central to computer science. The theory of computation concerns abstract models of computation and general classes of problems that can be solved using them. The fields of cryptography and computer security involve studying the means for secure communication and preventing security vulnerabilities. Computer graphics and computational geometry address the generation of images. Programming language theory considers different ways to describe computational processes, and database theory concerns the management of repositories of data. Human–computer interaction investigates the interfaces through which humans and computers interact, and software engineering focuses on the design and principles behind developing software. Areas such as operating systems, networks and embedded systems investigate the principles and design behind complex systems. Computer architecture describes the construction of computer components and computer-operated equipment. Artificial intelligence and machine learning aim to synthesize goal-orientated processes such as problem-solving, decision-making, environmental adaptation, planning and learning found in humans and animals. Within artificial intelligence, computer vision aims to understand and process image and video data, while natural language processing aims to understand and process textual and linguistic data. The fundamental concern of computer science is determining what can and cannot be automated. The Turing Award is generally recognized as the highest distinction in computer science. History The earliest foundations of what would become computer science predate the invention of the modern digital computer. Machines for calculating fixed numerical tasks such as the abacus have existed since antiquity, aiding in computations such as multiplication and division. Algorithms for performing computations have existed since antiquity, even before the development of sophisticated computing equipment. Wilhelm Schickard designed and constructed the first working mechanical calculator in 1623. In 1673, Gottfried Leibniz demonstrated a digital mechanical calculator, called the Stepped Reckoner. Leibniz may be considered the first computer scientist and information theorist, because of various reasons, including the fact that he documented the binary number system. In 1820, Thomas de Colmar launched the mechanical calculator industry when he invented his simplified arithmometer, the first calculating machine strong enough and reliable enough to be used daily in an office environment. Charles Babbage started the design of the first automatic mechanical calculator, his Difference Engine, in 1822, which eventually gave him the idea of the first programmable mechanical calculator, his Analytical Engine. He started developing this machine in 1834, and "in less than two years, he had sketched out many of the salient features of the modern computer". "A crucial step was the adoption of a punched card system derived from the Jacquard loom" making it infinitely programmable. In 1843, during the translation of a French article on the Analytical Engine, Ada Lovelace wrote, in one of the many notes she included, an algorithm to compute the Bernoulli numbers, which is considered to be the first published algorithm ever specifically tailored for implementation on a computer. Around 1885, Herman Hollerith invented the tabulator, which used punched cards to process statistical information; eventually his company became part of IBM. Following Babbage, although unaware of his earlier work, Percy Ludgate in 1909 published the 2nd of the only two designs for mechanical analytical engines in history. In 1914, the Spanish engineer Leonardo Torres Quevedo published his Essays on Automatics, and designed, inspired by Babbage, a theoretical electromechanical calculating machine which was to be controlled by a read-only program. The paper also introduced the idea of floating-point arithmetic. In 1920, to celebrate the 100th anniversary of the invention of the arithmometer, Torres presented in Paris the Electromechanical Arithmometer, a prototype that demonstrated the feasibility of an electromechanical analytical engine, on which commands could be typed and the results printed automatically. In 1937, one hundred years after Babbage's impossible dream, Howard Aiken convinced IBM, which was making all kinds of punched card equipment and was also in the calculator business to develop his giant programmable calculator, the ASCC/Harvard Mark I, based on Babbage's Analytical Engine, which itself used cards and a central computing unit. When the machine was finished, some hailed it as "Babbage's dream come true". During the 1940s, with the development of new and more powerful computing machines such as the Atanasoff–Berry computer and ENIAC, the term computer came to refer to the machines rather than their human predecessors. As it became clear that computers could be used for more than just mathematical calculations, the field of computer science broadened to study computation in general. In 1945, IBM founded the Watson Scientific Computing Laboratory at Columbia University in New York City. The renovated fraternity house on Manhattan's West Side was IBM's first laboratory devoted to pure science. The lab is the forerunner of IBM's Research Division, which today operates research facilities around the world. Ultimately, the close relationship between IBM and Columbia University was instrumental in the emergence of a new scientific discipline, with Columbia offering one of the first academic-credit courses in computer science in 1946. Computer science began to be established as a distinct academic discipline in the 1950s and early 1960s. The world's first computer science degree program, the Cambridge Diploma in Computer Science, began at the University of Cambridge Computer Laboratory in 1953. The first computer science department in the United States was formed at Purdue University in 1962. Since practical computers became available, many applications of computing have become distinct areas of study in their own rights. Etymology and scope Although first proposed in 1956, the term "computer science" appears in a 1959 article in Communications of the ACM, in which Louis Fein argues for the creation of a Graduate School in Computer Sciences analogous to the creation of Harvard Business School in 1921. Louis justifies the name by arguing that, like management science, the subject is applied and interdisciplinary in nature, while having the characteristics typical of an academic discipline. This effort, and those of others such as numerical analyst George Forsythe, were successful, and universities went on to create such departments, starting with Purdue in 1962. Despite its name, a significant amount of computer science does not involve the study of computers themselves. Because of this, several alternative names have been proposed. Certain departments of major universities prefer the term computing science, to emphasize precisely that difference. Danish scientist Peter Naur suggested the term datalogy, to reflect the fact that the scientific discipline revolves around data and data treatment, while not necessarily involving computers. The first scientific institution to use the term was the Department of Datalogy at the University of Copenhagen, founded in 1969, with Peter Naur being the first professor in datalogy. The term is used mainly in the Scandinavian countries. An alternative term, also proposed by Naur, is data science; this is now used for a multi-disciplinary field of data analysis, including statistics and databases. In the early days of computing, a number of terms for the practitioners of the field of computing were suggested (albeit facetiously) in the Communications of the ACM—turingineer, turologist, flow-charts-man, applied meta-mathematician, and applied epistemologist. Three months later in the same journal, comptologist was suggested, followed next year by hypologist. The term computics has also been suggested. In Europe, terms derived from contracted translations of the expression "automatic information" (e.g. "informazione automatica" in Italian) or "information and mathematics" are often used, e.g. informatique (French), Informatik (German), informatica (Italian, Dutch), informática (Spanish, Portuguese), informatika (Slavic languages and Hungarian) or pliroforiki (πληροφορική, which means informatics) in Greek. Similar words have also been adopted in the UK (as in the School of Informatics, University of Edinburgh). "In the U.S., however, informatics is linked with applied computing, or computing in the context of another domain." A folkloric quotation, often attributed to—but almost certainly not first formulated by—Edsger Dijkstra, states that "computer science is no more about computers than astronomy is about telescopes." The design and deployment of computers and computer systems is generally considered the province of disciplines other than computer science. For example, the study of computer hardware is usually considered part of computer engineering, while the study of commercial computer systems and their deployment is often called information technology or information systems. However, there has been exchange of ideas between the various computer-related disciplines. Computer science research also often intersects other disciplines, such as cognitive science, linguistics, mathematics, physics, biology, Earth science, statistics, philosophy, and logic. Computer science is considered by some to have a much closer relationship with mathematics than many scientific disciplines, with some observers saying that computing is a mathematical science. Early computer science was strongly influenced by the work of mathematicians such as Kurt Gödel, Alan Turing, John von Neumann, Rózsa Péter and Alonzo Church and there continues to be a useful interchange of ideas between the two fields in areas such as mathematical logic, category theory, domain theory, and algebra. The relationship between computer science and software engineering is a contentious issue, which is further muddied by disputes over what the term "software engineering" means, and how computer science is defined. David Parnas, taking a cue from the relationship between other engineering and science disciplines, has claimed that the principal focus of computer science is studying the properties of computation in general, while the principal focus of software engineering is the design of specific computations to achieve practical goals, making the two separate but complementary disciplines. The academic, political, and funding aspects of computer science tend to depend on whether a department is formed with a mathematical emphasis or with an engineering emphasis. Computer science departments with a mathematics emphasis and with a numerical orientation consider alignment with computational science. Both types of departments tend to make efforts to bridge the field educationally if not across all research. Philosophy Epistemology of computer science Despite the word science in its name, there is debate over whether or not computer science is a discipline of science, mathematics, or engineering. Allen Newell and Herbert A. Simon argued in 1975, Computer science is an empirical discipline. We would have called it an experimental science, but like astronomy, economics, and geology, some of its unique forms of observation and experience do not fit a narrow stereotype of the experimental method. Nonetheless, they are experiments. Each new machine that is built is an experiment. Actually constructing the machine poses a question to nature; and we listen for the answer by observing the machine in operation and analyzing it by all analytical and measurement means available. It has since been argued that computer science can be classified as an empirical science since it makes use of empirical testing to evaluate the correctness of programs, but a problem remains in defining the laws and theorems of computer science (if any exist) and defining the nature of experiments in computer science. Proponents of classifying computer science as an engineering discipline argue that the reliability of computational systems is investigated in the same way as bridges in civil engineering and airplanes in aerospace engineering. They also argue that while empirical sciences observe what presently exists, computer science observes what is possible to exist and while scientists discover laws from observation, no proper laws have been found in computer science and it is instead concerned with creating phenomena. Proponents of classifying computer science as a mathematical discipline argue that computer programs are physical realizations of mathematical entities and programs that can be deductively reasoned through mathematical formal methods. Computer scientists Edsger W. Dijkstra and Tony Hoare regard instructions for computer programs as mathematical sentences and interpret formal semantics for programming languages as mathematical axiomatic systems. Paradigms of computer science A number of computer scientists have argued for the distinction of three separate paradigms in computer science. Peter Wegner argued that those paradigms are science, technology, and mathematics. Peter Denning's working group argued that they are theory, abstraction (modeling), and design. Amnon H. Eden described them as the "rationalist paradigm" (which treats computer science as a branch of mathematics, which is prevalent in theoretical computer science, and mainly employs deductive reasoning), the "technocratic paradigm" (which might be found in engineering approaches, most prominently in software engineering), and the "scientific paradigm" (which approaches computer-related artifacts from the empirical perspective of natural sciences, identifiable in some branches of artificial intelligence). Computer science focuses on methods involved in design, specification, programming, verification, implementation and testing of human-made computing systems. Fields As a discipline, computer science spans a range of topics from theoretical studies of algorithms and the limits of computation to the practical issues of implementing computing systems in hardware and software. CSAB, formerly called Computing Sciences Accreditation Board—which is made up of representatives of the Association for Computing Machinery (ACM), and the IEEE Computer Society (IEEE CS)—identifies four areas that it considers crucial to the discipline of computer science: theory of computation, algorithms and data structures, programming methodology and languages, and computer elements and architecture. In addition to these four areas, CSAB also identifies fields such as software engineering, artificial intelligence, computer networking and communication, database systems, parallel computation, distributed computation, human–computer interaction, computer graphics, operating systems, and numerical and symbolic computation as being important areas of computer science. Theoretical computer science Theoretical computer science is mathematical and abstract in spirit, but it derives its motivation from practical and everyday computation. It aims to understand the nature of computation and, as a consequence of this understanding, provide more efficient methodologies. Theory of computation According to Peter Denning, the fundamental question underlying computer science is, "What can be automated?" Theory of computation is focused on answering fundamental questions about what can be computed and what amount of resources are required to perform those computations. In an effort to answer the first question, computability theory examines which computational problems are solvable on various theoretical models of computation. The second question is addressed by computational complexity theory, which studies the time and space costs associated with different approaches to solving a multitude of computational problems. The famous P = NP? problem, one of the Millennium Prize Problems, is an open problem in the theory of computation. Information and coding theory Information theory, closely related to probability and statistics, is related to the quantification of information. This was developed by Claude Shannon to find fundamental limits on signal processing operations such as compressing data and on reliably storing and communicating data. Coding theory is the study of the properties of codes (systems for converting information from one form to another) and their fitness for a specific application. Codes are used for data compression, cryptography, error detection and correction, and more recently also for network coding. Codes are studied for the purpose of designing efficient and reliable data transmission methods. Data structures and algorithms Data structures and algorithms are the studies of commonly used computational methods and their computational efficiency. Programming language theory and formal methods Programming language theory is a branch of computer science that deals with the design, implementation, analysis, characterization, and classification of programming languages and their individual features. It falls within the discipline of computer science, both depending on and affecting mathematics, software engineering, and linguistics. It is an active research area, with numerous dedicated academic journals. Formal methods are a particular kind of mathematically based technique for the specification, development and verification of software and hardware systems. The use of formal methods for software and hardware design is motivated by the expectation that, as in other engineering disciplines, performing appropriate mathematical analysis can contribute to the reliability and robustness of a design. They form an important theoretical underpinning for software engineering, especially where safety or security is involved. Formal methods are a useful adjunct to software testing since they help avoid errors and can also give a framework for testing. For industrial use, tool support is required. However, the high cost of using formal methods means that they are usually only used in the development of high-integrity and life-critical systems, where safety or security is of utmost importance. Formal methods are best described as the application of a fairly broad variety of theoretical computer science fundamentals, in particular logic calculi, formal languages, automata theory, and program semantics, but also type systems and algebraic data types to problems in software and hardware specification and verification. Applied computer science Computer graphics and visualization Computer graphics is the study of digital visual contents and involves the synthesis and manipulation of image data. The study is connected to many other fields in computer science, including computer vision, image processing, and computational geometry, and is heavily applied in the fields of special effects and video games. Image and sound processing Information can take the form of images, sound, video or other multimedia. Bits of information can be streamed via signals. Its processing is the central notion of informatics, the European view on computing, which studies information processing algorithms independently of the type of information carrier – whether it is electrical, mechanical or biological. This field plays important role in information theory, telecommunications, information engineering and has applications in medical image computing and speech synthesis, among others. What is the lower bound on the complexity of fast Fourier transform algorithms? is one of the unsolved problems in theoretical computer science. Computational science, finance and engineering Scientific computing (or computational science) is the field of study concerned with constructing mathematical models and quantitative analysis techniques and using computers to analyze and solve scientific problems. A major usage of scientific computing is simulation of various processes, including computational fluid dynamics, physical, electrical, and electronic systems and circuits, societies and social situations (notably war games) along with their habitats, and interactions among biological cells. Modern computers enable optimization of such designs as complete aircraft. Notable in electrical and electronic circuit design are SPICE, as well as software for physical realization of new (or modified) designs. The latter includes essential design software for integrated circuits. Human–computer interaction Human–computer interaction (HCI) is the field of study and research concerned with the design and use of computer systems, mainly based on the analysis of the interaction between humans and computer interfaces. HCI has several subfields that focus on the relationship between emotions, social behavior and brain activity with computers. Software engineering Software engineering is the study of designing, implementing, and modifying the software in order to ensure it is of high quality, affordable, maintainable, and fast to build. It is a systematic approach to software design, involving the application of engineering practices to software. Software engineering deals with the organizing and analyzing of software—it does not just deal with the creation or manufacture of new software, but its internal arrangement and maintenance. For example software testing, systems engineering, technical debt and software development processes. Artificial intelligence Artificial intelligence (AI) aims to or is required to synthesize goal-orientated processes such as problem-solving, decision-making, environmental adaptation, learning, and communication found in humans and animals. From its origins in cybernetics and in the Dartmouth Conference (1956), artificial intelligence research has been necessarily cross-disciplinary, drawing on areas of expertise such as applied mathematics, symbolic logic, semiotics, electrical engineering, philosophy of mind, neurophysiology, and social intelligence. AI is associated in the popular mind with robotic development, but the main field of practical application has been as an embedded component in areas of software development, which require computational understanding. The starting point in the late 1940s was Alan Turing's question "Can computers think?", and the question remains effectively unanswered, although the Turing test is still used to assess computer output on the scale of human intelligence. But the automation of evaluative and predictive tasks has been increasingly successful as a substitute for human monitoring and intervention in domains of computer application involving complex real-world data. Computer systems Computer architecture and microarchitecture Computer architecture, or digital computer organization, is the conceptual design and fundamental operational structure of a computer system. It focuses largely on the way by which the central processing unit performs internally and accesses addresses in memory. Computer engineers study computational logic and design of computer hardware, from individual processor components, microcontrollers, personal computers to supercomputers and embedded systems. The term "architecture" in computer literature can be traced to the work of Lyle R. Johnson and Frederick P. Brooks Jr., members of the Machine Organization department in IBM's main research center in 1959. Concurrent, parallel and distributed computing Concurrency is a property of systems in which several computations are executing simultaneously, and potentially interacting with each other. A number of mathematical models have been developed for general concurrent computation including Petri nets, process calculi and the parallel random access machine model. When multiple computers are connected in a network while using concurrency, this is known as a distributed system. Computers within that distributed system have their own private memory, and information can be exchanged to achieve common goals. Computer networks This branch of computer science aims studies the construction and behavior of computer networks. It addresses their performance, resilience, security, scalability, and cost-effectiveness, along with the variety of services they can provide. Computer security and cryptography Computer security is a branch of computer technology with the objective of protecting information from unauthorized access, disruption, or modification while maintaining the accessibility and usability of the system for its intended users. Historical cryptography is the art of writing and deciphering secret messages. Modern cryptography is the scientific study of problems relating to distributed computations that can be attacked. Technologies studied in modern cryptography include symmetric and asymmetric encryption, digital signatures, cryptographic hash functions, key-agreement protocols, blockchain, zero-knowledge proofs, and garbled circuits. Databases and data mining A database is intended to organize, store, and retrieve large amounts of data easily. Digital databases are managed using database management systems to store, create, maintain, and search data, through database models and query languages. Data mining is a process of discovering patterns in large data sets. Discoveries The philosopher of computing Bill Rapaport noted three Great Insights of Computer Science: Gottfried Wilhelm Leibniz's, George Boole's, Alan Turing's, Claude Shannon's, and Samuel Morse's insight: there are only two objects that a computer has to deal with in order to represent "anything". All the information about any computable problem can be represented using only 0 and 1 (or any other bistable pair that can flip-flop between two easily distinguishable states, such as "on/off", "magnetized/de-magnetized", "high-voltage/low-voltage", etc.). Alan Turing's insight: there are only five actions that a computer has to perform in order to do "anything". Every algorithm can be expressed in a language for a computer consisting of only five basic instructions: move left one location; move right one location; read symbol at current location; print 0 at current location; print 1 at current location. Corrado Böhm and Giuseppe Jacopini's insight: there are only three ways of combining these actions (into more complex ones) that are needed in order for a computer to do "anything". Only three rules are needed to combine any set of basic instructions into more complex ones: sequence: first do this, then do that; selection: IF such-and-such is the case, THEN do this, ELSE do that; repetition: WHILE such-and-such is the case, DO this. The three rules of Boehm's and Jacopini's insight can be further simplified with the use of goto (which means it is more elementary than structured programming). Programming paradigms Programming languages can be used to accomplish different tasks in different ways. Common programming paradigms include: Functional programming, a style of building the structure and elements of computer programs that treats computation as the evaluation of mathematical functions and avoids state and mutable data. It is a declarative programming paradigm, which means programming is done with expressions or declarations instead of statements. Imperative programming, a programming paradigm that uses statements that change a program's state. In much the same way that the imperative mood in natural languages expresses commands, an imperative program consists of commands for the computer to perform. Imperative programming focuses on describing how a program operates. Object-oriented programming, a programming paradigm based on the concept of "objects", which may contain data, in the form of fields, often known as attributes; and code, in the form of procedures, often known as methods. A feature of objects is that an object's procedures can access and often modify the data fields of the object with which they are associated. Thus object-oriented computer programs are made out of objects that interact with one another. Service-oriented programming, a programming paradigm that uses "services" as the unit of computer work, to design and implement integrated business applications and mission critical software programs. Many languages offer support for multiple paradigms, making the distinction more a matter of style than of technical capabilities. Research Conferences are important events for computer science research. During these conferences, researchers from the public and private sectors present their recent work and meet. Unlike in most other academic fields, in computer science, the prestige of conference papers is greater than that of journal publications. One proposed explanation for this is the quick development of this relatively new field requires rapid review and distribution of results, a task better handled by conferences than by journals. See also Notes References Further reading External links DBLP Computer Science Bibliography Association for Computing Machinery Institute of Electrical and Electronics Engineers

Artificial intelligence (AI) is the capability of computational systems to perform tasks typically associated with human intelligence, such as learning, reasoning, problem-solving, perception, and decision-making. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals. High-profile applications of AI include advanced web search engines (e.g., Google Search); recommendation systems (used by YouTube, Amazon, and Netflix); virtual assistants (e.g., Google Assistant, Siri, and Alexa); autonomous vehicles (e.g., Waymo); generative and creative tools (e.g., language models and AI art); and superhuman play and analysis in strategy games (e.g., chess and Go). However, many AI applications are not perceived as AI: "A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore." Various subfields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include learning, reasoning, knowledge representation, planning, natural language processing, perception, and support for robotics. To reach these goals, AI researchers have adapted and integrated a wide range of techniques, including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, operations research, and economics. AI also draws upon psychology, linguistics, philosophy, neuroscience, and other fields. Some companies, such as OpenAI, Google DeepMind and Meta, aim to create artificial general intelligence (AGI)—AI that can complete virtually any cognitive task at least as well as a human. Artificial intelligence was founded as an academic discipline in 1956, and the field went through multiple cycles of optimism throughout its history, followed by periods of disappointment and loss of funding, known as AI winters. Funding and interest vastly increased after 2012 when graphics processing units started being used to accelerate neural networks and deep learning outperformed previous AI techniques. This growth accelerated further after 2017 with the transformer architecture. In the 2020s, an ongoing period of rapid progress in advanced generative AI became known as the AI boom. Generative AI's ability to create and modify content has led to several unintended consequences and harms, which has raised ethical concerns about AI's long-term effects and potential existential risks, prompting discussions about regulatory policies to ensure the safety and benefits of the technology. Goals The general problem of simulating (or creating) intelligence has been broken into subproblems. These consist of particular traits or capabilities that researchers expect an intelligent system to display. The traits described below have received the most attention and cover the scope of AI research. Reasoning and problem-solving Early researchers developed algorithms that imitated step-by-step reasoning that humans use when they solve puzzles or make logical deductions. By the late 1980s and 1990s, methods were developed for dealing with uncertain or incomplete information, employing concepts from probability and economics. Many of these algorithms are insufficient for solving large reasoning problems because they experience a "combinatorial explosion": They become exponentially slower as the problems grow. Even humans rarely use the step-by-step deduction that early AI research could model. They solve most of their problems using fast, intuitive judgments. Accurate and efficient reasoning is an unsolved problem. Knowledge representation Knowledge representation and knowledge engineering allow AI programs to answer questions intelligently and make deductions about real-world facts. Formal knowledge representations are used in content-based indexing and retrieval, scene interpretation, clinical decision support, knowledge discovery (mining "interesting" and actionable inferences from large databases), and other areas. A knowledge base is a body of knowledge represented in a form that can be used by a program. An ontology is the set of objects, relations, concepts, and properties used by a particular domain of knowledge. Knowledge bases need to represent things such as objects, properties, categories, and relations between objects; situations, events, states, and time; causes and effects; knowledge about knowledge (what we know about what other people know); default reasoning (things that humans assume are true until they are told differently and will remain true even when other facts are changing); and many other aspects and domains of knowledge. Among the most difficult problems in knowledge representation are the breadth of commonsense knowledge (the set of atomic facts that the average person knows is enormous); and the sub-symbolic form of most commonsense knowledge (much of what people know is not represented as "facts" or "statements" that they could express verbally). There is also the difficulty of knowledge acquisition, the problem of obtaining knowledge for AI applications. Planning and decision-making An "agent" is anything that perceives and takes actions in the world. A rational agent has goals or preferences and takes actions to make them happen. In automated planning, the agent has a specific goal. In automated decision-making, the agent has preferences—there are some situations it would prefer to be in, and some situations it is trying to avoid. The decision-making agent assigns a number to each situation (called the "utility") that measures how much the agent prefers it. For each possible action, it can calculate the "expected utility": the utility of all possible outcomes of the action, weighted by the probability that the outcome will occur. It can then choose the action with the maximum expected utility. In classical planning, the agent knows exactly what the effect of any action will be. In most real-world problems, however, the agent may not be certain about the situation they are in (it is "unknown" or "unobservable") and it may not know for certain what will happen after each possible action (it is not "deterministic"). It must choose an action by making a probabilistic guess and then reassess the situation to see if the action worked. In some problems, the agent's preferences may be uncertain, especially if there are other agents or humans involved. These can be learned (e.g., with inverse reinforcement learning), or the agent can seek information to improve its preferences. Information value theory can be used to weigh the value of exploratory or experimental actions. The space of possible future actions and situations is typically intractably large, so the agents must take actions and evaluate situations while being uncertain of what the outcome will be. A Markov decision process has a transition model that describes the probability that a particular action will change the state in a particular way and a reward function that supplies the utility of each state and the cost of each action. A policy associates a decision with each possible state. The policy could be calculated (e.g., by iteration), be heuristic, or it can be learned. Game theory describes the rational behavior of multiple interacting agents and is used in AI programs that make decisions that involve other agents. Learning Machine learning is the study of programs that can improve their performance on a given task automatically. It has been a part of AI from the beginning. There are several kinds of machine learning. Unsupervised learning analyzes a stream of data and finds patterns and makes predictions without any other guidance. Supervised learning requires labeling the training data with the expected answers, and comes in two main varieties: classification (where the program must learn to predict what category the input belongs in) and regression (where the program must deduce a numeric function based on numeric input). In reinforcement learning, the agent is rewarded for good responses and punished for bad ones. The agent learns to choose responses that are classified as "good". Transfer learning is when the knowledge gained from one problem is applied to a new problem. Deep learning is a type of machine learning that runs inputs through biologically inspired artificial neural networks for all of these types of learning. Computational learning theory can assess learners by computational complexity, by sample complexity (how much data is required), or by other notions of optimization. Natural language processing Natural language processing (NLP) allows programs to read, write and communicate in human languages. Specific problems include speech recognition, speech synthesis, machine translation, information extraction, information retrieval and question answering. Early work, based on Noam Chomsky's generative grammar and semantic networks, had difficulty with word-sense disambiguation unless restricted to small domains called "micro-worlds" (due to the common sense knowledge problem). Margaret Masterman believed that it was meaning and not grammar that was the key to understanding languages, and that thesauri and not dictionaries should be the basis of computational language structure. Modern deep learning techniques for NLP include word embedding (representing words, typically as vectors encoding their meaning), transformers (a deep learning architecture using an attention mechanism), and others. In 2019, generative pre-trained transformer (or "GPT") language models began to generate coherent text, and by 2023, these models were able to get human-level scores on the bar exam, SAT test, GRE test, and many other real-world applications. Perception Machine perception is the ability to use input from sensors (such as cameras, microphones, wireless signals, active lidar, sonar, radar, and tactile sensors) to deduce aspects of the world. Computer vision is the ability to analyze visual input. The field includes speech recognition, image classification, facial recognition, object recognition, object tracking, and robotic perception. Social intelligence Affective computing is a field that comprises systems that recognize, interpret, process, or simulate human feeling, emotion, and mood. For example, some virtual assistants are programmed to speak conversationally or even to banter humorously; it makes them appear more sensitive to the emotional dynamics of human interaction, or to otherwise facilitate human–computer interaction. However, this tends to give naïve users an unrealistic conception of the intelligence of existing computer agents. Moderate successes related to affective computing include textual sentiment analysis and, more recently, multimodal sentiment analysis, wherein AI classifies the effects displayed by a videotaped subject. General intelligence A machine with artificial general intelligence would be able to solve a wide variety of problems with breadth and versatility similar to human intelligence. Techniques AI research uses a wide variety of techniques to accomplish the goals above. Search and optimization AI can solve many problems by intelligently searching through many possible solutions. There are two very different kinds of search used in AI: state space search and local search. State space search State space search searches through a tree of possible states to try to find a goal state. For example, planning algorithms search through trees of goals and subgoals, attempting to find a path to a target goal, a process called means-ends analysis. Simple exhaustive searches are rarely sufficient for most real-world problems: the search space (the number of places to search) quickly grows to astronomical numbers. The result is a search that is too slow or never completes. "Heuristics" or "rules of thumb" can help prioritize choices that are more likely to reach a goal. Adversarial search is used for game-playing programs, such as chess or Go. It searches through a tree of possible moves and countermoves, looking for a winning position. Local search Local search uses mathematical optimization to find a solution to a problem. It begins with some form of guess and refines it incrementally. Gradient descent is a type of local search that optimizes a set of numerical parameters by incrementally adjusting them to minimize a loss function. Variants of gradient descent are commonly used to train neural networks, through the backpropagation algorithm. Another type of local search is evolutionary computation, which aims to iteratively improve a set of candidate solutions by "mutating" and "recombining" them, selecting only the fittest to survive each generation. Distributed search processes can coordinate via swarm intelligence algorithms. Two popular swarm algorithms used in search are particle swarm optimization (inspired by bird flocking) and ant colony optimization (inspired by ant trails). Logic Formal logic is used for reasoning and knowledge representation. Formal logic comes in two main forms: propositional logic (which operates on statements that are true or false and uses logical connectives such as "and", "or", "not" and "implies") and predicate logic (which also operates on objects, predicates and relations and uses quantifiers such as "Every X is a Y" and "There are some Xs that are Ys"). Deductive reasoning in logic is the process of proving a new statement (conclusion) from other statements that are given and assumed to be true (the premises). Proofs can be structured as proof trees, in which nodes are labelled by sentences, and children nodes are connected to parent nodes by inference rules. Given a problem and a set of premises, problem-solving reduces to searching for a proof tree whose root node is labelled by a solution of the problem and whose leaf nodes are labelled by premises or axioms. In the case of Horn clauses, problem-solving search can be performed by reasoning forwards from the premises or backwards from the problem. In the more general case of the clausal form of first-order logic, resolution is a single, axiom-free rule of inference, in which a problem is solved by proving a contradiction from premises that include the negation of the problem to be solved. Inference in both Horn clause logic and first-order logic is undecidable, and therefore intractable. However, backward reasoning with Horn clauses, which underpins computation in the logic programming language Prolog, is Turing complete. Moreover, its efficiency is competitive with computation in other symbolic programming languages. Fuzzy logic assigns a "degree of truth" between 0 and 1. It can therefore handle propositions that are vague and partially true. Non-monotonic logics, including logic programming with negation as failure, are designed to handle default reasoning. Other specialized versions of logic have been developed to describe many complex domains. Probabilistic methods for uncertain reasoning Many problems in AI (including reasoning, planning, learning, perception, and robotics) require the agent to operate with incomplete or uncertain information. AI researchers have devised a number of tools to solve these problems using methods from probability theory and economics. Precise mathematical tools have been developed that analyze how an agent can make choices and plan, using decision theory, decision analysis, and information value theory. These tools include models such as Markov decision processes, dynamic decision networks, game theory and mechanism design. Bayesian networks are a tool that can be used for reasoning (using the Bayesian inference algorithm), learning (using the expectation–maximization algorithm), planning (using decision networks) and perception (using dynamic Bayesian networks). Probabilistic algorithms can also be used for filtering, prediction, smoothing, and finding explanations for streams of data, thus helping perception systems analyze processes that occur over time (e.g., hidden Markov models or Kalman filters). Classifiers and statistical learning methods The simplest AI applications can be divided into two types: classifiers (e.g., "if shiny then diamond"), on one hand, and controllers (e.g., "if diamond then pick up"), on the other hand. Classifiers are functions that use pattern matching to determine the closest match. They can be fine-tuned based on chosen examples using supervised learning. Each pattern (also called an "observation") is labeled with a certain predefined class. All the observations combined with their class labels are known as a data set. When a new observation is received, that observation is classified based on previous experience. There are many kinds of classifiers in use. The decision tree is the simplest and most widely used symbolic machine learning algorithm. K-nearest neighbor algorithm was the most widely used analogical AI until the mid-1990s, and Kernel methods such as the support vector machine (SVM) displaced k-nearest neighbor in the 1990s. The naive Bayes classifier is reportedly the "most widely used learner" at Google, due in part to its scalability. Neural networks are also used as classifiers. Artificial neural networks An artificial neural network is based on a collection of nodes also known as artificial neurons, which loosely model the neurons in a biological brain. It is trained to recognise patterns; once trained, it can recognise those patterns in fresh data. There is an input, at least one hidden layer of nodes and an output. Each node applies a function and once the weight crosses its specified threshold, the data is transmitted to the next layer. A network is typically called a deep neural network if it has at least 2 hidden layers. Learning algorithms for neural networks use local search to choose the weights that will get the right output for each input during training. The most common training technique is the backpropagation algorithm. Neural networks learn to model complex relationships between inputs and outputs and find patterns in data. In theory, a neural network can learn any function. In feedforward neural networks the signal passes in only one direction. The term perceptron typically refers to a single-layer neural network. In contrast, deep learning uses many layers. Recurrent neural networks (RNNs) feed the output signal back into the input, which allows short-term memories of previous input events. Long short-term memory networks (LSTMs) are recurrent neural networks that better preserve longterm dependencies and are less sensitive to the vanishing gradient problem. Convolutional neural networks (CNNs) use layers of kernels to more efficiently process local patterns. This local processing is especially important in image processing, where the early CNN layers typically identify simple local patterns such as edges and curves, with subsequent layers detecting more complex patterns like textures, and eventually whole objects. Deep learning Deep learning uses several layers of neurons between the network's inputs and outputs. The multiple layers can progressively extract higher-level features from the raw input. For example, in image processing, lower layers may identify edges, while higher layers may identify the concepts relevant to a human such as digits, letters, or faces. Deep learning has profoundly improved the performance of programs in many important subfields of artificial intelligence, including computer vision, speech recognition, natural language processing, image classification, and others. The reason that deep learning performs so well in so many applications is not known as of 2021. The sudden success of deep learning in 2012–2015 did not occur because of some new discovery or theoretical breakthrough (deep neural networks and backpropagation had been described by many people, as far back as the 1950s) but because of two factors: the incredible increase in computer power (including the hundred-fold increase in speed by switching to GPUs) and the availability of vast amounts of training data, especially the giant curated datasets used for benchmark testing, such as ImageNet. GPT Generative pre-trained transformers (GPT) are large language models (LLMs) that generate text based on the semantic relationships between words in sentences. Text-based GPT models are pre-trained on a large corpus of text that can be from the Internet. The pretraining consists of predicting the next token (a token being usually a word, subword, or punctuation). Throughout this pretraining, GPT models accumulate knowledge about the world and can then generate human-like text by repeatedly predicting the next token. Typically, a subsequent training phase makes the model more truthful, useful, and harmless, usually with a technique called reinforcement learning from human feedback (RLHF). Current GPT models are prone to generating falsehoods called "hallucinations". These can be reduced with RLHF and quality data, but the problem has been getting worse for reasoning systems. Such systems are used in chatbots, which allow people to ask a question or request a task in simple text. Current models and services include ChatGPT, Claude, Gemini, Copilot, and Meta AI. Multimodal GPT models can process different types of data (modalities) such as images, videos, sound, and text. Hardware and software In the late 2010s, graphics processing units (GPUs) that were increasingly designed with AI-specific enhancements and used with specialized TensorFlow software had replaced previously used central processing unit (CPUs) as the dominant means for large-scale (commercial and academic) machine learning models' training. Specialized programming languages such as Prolog were used in early AI research, but general-purpose programming languages like Python have become predominant. The transistor density in integrated circuits has been observed to roughly double every 18 months—a trend known as Moore's law, named after the Intel co-founder Gordon Moore, who first identified it. Improvements in GPUs have been even faster, a trend sometimes called Huang's law, named after Nvidia co-founder and CEO Jensen Huang. Applications AI and machine learning technology is used in most of the essential applications of the 2020s, including: search engines (such as Google Search), targeting online advertisements, recommendation systems (offered by Netflix, YouTube or Amazon), driving internet traffic, targeted advertising (AdSense, Facebook), virtual assistants (such as Siri or Alexa), autonomous vehicles (including drones, ADAS and self-driving cars), automatic language translation (Microsoft Translator, Google Translate), facial recognition (Apple's FaceID or Microsoft's DeepFace and Google's FaceNet) and image labeling (used by Facebook, Apple's Photos and TikTok). The deployment of AI may be overseen by a chief automation officer (CAO). Health and medicine The application of AI in medicine and medical research has the potential to increase patient care and quality of life. Through the lens of the Hippocratic Oath, medical professionals are ethically compelled to use AI, if applications can more accurately diagnose and treat patients. For medical research, AI is an important tool for processing and integrating big data. This is particularly important for organoid and tissue engineering development which use microscopy imaging as a key technique in fabrication. It has been suggested that AI can overcome discrepancies in funding allocated to different fields of research. New AI tools can deepen the understanding of biomedically relevant pathways. For example, AlphaFold 2 (2021) demonstrated the ability to approximate, in hours rather than months, the 3D structure of a protein. In 2023, it was reported that AI-guided drug discovery helped find a class of antibiotics capable of killing two different types of drug-resistant bacteria. In 2024, researchers used machine learning to accelerate the search for Parkinson's disease drug treatments. Their aim was to identify compounds that block the clumping, or aggregation, of alpha-synuclein (the protein that characterises Parkinson's disease). They were able to speed up the initial screening process ten-fold and reduce the cost by a thousand-fold. Games Game playing programs have been used since the 1950s to demonstrate and test AI's most advanced techniques. Deep Blue became the first computer chess-playing system to beat a reigning world chess champion, Garry Kasparov, on 11 May 1997. In 2011, in a Jeopardy! quiz show exhibition match, IBM's question answering system, Watson, defeated the two greatest Jeopardy! champions, Brad Rutter and Ken Jennings, by a significant margin. In March 2016, AlphaGo won 4 out of 5 games of Go in a match with Go champion Lee Sedol, becoming the first computer Go-playing system to beat a professional Go player without handicaps. Then, in 2017, it defeated Ke Jie, who was the best Go player in the world. Other programs handle imperfect-information games, such as the poker-playing program Pluribus. DeepMind developed increasingly generalistic reinforcement learning models, such as with MuZero, which could be trained to play chess, Go, or Atari games. In 2019, DeepMind's AlphaStar achieved grandmaster level in StarCraft II, a particularly challenging real-time strategy game that involves incomplete knowledge of what happens on the map. In 2021, an AI agent competed in a PlayStation Gran Turismo competition, winning against four of the world's best Gran Turismo drivers using deep reinforcement learning. In 2024, Google DeepMind introduced SIMA, a type of AI capable of autonomously playing nine previously unseen open-world video games by observing screen output, as well as executing short, specific tasks in response to natural language instructions. Mathematics Large language models, such as GPT-4, Gemini, Claude, Llama or Mistral, are increasingly used in mathematics. These probabilistic models are versatile, but can also produce wrong answers in the form of hallucinations. They sometimes need a large database of mathematical problems to learn from, but also methods such as supervised fine-tuning or trained classifiers with human-annotated data to improve answers for new problems and learn from corrections. A February 2024 study showed that the performance of some language models for reasoning capabilities in solving math problems not included in their training data was low, even for problems with only minor deviations from trained data. One technique to improve their performance involves training the models to produce correct reasoning steps, rather than just the correct result. The Alibaba Group developed a version of its Qwen models called Qwen2-Math, that achieved state-of-the-art performance on several mathematical benchmarks, including 84% accuracy on the MATH dataset of competition mathematics problems. In January 2025, Microsoft proposed the technique rStar-Math that leverages Monte Carlo tree search and step-by-step reasoning, enabling a relatively small language model like Qwen-7B to solve 53% of the AIME 2024 and 90% of the MATH benchmark problems. Alternatively, dedicated models for mathematical problem solving with higher precision for the outcome including proof of theorems have been developed such as AlphaTensor, AlphaGeometry, AlphaProof and AlphaEvolve all from Google DeepMind, Llemma from EleutherAI or Julius. When natural language is used to describe mathematical problems, converters can transform such prompts into a formal language such as Lean to define mathematical tasks. The experimental model Gemini Deep Think accepts natural language prompts directly and achieved gold medal results in the International Math Olympiad of 2025. Some models have been developed to solve challenging problems and reach good results in benchmark tests, others to serve as educational tools in mathematics. Topological deep learning integrates various topological approaches. Finance Finance is one of the fastest growing sectors where applied AI tools are being deployed: from retail online banking to investment advice and insurance, where automated "robot advisers" have been in use for some years. According to Nicolas Firzli, director of the World Pensions & Investments Forum, it may be too early to see the emergence of highly innovative AI-informed financial products and services. He argues that "the deployment of AI tools will simply further automatise things: destroying tens of thousands of jobs in banking, financial planning, and pension advice in the process, but I'm not sure it will unleash a new wave of [e.g., sophisticated] pension innovation." Military Various countries are deploying AI military applications. The main applications enhance command and control, communications, sensors, integration and interoperability. Research is targeting intelligence collection and analysis, logistics, cyber operations, information operations, and semiautonomous and autonomous vehicles. AI technologies enable coordination of sensors and effectors, threat detection and identification, marking of enemy positions, target acquisition, coordination and deconfliction of distributed Joint Fires between networked combat vehicles, both human-operated and autonomous. AI has been used in military operations in Iraq, Syria, Israel and Ukraine. Generative AI Agents AI agents are software entities designed to perceive their environment, make decisions, and take actions autonomously to achieve specific goals. These agents can interact with users, their environment, or other agents. AI agents are used in various applications, including virtual assistants, chatbots, autonomous vehicles, game-playing systems, and industrial robotics. AI agents operate within the constraints of their programming, available computational resources, and hardware limitations. This means they are restricted to performing tasks within their defined scope and have finite memory and processing capabilities. In real-world applications, AI agents often face time constraints for decision-making and action execution. Many AI agents incorporate learning algorithms, enabling them to improve their performance over time through experience or training. Using machine learning, AI agents can adapt to new situations and optimise their behaviour for their designated tasks. Web search Microsoft introduced Copilot Search in February 2023 under the name Bing Chat, as a built-in feature for Microsoft Edge and Bing mobile app. Copilot Search provides AI-generated summaries and step-by-step reasoning based of information from web publishers, ranked in Bing Search. For safety, Copilot uses AI-based classifiers and filters to reduce potentially harmful content. Google officially pushed its AI Search at its Google I/O event on May 20, 2025. It keeps people looking at Google instead of clicking on a search result. AI Overviews uses Gemini 2.5 to provide contextual answers to user queries based on web content. Sexuality Applications of AI in this domain include AI-enabled menstruation and fertility trackers that analyze user data to offer predictions, AI-integrated sex toys (e.g., teledildonics), AI-generated sexual education content, and AI agents that simulate sexual and romantic partners (e.g., Replika). AI is also used for the production of non-consensual deepfake pornography, raising significant ethical and legal concerns. AI technologies have also been used to attempt to identify online gender-based violence and online sexual grooming of minors. Other industry-specific tasks There are also thousands of successful AI applications used to solve specific problems for specific industries or institutions. In a 2017 survey, one in five companies reported having incorporated "AI" in some offerings or processes. A few examples are energy storage, medical diagnosis, military logistics, applications that predict the result of judicial decisions, foreign policy, or supply chain management. AI applications for evacuation and disaster management are growing. AI has been used to investigate patterns in large-scale and small-scale evacuations using historical data from GPS, videos or social media. Furthermore, AI can provide real-time information on the evacuation conditions. In agriculture, AI has helped farmers to increase yield and identify areas that need irrigation, fertilization, pesticide treatments. Agronomists use AI to conduct research and development. AI has been used to predict the ripening time for crops such as tomatoes, monitor soil moisture, operate agricultural robots, conduct predictive analytics, classify livestock pig call emotions, automate greenhouses, detect diseases and pests, and save water. Artificial intelligence is used in astronomy to analyze increasing amounts of available data and applications, mainly for "classification, regression, clustering, forecasting, generation, discovery, and the development of new scientific insights." For example, it is used for discovering exoplanets, forecasting solar activity, and distinguishing between signals and instrumental effects in gravitational wave astronomy. Additionally, it could be used for activities in space, such as space exploration, including the analysis of data from space missions, real-time science decisions of spacecraft, space debris avoidance, and more autonomous operation. During the 2024 Indian elections, US$50 million was spent on authorized AI-generated content, notably by creating deepfakes of allied (including sometimes deceased) politicians to better engage with voters, and by translating speeches to various local languages. Ethics AI has potential benefits and potential risks. AI may be able to advance science and find solutions for serious problems: Demis Hassabis of DeepMind hopes to "solve intelligence, and then use that to solve everything else". However, as the use of AI has become widespread, several unintended consequences and risks have been identified. In-production systems can sometimes not factor ethics and bias into their AI training processes, especially when the AI algorithms are inherently unexplainable in deep learning. Risks and harm Privacy and copyright Machine learning algorithms require large amounts of data. The techniques used to acquire this data have raised concerns about privacy, surveillance and copyright. AI-powered devices and services, such as virtual assistants and IoT products, continuously collect personal information, raising concerns about intrusive data gathering and unauthorized access by third parties. The loss of privacy is further exacerbated by AI's ability to process and combine vast amounts of data, potentially leading to a surveillance society where individual activities are constantly monitored and analyzed without adequate safeguards or transparency. Sensitive user data collected may include online activity records, geolocation data, video, or audio. For example, in order to build speech recognition algorithms, Amazon has recorded millions of private conversations and allowed temporary workers to listen to and transcribe some of them. Opinions about this widespread surveillance range from those who see it as a necessary evil to those for whom it is clearly unethical and a violation of the right to privacy. AI developers argue that this is the only way to deliver valuable applications and have developed several techniques that attempt to preserve privacy while still obtaining the data, such as data aggregation, de-identification and differential privacy. Since 2016, some privacy experts, such as Cynthia Dwork, have begun to view privacy in terms of fairness. Brian Christian wrote that experts have pivoted "from the question of 'what they know' to the question of 'what they're doing with it'." Generative AI is often trained on unlicensed copyrighted works, including in domains such as images or computer code; the output is then used under the rationale of "fair use". Experts disagree about how well and under what circumstances this rationale will hold up in courts of law; relevant factors may include "the purpose and character of the use of the copyrighted work" and "the effect upon the potential market for the copyrighted work". Website owners who do not wish to have their content scraped can indicate it in a "robots.txt" file. In 2023, leading authors (including John Grisham and Jonathan Franzen) sued AI companies for using their work to train generative AI. Another discussed approach is to envision a separate sui generis system of protection for creations generated by AI to ensure fair attribution and compensation for human authors. Dominance by tech giants The commercial AI scene is dominated by Big Tech companies such as Alphabet Inc., Amazon, Apple Inc., Meta Platforms, and Microsoft. Some of these players already own the vast majority of existing cloud infrastructure and computing power from data centers, allowing them to entrench further in the marketplace. Power needs and environmental impacts In January 2024, the International Energy Agency (IEA) released Electricity 2024, Analysis and Forecast to 2026, forecasting electric power use. This is the first IEA report to make projections for data centers and power consumption for artificial intelligence and cryptocurrency. The report states that power demand for these uses might double by 2026, with additional electric power usage equal to electricity used by the whole Japanese nation. Prodigious power consumption by AI is responsible for the growth of fossil fuel use, and might delay closings of obsolete, carbon-emitting coal energy facilities. There is a feverish rise in the construction of data centers throughout the US, making large technology firms (e.g., Microsoft, Meta, Google, Amazon) into voracious consumers of electric power. Projected electric consumption is so immense that there is concern that it will be fulfilled no matter the source. A ChatGPT search involves the use of 10 times the electrical energy as a Google search. The large firms are in haste to find power sources – from nuclear energy to geothermal to fusion. The tech firms argue that – in the long view – AI will be eventually kinder to the environment, but they need the energy now. AI makes the power grid more efficient and "intelligent", will assist in the growth of nuclear power, and track overall carbon emissions, according to technology firms. A 2024 Goldman Sachs Research Paper, AI Data Centers and the Coming US Power Demand Surge, found "US power demand (is) likely to experience growth not seen in a generation...." and forecasts that, by 2030, US data centers will consume 8% of US power, as opposed to 3% in 2022, presaging growth for the electrical power generation industry by a variety of means. Data centers' need for more and more electrical power is such that they might max out the electrical grid. The Big Tech companies counter that AI can be used to maximize the utilization of the grid by all. In 2024, the Wall Street Journal reported that big AI companies have begun negotiations with the US nuclear power providers to provide electricity to the data centers. In March 2024 Amazon purchased a Pennsylvania nuclear-powered data center for US$650 million. Nvidia CEO Jensen Huang said nuclear power is a good option for the data centers. In September 2024, Microsoft announced an agreement with Constellation Energy to re-open the Three Mile Island nuclear power plant to provide Microsoft with 100% of all electric power produced by the plant for 20 years. Reopening the plant, which suffered a partial nuclear meltdown of its Unit 2 reactor in 1979, will require Constellation to get through strict regulatory processes which will include extensive safety scrutiny from the US Nuclear Regulatory Commission. If approved (this will be the first ever US re-commissioning of a nuclear plant), over 835 megawatts of power – enough for 800,000 homes – of energy will be produced. The cost for re-opening and upgrading is estimated at US$1.6 billion and is dependent on tax breaks for nuclear power contained in the 2022 US Inflation Reduction Act. The US government and the state of Michigan are investing almost US$2 billion to reopen the Palisades Nuclear reactor on Lake Michigan. Closed since 2022, the plant is planned to be reopened in October 2025. The Three Mile Island facility will be renamed the Crane Clean Energy Center after Chris Crane, a nuclear proponent and former CEO of Exelon who was responsible for Exelon's spinoff of Constellation. After the last approval in September 2023, Taiwan suspended the approval of data centers north of Taoyuan with a capacity of more than 5 MW in 2024, due to power supply shortages. Taiwan aims to phase out nuclear power by 2025. On the other hand, Singapore imposed a ban on the opening of data centers in 2019 due to electric power, but in 2022, lifted this ban. Although most nuclear plants in Japan have been shut down after the 2011 Fukushima nuclear accident, according to an October 2024 Bloomberg article in Japanese, cloud gaming services company Ubitus, in which Nvidia has a stake, is looking for land in Japan near a nuclear power plant for a new data center for generative AI. Ubitus CEO Wesley Kuo said nuclear power plants are the most efficient, cheap and stable power for AI. On 1 November 2024, the Federal Energy Regulatory Commission (FERC) rejected an application submitted by Talen Energy for approval to supply some electricity from the nuclear power station Susquehanna to Amazon's data center. According to the Commission Chairman Willie L. Phillips, it is a burden on the electricity grid as well as a significant cost shifting concern to households and other business sectors. In 2025, a report prepared by the International Energy Agency estimated the greenhouse gas emissions from the energy consumption of AI at 180 million tons. By 2035, these emissions could rise to 300–500 million tonnes depending on what measures will be taken. This is below 1.5% of the energy sector emissions. The emissions reduction potential of AI was estimated at 5% of the energy sector emissions, but rebound effects (for example if people switch from public transport to autonomous cars) can reduce it. Misinformation YouTube, Facebook and others use recommender systems to guide users to more content. These AI programs were given the goal of maximizing user engagement (that is, the only goal was to keep people watching). The AI learned that users tended to choose misinformation, conspiracy theories, and extreme partisan content, and, to keep them watching, the AI recommended more of it. Users also tended to watch more content on the same subject, so the AI led people into filter bubbles where they received multiple versions of the same misinformation. This convinced many users that the misinformation was true, and ultimately undermined trust in institutions, the media and the government. The AI program had correctly learned to maximize its goal, but the result was harmful to society. After the U.S. election in 2016, major technology companies took some steps to mitigate the problem. In the early 2020s, generative AI began to create images, audio, and texts that are virtually indistinguishable from real photographs, recordings, or human writing, while realistic AI-generated videos became feasible in the mid-2020s. It is possible for bad actors to use this technology to create massive amounts of misinformation or propaganda; one such potential malicious use is deepfakes for computational propaganda. AI pioneer Geoffrey Hinton expressed concern about AI enabling "authoritarian leaders to manipulate their electorates" on a large scale, among other risks. AI researchers at Microsoft, OpenAI, universities and other organisations have suggested using "personhood credentials" as a way to overcome online deception enabled by AI models. Algorithmic bias and fairness Machine learning applications will be biased if they learn from biased data. The developers may not be aware that the bias exists. Bias can be introduced by the way training data is selected and by the way a model is deployed. If a biased algorithm is used to make decisions that can seriously harm people (as it can in medicine, finance, recruitment, housing or policing) then the algorithm may cause discrimination. The field of fairness studies how to prevent harms from algorithmic biases. On June 28, 2015, Google Photos's new image labeling feature mistakenly identified Jacky Alcine and a friend as "gorillas" because they were black. The system was trained on a dataset that contained very few images of black people, a problem called "sample size disparity". Google "fixed" this problem by preventing the system from labelling anything as a "gorilla". Eight years later, in 2023, Google Photos still could not identify a gorilla, and neither could similar products from Apple, Facebook, Microsoft and Amazon. COMPAS is a commercial program widely used by U.S. courts to assess the likelihood of a defendant becoming a recidivist. In 2016, Julia Angwin at ProPublica discovered that COMPAS exhibited racial bias, despite the fact that the program was not told the races of the defendants. Although the error rate for both whites and blacks was calibrated equal at exactly 61%, the errors for each race were different—the system consistently overestimated the chance that a black person would re-offend and would underestimate the chance that a white person would not re-offend. In 2017, several researchers showed that it was mathematically impossible for COMPAS to accommodate all possible measures of fairness when the base rates of re-offense were different for whites and blacks in the data. A program can make biased decisions even if the data does not explicitly mention a problematic feature (such as "race" or "gender"). The feature will correlate with other features (like "address", "shopping history" or "first name"), and the program will make the same decisions based on these features as it would on "race" or "gender". Moritz Hardt said "the most robust fact in this research area is that fairness through blindness doesn't work." Criticism of COMPAS highlighted that machine learning models are designed to make "predictions" that are only valid if we assume that the future will resemble the past. If they are trained on data that includes the results of racist decisions in the past, machine learning models must predict that racist decisions will be made in the future. If an application then uses these predictions as recommendations, some of these "recommendations" will likely be racist. Thus, machine learning is not well suited to help make decisions in areas where there is hope that the future will be better than the past. It is descriptive rather than prescriptive. Bias and unfairness may go undetected because the developers are overwhelmingly white and male: among AI engineers, about 4% are black and 20% are women. There are various conflicting definitions and mathematical models of fairness. These notions depend on ethical assumptions, and are influenced by beliefs about society. One broad category is distributive fairness, which focuses on the outcomes, often identifying groups and seeking to compensate for statistical disparities. Representational fairness tries to ensure that AI systems do not reinforce negative stereotypes or render certain groups invisible. Procedural fairness focuses on the decision process rather than the outcome. The most relevant notions of fairness may depend on the context, notably the type of AI application and the stakeholders. The subjectivity in the notions of bias and fairness makes it difficult for companies to operationalize them. Having access to sensitive attributes such as race or gender is also considered by many AI ethicists to be necessary in order to compensate for biases, but it may conflict with anti-discrimination laws. At its 2022 Conference on Fairness, Accountability, and Transparency (ACM FAccT 2022), the Association for Computing Machinery, in Seoul, South Korea, presented and published findings that recommend that until AI and robotics systems are demonstrated to be free of bias mistakes, they are unsafe, and the use of self-learning neural networks trained on vast, unregulated sources of flawed internet data should be curtailed. Lack of transparency Many AI systems are so complex that their designers cannot explain how they reach their decisions. Particularly with deep neural networks, in which there are many non-linear relationships between inputs and outputs. But some popular explainability techniques exist. It is impossible to be certain that a program is operating correctly if no one knows how exactly it works. There have been many cases where a machine learning program passed rigorous tests, but nevertheless learned something different than what the programmers intended. For example, a system that could identify skin diseases better than medical professionals was found to actually have a strong tendency to classify images with a ruler as "cancerous", because pictures of malignancies typically include a ruler to show the scale. Another machine learning system designed to help effectively allocate medical resources was found to classify patients with asthma as being at "low risk" of dying from pneumonia. Having asthma is actually a severe risk factor, but since the patients having asthma would usually get much more medical care, they were relatively unlikely to die according to the training data. The correlation between asthma and low risk of dying from pneumonia was real, but misleading. People who have been harmed by an algorithm's decision have a right to an explanation. Doctors, for example, are expected to clearly and completely explain to their colleagues the reasoning behind any decision they make. Early drafts of the European Union's General Data Protection Regulation in 2016 included an explicit statement that this right exists. Industry experts noted that this is an unsolved problem with no solution in sight. Regulators argued that nevertheless the harm is real: if the problem has no solution, the tools should not be used. DARPA established the XAI ("Explainable Artificial Intelligence") program in 2014 to try to solve these problems. Several approaches aim to address the transparency problem. SHAP enables to visualise the contribution of each feature to the output. LIME can locally approximate a model's outputs with a simpler, interpretable model. Multitask learning provides a large number of outputs in addition to the target classification. These other outputs can help developers deduce what the network has learned. Deconvolution, DeepDream and other generative methods can allow developers to see what different layers of a deep network for computer vision have learned, and produce output that can suggest what the network is learning. For generative pre-trained transformers, Anthropic developed a technique based on dictionary learning that associates patterns of neuron activations with human-understandable concepts. Bad actors and weaponized AI Artificial intelligence provides a number of tools that are useful to bad actors, such as authoritarian governments, terrorists, criminals or rogue states. A lethal autonomous weapon is a machine that locates, selects and engages human targets without human supervision. Widely available AI tools can be used by bad actors to develop inexpensive autonomous weapons and, if produced at scale, they are potentially weapons of mass destruction. Even when used in conventional warfare, they currently cannot reliably choose targets and could potentially kill an innocent person. In 2014, 30 nations (including China) supported a ban on autonomous weapons under the United Nations' Convention on Certain Conventional Weapons, however the United States and others disagreed. By 2015, over fifty countries were reported to be researching battlefield robots. AI tools make it easier for authoritarian governments to efficiently control their citizens in several ways. Face and voice recognition allow widespread surveillance. Machine learning, operating this data, can classify potential enemies of the state and prevent them from hiding. Recommendation systems can precisely target propaganda and misinformation for maximum effect. Deepfakes and generative AI aid in producing misinformation. Advanced AI can make authoritarian centralized decision-making more competitive than liberal and decentralized systems such as markets. It lowers the cost and difficulty of digital warfare and advanced spyware. All these technologies have been available since 2020 or earlier—AI facial recognition systems are already being used for mass surveillance in China. There are many other ways in which AI is expected to help bad actors, some of which can not be foreseen. For example, machine-learning AI is able to design tens of thousands of toxic molecules in a matter of hours. Technological unemployment Economists have frequently highlighted the risks of redundancies from AI, and speculated about unemployment if there is no adequate social policy for full employment. In the past, technology has tended to increase rather than reduce total employment, but economists acknowledge that "we're in uncharted territory" with AI. A survey of economists showed disagreement about whether the increasing use of robots and AI will cause a substantial increase in long-term unemployment, but they generally agree that it could be a net benefit if productivity gains are redistributed. Risk estimates vary; for example, in the 2010s, Michael Osborne and Carl Benedikt Frey estimated 47% of U.S. jobs are at "high risk" of potential automation, while an OECD report classified only 9% of U.S. jobs as "high risk". The methodology of speculating about future employment levels has been criticised as lacking evidential foundation, and for implying that technology, rather than social policy, creates unemployment, as opposed to redundancies. In April 2023, it was reported that 70% of the jobs for Chinese video game illustrators had been eliminated by generative artificial intelligence. Unlike previous waves of automation, many middle-class jobs may be eliminated by artificial intelligence; The Economist stated in 2015 that "the worry that AI could do to white-collar jobs what steam power did to blue-collar ones during the Industrial Revolution" is "worth taking seriously". Jobs at extreme risk range from paralegals to fast food cooks, while job demand is likely to increase for care-related professions ranging from personal healthcare to the clergy. From the early days of the development of artificial intelligence, there have been arguments, for example, those put forward by Joseph Weizenbaum, about whether tasks that can be done by computers actually should be done by them, given the difference between computers and humans, and between quantitative calculation and qualitative, value-based judgement. Existential risk It has been argued AI will become so powerful that humanity may irreversibly lose control of it. This could, as physicist Stephen Hawking stated, "spell the end of the human race". This scenario has been common in science fiction, when a computer or robot suddenly develops a human-like "self-awareness" (or "sentience" or "consciousness") and becomes a malevolent character. These sci-fi scenarios are misleading in several ways. First, AI does not require human-like sentience to be an existential risk. Modern AI programs are given specific goals and use learning and intelligence to achieve them. Philosopher Nick Bostrom argued that if one gives almost any goal to a sufficiently powerful AI, it may choose to destroy humanity to achieve it (he used the example of a paperclip maximizer). Stuart Russell gives the example of household robot that tries to find a way to kill its owner to prevent it from being unplugged, reasoning that "you can't fetch the coffee if you're dead." In order to be safe for humanity, a superintelligence would have to be genuinely aligned with humanity's morality and values so that it is "fundamentally on our side". Second, Yuval Noah Harari argues that AI does not require a robot body or physical control to pose an existential risk. The essential parts of civilization are not physical. Things like ideologies, law, government, money and the economy are built on language; they exist because there are stories that billions of people believe. The current prevalence of misinformation suggests that an AI could use language to convince people to believe anything, even to take actions that are destructive. The opinions amongst experts and industry insiders are mixed, with sizable fractions both concerned and unconcerned by risk from eventual superintelligent AI. Personalities such as Stephen Hawking, Bill Gates, and Elon Musk, as well as AI pioneers such as Yoshua Bengio, Stuart Russell, Demis Hassabis, and Sam Altman, have expressed concerns about existential risk from AI. In May 2023, Geoffrey Hinton announced his resignation from Google in order to be able to "freely speak out about the risks of AI" without "considering how this impacts Google". He notably mentioned risks of an AI takeover, and stressed that in order to avoid the worst outcomes, establishing safety guidelines will require cooperation among those competing in use of AI. In 2023, many leading AI experts endorsed the joint statement that "Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war". Some other researchers were more optimistic. AI pioneer Jürgen Schmidhuber did not sign the joint statement, emphasising that in 95% of all cases, AI research is about making "human lives longer and healthier and easier." While the tools that are now being used to improve lives can also be used by bad actors, "they can also be used against the bad actors." Andrew Ng also argued that "it's a mistake to fall for the doomsday hype on AI—and that regulators who do will only benefit vested interests." Yann LeCun "scoffs at his peers' dystopian scenarios of supercharged misinformation and even, eventually, human extinction." In the early 2010s, experts argued that the risks are too distant in the future to warrant research or that humans will be valuable from the perspective of a superintelligent machine. However, after 2016, the study of current and future risks and possible solutions became a serious area of research. Ethical machines and alignment Friendly AI are machines that have been designed from the beginning to minimize risks and to make choices that benefit humans. Eliezer Yudkowsky, who coined the term, argues that developing friendly AI should be a higher research priority: it may require a large investment and it must be completed before AI becomes an existential risk. Machines with intelligence have the potential to use their intelligence to make ethical decisions. The field of machine ethics provides machines with ethical principles and procedures for resolving ethical dilemmas. The field of machine ethics is also called computational morality, and was founded at an AAAI symposium in 2005. Other approaches include Wendell Wallach's "artificial moral agents" and Stuart J. Russell's three principles for developing provably beneficial machines. Open source Active organizations in the AI open-source community include Hugging Face, Google, EleutherAI and Meta. Various AI models, such as Llama 2, Mistral or Stable Diffusion, have been made open-weight, meaning that their architecture and trained parameters (the "weights") are publicly available. Open-weight models can be freely fine-tuned, which allows companies to specialize them with their own data and for their own use-case. Open-weight models are useful for research and innovation but can also be misused. Since they can be fine-tuned, any built-in security measure, such as objecting to harmful requests, can be trained away until it becomes ineffective. Some researchers warn that future AI models may develop dangerous capabilities (such as the potential to drastically facilitate bioterrorism) and that once released on the Internet, they cannot be deleted everywhere if needed. They recommend pre-release audits and cost-benefit analyses. Frameworks Artificial intelligence projects can be guided by ethical considerations during the design, development, and implementation of an AI system. An AI framework such as the Care and Act Framework, developed by the Alan Turing Institute and based on the SUM values, outlines four main ethical dimensions, defined as follows: Respect the dignity of individual people Connect with other people sincerely, openly, and inclusively Care for the wellbeing of everyone Protect social values, justice, and the public interest Other developments in ethical frameworks include those decided upon during the Asilomar Conference, the Montreal Declaration for Responsible AI, and the IEEE's Ethics of Autonomous Systems initiative, among others; however, these principles are not without criticism, especially regarding the people chosen to contribute to these frameworks. Promotion of the wellbeing of the people and communities that these technologies affect requires consideration of the social and ethical implications at all stages of AI system design, development and implementation, and collaboration between job roles such as data scientists, product managers, data engineers, domain experts, and delivery managers. The UK AI Safety Institute released in 2024 a testing toolset called 'Inspect' for AI safety evaluations available under an MIT open-source licence which is freely available on GitHub and can be improved with third-party packages. It can be used to evaluate AI models in a range of areas including core knowledge, ability to reason, and autonomous capabilities. Regulation The regulation of artificial intelligence is the development of public sector policies and laws for promoting and regulating AI; it is therefore related to the broader regulation of algorithms. The regulatory and policy landscape for AI is an emerging issue in jurisdictions globally. According to AI Index at Stanford, the annual number of AI-related laws passed in the 127 survey countries jumped from one passed in 2016 to 37 passed in 2022 alone. Between 2016 and 2020, more than 30 countries adopted dedicated strategies for AI. Most EU member states had released national AI strategies, as had Canada, China, India, Japan, Mauritius, the Russian Federation, Saudi Arabia, United Arab Emirates, U.S., and Vietnam. Others were in the process of elaborating their own AI strategy, including Bangladesh, Malaysia and Tunisia. The Global Partnership on Artificial Intelligence was launched in June 2020, stating a need for AI to be developed in accordance with human rights and democratic values, to ensure public confidence and trust in the technology. Henry Kissinger, Eric Schmidt, and Daniel Huttenlocher published a joint statement in November 2021 calling for a government commission to regulate AI. In 2023, OpenAI leaders published recommendations for the governance of superintelligence, which they believe may happen in less than 10 years. In 2023, the United Nations also launched an advisory body to provide recommendations on AI governance; the body comprises technology company executives, government officials and academics. On 1 August 2024, the EU Artificial Intelligence Act entered into force, establishing the first comprehensive EU-wide AI regulation. In 2024, the Council of Europe created the first international legally binding treaty on AI, called the "Framework Convention on Artificial Intelligence and Human Rights, Democracy and the Rule of Law". It was adopted by the European Union, the United States, the United Kingdom, and other signatories. In a 2022 Ipsos survey, attitudes towards AI varied greatly by country; 78% of Chinese citizens, but only 35% of Americans, agreed that "products and services using AI have more benefits than drawbacks". A 2023 Reuters/Ipsos poll found that 61% of Americans agree, and 22% disagree, that AI poses risks to humanity. In a 2023 Fox News poll, 35% of Americans thought it "very important", and an additional 41% thought it "somewhat important", for the federal government to regulate AI, versus 13% responding "not very important" and 8% responding "not at all important". In November 2023, the first global AI Safety Summit was held in Bletchley Park in the UK to discuss the near and far term risks of AI and the possibility of mandatory and voluntary regulatory frameworks. 28 countries including the United States, China, and the European Union issued a declaration at the start of the summit, calling for international co-operation to manage the challenges and risks of artificial intelligence. In May 2024 at the AI Seoul Summit, 16 global AI tech companies agreed to safety commitments on the development of AI. History The study of mechanical or "formal" reasoning began with philosophers and mathematicians in antiquity. The study of logic led directly to Alan Turing's theory of computation, which suggested that a machine, by shuffling symbols as simple as "0" and "1", could simulate any conceivable form of mathematical reasoning. This, along with concurrent discoveries in cybernetics, information theory and neurobiology, led researchers to consider the possibility of building an "electronic brain". They developed several areas of research that would become part of AI, such as McCulloch and Pitts design for "artificial neurons" in 1943, and Turing's influential 1950 paper 'Computing Machinery and Intelligence', which introduced the Turing test and showed that "machine intelligence" was plausible. The field of AI research was founded at a workshop at Dartmouth College in 1956. The attendees became the leaders of AI research in the 1960s. They and their students produced programs that the press described as "astonishing": computers were learning checkers strategies, solving word problems in algebra, proving logical theorems and speaking English. Artificial intelligence laboratories were set up at a number of British and U.S. universities in the latter 1950s and early 1960s. Researchers in the 1960s and the 1970s were convinced that their methods would eventually succeed in creating a machine with general intelligence and considered this the goal of their field. In 1965 Herbert Simon predicted, "machines will be capable, within twenty years, of doing any work a man can do". In 1967 Marvin Minsky agreed, writing that "within a generation ... the problem of creating 'artificial intelligence' will substantially be solved". They had, however, underestimated the difficulty of the problem. In 1974, both the U.S. and British governments cut off exploratory research in response to the criticism of Sir James Lighthill and ongoing pressure from the U.S. Congress to fund more productive projects. Minsky and Papert's book Perceptrons was understood as proving that artificial neural networks would never be useful for solving real-world tasks, thus discrediting the approach altogether. The "AI winter", a period when obtaining funding for AI projects was difficult, followed. In the early 1980s, AI research was revived by the commercial success of expert systems, a form of AI program that simulated the knowledge and analytical skills of human experts. By 1985, the market for AI had reached over a billion dollars. At the same time, Japan's fifth generation computer project inspired the U.S. and British governments to restore funding for academic research. However, beginning with the collapse of the Lisp Machine market in 1987, AI once again fell into disrepute, and a second, longer-lasting winter began. Up to this point, most of AI's funding had gone to projects that used high-level symbols to represent mental objects like plans, goals, beliefs, and known facts. In the 1980s, some researchers began to doubt that this approach would be able to imitate all the processes of human cognition, especially perception, robotics, learning and pattern recognition, and began to look into "sub-symbolic" approaches. Rodney Brooks rejected "representation" in general and focussed directly on engineering machines that move and survive. Judea Pearl, Lotfi Zadeh, and others developed methods that handled incomplete and uncertain information by making reasonable guesses rather than precise logic. But the most important development was the revival of "connectionism", including neural network research, by Geoffrey Hinton and others. In 1990, Yann LeCun successfully showed that convolutional neural networks can recognize handwritten digits, the first of many successful applications of neural networks. AI gradually restored its reputation in the late 1990s and early 21st century by exploiting formal mathematical methods and by finding specific solutions to specific problems. This "narrow" and "formal" focus allowed researchers to produce verifiable results and collaborate with other fields (such as statistics, economics and mathematics). By 2000, solutions developed by AI researchers were being widely used, although in the 1990s they were rarely described as "artificial intelligence" (a tendency known as the AI effect). However, several academic researchers became concerned that AI was no longer pursuing its original goal of creating versatile, fully intelligent machines. Beginning around 2002, they founded the subfield of artificial general intelligence (or "AGI"), which had several well-funded institutions by the 2010s. Deep learning began to dominate industry benchmarks in 2012 and was adopted throughout the field. For many specific tasks, other methods were abandoned. Deep learning's success was based on both hardware improvements (faster computers, graphics processing units, cloud computing) and access to large amounts of data (including curated datasets, such as ImageNet). Deep learning's success led to an enormous increase in interest and funding in AI. The amount of machine learning research (measured by total publications) increased by 50% in the years 2015–2019. In 2016, issues of fairness and the misuse of technology were catapulted into center stage at machine learning conferences, publications vastly increased, funding became available, and many researchers re-focussed their careers on these issues. The alignment problem became a serious field of academic study. In the late 2010s and early 2020s, AGI companies began to deliver programs that created enormous interest. In 2015, AlphaGo, developed by DeepMind, beat the world champion Go player. The program taught only the game's rules and developed a strategy by itself. GPT-3 is a large language model that was released in 2020 by OpenAI and is capable of generating high-quality human-like text. ChatGPT, launched on November 30, 2022, became the fastest-growing consumer software application in history, gaining over 100 million users in two months. It marked what is widely regarded as AI's breakout year, bringing it into the public consciousness. These programs, and others, inspired an aggressive AI boom, where large companies began investing billions of dollars in AI research. According to AI Impacts, about US$50 billion annually was invested in "AI" around 2022 in the U.S. alone and about 20% of the new U.S. Computer Science PhD graduates have specialized in "AI". About 800,000 "AI"-related U.S. job openings existed in 2022. According to PitchBook research, 22% of newly funded startups in 2024 claimed to be AI companies. Philosophy Philosophical debates have historically sought to determine the nature of intelligence and how to make intelligent machines. Another major focus has been whether machines can be conscious, and the associated ethical implications. Many other topics in philosophy are relevant to AI, such as epistemology and free will. Rapid advancements have intensified public discussions on the philosophy and ethics of AI. Defining artificial intelligence Alan Turing wrote in 1950 "I propose to consider the question 'can machines think'?" He advised changing the question from whether a machine "thinks", to "whether or not it is possible for machinery to show intelligent behaviour". He devised the Turing test, which measures the ability of a machine to simulate human conversation. Since we can only observe the behavior of the machine, it does not matter if it is "actually" thinking or literally has a "mind". Turing notes that we can not determine these things about other people but "it is usual to have a polite convention that everyone thinks." Russell and Norvig agree with Turing that intelligence must be defined in terms of external behavior, not internal structure. However, they are critical that the test requires the machine to imitate humans. "Aeronautical engineering texts", they wrote, "do not define the goal of their field as making 'machines that fly so exactly like pigeons that they can fool other pigeons.'" AI founder John McCarthy agreed, writing that "Artificial intelligence is not, by definition, simulation of human intelligence". McCarthy defines intelligence as "the computational part of the ability to achieve goals in the world". Another AI founder, Marvin Minsky, similarly describes it as "the ability to solve hard problems". The leading AI textbook defines it as the study of agents that perceive their environment and take actions that maximize their chances of achieving defined goals. These definitions view intelligence in terms of well-defined problems with well-defined solutions, where both the difficulty of the problem and the performance of the program are direct measures of the "intelligence" of the machine—and no other philosophical discussion is required, or may not even be possible. Another definition has been adopted by Google, a major practitioner in the field of AI. This definition stipulates the ability of systems to synthesize information as the manifestation of intelligence, similar to the way it is defined in biological intelligence. As a result of the many circulating definitions scholars have started to critically analyze and order the AI discourse itself including discussing the many AI narratives and myths to be found within societal, political and academic discourses. Similarly, in practice, some authors have suggested that the term 'AI' is often used too broadly and vaguely. This raises the question of where the line should be drawn between AI and classical algorithms, with many companies during the early 2020s AI boom using the term as a marketing buzzword, often even if they did "not actually use AI in a material way". There has been debate over whether large language models exhibit genuine intelligence or merely simulate it by imitating human text. Evaluating approaches to AI No established unifying theory or paradigm has guided AI research for most of its history. The unprecedented success of statistical machine learning in the 2010s eclipsed all other approaches (so much so that some sources, especially in the business world, use the term "artificial intelligence" to mean "machine learning with neural networks"). This approach is mostly sub-symbolic, soft and narrow. Critics argue that these questions may have to be revisited by future generations of AI researchers. Symbolic AI and its limits Symbolic AI (or "GOFAI") simulated the high-level conscious reasoning that people use when they solve puzzles, express legal reasoning and do mathematics. They were highly successful at "intelligent" tasks such as algebra or IQ tests. In the 1960s, Newell and Simon proposed the physical symbol systems hypothesis: "A physical symbol system has the necessary and sufficient means of general intelligent action." However, the symbolic approach failed on many tasks that humans solve easily, such as learning, recognizing an object or commonsense reasoning. Moravec's paradox is the discovery that high-level "intelligent" tasks were easy for AI, but low level "instinctive" tasks were extremely difficult. Philosopher Hubert Dreyfus had argued since the 1960s that human expertise depends on unconscious instinct rather than conscious symbol manipulation, and on having a "feel" for the situation, rather than explicit symbolic knowledge. Although his arguments had been ridiculed and ignored when they were first presented, eventually, AI research came to agree with him. The issue is not resolved: sub-symbolic reasoning can make many of the same inscrutable mistakes that human intuition does, such as algorithmic bias. Critics such as Noam Chomsky argue continuing research into symbolic AI will still be necessary to attain general intelligence, in part because sub-symbolic AI is a move away from explainable AI: it can be difficult or impossible to understand why a modern statistical AI program made a particular decision. The emerging field of neuro-symbolic artificial intelligence attempts to bridge the two approaches. Neat vs. scruffy "Neats" hope that intelligent behavior is described using simple, elegant principles (such as logic, optimization, or neural networks). "Scruffies" expect that it necessarily requires solving a large number of unrelated problems. Neats defend their programs with theoretical rigor, scruffies rely mainly on incremental testing to see if they work. This issue was actively discussed in the 1970s and 1980s, but eventually was seen as irrelevant. Modern AI has elements of both. Soft vs. hard computing Finding a provably correct or optimal solution is intractable for many important problems. Soft computing is a set of techniques, including genetic algorithms, fuzzy logic and neural networks, that are tolerant of imprecision, uncertainty, partial truth and approximation. Soft computing was introduced in the late 1980s and most successful AI programs in the 21st century are examples of soft computing with neural networks. Narrow vs. general AI AI researchers are divided as to whether to pursue the goals of artificial general intelligence and superintelligence directly or to solve as many specific problems as possible (narrow AI) in hopes these solutions will lead indirectly to the field's long-term goals. General intelligence is difficult to define and difficult to measure, and modern AI has had more verifiable successes by focusing on specific problems with specific solutions. The sub-field of artificial general intelligence studies this area exclusively. Machine consciousness, sentience, and mind There is no settled consensus in philosophy of mind on whether a machine can have a mind, consciousness and mental states in the same sense that human beings do. This issue considers the internal experiences of the machine, rather than its external behavior. Mainstream AI research considers this issue irrelevant because it does not affect the goals of the field: to build machines that can solve problems using intelligence. Russell and Norvig add that "[t]he additional project of making a machine conscious in exactly the way humans are is not one that we are equipped to take on." However, the question has become central to the philosophy of mind. It is also typically the central question at issue in artificial intelligence in fiction. Consciousness David Chalmers identified two problems in understanding the mind, which he named the "hard" and "easy" problems of consciousness. The easy problem is understanding how the brain processes signals, makes plans and controls behavior. The hard problem is explaining how this feels or why it should feel like anything at all, assuming we are right in thinking that it truly does feel like something (Dennett's consciousness illusionism says this is an illusion). While human information processing is easy to explain, human subjective experience is difficult to explain. For example, it is easy to imagine a color-blind person who has learned to identify which objects in their field of view are red, but it is not clear what would be required for the person to know what red looks like. Computationalism and functionalism Computationalism is the position in the philosophy of mind that the human mind is an information processing system and that thinking is a form of computing. Computationalism argues that the relationship between mind and body is similar or identical to the relationship between software and hardware and thus may be a solution to the mind–body problem. This philosophical position was inspired by the work of AI researchers and cognitive scientists in the 1960s and was originally proposed by philosophers Jerry Fodor and Hilary Putnam. Philosopher John Searle characterized this position as "strong AI": "The appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds." Searle challenges this claim with his Chinese room argument, which attempts to show that even a computer capable of perfectly simulating human behavior would not have a mind. AI welfare and rights It is difficult or impossible to reliably evaluate whether an advanced AI is sentient (has the ability to feel), and if so, to what degree. But if there is a significant chance that a given machine can feel and suffer, then it may be entitled to certain rights or welfare protection measures, similarly to animals. Sapience (a set of capacities related to high intelligence, such as discernment or self-awareness) may provide another moral basis for AI rights. Robot rights are also sometimes proposed as a practical way to integrate autonomous agents into society. In 2017, the European Union considered granting "electronic personhood" to some of the most capable AI systems. Similarly to the legal status of companies, it would have conferred rights but also responsibilities. Critics argued in 2018 that granting rights to AI systems would downplay the importance of human rights, and that legislation should focus on user needs rather than speculative futuristic scenarios. They also noted that robots lacked the autonomy to take part in society on their own. Progress in AI increased interest in the topic. Proponents of AI welfare and rights often argue that AI sentience, if it emerges, would be particularly easy to deny. They warn that this may be a moral blind spot analogous to slavery or factory farming, which could lead to large-scale suffering if sentient AI is created and carelessly exploited. Future Superintelligence and the singularity A superintelligence is a hypothetical agent that would possess intelligence far surpassing that of the brightest and most gifted human mind. If research into artificial general intelligence produced sufficiently intelligent software, it might be able to reprogram and improve itself. The improved software would be even better at improving itself, leading to what I. J. Good called an "intelligence explosion" and Vernor Vinge called a "singularity". However, technologies cannot improve exponentially indefinitely, and typically follow an S-shaped curve, slowing when they reach the physical limits of what the technology can do. Transhumanism Robot designer Hans Moravec, cyberneticist Kevin Warwick and inventor Ray Kurzweil have predicted that humans and machines may merge in the future into cyborgs that are more capable and powerful than either. This idea, called transhumanism, has roots in the writings of Aldous Huxley and Robert Ettinger. Edward Fredkin argues that "artificial intelligence is the next step in evolution", an idea first proposed by Samuel Butler's "Darwin among the Machines" as far back as 1863, and expanded upon by George Dyson in his 1998 book Darwin Among the Machines: The Evolution of Global Intelligence. In fiction Thought-capable artificial beings have appeared as storytelling devices since antiquity, and have been a persistent theme in science fiction. A common trope in these works began with Mary Shelley's Frankenstein, where a human creation becomes a threat to its masters. This includes such works as Arthur C. Clarke's and Stanley Kubrick's 2001: A Space Odyssey (both 1968), with HAL 9000, the murderous computer in charge of the Discovery One spaceship, as well as The Terminator (1984) and The Matrix (1999). In contrast, the rare loyal robots such as Gort from The Day the Earth Stood Still (1951) and Bishop from Aliens (1986) are less prominent in popular culture. Isaac Asimov introduced the Three Laws of Robotics in many stories, most notably with the "Multivac" super-intelligent computer. Asimov's laws are often brought up during lay discussions of machine ethics; while almost all artificial intelligence researchers are familiar with Asimov's laws through popular culture, they generally consider the laws useless for many reasons, one of which is their ambiguity. Several works use AI to force us to confront the fundamental question of what makes us human, showing us artificial beings that have the ability to feel, and thus to suffer. This appears in Karel Čapek's R.U.R., the films A.I. Artificial Intelligence and Ex Machina, as well as the novel Do Androids Dream of Electric Sheep?, by Philip K. Dick. Dick considers the idea that our understanding of human subjectivity is altered by technology created with artificial intelligence. See also Artificial consciousness – Field in cognitive science Artificial intelligence and elections – Use and impact of AI on political elections Artificial intelligence content detection – Software to detect AI-generated content Association for the Advancement of Artificial Intelligence (AAAI) Behavior selection algorithm – Algorithm that selects actions for intelligent agents Business process automation – Automation of business processes Case-based reasoning – Process of solving new problems based on the solutions of similar past problems Computational intelligence – Ability of a computer to learn a specific task from data or experimental observation Digital immortality – Hypothetical concept of storing a personality in digital form Emergent algorithm – Algorithm exhibiting emergent behavior Female gendering of AI technologies – Gender biases in digital technologyPages displaying short descriptions of redirect targets Glossary of artificial intelligence – List of definitions of terms and concepts commonly used in the study of artificial intelligence Intelligence amplification – Use of information technology to augment human intelligence Intelligent agent – Software agent which acts autonomously Intelligent automation – Software process that combines robotic process automation and artificial intelligence List of artificial intelligence journals List of artificial intelligence projects Mind uploading – Hypothetical process of digitally emulating a brain Organoid intelligence – Use of brain cells and brain organoids for intelligent computing Robotic process automation – Form of business process automation technology The Last Day – 1967 Welsh science fiction novel Wetware computer – Computer composed of organic material DARWIN EU - A European Union initiative coordinated by the European Medicines Agency (EMA) to generate and utilize real-world evidence (RWE) to support the evaluation and supervision of medicines across the EU. Artificial intelligence in Wikimedia projects - Use of artificial intelligence to develop Wikipedia and other Wikimedia projects AI-generated content on Wikipedia - Use of artificial intelligence to generate articles or text on Wikipedia Explanatory notes References AI textbooks The two most widely used textbooks in 2023 (see the Open Syllabus): Russell, Stuart J.; Norvig, Peter (2021). Artificial Intelligence: A Modern Approach (4th ed.). Hoboken: Pearson. ISBN 978-0-1346-1099-3. LCCN 20190474. Rich, Elaine; Knight, Kevin; Nair, Shivashankar B (2010). Artificial Intelligence (3rd ed.). New Delhi: Tata McGraw Hill India. ISBN 978-0-0700-8770-5. The four most widely used AI textbooks in 2008: Other textbooks: Ertel, Wolfgang (2017). Introduction to Artificial Intelligence (2nd ed.). Springer. ISBN 978-3-3195-8486-7. Ciaramella, Alberto; Ciaramella, Marco (2024). Introduction to Artificial Intelligence: from data analysis to generative AI (1st ed.). Intellisemantic Editions. ISBN 978-8-8947-8760-3. History of AI Other sources Further reading External links "Artificial Intelligence". Internet Encyclopedia of Philosophy.

Medicine is the science and practice of caring for patients, managing the diagnosis, prognosis, prevention, treatment, palliation of their injury or disease, and promoting their health. Medicine encompasses a variety of health care practices evolved to maintain and restore health by the prevention and treatment of illness. Contemporary medicine applies biomedical sciences, biomedical research, genetics, and medical technology to diagnose, treat, and prevent injury and disease, typically through pharmaceuticals or surgery, but also through therapies as diverse as psychotherapy, external splints and traction, medical devices, biologics, and ionizing radiation, amongst others. Medicine has been practiced since prehistoric times, and for most of this time it was an art (an area of creativity and skill), frequently having connections to the religious and philosophical beliefs of local culture. For example, a medicine man would apply herbs and say prayers for healing, or an ancient philosopher and physician would apply bloodletting according to the theories of humorism. In recent centuries, since the advent of modern science, most medicine has become a combination of art and science (both basic and applied, under the umbrella of medical science). For example, while stitching technique for sutures is an art learned through practice, knowledge of what happens at the cellular and molecular level in the tissues being stitched arises through science. Prescientific forms of medicine, now known as traditional medicine or folk medicine, remain commonly used in the absence of scientific medicine and are thus called alternative medicine. Alternative treatments outside of scientific medicine with ethical, safety and efficacy concerns are termed quackery. Etymology Medicine (UK: , US: ) is the science and practice of the diagnosis, prognosis, treatment, and prevention of disease. The word "medicine" is derived from Latin medicus, meaning "a physician". The word "physic" itself, from which "physician" derives, was the old word for what is now called a medicine, and also the field of medicine. Clinical practice Medical availability and clinical practice vary across the world due to regional differences in culture and technology. Modern scientific medicine is highly developed in the Western world, while in developing countries such as parts of Africa or Asia, the population may rely more heavily on traditional medicine with limited evidence and efficacy and no required formal training for practitioners. In the developed world, evidence-based medicine is not universally used in clinical practice; for example, a 2007 survey of literature reviews found that about 49% of the interventions lacked sufficient evidence to support either benefit or harm. In modern clinical practice, physicians and physician assistants personally assess patients to diagnose, prognose, treat, and prevent disease using clinical judgment. The doctor-patient relationship typically begins with an interaction with an examination of the patient's medical history and medical record, followed by a medical interview and a physical examination. Basic diagnostic medical devices (e.g., stethoscope, tongue depressor) are typically used. After examining for signs and interviewing for symptoms, the doctor may order medical tests (e.g., blood tests), take a biopsy, or prescribe pharmaceutical drugs or other therapies. Differential diagnosis methods help to rule out conditions based on the information provided. During the encounter, properly informing the patient of all relevant facts is an important part of the relationship and the development of trust. The medical encounter is then documented in the medical record, which is a legal document in many jurisdictions. Follow-ups may be shorter but follow the same general procedure, and specialists follow a similar process. The diagnosis and treatment may take only a few minutes or a few weeks, depending on the complexity of the issue. The components of the medical interview and encounter are: Chief complaint (CC): the reason for the current medical visit. These are the symptoms. They are in the patient's own words and are recorded along with the duration of each one. Also called chief concern or presenting complaint. Current activity: occupation, hobbies, what the patient actually does. Family history (FH): listing of diseases in the family that may impact the patient. A family tree is sometimes used. History of present illness (HPI): the chronological order of events of symptoms and further clarification of each symptom. Distinguishable from history of previous illness, often called past medical history (PMH). Medical history comprises HPI and PMH. Medications (Rx): what drugs the patient takes including prescribed, over-the-counter, and home remedies, as well as alternative and herbal medicines or remedies. Allergies are also recorded. Past medical history (PMH/PMHx): concurrent medical problems, past hospitalizations and operations, injuries, past infectious diseases or vaccinations, history of known allergies. Review of systems (ROS) or systems inquiry: a set of additional questions to ask, which may be missed on HPI: a general enquiry (have you noticed any weight loss, change in sleep quality, fevers, lumps and bumps? etc.), followed by questions on the body's main organ systems (heart, lungs, digestive tract, urinary tract, etc.). Social history (SH): birthplace, residences, marital history, social and economic status, habits (including diet, medications, tobacco, alcohol). The physical examination is the examination of the patient for medical signs of disease that are objective and observable, in contrast to symptoms that are volunteered by the patient and are not necessarily objectively observable. The healthcare provider uses sight, hearing, touch, and sometimes smell (e.g., in infection, uremia, diabetic ketoacidosis). Four actions are the basis of physical examination: inspection, palpation (feel), percussion (tap to determine resonance characteristics), and auscultation (listen), generally in that order, although auscultation occurs prior to percussion and palpation for abdominal assessments. The clinical examination involves the study of: Abdomen and rectum Cardiovascular (heart and blood vessels) General appearance of the patient and specific indicators of disease (nutritional status, presence of jaundice, pallor or clubbing) Genitalia (and pregnancy if the patient is or could be pregnant) Head, eye, ear, nose, and throat (HEENT) Musculoskeletal (including spine and extremities) Neurological (consciousness, awareness, brain, vision, cranial nerves, spinal cord and peripheral nerves) Psychiatric (orientation, mental state, mood, evidence of abnormal perception or thought). Respiratory (large airways and lungs) Skin Vital signs including height, weight, body temperature, blood pressure, pulse, respiration rate, and hemoglobin oxygen saturation It is to likely focus on areas of interest highlighted in the medical history and may not include everything listed above. The treatment plan may include ordering additional medical laboratory tests and medical imaging studies, starting therapy, referral to a specialist, or watchful observation. A follow-up may be advised. Depending upon the health insurance plan and the managed care system, various forms of "utilization review", such as prior authorization of tests, may place barriers on accessing expensive services. The medical decision-making (MDM) process includes the analysis and synthesis of all the above data to come up with a list of possible diagnoses (the differential diagnoses), along with an idea of what needs to be done to obtain a definitive diagnosis that would explain the patient's problem. On subsequent visits, the process may be repeated in an abbreviated manner to obtain any new history, symptoms, physical findings, lab or imaging results, or specialist consultations. Institutions Contemporary medicine is, in general, conducted within health care systems. Legal, credentialing, and financing frameworks are established by individual governments, augmented on occasion by international organizations, such as churches. The characteristics of any given health care system have a significant impact on the way medical care is provided. From ancient times, Christian emphasis on practical charity gave rise to the development of systematic nursing and hospitals, and the Catholic Church today remains the largest non-government provider of medical services in the world. Advanced industrial countries (with the exception of the United States) and many developing countries provide medical services through a system of universal health care that aims to guarantee care for all through a single-payer health care system or compulsory private or cooperative health insurance. This is intended to ensure that the entire population has access to medical care on the basis of need rather than ability to pay. Delivery may be via private medical practices, state-owned hospitals and clinics, or charities, most commonly a combination of all three. Most tribal societies provide no guarantee of healthcare for the population as a whole. In such societies, healthcare is available to those who can afford to pay for it, have self-insured it (either directly or as part of an employment contract), or may be covered by care financed directly by the government or tribe. Transparency of information is another factor defining a delivery system. Access to information on conditions, treatments, quality, and pricing greatly affects the choice of patients/consumers and, therefore, the incentives of medical professionals. While the US healthcare system has come under fire for its lack of openness, new legislation may encourage greater openness. There is a perceived tension between the need for transparency on the one hand and such issues as patient confidentiality and the possible exploitation of information for commercial gain on the other. The health professionals who provide care in medicine comprise multiple professions, such as medics, nurses, physiotherapists, and psychologists. These professions will have their own ethical standards, professional education, and bodies. The medical profession has been conceptualized from a sociological perspective. Delivery Provision of medical care is classified into primary, secondary, and tertiary care categories. Primary care medical services are provided by physicians, physician assistants, nurse practitioners, or other health professionals who have first contact with a patient seeking medical treatment or care. These occur in physician offices, medical practices, clinics, nursing homes, schools, patients' homes, and in other places that are typically geographically close to where patients live, work or study. About 90% of medical visits can be satisfactorily and effectively dealt with by primary care provider(s). Primary care visits might include treatment of minor, acute or chronic illnesses, preventive care, and health education. Primary care is directed to the health of entire populations and thus providers care for patients of all ages and sexes. Secondary care medical services are provided by medical specialists in their offices, practices or clinics, or at local community hospitals, to patients referred by the primary care provider who first diagnosed or treated the patient. 'Referrals' are made of those patients who required the particular expertise of, or specific procedures performed by, specialists. Secondary care services include both ambulatory care and inpatient services, emergency departments, some intensive care medicine, some surgeries and related services, physical therapy, labor and delivery, endoscopy units, diagnostic laboratory and medical imaging services, hospice centers, and others depending on the health services systems within which the care is being delivered. Some primary care providers may also take care of hospitalized patients and deliver babies in a secondary care setting. Tertiary care medical services are provided by specialist teams of providers in larger, more specialised hospitals or regional medical centers, which are equipped with diagnostic and treatment facilities not typically available at local (often smaller) hospitals. This allows for the treatment and care of patients with more complex or urgent or serious medical conditions, which in turn may require more expertise (including multi-disciplinary teams) and resources (facilities, staff, bed days) to effectively treat. Tertiary care may include that provided at burn treatment or trauma centers, advanced neonatology unit services, organ transplants, high-risk pregnancy and child delivery, radiation oncology, and very many other forms of specialist and intensive care. Modern medical care also depends on the keeping and use of information, including about a particular patient—still kept in many health care settings on paper 'medical records', but increasingly nowadays by electronic means. In low-income countries, modern healthcare is often too expensive for the average person. International healthcare policy researchers have advocated that "user fees" be removed in these areas to ensure access; however, even with removal of patient fee obligations, significant costs and barriers remain for the poor and the sick in accessing sufficient care. Separation of prescribing and dispensing is a practice in medicine and pharmacy in which the physician who provides a medical prescription is different from the pharmacist who provides the prescription drug to the patient. In the Western world there are centuries of tradition and practice differentiating pharmacists from physicians, and two quite separate professions developed. In many Asian countries, on the other hand, it is traditional for physicians to also deliver drugs directly to patients, at least in some cases. This model is also being used increasingly in the west: especially for simply-treated conditions (eg, those needing general antibiotics), in remote locations, with vulnerable communities of patients, and in small or integrated medical facilities. Branches Working together as an interdisciplinary team, many highly trained health professionals besides medical practitioners are involved in the delivery of modern health care. Examples include: nurses, emergency medical technicians and paramedics, laboratory scientists, pharmacists, podiatrists, physiotherapists, respiratory therapists, speech therapists, occupational therapists, radiographers, dietitians, and bioengineers, medical physicists, surgeons, surgeon's assistant, surgical technologist. The scope and sciences underpinning human medicine overlap many other fields. A patient admitted to the hospital is usually under the care of a specific team based on their main presenting problem, e.g., the cardiology team, who then may interact with other specialties, e.g., surgical, radiology, to help diagnose or treat the main problem or any subsequent complications/developments. Physicians have many specializations and subspecializations into certain branches of medicine, which are listed below. There are variations from country to country regarding which specialties certain subspecialties are in. The main branches of medicine are: Basic sciences of medicine; this is what every physician is educated in, and some return to in biomedical research. Interdisciplinary fields, where different medical specialties are mixed to function in certain occasions. Medical specialties Basic sciences Anatomy is the study of the physical structure of organisms. In contrast to macroscopic or gross anatomy, cytology and histology are concerned with microscopic structures. Biochemistry is the study of the chemistry taking place in living organisms, especially the structure and function of their chemical components. Biomechanics is the study of the structure and function of biological systems by means of the methods of Mechanics. Biophysics is an interdisciplinary science that uses the methods of physics and physical chemistry to study biological systems. Biostatistics is the application of statistics to biological fields in the broadest sense. A knowledge of biostatistics is essential in the planning, evaluation, and interpretation of medical research. It is also fundamental to epidemiology and evidence-based medicine. Cytology is the microscopic study of individual cells. Embryology is the study of the early development of organisms. Endocrinology is the study of hormones and their effect throughout the body of animals. Epidemiology is the study of the demographics of disease processes, and includes, but is not limited to, the study of epidemics. Genetics is the study of genes, and their role in biological inheritance. Gynecology is the study of female reproductive system. Histology is the study of the structures of biological tissues by light microscopy, electron microscopy and immunohistochemistry. Immunology is the study of the immune system, which includes the innate and adaptive immune system in humans, for example. Lifestyle medicine is the study of the chronic conditions, and how to prevent, treat and reverse them. Medical physics is the study of the applications of physics principles in medicine. Microbiology is the study of microorganisms, including protozoa, bacteria, fungi, and viruses. Molecular biology is the study of molecular underpinnings of the process of replication, transcription and translation of the genetic material. Neuroscience includes those disciplines of science that are related to the study of the nervous system. A main focus of neuroscience is the biology and physiology of the human brain and spinal cord. Some related clinical specialties include neurology, neurosurgery and psychiatry. Nutrition science (theoretical focus) and dietetics (practical focus) is the study of the relationship of food and drink to health and disease, especially in determining an optimal diet. Medical nutrition therapy is done by dietitians and is prescribed for diabetes, cardiovascular diseases, weight and eating disorders, allergies, malnutrition, and neoplastic diseases. Pathology as a science is the study of disease – the causes, course, progression and resolution thereof. Pharmacology is the study of drugs and their actions. Photobiology is the study of the interactions between non-ionizing radiation and living organisms. Physiology is the study of the normal functioning of the body and the underlying regulatory mechanisms. Radiobiology is the study of the interactions between ionizing radiation and living organisms. Toxicology is the study of hazardous effects of drugs and poisons. Specialties In the broadest meaning of "medicine", there are many different specialties. In the UK, most specialities have their own body or college, which has its own entrance examination. These are collectively known as the Royal Colleges, although not all currently use the term "Royal". The development of a speciality is often driven by new technology (such as the development of effective anaesthetics) or ways of working (such as emergency departments); the new specialty leads to the formation of a unifying body of doctors and the prestige of administering their own examination. Within medical circles, specialities usually fit into one of two broad categories: "Medicine" and "Surgery". "Medicine" refers to the practice of non-operative medicine, and most of its subspecialties require preliminary training in Internal Medicine. In the UK, this was traditionally evidenced by passing the examination for the Membership of the Royal College of Physicians (MRCP) or the equivalent college in Scotland or Ireland. "Surgery" refers to the practice of operative medicine, and most subspecialties in this area require preliminary training in General Surgery, which in the UK leads to membership of the Royal College of Surgeons of England (MRCS). At present, some specialties of medicine do not fit easily into either of these categories, such as radiology, pathology, or anesthesia. Most of these have branched from one or other of the two camps above; for example anaesthesia developed first as a faculty of the Royal College of Surgeons (for which MRCS/FRCS would have been required) before becoming the Royal College of Anaesthetists and membership of the college is attained by sitting for the examination of the Fellowship of the Royal College of Anesthetists (FRCA). Surgical specialty Surgery is an ancient medical specialty that uses operative manual and instrumental techniques on a patient to investigate or treat a pathological condition such as disease or injury, to help improve bodily function or appearance or to repair unwanted ruptured areas (for example, a perforated ear drum). Surgeons must also manage pre-operative, post-operative, and potential surgical candidates on the hospital wards. In some centers, anesthesiology is part of the division of surgery (for historical and logistical reasons), although it is not a surgical discipline. Other medical specialties may employ surgical procedures, such as ophthalmology and dermatology, but are not considered surgical sub-specialties per se. Surgical training in the U.S. requires a minimum of five years of residency after medical school. Sub-specialties of surgery often require seven or more years. In addition, fellowships can last an additional one to three years. Because post-residency fellowships can be competitive, many trainees devote two additional years to research. Thus in some cases surgical training will not finish until more than a decade after medical school. Furthermore, surgical training can be very difficult and time-consuming. Surgical subspecialties include those a physician may specialize in after undergoing general surgery residency training as well as several surgical fields with separate residency training. Surgical subspecialties that one may pursue following general surgery residency training: Bariatric surgery Cardiovascular surgery – may also be pursued through a separate cardiovascular surgery residency track Colorectal surgery Endocrine surgery General surgery Hand surgery Hepatico-Pancreatico-Biliary Surgery Minimally invasive surgery Pediatric surgery Plastic surgery – may also be pursued through a separate plastic surgery residency track Surgical critical care Surgical oncology Transplant surgery Trauma surgery Vascular surgery – may also be pursued through a separate vascular surgery residency track Other surgical specialties within medicine with their own individual residency training: Dermatology Neurosurgery Ophthalmology Oral and maxillofacial surgery Orthopedic surgery Otorhinolaryngology Podiatric surgery – do not undergo medical school training, but rather separate training in podiatry school Urology Internal medicine specialty Internal medicine is the medical specialty dealing with the prevention, diagnosis, and treatment of adult diseases. According to some sources, an emphasis on internal structures is implied. In North America, specialists in internal medicine are commonly called "internists". Elsewhere, especially in Commonwealth nations, such specialists are often called physicians. These terms, internist or physician (in the narrow sense, common outside North America), generally exclude practitioners of gynecology and obstetrics, pathology, psychiatry, and especially surgery and its subspecialities. Because their patients are often seriously ill or require complex investigations, internists do much of their work in hospitals. Formerly, many internists were not subspecialized; such general physicians would see any complex nonsurgical problem; this style of practice has become much less common. In modern urban practice, most internists are subspecialists: that is, they generally limit their medical practice to problems of one organ system or to one particular area of medical knowledge. For example, gastroenterologists and nephrologists specialize respectively in diseases of the gut and the kidneys. In the Commonwealth of Nations and some other countries, specialist pediatricians and geriatricians are also described as specialist physicians (or internists) who have subspecialized by age of patient rather than by organ system. Elsewhere, especially in North America, general pediatrics is often a form of primary care. There are many subspecialities (or subdisciplines) of internal medicine: Training in internal medicine (as opposed to surgical training), varies considerably across the world: see the articles on medical education for more details. In North America, it requires at least three years of residency training after medical school, which can then be followed by a one- to three-year fellowship in the subspecialties listed above. In general, resident work hours in medicine are less than those in surgery, averaging about 60 hours per week in the US. This difference does not apply in the UK where all doctors are now required by law to work less than 48 hours per week on average. Diagnostic specialties Clinical laboratory sciences are the clinical diagnostic services that apply laboratory techniques to diagnosis and management of patients. In the United States, these services are supervised by a pathologist. The personnel that work in these medical laboratory departments are technically trained staff who do not hold medical degrees, but who usually hold an undergraduate medical technology degree, who actually perform the tests, assays, and procedures needed for providing the specific services. Subspecialties include transfusion medicine, cellular pathology, clinical chemistry, hematology, clinical microbiology and clinical immunology. Clinical neurophysiology is concerned with testing the physiology or function of the central and peripheral aspects of the nervous system. These kinds of tests can be divided into recordings of: (1) spontaneous or continuously running electrical activity, or (2) stimulus evoked responses. Subspecialties include electroencephalography, electromyography, evoked potential, nerve conduction study and polysomnography. Sometimes these tests are performed by techs without a medical degree, but the interpretation of these tests is done by a medical professional. Diagnostic radiology is concerned with imaging of the body, e.g. by x-rays, x-ray computed tomography, ultrasonography, and nuclear magnetic resonance tomography. Interventional radiologists can access areas in the body under imaging for an intervention or diagnostic sampling. Nuclear medicine is concerned with studying human organ systems by administering radiolabelled substances (radiopharmaceuticals) to the body, which can then be imaged outside the body by a gamma camera or a PET scanner. Each radiopharmaceutical consists of two parts: a tracer that is specific for the function under study (e.g., neurotransmitter pathway, metabolic pathway, blood flow, or other), and a radionuclide (usually either a gamma-emitter or a positron emitter). There is a degree of overlap between nuclear medicine and radiology, as evidenced by the emergence of combined devices such as the PET/CT scanner. Pathology as a medical specialty is the branch of medicine that deals with the study of diseases and the morphologic, physiologic changes produced by them. As a diagnostic specialty, pathology can be considered the basis of modern scientific medical knowledge and plays a large role in evidence-based medicine. Many modern molecular tests such as flow cytometry, polymerase chain reaction (PCR), immunohistochemistry, cytogenetics, gene rearrangements studies and fluorescent in situ hybridization (FISH) fall within the territory of pathology. Other major specialties The following are some major medical specialties that do not directly fit into any of the above-mentioned groups: Anesthesiology (also known as anaesthetics): concerned with the perioperative management of the surgical patient. The anesthesiologist's role during surgery is to prevent derangement in the vital organs' (i.e. brain, heart, kidneys) functions and postoperative pain. Outside of the operating room, the anesthesiology physician also serves the same function in the labor and delivery ward, and some are specialized in critical medicine. Emergency medicine is concerned with the diagnosis and treatment of acute or life-threatening conditions, including trauma, surgical, medical, pediatric, and psychiatric emergencies. Family medicine, family practice, general practice or primary care is, in many countries, the first port-of-call for patients with non-emergency medical problems. Family physicians often provide services across a broad range of settings including office based practices, emergency department coverage, inpatient care, and nursing home care. Medical genetics is concerned with the diagnosis and management of hereditary disorders. Neurology is concerned with diseases of the nervous system. In the UK, neurology is a subspecialty of general medicine. Obstetrics and gynecology (often abbreviated as OB/GYN (American English) or Obs & Gynae (British English)) are concerned respectively with childbirth and the female reproductive and associated organs. Reproductive medicine and fertility medicine are generally practiced by gynecological specialists. Pediatrics (AE) or paediatrics (BE) is devoted to the care of infants, children, and adolescents. Like internal medicine, there are many pediatric subspecialties for specific age ranges, organ systems, disease classes, and sites of care delivery. Pharmaceutical medicine is the medical scientific discipline concerned with the discovery, development, evaluation, registration, monitoring and medical aspects of marketing of medicines for the benefit of patients and public health. Physical medicine and rehabilitation (or physiatry) is concerned with functional improvement after injury, illness, or congenital disorders. Podiatric medicine is the study of, diagnosis, and medical and surgical treatment of disorders of the foot, ankle, lower limb, hip and lower back. Preventive medicine is the branch of medicine concerned with preventing disease. Community health or public health is an aspect of health services concerned with threats to the overall health of a community based on population health analysis. Psychiatry is the branch of medicine concerned with the bio-psycho-social study of the etiology, diagnosis, treatment and prevention of cognitive, perceptual, emotional and behavioral disorders. Related fields include psychotherapy and clinical psychology. Interdisciplinary fields Some interdisciplinary sub-specialties of medicine include: Addiction medicine deals with the treatment of addiction. Aerospace medicine deals with medical problems related to flying and space travel. Biomedical Engineering is a field dealing with the application of engineering principles to medical practice. Clinical pharmacology is concerned with how systems of therapeutics interact with patients. Conservation medicine studies the relationship between human and non-human animal health, and environmental conditions. Also known as ecological medicine, environmental medicine, or medical geology. Disaster medicine deals with medical aspects of emergency preparedness, disaster mitigation and management. Diving medicine (or hyperbaric medicine) is the prevention and treatment of diving-related problems. Evolutionary medicine is a perspective on medicine derived through applying evolutionary theory. Forensic medicine deals with medical questions in legal context, such as determination of the time and cause of death, type of weapon used to inflict trauma, reconstruction of the facial features using remains of deceased (skull) thus aiding identification. Gender-based medicine studies the biological and physiological differences between the human sexes and how that affects differences in disease. Health informatics is a relatively recent field that deal with the application of computers and information technology to medicine. Hospice and Palliative Medicine is a relatively modern branch of clinical medicine that deals with pain and symptom relief and emotional support in patients with terminal illnesses including cancer and heart failure. Hospital medicine is the general medical care of hospitalized patients. Physicians whose primary professional focus is hospital medicine are called hospitalists in the United States and Canada. The term Most Responsible Physician (MRP) or attending physician is also used interchangeably to describe this role. Laser medicine involves the use of lasers in the diagnostics or treatment of various conditions. Many other health science fields, e.g. dietetics Medical ethics deals with ethical and moral principles that apply values and judgments to the practice of medicine. Medical humanities includes the humanities (literature, philosophy, ethics, history and religion), social science (anthropology, cultural studies, psychology, sociology), and the arts (literature, theater, film, and visual arts) and their application to medical education and practice. Nosokinetics is the science/subject of measuring and modelling the process of care in health and social care systems. Nosology is the classification of diseases for various purposes. Occupational medicine is the provision of health advice to organizations and individuals to ensure that the highest standards of health and safety at work can be achieved and maintained. Pain management (also called pain medicine, or algiatry) is the medical discipline concerned with the relief of pain. Pharmacogenomics is a form of individualized medicine. Podiatric medicine is the study of, diagnosis, and medical treatment of disorders of the foot, ankle, lower limb, hip and lower back. Sexual medicine is concerned with diagnosing, assessing and treating all disorders related to sexuality. Sports medicine deals with the treatment and prevention and rehabilitation of sports/exercise injuries such as muscle spasms, muscle tears, injuries to ligaments (ligament tears or ruptures) and their repair in athletes, amateur and professional. Therapeutics is the field, more commonly referenced in earlier periods of history, of the various remedies that can be used to treat disease and promote health. Travel medicine or emporiatrics deals with health problems of international travelers or travelers across highly different environments. Tropical medicine deals with the prevention and treatment of tropical diseases. It is studied separately in temperate climates where those diseases are quite unfamiliar to medical practitioners and their local clinical needs. Urgent care focuses on delivery of unscheduled, walk-in care outside of the hospital emergency department for injuries and illnesses that are not severe enough to require care in an emergency department. In some jurisdictions this function is combined with the emergency department. Veterinary medicine; veterinarians apply similar techniques as physicians to the care of non-human animals. Wilderness medicine entails the practice of medicine in the wild, where conventional medical facilities may not be available. Education and legal controls Medical education and training varies around the world. It typically involves entry level education at a university medical school, followed by a period of supervised practice or internship, or residency. This can be followed by postgraduate vocational training. A variety of teaching methods have been employed in medical education, still itself a focus of active research. In Canada and the United States of America, a Doctor of Medicine degree, often abbreviated M.D., or a Doctor of Osteopathic Medicine degree, often abbreviated as D.O. and unique to the United States, must be completed in and delivered from a recognized university. Since knowledge, techniques, and medical technology continue to evolve at a rapid rate, many regulatory authorities require continuing medical education. Medical practitioners upgrade their knowledge in various ways, including medical journals, seminars, conferences, and online programs. A database of objectives covering medical knowledge, as suggested by national societies across the United States, can be searched at http://data.medobjectives.marian.edu/ Archived 4 October 2018 at the Wayback Machine. In most countries, it is a legal requirement for a medical doctor to be licensed or registered. In general, this entails a medical degree from a university and accreditation by a medical board or an equivalent national organization, which may ask the applicant to pass exams. This restricts the considerable legal authority of the medical profession to physicians that are trained and qualified by national standards. It is also intended as an assurance to patients and as a safeguard against charlatans that practice inadequate medicine for personal gain. While the laws generally require medical doctors to be trained in "evidence based", Western, or Hippocratic Medicine, they are not intended to discourage different paradigms of health. In the European Union, the profession of doctor of medicine is regulated. A profession is said to be regulated when access and exercise is subject to the possession of a specific professional qualification. The regulated professions database contains a list of regulated professions for doctor of medicine in the EU member states, EEA countries and Switzerland. This list is covered by the Directive 2005/36/EC. Doctors who are negligent or intentionally harmful in their care of patients can face charges of medical malpractice and be subject to civil, criminal, or professional sanctions. Medical ethics Medical ethics is a system of moral principles that apply values and judgments to the practice of medicine. As a scholarly discipline, medical ethics encompasses its practical application in clinical settings as well as work on its history, philosophy, theology, and sociology. Six of the values that commonly apply to medical ethics discussions are: autonomy – the patient has the right to refuse or choose their treatment. (Latin: Voluntas aegroti suprema lex.) beneficence – a practitioner should act in the best interest of the patient. (Latin: Salus aegroti suprema lex.) justice – concerns the distribution of scarce health resources, and the decision of who gets what treatment (fairness and equality). non-maleficence – "first, do no harm" (Latin: primum non-nocere). respect for persons – the patient (and the person treating the patient) have the right to be treated with dignity. truthfulness and honesty – the concept of informed consent has increased in importance since the historical events of the Doctors' Trial of the Nuremberg trials, Tuskegee syphilis experiment, and others. Values such as these do not give answers as to how to handle a particular situation, but provide a useful framework for understanding conflicts. When moral values are in conflict, the result may be an ethical dilemma or crisis. Sometimes, no good solution to a dilemma in medical ethics exists, and occasionally, the values of the medical community (i.e., the hospital and its staff) conflict with the values of the individual patient, family, or larger non-medical community. Conflicts can also arise between health care providers, or among family members. For example, some argue that the principles of autonomy and beneficence clash when patients refuse blood transfusions, considering them life-saving; and truth-telling was not emphasized to a large extent before the HIV era. History Ancient world Prehistoric medicine incorporated plants (herbalism), animal parts, and minerals. In many cases these materials were used ritually as magical substances by priests, shamans, or medicine men. Well-known spiritual systems include animism (the notion of inanimate objects having spirits), spiritualism (an appeal to gods or communion with ancestor spirits); shamanism (the vesting of an individual with mystic powers); and divination (magically obtaining the truth). The field of medical anthropology examines the ways in which culture and society are organized around or impacted by issues of health, health care and related issues. The earliest known medical texts in the world were found in the ancient Syrian city of Ebla and date back to 2500 BCE. Other early records on medicine have been discovered from ancient Egyptian medicine, Babylonian Medicine, Ayurvedic medicine (in the Indian subcontinent), classical Chinese medicine (Alternative medicine) predecessor to the modern traditional Chinese medicine), and ancient Greek medicine and Roman medicine. In Egypt, Imhotep (3rd millennium BCE) is the first physician in history known by name. The oldest Egyptian medical text is the Kahun Gynaecological Papyrus from around 2000 BCE, which describes gynaecological diseases. The Edwin Smith Papyrus dating back to 1600 BCE is an early work on surgery, while the Ebers Papyrus dating back to 1500 BCE is akin to a textbook on medicine. In China, archaeological evidence of medicine in Chinese dates back to the Bronze Age Shang dynasty, based on seeds for herbalism and tools presumed to have been used for surgery. The Huangdi Neijing, the progenitor of Chinese medicine, is a medical text written beginning in the 2nd century BCE and compiled in the 3rd century. In India, the oldest known surgical text, the Sushruta Samhita written by the surgeon Sushruta, described numerous surgical operations, including the earliest forms of plastic surgery as well as methods of sterilization for surgical instruments. The earliest records of dedicated hospitals come from Mihintale in Sri Lanka where evidence of dedicated medicinal treatment facilities for patients are found. In Greece, the ancient Greek physician Hippocrates, the "father of modern medicine", laid the foundation for a rational approach to medicine. Hippocrates introduced the Hippocratic Oath for physicians, which is still relevant and in use today, and was the first to categorize illnesses as acute, chronic, endemic and epidemic, and use terms such as, "exacerbation, relapse, resolution, crisis, paroxysm, peak, and convalescence". The Greek physician Galen was also one of the greatest surgeons of the ancient world and performed many audacious operations, including brain and eye surgeries. After the fall of the Western Roman Empire and the onset of the Early Middle Ages, the Greek tradition of medicine went into decline in Western Europe, although it continued uninterrupted in the Eastern Roman (Byzantine) Empire. Most of our knowledge of ancient Hebrew medicine during the 1st millennium BC comes from the Torah, i.e. the Five Books of Moses, which contain various health related laws and rituals. The Hebrew contribution to the development of modern medicine started in the Byzantine Era, with the physician Asaph the Jew. Middle Ages The concept of hospital as institution to offer medical care and possibility of a cure for the patients due to the ideals of Christian charity, rather than just merely a place to die, appeared in the Byzantine Empire. Although the concept of uroscopy was known to Galen, he did not see the importance of using it to localize the disease. It was under the Byzantines with physicians such of Theophilus Protospatharius that they realized the potential in uroscopy to determine disease in a time when no microscope or stethoscope existed. That practice eventually spread to the rest of Europe. After 750 CE, the Muslim world had the works of Hippocrates, Galen and Sushruta translated into Arabic, and Islamic physicians engaged in some significant medical research. Notable Islamic medical pioneers include the Persian polymath, Avicenna, who, along with Imhotep and Hippocrates, has also been called the "father of medicine". He wrote The Canon of Medicine which became a standard medical text at many medieval European universities, considered one of the most famous books in the history of medicine. Others include Abulcasis, Avenzoar, Ibn al-Nafis, and Averroes. Persian physician Rhazes was one of the first to question the Greek theory of humorism, which nevertheless remained influential in both medieval Western and medieval Islamic medicine. Some volumes of Rhazes's work Al-Mansuri, namely "On Surgery" and "A General Book on Therapy", became part of the medical curriculum in European universities. Additionally, he has been described as a doctor's doctor, the father of pediatrics, and a pioneer of ophthalmology. For example, he was the first to recognize the reaction of the eye's pupil to light. The Persian Bimaristan hospitals were an early example of public hospitals. In Europe, Charlemagne decreed that a hospital should be attached to each cathedral and monastery and the historian Geoffrey Blainey likened the activities of the Catholic Church in health care during the Middle Ages to an early version of a welfare state: "It conducted hospitals for the old and orphanages for the young; hospices for the sick of all ages; places for the lepers; and hostels or inns where pilgrims could buy a cheap bed and meal". It supplied food to the population during famine and distributed food to the poor. This welfare system the church funded through collecting taxes on a large scale and possessing large farmlands and estates. The Benedictine order was noted for setting up hospitals and infirmaries in their monasteries, growing medical herbs and becoming the chief medical care givers of their districts, as at the great Abbey of Cluny. The Church also established a network of cathedral schools and universities where medicine was studied. The Schola Medica Salernitana in Salerno, looking to the learning of Greek and Arab physicians, grew to be the finest medical school in medieval Europe. However, the fourteenth and fifteenth century Black Death devastated both the Middle East and Europe, and it has even been argued that Western Europe was generally more effective in recovering from the pandemic than the Middle East. In the early modern period, important early figures in medicine and anatomy emerged in Europe, including Gabriele Falloppio and William Harvey. The major shift in medical thinking was the gradual rejection, especially during the Black Death in the 14th and 15th centuries, of what may be called the "traditional authority" approach to science and medicine. This was the notion that because some prominent person in the past said something must be so, then that was the way it was, and anything one observed to the contrary was an anomaly (which was paralleled by a similar shift in European society in general – see Copernicus's rejection of Ptolemy's theories on astronomy). Physicians like Vesalius improved upon or disproved some of the theories from the past. The main tomes used both by medicine students and expert physicians were Materia Medica and Pharmacopoeia. Andreas Vesalius was the author of De humani corporis fabrica, an important book on human anatomy. Bacteria and microorganisms were first observed with a microscope by Antonie van Leeuwenhoek in 1676, initiating the scientific field microbiology. Independently from Ibn al-Nafis, Michael Servetus rediscovered the pulmonary circulation, but this discovery did not reach the public because it was written down for the first time in the "Manuscript of Paris" in 1546, and later published in the theological work for which he paid with his life in 1553. Later this was described by Renaldus Columbus and Andrea Cesalpino. Herman Boerhaave is sometimes referred to as a "father of physiology" due to his exemplary teaching in Leiden and textbook 'Institutiones medicae' (1708). Pierre Fauchard has been called "the father of modern dentistry". Modern Veterinary medicine was, for the first time, truly separated from human medicine in 1761, when the French veterinarian Claude Bourgelat founded the world's first veterinary school in Lyon, France. Before this, medical doctors treated both humans and other animals. Modern scientific biomedical research (where results are testable and reproducible) began to replace early Western traditions based on herbalism, the Greek "four humours" and other such pre-modern notions. The modern era really began with Edward Jenner's discovery of the smallpox vaccine at the end of the 18th century (inspired by the method of variolation originated in ancient China), Robert Koch's discoveries around 1880 of the transmission of disease by bacteria, and then the discovery of antibiotics around 1900. The post-18th century modernity period brought more groundbreaking researchers from Europe. From Germany and Austria, doctors Rudolf Virchow, Wilhelm Conrad Röntgen, Karl Landsteiner and Otto Loewi made notable contributions. In the United Kingdom, Alexander Fleming, Joseph Lister, Francis Crick and Florence Nightingale are considered important. Spanish doctor Santiago Ramón y Cajal is considered the father of modern neuroscience. From New Zealand and Australia came Maurice Wilkins, Howard Florey, and Frank Macfarlane Burnet. Others that did significant work include William Williams Keen, William Coley, James D. Watson (United States); Salvador Luria (Italy); Alexandre Yersin (Switzerland); Kitasato Shibasaburō (Japan); Jean-Martin Charcot, Claude Bernard, Paul Broca (France); Adolfo Lutz (Brazil); Nikolai Korotkov (Russia); Sir William Osler (Canada); and Harvey Cushing (United States). As science and technology developed, medicine became more reliant upon medications. Throughout history and in Europe right until the late 18th century, not only plant products were used as medicine, but also animal (including human) body parts and fluids. Pharmacology developed in part from herbalism and some drugs are still derived from plants (atropine, ephedrine, warfarin, aspirin, digoxin, vinca alkaloids, taxol, hyoscine, etc.). Vaccines were discovered by Edward Jenner and Louis Pasteur. The first antibiotic was arsphenamine (Salvarsan) discovered by Paul Ehrlich in 1908 after he observed that bacteria took up toxic dyes that human cells did not. The first major class of antibiotics was the sulfa drugs, derived by German chemists originally from azo dyes. Pharmacology has become increasingly sophisticated; modern biotechnology allows drugs targeted towards specific physiological processes to be developed, sometimes designed for compatibility with the body to reduce side-effects. Genomics and knowledge of human genetics and human evolution is having increasingly significant influence on medicine, as the causative genes of most monogenic genetic disorders have now been identified, and the development of techniques in molecular biology, evolution, and genetics are influencing medical technology, practice and decision-making. Evidence-based medicine is a contemporary movement to establish the most effective algorithms of practice (ways of doing things) through the use of systematic reviews and meta-analysis. The movement is facilitated by modern global information science, which allows as much of the available evidence as possible to be collected and analyzed according to standard protocols that are then disseminated to healthcare providers. The Cochrane Collaboration leads this movement. A 2001 review of 160 Cochrane systematic reviews revealed that, according to two readers, 21.3% of the reviews concluded insufficient evidence, 20% concluded evidence of no effect, and 22.5% concluded positive effect. Quality, efficiency, and access Evidence-based medicine, prevention of medical error (and other "iatrogenesis"), and avoidance of unnecessary health care are a priority in modern medical systems. These topics generate significant political and public policy attention, particularly in the United States where healthcare is regarded as excessively costly but population health metrics lag similar nations. Health spending varies by country, which results in differences in access to care and access to medicines. Health care rationing varies by country. Most developed countries provide health care to all citizens, with a few exceptions such as the United States where restrictions on health insurance coverage may limit affordability. Telemedicine Telemedicine (also Telehealth) refers to preventive, promotive, and curative care delivery, including remote clinical services, such as diagnosis, monitoring, administration and provider education. The main categories of telehealth: Telenursing is experiencing significant growth globally due to factors such as the need to reduce healthcare costs, an increasing aging and chronically ill population, and expanded healthcare coverage to distant, rural, small, or sparsely populated regions. Telenursing can help address nurse shortages, reduce travel time and distances, and minimize hospital admissions. Telepalliative care is a remote approach to optimising quality of life and relieving suffering for people with serious, complex and often fatal illnesses. The World Health Organization (WHO) recommends integrating palliative care as early as possible for any chronic and fatal illness. Telepalliative care typically utilizes telecommunication technologies like video conferencing, messaging for follow-ups, and digital symptom assessments through questionnaires that generate alerts for healthcare professionals. Telepharmacy involves delivering pharmaceutical care via telecommunications to patients in locations where direct contact with a pharmacist may not be possible or difficult. Telepsychiatry (telemental health) uses telecommunications technology to provide remote psychiatric care for individuals with mental health conditions. Telepsychology is the use of communication technology for the remote administration of psychological tests and psychotherapy. Teleneurotherapy utilizes computers and communications technology to deliver neurotherapy remotely. Research indicates that systematic physical stimuli from standard electronic devices, such as tablets with headphones, may treat injured nervous systems online by modulating neuronal plasticity. Evidence suggests that teleneurotherapy could enhance neurological treatment if it incorporates the therapeutic effect of a systematic abiotic impact of physical forces with key parameters of mother-fetus interaction. Recent research has shown therapeutic effects when implementing the APIN method in the online treatment of patients with various neurological conditions. Telenutrition refers to the use of video conferencing or telephony to provide online consultations by nutritionists or dieticians. Telerehabilitation (or e-rehabilitation) is the delivery of rehabilitation services over telecommunication networks and the Internet. Most services fall into two categories: clinical assessment (evaluating the patient's functional abilities in their environment) and clinical therapy. See also Notes == References ==

Engineering is the practice of using natural science, mathematics, and the engineering design process to solve problems within technology, increase efficiency and productivity, and improve systems. Modern engineering comprises many subfields which include designing and improving infrastructure, machinery, vehicles, electronics, materials, and energy systems. The discipline of engineering encompasses a broad range of more specialized fields of engineering, each with a more specific emphasis for applications of mathematics and science. See glossary of engineering. The word engineering is derived from the Latin ingenium. Definition The American Engineers' Council for Professional Development (the predecessor of the Accreditation Board for Engineering and Technology aka ABET) has defined "engineering" as: The creative application of scientific principles to design or develop structures, machines, apparatus, or manufacturing processes, or works utilizing them singly or in combination; or to construct or operate the same with full cognizance of their design; or to forecast their behavior under specific operating conditions; all as respects an intended function, economics of operation and safety to life and property. History Engineering has existed since ancient times, when humans devised inventions such as the wedge, lever, wheel and pulley, etc. The term engineering is derived from the word engineer, which itself dates back to the 14th century when an engine'er (literally, one who builds or operates a siege engine) referred to "a constructor of military engines". In this context, now obsolete, an "engine" referred to a military machine, i.e., a mechanical contraption used in war (for example, a catapult). Notable examples of the obsolete usage which have survived to the present day are military engineering corps, e.g., the U.S. Army Corps of Engineers. The word "engine" itself is of even older origin, ultimately deriving from the Latin ingenium (c. 1250), meaning "innate quality, especially mental power, hence a clever invention." Later, as the design of civilian structures, such as bridges and buildings, matured as a technical discipline, the term civil engineering entered the lexicon as a way to distinguish between those specializing in the construction of such non-military projects and those involved in the discipline of military engineering. Ancient era The pyramids in ancient Egypt, ziggurats of Mesopotamia, the Acropolis and Parthenon in Greece, the Roman aqueducts, Via Appia and Colosseum, Teotihuacán, and the Brihadeeswarar Temple of Thanjavur, among many others, stand as a testament to the ingenuity and skill of ancient civil and military engineers. Other monuments, no longer standing, such as the Hanging Gardens of Babylon and the Pharos of Alexandria, were important engineering achievements of their time and were considered among the Seven Wonders of the Ancient World. The six classic simple machines were known in the ancient Near East. The wedge and the inclined plane (ramp) were known since prehistoric times. The wheel, along with the wheel and axle mechanism, was invented in Mesopotamia (modern Iraq) during the 5th millennium BC. The lever mechanism first appeared around 5,000 years ago in the Near East, where it was used in a simple balance scale, and to move large objects in ancient Egyptian technology. The lever was also used in the shadoof water-lifting device, the first crane machine, which appeared in Mesopotamia c. 3000 BC, and then in ancient Egyptian technology c. 2000 BC. The earliest evidence of pulleys date back to Mesopotamia in the early 2nd millennium BC, and ancient Egypt during the Twelfth Dynasty (1991–1802 BC). The screw, the last of the simple machines to be invented, first appeared in Mesopotamia during the Neo-Assyrian period (911–609) BC. The Egyptian pyramids were built using three of the six simple machines, the inclined plane, the wedge, and the lever, to create structures like the Great Pyramid of Giza. The earliest civil engineer known by name is Imhotep. As one of the officials of the Pharaoh, Djosèr, he probably designed and supervised the construction of the Pyramid of Djoser (the Step Pyramid) at Saqqara in Egypt around 2630–2611 BC. The earliest practical water-powered machines, the water wheel and watermill, first appeared in the Persian Empire, in what are now Iraq and Iran, by the early 4th century BC. Kush developed the Sakia during the 4th century BC, which relied on animal power instead of human energy. Hafirs were developed as a type of reservoir in Kush to store and contain water as well as boost irrigation. Sappers were employed to build causeways during military campaigns. Kushite ancestors built speos during the Bronze Age between 3700 and 3250 BC. Bloomeries and blast furnaces were also created during the 7th centuries BC in Kush. Ancient Greece developed machines in both civilian and military domains. The Antikythera mechanism, an early known mechanical analog computer, and the mechanical inventions of Archimedes, are examples of Greek mechanical engineering. Some of Archimedes' inventions, as well as the Antikythera mechanism, required sophisticated knowledge of differential gearing or epicyclic gearing, two key principles in machine theory that helped design the gear trains of the Industrial Revolution, and are widely used in fields such as robotics and automotive engineering. Ancient Chinese, Greek, Roman and Hunnic armies employed military machines and inventions such as artillery which was developed by the Greeks around the 4th century BC, the trireme, the ballista and the catapult, the trebuchet by Chinese circa 6th-5th century BCE. Middle Ages The earliest practical wind-powered machines, the windmill and wind pump, first appeared in the Muslim world during the Islamic Golden Age, in what are now Iran, Afghanistan, and Pakistan, by the 9th century AD. The earliest practical steam-powered machine was a steam jack driven by a steam turbine, described in 1551 by Taqi al-Din Muhammad ibn Ma'ruf in Ottoman Egypt. The cotton gin was invented in India by the 6th century AD, and the spinning wheel was invented in the Islamic world by the early 11th century, both of which were fundamental to the growth of the cotton industry. The spinning wheel was also a precursor to the spinning jenny, which was a key development during the early Industrial Revolution in the 18th century. The earliest programmable machines were developed in the Muslim world. A music sequencer, a programmable musical instrument, was the earliest type of programmable machine. The first music sequencer was an automated flute player invented by the Banu Musa brothers, described in their Book of Ingenious Devices, in the 9th century. In 1206, Al-Jazari invented programmable automata/robots. He described four automaton musicians, including drummers operated by a programmable drum machine, where they could be made to play different rhythms and different drum patterns. Before the development of modern engineering, mathematics was used by artisans and craftsmen, such as millwrights, clockmakers, instrument makers and surveyors. Aside from these professions, universities were not believed to have had much practical significance to technology. A standard reference for the state of mechanical arts during the Renaissance is given in the mining engineering treatise De re metallica (1556), which also contains sections on geology, mining, and chemistry. De re metallica was the standard chemistry reference for the next 180 years. Modern era The science of classical mechanics, sometimes called Newtonian mechanics, formed the scientific basis of much of modern engineering. With the rise of engineering as a profession in the 18th century, the term became more narrowly applied to fields in which mathematics and science were applied to these ends. Similarly, in addition to military and civil engineering, the fields then known as the mechanic arts became incorporated into engineering. Canal building was an important engineering work during the early phases of the Industrial Revolution. John Smeaton was the first self-proclaimed civil engineer and is often regarded as the "father" of civil engineering. He was an English civil engineer responsible for the design of bridges, canals, harbors, and lighthouses. He was also a capable mechanical engineer and an eminent physicist. Using a model water wheel, Smeaton conducted experiments for seven years, determining ways to increase efficiency. Smeaton introduced iron axles and gears to water wheels. Smeaton also made mechanical improvements to the Newcomen steam engine. Smeaton designed the third Eddystone Lighthouse (1755–59) where he pioneered the use of 'hydraulic lime' (a form of mortar which will set under water) and developed a technique involving dovetailed blocks of granite in the building of the lighthouse. He is important in the history, rediscovery of, and development of modern cement, because he identified the compositional requirements needed to obtain "hydraulicity" in lime; work which led ultimately to the invention of Portland cement. Applied science led to the development of the steam engine. The sequence of events began with the invention of the barometer and the measurement of atmospheric pressure by Evangelista Torricelli in 1643, demonstration of the force of atmospheric pressure by Otto von Guericke using the Magdeburg hemispheres in 1656, laboratory experiments by Denis Papin, who built experimental model steam engines and demonstrated the use of a piston, which he published in 1707. Edward Somerset, 2nd Marquess of Worcester published a book of 100 inventions containing a method for raising waters similar to a coffee percolator. Samuel Morland, a mathematician and inventor who worked on pumps, left notes at the Vauxhall Ordinance Office on a steam pump design that Thomas Savery read. In 1698 Savery built a steam pump called "The Miner's Friend". It employed both vacuum and pressure. Iron merchant Thomas Newcomen, who built the first commercial piston steam engine in 1712, was not known to have any scientific training. The application of steam-powered cast iron blowing cylinders for providing pressurized air for blast furnaces lead to a large increase in iron production in the late 18th century. The higher furnace temperatures made possible with steam-powered blast allowed for the use of more lime in blast furnaces, which enabled the transition from charcoal to coke. These innovations lowered the cost of iron, making horse railways and iron bridges practical. The puddling process, patented by Henry Cort in 1784 produced large scale quantities of wrought iron. Hot blast, patented by James Beaumont Neilson in 1828, greatly lowered the amount of fuel needed to smelt iron. With the development of the high pressure steam engine, the power to weight ratio of steam engines made practical steamboats and locomotives possible. New steel making processes, such as the Bessemer process and the open hearth furnace, ushered in an area of heavy engineering in the late 19th century. One of the most famous engineers of the mid-19th century was Isambard Kingdom Brunel, who built railroads, dockyards and steamships. The Industrial Revolution created a demand for machinery with metal parts, which led to the development of several machine tools. Boring cast iron cylinders with precision was not possible until John Wilkinson invented his boring machine, which is considered the first machine tool. Other machine tools included the screw cutting lathe, milling machine, turret lathe and the metal planer. Precision machining techniques were developed in the first half of the 19th century. These included the use of gigs to guide the machining tool over the work and fixtures to hold the work in the proper position. Machine tools and machining techniques capable of producing interchangeable parts lead to large scale factory production by the late 19th century. The United States Census of 1850 listed the occupation of "engineer" for the first time with a count of 2,000. There were fewer than 50 engineering graduates in the U.S. before 1865. The first PhD in engineering (technically, applied science and engineering) awarded in the United States went to Josiah Willard Gibbs at Yale University in 1863; it was also the second PhD awarded in science in the U.S. In 1870 there were a dozen U.S. mechanical engineering graduates, with that number increasing to 43 per year in 1875. In 1890, there were 6,000 engineers in civil, mining, mechanical and electrical. There was no chair of applied mechanism and applied mechanics at Cambridge until 1875, and no chair of engineering at Oxford until 1907. Germany established technical universities earlier. The foundations of electrical engineering in the 1800s included the experiments of Alessandro Volta, Michael Faraday, Georg Ohm and others and the invention of the electric telegraph in 1816 and the electric motor in 1872. The theoretical work of James Maxwell (see: Maxwell's equations) and Heinrich Hertz in the late 19th century gave rise to the field of electronics. The later inventions of the vacuum tube and the transistor further accelerated the development of electronics to such an extent that electrical and electronics engineers currently outnumber their colleagues of any other engineering specialty. Chemical engineering developed in the late nineteenth century. Industrial scale manufacturing demanded new materials and new processes and by 1880 the need for large scale production of chemicals was such that a new industry was created, dedicated to the development and large scale manufacturing of chemicals in new industrial plants. The role of the chemical engineer was the design of these chemical plants and processes. Originally deriving from the manufacture of ceramics and its putative derivative metallurgy, materials science is one of the oldest forms of engineering. Modern materials science evolved directly from metallurgy, which itself evolved from the use of fire. Important elements of modern materials science were products of the Space Race; the understanding and engineering of the metallic alloys, and silica and carbon materials, used in building space vehicles enabling the exploration of space. Materials science has driven, and been driven by, the development of revolutionary technologies such as rubbers, plastics, semiconductors, and biomaterials. Aeronautical engineering deals with aircraft design process design while aerospace engineering is a more modern term that expands the reach of the discipline by including spacecraft design. Its origins can be traced back to the aviation pioneers around the start of the 20th century although the work of Sir George Cayley has recently been dated as being from the last decade of the 18th century. Early knowledge of aeronautical engineering was largely empirical with some concepts and skills imported from other branches of engineering. Only a decade after the successful flights by the Wright brothers, there was extensive development of aeronautical engineering through development of military aircraft that were used in World War I. Meanwhile, research to provide fundamental background science continued by combining theoretical physics with experiments. Branches of engineering Engineering is a broad discipline that is often broken down into several sub-disciplines. Although most engineers will usually be trained in a specific discipline, some engineers become multi-disciplined through experience. Engineering is often characterized as having five main branches: chemical engineering, civil engineering, electrical engineering, materials science and engineering, and mechanical engineering. Below is a list of recognized branches of engineering. There are additional sub-disciplines as well. Interdisciplinary engineering Interdisciplinary engineering draws from more than one of the principle branches of the practice. Historically, naval engineering and mining engineering were major branches. Other engineering fields are manufacturing engineering, acoustical engineering, corrosion engineering, instrumentation and control, automotive, information engineering, petroleum, systems, audio, software, architectural, biosystems, and textile engineering. These and other branches of engineering are represented in the 36 licensed member institutions of the UK Engineering Council. New specialties sometimes combine with the traditional fields and form new branches – for example, Earth systems engineering and management involves a wide range of subject areas including engineering studies, environmental science, engineering ethics and philosophy of engineering. Practice One who practices engineering is called an engineer, and those licensed to do so may have more formal designations such as Professional Engineer, Chartered Engineer, Incorporated Engineer, Ingenieur, European Engineer. There can also be what is called by the FAA a Designated Engineering Representative. Methodology In the engineering design process, engineers apply mathematics and sciences such as physics to find novel solutions to problems or to improve existing solutions. Engineers need proficient knowledge of relevant sciences for their design projects. As a result, many engineers continue to learn new material throughout their careers. If multiple solutions exist, engineers weigh each design choice based on their merit and choose the solution that best matches the requirements. The task of the engineer is to identify, understand, and interpret the constraints on a design in order to yield a successful result. It is generally insufficient to build a technically successful product, rather, it must also meet further requirements. Constraints may include available resources, physical, imaginative or technical limitations, flexibility for future modifications and additions, and other factors, such as requirements for cost, safety, marketability, productivity, and serviceability. By understanding the constraints, engineers derive specifications for the limits within which a viable object or system may be produced and operated. Problem solving Engineers use their knowledge of science, mathematics, logic, economics, and appropriate experience or tacit knowledge to find suitable solutions to a particular problem. Creating an appropriate mathematical model of a problem often allows them to analyze it (sometimes definitively), and to test potential solutions. More than one solution to a design problem usually exists so the different design choices have to be evaluated on their merits before the one judged most suitable is chosen. Genrich Altshuller, after gathering statistics on a large number of patents, suggested that compromises are at the heart of "low-level" engineering designs, while at a higher level the best design is one which eliminates the core contradiction causing the problem. Engineers typically attempt to predict how well their designs will perform to their specifications prior to full-scale production. They use, among other things: prototypes, scale models, simulations, destructive tests, nondestructive tests, and stress tests. Testing ensures that products will perform as expected but only in so far as the testing has been representative of use in service. For products, such as aircraft, that are used differently by different users failures and unexpected shortcomings (and necessary design changes) can be expected throughout the operational life of the product. Engineers take on the responsibility of producing designs that will perform as well as expected and, except those employed in specific areas of the arms industry, will not harm people. Engineers typically include a factor of safety in their designs to reduce the risk of unexpected failure. The study of failed products is known as forensic engineering. It attempts to identify the cause of failure to allow a redesign of the product and so prevent a re-occurrence. Careful analysis is needed to establish the cause of failure of a product. The consequences of a failure may vary in severity from the minor cost of a machine breakdown to large loss of life in the case of accidents involving aircraft and large stationary structures like buildings and dams. Computer use As with all modern scientific and technological endeavors, computers and software play an increasingly important role. As well as the typical business application software there are a number of computer aided applications (computer-aided technologies) specifically for engineering. Computers can be used to generate models of fundamental physical processes, which can be solved using numerical methods. One of the most widely used design tools in the profession is computer-aided design (CAD) software. It enables engineers to create 3D models, 2D drawings, and schematics of their designs. CAD together with digital mockup (DMU) and CAE software such as finite element method analysis or analytic element method allows engineers to create models of designs that can be analyzed without having to make expensive and time-consuming physical prototypes. These allow products and components to be checked for flaws; assess fit and assembly; study ergonomics; and to analyze static and dynamic characteristics of systems such as stresses, temperatures, electromagnetic emissions, electrical currents and voltages, digital logic levels, fluid flows, and kinematics. Access and distribution of all this information is generally organized with the use of product data management software. There are also many tools to support specific engineering tasks such as computer-aided manufacturing (CAM) software to generate CNC machining instructions; manufacturing process management software for production engineering; EDA for printed circuit board (PCB) and circuit schematics for electronic engineers; MRO applications for maintenance management; and Architecture, engineering and construction (AEC) software for civil engineering. In recent years the use of computer software to aid the development of goods has collectively come to be known as product lifecycle management (PLM). Social context The engineering profession engages in a range of activities, from collaboration at the societal level, and smaller individual projects. Almost all engineering projects are obligated to a funding source: a company, a set of investors, or a government. The types of engineering that are less constrained by such a funding source, are pro bono, and open-design engineering. Engineering has interconnections with society, culture and human behavior. Most products and constructions used by modern society, are influenced by engineering. Engineering activities have an impact on the environment, society, economies, and public safety. Engineering projects can be controversial. Examples from different engineering disciplines include: the development of nuclear weapons, the Three Gorges Dam, the design and use of sport utility vehicles and the extraction of oil. In response, some engineering companies have enacted serious corporate and social responsibility policies. The attainment of many of the Millennium Development Goals requires the achievement of sufficient engineering capacity to develop infrastructure and sustainable technological development. Overseas development and relief NGOs make considerable use of engineers, to apply solutions in disaster and development scenarios. Some charitable organizations use engineering directly for development: Engineers Without Borders Engineers Against Poverty Registered Engineers for Disaster Relief Engineers for a Sustainable World Engineering for Change Engineering Ministries International Engineering companies in more developed economies face challenges with regard to the number of engineers being trained, compared with those retiring. This problem is prominent in the UK where engineering has a poor image and low status. There are negative economic and political issues that this can cause, as well as ethical issues. It is agreed the engineering profession faces an "image crisis". The UK holds the most engineering companies compared to other European countries, together with the United States. Code of ethics Many engineering societies have established codes of practice and codes of ethics to guide members and inform the public at large. The National Society of Professional Engineers code of ethics states: Engineering is an important and learned profession. As members of this profession, engineers are expected to exhibit the highest standards of honesty and integrity. Engineering has a direct and vital impact on the quality of life for all people. Accordingly, the services provided by engineers require honesty, impartiality, fairness, and equity, and must be dedicated to the protection of the public health, safety, and welfare. Engineers must perform under a standard of professional behavior that requires adherence to the highest principles of ethical conduct. In Canada, engineers wear the Iron Ring as a symbol and reminder of the obligations and ethics associated with their profession. Relationships with other disciplines Science Scientists study the world as it is; engineers create the world that has never been. There exists an overlap between the sciences and engineering practice; in engineering, one applies science. Both areas of endeavor rely on accurate observation of materials and phenomena. Both use mathematics and classification criteria to analyze and communicate observations. Scientists may also have to complete engineering tasks, such as designing experimental apparatus or building prototypes. Conversely, in the process of developing technology, engineers sometimes find themselves exploring new phenomena, thus becoming, for the moment, scientists or more precisely "engineering scientists". In the book What Engineers Know and How They Know It, Walter Vincenti asserts that engineering research has a character different from that of scientific research. First, it often deals with areas in which the basic physics or chemistry are well understood, but the problems themselves are too complex to solve in an exact manner. There is a "real and important" difference between engineering and physics as similar to any science field has to do with technology. Physics is an exploratory science that seeks knowledge of principles while engineering uses knowledge for practical applications of principles. The former equates an understanding into a mathematical principle while the latter measures variables involved and creates technology. For technology, physics is an auxiliary and in a way technology is considered as applied physics. Though physics and engineering are interrelated, it does not mean that a physicist is trained to do an engineer's job. A physicist would typically require additional and relevant training. Physicists and engineers engage in different lines of work. But PhD physicists who specialize in sectors of engineering physics and applied physics are titled as Technology officer, R&D Engineers and System Engineers. An example of this is the use of numerical approximations to the Navier–Stokes equations to describe aerodynamic flow over an aircraft, or the use of the finite element method to calculate the stresses in complex components. Second, engineering research employs many semi-empirical methods that are foreign to pure scientific research, one example being the method of parameter variation. As stated by Fung et al. in the revision to the classic engineering text Foundations of Solid Mechanics: Engineering is quite different from science. Scientists try to understand nature. Engineers try to make things that do not exist in nature. Engineers stress innovation and invention. To embody an invention the engineer must put his idea in concrete terms, and design something that people can use. That something can be a complex system, device, a gadget, a material, a method, a computing program, an innovative experiment, a new solution to a problem, or an improvement on what already exists. Since a design has to be realistic and functional, it must have its geometry, dimensions, and characteristics data defined. In the past engineers working on new designs found that they did not have all the required information to make design decisions. Most often, they were limited by insufficient scientific knowledge. Thus they studied mathematics, physics, chemistry, biology and mechanics. Often they had to add to the sciences relevant to their profession. Thus engineering sciences were born. Although engineering solutions make use of scientific principles, engineers must also take into account safety, efficiency, economy, reliability, and constructability or ease of fabrication as well as the environment, ethical and legal considerations such as patent infringement or liability in the case of failure of the solution. Medicine and biology The study of the human body, albeit from different directions and for different purposes, is an important common link between medicine and some engineering disciplines. Medicine aims to sustain, repair, enhance and even replace functions of the human body, if necessary, through the use of technology. Modern medicine can replace several of the body's functions through the use of artificial organs and can significantly alter the function of the human body through artificial devices such as, for example, brain implants and pacemakers. The fields of bionics and medical bionics are dedicated to the study of synthetic implants pertaining to natural systems. Conversely, some engineering disciplines view the human body as a biological machine worth studying and are dedicated to emulating many of its functions by replacing biology with technology. This has led to fields such as artificial intelligence, neural networks, fuzzy logic, and robotics. There are also substantial interdisciplinary interactions between engineering and medicine. Both fields provide solutions to real world problems. This often requires moving forward before phenomena are completely understood in a more rigorous scientific sense and therefore experimentation and empirical knowledge is an integral part of both. Medicine, in part, studies the function of the human body. The human body, as a biological machine, has many functions that can be modeled using engineering methods. The heart for example functions much like a pump, the skeleton is like a linked structure with levers, the brain produces electrical signals etc. These similarities as well as the increasing importance and application of engineering principles in medicine, led to the development of the field of biomedical engineering that uses concepts developed in both disciplines. Newly emerging branches of science, such as systems biology, are adapting analytical tools traditionally used for engineering, such as systems modeling and computational analysis, to the description of biological systems. Art There are connections between engineering and art, for example, architecture, landscape architecture and industrial design (even to the extent that these disciplines may sometimes be included in a university's Faculty of Engineering). The Art Institute of Chicago, for instance, held an exhibition about the art of NASA's aerospace design. Robert Maillart's bridge design is perceived by some to have been deliberately artistic. At the University of South Florida, an engineering professor, through a grant with the National Science Foundation, has developed a course that connects art and engineering. Among famous historical figures, Leonardo da Vinci is a well-known Renaissance artist and engineer, and a prime example of the nexus between art and engineering. Business Business engineering deals with the relationship between professional engineering, IT systems, business administration and change management. Engineering management or "Management engineering" is a specialized field of management concerned with engineering practice or the engineering industry sector. The demand for management-focused engineers (or from the opposite perspective, managers with an understanding of engineering), has resulted in the development of specialized engineering management degrees that develop the knowledge and skills needed for these roles. During an engineering management course, students will develop industrial engineering skills, knowledge, and expertise, alongside knowledge of business administration, management techniques, and strategic thinking. Engineers specializing in change management must have in-depth knowledge of the application of industrial and organizational psychology principles and methods. Professional engineers often train as certified management consultants in the very specialized field of management consulting applied to engineering practice or the engineering sector. This work often deals with large scale complex business transformation or business process management initiatives in aerospace and defence, automotive, oil and gas, machinery, pharmaceutical, food and beverage, electrical and electronics, power distribution and generation, utilities and transportation systems. This combination of technical engineering practice, management consulting practice, industry sector knowledge, and change management expertise enables professional engineers who are also qualified as management consultants to lead major business transformation initiatives. These initiatives are typically sponsored by C-level executives. Other fields In political science, the term engineering has been borrowed for the study of the subjects of social engineering and political engineering, which deal with forming political and social structures using engineering methodology coupled with political science principles. Marketing engineering and financial engineering have similarly borrowed the term. See also Lists Glossaries Related subjects References Further reading External links The dictionary definition of engineering at Wiktionary Learning materials related to Engineering at Wikiversity Quotations related to Engineering at Wikiquote Works related to Engineering at Wikisource

Astronomy is a natural science that studies celestial objects and the phenomena that occur in the cosmos. It uses mathematics, physics, and chemistry to explain their origin and their overall evolution. Objects of interest include planets, moons, stars, nebulae, galaxies, meteoroids, asteroids, and comets. Relevant phenomena include supernova explosions, gamma ray bursts, quasars, blazars, pulsars, and cosmic microwave background radiation. More generally, astronomy studies everything that originates beyond Earth's atmosphere. Cosmology is the branch of astronomy that studies the universe as a whole. Astronomy is one of the oldest natural sciences. The early civilizations in recorded history made methodical observations of the night sky. These include the Egyptians, Babylonians, Greeks, Indians, Chinese, Maya, and many ancient indigenous peoples of the Americas. In the past, astronomy included disciplines as diverse as astrometry, celestial navigation, observational astronomy, and the making of calendars. Professional astronomy is split into observational and theoretical branches. Observational astronomy is focused on acquiring data from observations of astronomical objects. This data is then analyzed using basic principles of physics. Theoretical astronomy is oriented toward the development of computer or analytical models to describe astronomical objects and phenomena. These two fields complement each other. Theoretical astronomy seeks to explain observational results and observations are used to confirm theoretical results. Astronomy is one of the few sciences in which amateurs play an active role. This is especially true for the discovery and observation of transient events. Amateur astronomers have helped with many important discoveries, such as finding new comets. Etymology Astronomy (from the Greek ἀστρονομία from ἄστρον astron, "star" and -νομία -nomia from νόμος nomos, "law" or "rule") means study of celestial objects. Astronomy should not be confused with astrology, the belief system which claims that human affairs are correlated with the positions of celestial objects. The two fields share a common origin but became distinct, astronomy being supported by physics while astrology is not. Use of terms "astronomy" and "astrophysics" "Astronomy" and "astrophysics" are broadly synonymous in modern usage. In dictionary definitions, "astronomy" is "the study of objects and matter outside the Earth's atmosphere and of their physical and chemical properties", while "astrophysics" is the branch of astronomy dealing with "the behavior, physical properties, and dynamic processes of celestial objects and phenomena". Sometimes, as in the introduction of the introductory textbook The Physical Universe by Frank Shu, "astronomy" means the qualitative study of the subject, whereas "astrophysics" is the physics-oriented version of the subject. Some fields, such as astrometry, are in this sense purely astronomy rather than also astrophysics. Research departments may use "astronomy" and "astrophysics" according to whether the department is historically affiliated with a physics department, and many professional astronomers have physics rather than astronomy degrees. Thus, in modern use, the two terms are often used interchangeably. History Pre-historic The initial development of astronomy was driven by practical needs like agricultural calendars. Before recorded history archeological sites such as Stonehenge provide evidence of ancient interest in astronomical observations. Evidence also comes from artefacts such as the Nebra sky disc which serves as an astronomical calendar, defining a year as twelve lunar months, 354 days, with intercalary months to make up the solar year. The disc is inlaid with symbols interpreted as a sun, moon, and stars including a cluster of seven stars. Classical Civilizations such as Egypt, Mesopotamia, Greece, India, China together – with cross-cultural influences – created astronomical observatories and developed ideas on the nature of the Universe, along with calendars and astronomical instruments. A key early development was the beginning of mathematical and scientific astronomy among the Babylonians, laying the foundations for astronomical traditions in other civilizations. The Babylonians discovered that lunar eclipses recurred in the saros cycle of 223 synodic months. Following the Babylonians, significant advances were made in ancient Greece and the Hellenistic world. Greek astronomy sought a rational, physical explanation for celestial phenomena. In the 3rd century BC, Aristarchus of Samos estimated the size and distance of the Moon and Sun, and he proposed a model of the Solar System where the Earth and planets rotated around the Sun, now called the heliocentric model. In the 2nd century BC, Hipparchus calculated the size and distance of the Moon and invented the earliest known astronomical devices such as the astrolabe. He also observed the small drift in the positions of the equinoxes and solstices with respect to the fixed stars that we now know is caused by precession. Hipparchus also created a catalog of 1020 stars, and most of the constellations of the northern hemisphere derive from Greek astronomy. The Antikythera mechanism (c. 150–80 BC) was an early analog computer designed to calculate the location of the Sun, Moon, and planets for a given date. Technological artifacts of similar complexity did not reappear until the 14th century, when mechanical astronomical clocks appeared in Europe. After the classical Greek era, astronomy was dominated by the geocentric model of the Universe, or the Ptolemaic system, named after Claudius Ptolemy. His 13-volume astronomy work, named the Almagest in its Arabic translation, became the primary reference for over a thousand years. In this system, the Earth was believed to be the center of the Universe with the Sun, the Moon and the stars rotating around it. While the system would eventually be discredited it gave the most accurate predictions for the positions of astronomical bodies available at that time. Post-classical Astronomy flourished in the medieval Islamic world. Astronomical observatories were established there by the early 9th century. In 964, the Andromeda Galaxy, the largest galaxy in the Local Group, was described by the Persian Muslim astronomer Abd al-Rahman al-Sufi in his Book of Fixed Stars. The SN 1006 supernova, the brightest apparent magnitude stellar event in the last 1000 years, was observed by the Egyptian Arabic astronomer Ali ibn Ridwan and Chinese astronomers in 1006. Iranian scholar Al-Biruni observed that, contrary to Ptolemy, the Sun's apogee (highest point in the heavens) was mobile, not fixed. Arabic astronomers introduced many Arabic names now used for individual stars. The ruins at Great Zimbabwe and Timbuktu may have housed astronomical observatories. In Post-classical West Africa, astronomers studied the movement of stars and relation to seasons, crafting charts of the heavens and diagrams of orbits of the other planets based on complex mathematical calculations. Songhai historian Mahmud Kati documented a meteor shower in 1583. In medieval Europe, Richard of Wallingford (1292–1336) invented the first astronomical clock, the Rectangulus which allowed for the measurement of angles between planets and other astronomical bodies, as well as an equatorium called the Albion which could be used for astronomical calculations such as lunar, solar and planetary longitudes. Nicole Oresme (1320–1382) discussed evidence for the rotation of the Earth. Jean Buridan (1300–1361) developed the theory of impetus, describing motions including of the celestial bodies. For over six centuries (from the recovery of ancient learning during the late Middle Ages into the Enlightenment), the Roman Catholic Church gave more financial and social support to the study of astronomy than probably all other institutions. Among the Church's motives was finding the date for Easter. Early telescopic During the Renaissance, Nicolaus Copernicus proposed a heliocentric model of the solar system. In 1610, Galileo Galilei observed phases on the planet Venus similar to those of the Moon, supporting the heliocentric model. Around the same time the heliocentric model was organized quantitatively by Johannes Kepler. Analyzing two decades of careful observations by Tycho Brahe, Kepler devised a system that described the details of the motion of the planets around the Sun. While Kepler discarded the uniform circular motion of Copernicus in favor of elliptical motion, he did not succeed in formulating a theory behind the laws he wrote down. It was Isaac Newton, with his invention of celestial dynamics and his law of gravitation, who finally explained the motions of the planets. Newton also developed the reflecting telescope. Newton, in collaboration with Richard Bentley proposed that stars are like the Sun only much further away. The new telescopes also altered ideas about stars. By 1610 Galileo discovered that the band of light crossing the sky at night that we call the Milky Way was composed of numerous stars. In 1668 James Gregory compared the luminosity of Jupiter to Sirius to estimate its distance at over 83,000 AU. The English astronomer John Flamsteed, Britain's first Astronomer Royal, catalogued over 3000 stars but the data were published against his wishes in 1712. The astronomer William Herschel made a detailed catalog of nebulosity and clusters, and in 1781 discovered the planet Uranus, the first new planet found. Friedrich Bessel developed the technique of stellar parallax in 1838 but it was so difficult to apply that only about 100 stars were measured by 1900. During the 18–19th centuries, the study of the three-body problem by Leonhard Euler, Alexis Claude Clairaut, and Jean le Rond d'Alembert led to more accurate predictions about the motions of the Moon and planets. This work was further refined by Joseph-Louis Lagrange and Pierre Simon Laplace, allowing the masses of the planets and moons to be estimated from their perturbations. Significant advances in astronomy came about with the introduction of new technology, including the spectroscope and astrophotography. In 1814–15, Joseph von Fraunhofer discovered some 574 dark lines in the spectrum of the sun and of other stars. In 1859, Gustav Kirchhoff ascribed these lines to the presence of different elements. Galaxies In the late 1700s William Herschel mapped the distribution of stars in different directions from Earth, concluding that the universe consisted of the Sun near the center of disk of stars, the Milky Way. After John Mitchell demonstrated that stars differ in intrinsic luminosity and after Herschel's own observations with more powerful telescopes that additional stars appeared in all directions, astronomers began to consider that some of the fuzzy spiral_nebula were distant island Universes. The existence of galaxies, including the Earth's galaxy, the Milky Way, as a group of stars was only demonstrated in the 20th century. In 1912, Henrietta Leavitt discovered Cepheid variable stars with well-defined, periodic luminosity changes which can be used to fix the star's true luminosity which then becomes an accurate tool for distance estimates. Using Cepheid variable stars, Harlow Shapley constructed the first accurate map of the Milky Way. Using the Hooker Telescope, Edwin Hubble identified Cepheid variables in several spiral nebulae and in 1922–1923 proved conclusively that Andromeda Nebula and Triangulum among others, were entire galaxies outside our own, thus proving that the universe consists of a multitude of galaxies. Cosmology Albert Einstein's 1917 publication of general relativity began the modern era of theoretical models of the universe as a whole. In 1922, Alexander Friedman published simplified models for the universe showing static, expanding and contracting solutions. In 1929 Hubble published observations that the galaxies are all moving away from Earth with a velocity proportional to distance, a relation now known as Hubble's law. This relation is expected if the universe is expanding. The consequence that the universe was once very dense and hot, a Big Bang concept expounded by Georges Lemaître in 1927, was discussed but no experimental evidence was available to support it. From the 1940s on, nuclear reaction rates under high density conditions were studied leading to the development of a successful model of big bang nucleosynthesis in the late 1940s and early 1950s. Then in 1965 cosmic microwave background radiation was discovered, cementing the evidence for the Big Bang. Theoretical astronomy predicted the existence of objects such as black holes and neutron stars. These have been used to explain phenomena such as quasars and pulsars. Space telescopes have enabled measurements in parts of the electromagnetic spectrum normally blocked or blurred by the atmosphere. The LIGO project detected evidence of gravitational waves in 2015. Observational astronomy Observational astronomy relies on many different wavelengths of electromagnetic radiation and the forms of astronomy are categorized according to the corresponding region of the electromagnetic spectrum on which the observations are made. Specific information on these subfields is given below. Radio Radio astronomy uses radiation with long wavelengths, mainly between 1 millimeter and 15 meters (frequencies from 20 MHz to 300 GHz), far outside the visible range. Hydrogen, otherwise an invisible gas, produces a spectral line at 21 cm (1420 MHz) which is observable at radio wavelengths. Objects observable at radio wavelengths include interstellar gas, pulsars, fast radio bursts, supernovae, and active galactic nuclei. Infrared Infrared astronomy detects infrared radiation with wavelengths longer than red visible light, outside the range of our vision. The infrared spectrum is useful for studying objects that are too cold to radiate visible light, such as planets, circumstellar disks or nebulae whose light is blocked by dust. The longer wavelengths of infrared can penetrate clouds of dust that block visible light, allowing the observation of young stars embedded in molecular clouds and the cores of galaxies. Observations from the Wide-field Infrared Survey Explorer (WISE) have been particularly effective at unveiling numerous galactic protostars and their host star clusters. With the exception of infrared wavelengths close to visible light, such radiation is heavily absorbed by the atmosphere, or masked, as the atmosphere itself produces significant infrared emission. Consequently, infrared observatories have to be located in high, dry places on Earth or in space. Some molecules radiate strongly in the infrared. This allows the study of the chemistry of space. The James Webb Space Telescope senses infrared radiation to detect very distant galaxies. Visible light from these galaxies was emitted billions of years ago and the expansion of the universe shifted the light in to the infrared range. By studying these distant galaxies astronomers hope to learn about the formation of the first galaxies. Optical Historically, optical astronomy, which has been also called visible light astronomy, is the oldest form of astronomy. Images of observations were originally drawn by hand. In the late 19th century and most of the 20th century, images were made using photographic equipment. Modern images are made using digital detectors, particularly using charge-coupled devices (CCDs) and recorded on modern medium. Although visible light itself extends from approximately 380 to 700 nm that same equipment can be used to observe some near-ultraviolet and near-infrared radiation. Ultraviolet Ultraviolet astronomy employs ultraviolet wavelengths which are absorbed by the Earth's atmosphere, requiring observations from the upper atmosphere or from space. Ultraviolet astronomy is best suited to the study of thermal radiation and spectral emission lines from hot blue OB stars that are very bright at these wavelengths. X-ray X-ray astronomy uses X-radiation, produced by extremely hot and high-energy processes. Since X-rays are absorbed by the Earth's atmosphere, observations must be performed at high altitude, such as from balloons, rockets, or specialized satellites. X-ray sources include X-ray binaries, supernova remnants, clusters of galaxies, and active galactic nuclei. Since the Sun's surface is relatively cool, X-ray images of the Sun and other stars give valuable information on the hot solar corona. Gamma-ray Gamma ray astronomy observes astronomical objects at the shortest wavelengths (highest energy) of the electromagnetic spectrum. Gamma rays may be observed directly by satellites such as the Compton Gamma Ray Observatory, or by specialized telescopes called atmospheric Cherenkov telescopes. Cherenkov telescopes do not detect the gamma rays directly but instead detect the flashes of visible light produced when gamma rays are absorbed by the Earth's atmosphere. Gamma-ray astronomy provides information on the origin of cosmic rays, possible annihilation events for dark matter, relativistic particles outflows from active galactic nuclei (AGN), and, using AGN as distant sources, properties of intergalactic space. Gamma-ray bursts, which radiate transiently, are extremely energetic events, and are the brightest (most luminous) phenomena in the universe. Non-electomagnetic observation Some events originating from great distances may be observed from the Earth using systems that do not rely on electromagnetic radiation. In neutrino astronomy, astronomers use heavily shielded underground facilities such as SAGE, GALLEX, and Kamioka II/III for the detection of neutrinos. The vast majority of the neutrinos streaming through the Earth originate from the Sun, but 24 neutrinos were also detected from supernova 1987A. Cosmic rays, which consist of very high energy particles (atomic nuclei) that can decay or be absorbed when they enter the Earth's atmosphere, result in a cascade of secondary particles which can be detected by current observatories. Gravitational-wave astronomy employs gravitational-wave detectors to collect observational data about distant massive objects. A few observatories have been constructed, such as the Laser Interferometer Gravitational Observatory LIGO. LIGO made its first detection on 14 September 2015, observing gravitational waves from a binary black hole. A second gravitational wave was detected on 26 December 2015 and additional observations should continue but gravitational waves require extremely sensitive instruments. The combination of observations made using electromagnetic radiation, neutrinos or gravitational waves and other complementary information, is known as multi-messenger astronomy. Astrometry and celestial mechanics One of the oldest fields in astronomy, and in all of science, is the measurement of the positions of celestial objects known as astrometry. Historically, accurate knowledge of the positions of the Sun, Moon, planets and stars has been essential in celestial navigation (the use of celestial objects to guide navigation) and in the making of calendars. Careful measurement of the positions of the planets has led to a solid understanding of gravitational perturbations, and an ability to determine past and future positions of the planets with great accuracy, a field known as celestial mechanics. The measurement of stellar parallax of nearby stars provides a fundamental baseline in the cosmic distance ladder that is used to measure the scale of the Universe. Parallax measurements of nearby stars provide an absolute baseline for the properties of more distant stars, as their properties can be compared. Measurements of the radial velocity and proper motion of stars allow astronomers to plot the movement of these systems through the Milky Way galaxy. Theoretical astronomy Theoretical astronomers use several tools including analytical models and computational numerical simulations; each has its particular advantages. Analytical models of a process are better for giving broader insight into the heart of what is going on. Numerical models reveal the existence of phenomena and effects otherwise unobserved. Modern theoretical astronomy reflects dramatic advances in observation since the 1990s, including studies of the cosmic microwave background, distant supernovae and galaxy redshifts, which have led to the development of a standard model of cosmology. This model requires the universe to contain large amounts of dark matter and dark energy whose nature is currently not well understood, but the model gives detailed predictions that are in excellent agreement with many diverse observations. Subfields by scale Physical cosmology Physical cosmology, the study of large-scale structure of the Universe, seeks to understand the formation and evolution of the cosmos. Fundamental to modern cosmology is the well-accepted theory of the Big Bang, the concept that the universe begin extremely dense and hot, then expanded over the course of 13.8 billion years to its present condition. The concept of the Big Bang became widely accepted after the discovery of the microwave background radiation in 1965. Fundamental to the structure of the Universe is the existence of dark matter and dark energy. These are now thought to be its dominant components, forming 96% of the mass of the Universe. For this reason, much effort is expended in trying to understand the physics of these components. Extragalactic The study of objects outside our galaxy is concerned with the formation and evolution of galaxies, their morphology (description) and classification, the observation of active galaxies, and at a larger scale, the groups and clusters of galaxies. These assist the understanding of the large-scale structure of the cosmos. Galactic Galactic astronomy studies galaxies including the Milky Way, a barred spiral galaxy that is a prominent member of the Local Group of galaxies and contains the Solar System. It is a rotating mass of gas, dust, stars and other objects, held together by mutual gravitational attraction. As the Earth is within the dusty outer arms, large portions of the Milky Way are obscured from view. Kinematic studies of matter in the Milky Way and other galaxies show there is more mass than can be accounted for by visible matter. A dark matter halo appears to dominate the mass, although the nature of this dark matter remains undetermined. Stellar The study of stars and stellar evolution is fundamental to our understanding of the Universe. The astrophysics of stars has been determined through observation and theoretical understanding; and from computer simulations of the interior. Aspects studied include star formation in giant molecular clouds; the formation of protostars; and the transition to nuclear fusion and main-sequence stars, carrying out nucleosynthesis. Further processes studied include stellar evolution, ending either with supernovae or white dwarfs. The ejection of the outer layers forms a planetary nebula. The remnant of a supernova is a dense neutron star, or, if the stellar mass was at least three times that of the Sun, a black hole. Solar Solar astronomy is the study of the Sun, a typical main-sequence dwarf star of stellar class G2 V, and about 4.6 billion years (Gyr) old. Processes studied by the science include the sunspot cycle, the sun's changes in luminosity, both steady and periodic, and the behavior of the sun's various layers, namely its core with its nuclear fusion, the radiation zone, the convection zone, the photosphere, the chromosphere, and the corona. Planetary science Planetary science is the study of the assemblage of planets, moons, dwarf planets, comets, asteroids, and other bodies orbiting the Sun, as well as exoplanets orbiting distant stars. The Solar System has been relatively well-studied, initially through telescopes and then later by spacecraft. Processes studied include planetary differentiation; the generation of, and effects created by, a planetary magnetic field; and the creation of heat within a planet, such as by collisions, radioactive decay, and tidal heating. In turn, that heat can drive geologic processes such as volcanism, tectonics, and surface erosion, studied by branches of geology. Interdisciplinary subfields Astrochemistry Astrochemistry is an overlap of astronomy and chemistry. It studies the abundance and reactions of molecules in the Universe, and their interaction with radiation. The word "astrochemistry" may be applied to both the Solar System and the interstellar medium. Studies in this field contribute for example to the understanding of the formation of the Solar System. Astrobiology Astrobiology (or exobiology) studies the origin of life and its development other than on earth. It considers whether extraterrestrial life exists, and how humans can detect it if it does. It makes use of astronomy, biochemistry, geology, microbiology, physics, and planetary science to investigate the possibility of life on other worlds and help recognize biospheres that might be different from that on Earth. The origin and early evolution of life is an inseparable part of the discipline of astrobiology. That encompasses research on the origin of planetary systems, origins of organic compounds in space, rock-water-carbon interactions, abiogenesis on Earth, planetary habitability, research on biosignatures for life detection, and studies on the potential for life to adapt to challenges on Earth and in outer space. Other Astronomy and astrophysics have developed interdisciplinary links with other major scientific fields. Archaeoastronomy is the study of ancient or traditional astronomies in their cultural context, using archaeological and anthropological evidence. Astrostatistics is the application of statistics to the analysis of large quantities of observational astrophysical data. As "forensic astronomy", finally, methods from astronomy have been used to solve problems of art history and occasionally of law. Amateur Astronomy is one of the sciences to which amateurs can contribute the most. Collectively, amateur astronomers observe celestial objects and phenomena, sometimes with consumer-level equipment or equipment that they build themselves. Common targets include the Sun, the Moon, planets, stars, comets, meteor showers, and deep-sky objects such as star clusters, galaxies, and nebulae. Astronomy clubs throughout the world have programs to help their members set up and run observational programs such as to observe all the objects in the Messier (110 objects) or Herschel 400 catalogues. Most amateurs work at visible wavelengths, but some have experimented with wavelengths outside the visible spectrum. The pioneer of amateur radio astronomy, Karl Jansky, discovered a radio source at the centre of the Milky Way. Some amateur astronomers use homemade telescopes or radio telescopes originally built for astronomy research (e.g. the One-Mile Telescope). Amateurs can make occultation measurements to refine the orbits of minor planets. They can discover comets, and perform regular observations of variable stars. Improvements in digital technology have allowed amateurs to make advances in astrophotography. Unsolved problems In the 21st century, there remain important unanswered questions in astronomy. Some are cosmic in scope: for example, what are the dark matter and dark energy that dominate the evolution and fate of the cosmos? What will be the ultimate fate of the universe? Why is the abundance of lithium in the cosmos four times lower than predicted by the standard Big Bang model? Others pertain to more specific classes of phenomena. For example, is the Solar System normal or atypical? What is the origin of the stellar mass spectrum, i.e. why do astronomers observe the same distribution of stellar masses—the initial mass function—regardless of initial conditions? Likewise, questions remain about the formation of the first galaxies, the origin of supermassive black holes, the source of ultra-high-energy cosmic rays, and whether there is other life in the Universe, especially other intelligent life. See also Cosmogony – Theory or model concerning the origin of the universe Outline of astronomy – Overview of the scientific field of astronomy Outline of space science – Overview of and topical guide to space science Space exploration – Investigation of space, planets, and moons Lists Glossary of astronomy List of astronomers List of astronomical instruments List of astronomical observatories List of astronomy acronyms List of astronomical societies List of software for astronomy research and education References Sources Forbes, George (1909). History of Astronomy. London: Plain Label Books. ISBN 978-1-60303-159-2. Archived from the original on 28 August 2018. Retrieved 7 April 2019. {{cite book}}: ISBN / Date incompatibility (help) Harpaz, Amos (1994). Stellar Evolution. A K Peters. ISBN 978-1-56881-012-6. Unsöld, A.; Baschek, B. (2001). The New Cosmos: An Introduction to Astronomy and Astrophysics. Springer. ISBN 978-3-540-67877-9. External links NASA/IPAC Extragalactic Database (NED) (NED-Distances) Core books and Core journals in Astronomy, from the Smithsonian/NASA Astrophysics Data System

Environmental science is an interdisciplinary academic field that integrates physics, biology, meteorology, mathematics and geography (including ecology, chemistry, plant science, zoology, mineralogy, oceanography, limnology, soil science, geology and physical geography, and atmospheric science) to the study of the environment, and the solution of environmental problems. Environmental science emerged from the fields of natural history and medicine during the Enlightenment. Today it provides an integrated, quantitative, and interdisciplinary approach to the study of environmental systems. Environmental Science is the study of the environment, the processes it undergoes, and the issues that arise generally from the interaction of humans and the natural world. It is an interdisciplinary science because it is an integration of various fields such as: biology, chemistry, physics, geology, engineering, sociology, and most especially ecology. All these scientific disciplines are relevant to the identification and resolution of environmental problems. Environmental science came alive as a substantive, active field of scientific investigation in the 1960s and 1970s driven by (a) the need for a multi-disciplinary approach to analyze complex environmental problems, (b) the arrival of substantive environmental laws requiring specific environmental protocols of investigation and (c) the growing public awareness of a need for action in addressing environmental problems. Events that spurred this development included the publication of Rachel Carson's landmark environmental book Silent Spring along with major environmental issues becoming very public, such as the 1969 Santa Barbara oil spill, and the Cuyahoga River of Cleveland, Ohio, "catching fire" (also in 1969), and helped increase the visibility of environmental issues and create this new field of study. Terminology In common usage, "environmental science" and "ecology" are often used interchangeably, but technically, ecology refers only to the study of organisms and their interactions with each other as well as how they interrelate with environment. Ecology could be considered a subset of environmental science, which also could involve purely chemical or public health issues (for example) ecologists would be unlikely to study. In practice, there are considerable similarities between the work of ecologists and other environmental scientists. There is substantial overlap between ecology and environmental science with the disciplines of fisheries, forestry, and wildlife. Environmental studies incorporates more of the social sciences for understanding human relationships, perceptions and policies towards the environment. Environmental engineering focuses on design and technology for improving environmental quality in every aspect. History Ancient civilizations Historical concern for environmental issues is well documented in archives around the world. Ancient civilizations were mainly concerned with what is now known as environmental science insofar as it related to agriculture and natural resources. Scholars believe that early interest in the environment began around 6000 BCE when ancient civilizations in Israel and Jordan collapsed due to deforestation. As a result, in 2700 BCE the first legislation limiting deforestation was established in Mesopotamia. Two hundred years later, in 2500 BCE, a community residing in the Indus River Valley observed the nearby river system in order to improve sanitation. This involved manipulating the flow of water to account for public health. In the Western Hemisphere, numerous ancient Central American city-states collapsed around 1500 BCE due to soil erosion from intensive agriculture. Those remaining from these civilizations took greater attention to the impact of farming practices on the sustainability of the land and its stable food production. Furthermore, in 1450 BCE the Minoan civilization on the Greek island of Crete declined due to deforestation and the resulting environmental degradation of natural resources. Pliny the Elder somewhat addressed the environmental concerns of ancient civilizations in the text Naturalis Historia, written between 77 and 79 ACE, which provided an overview of many related subsets of the discipline. Although warfare and disease were of primary concern in ancient society, environmental issues played a crucial role in the survival and power of different civilizations. As more communities recognized the importance of the natural world to their long-term success, an interest in studying the environment came into existence. Beginnings of environmental science 18th century In 1735, the concept of binomial nomenclature is introduced by Carolus Linnaeus as a way to classify all living organisms, influenced by earlier works of Aristotle. His text, Systema Naturae, represents one of the earliest culminations of knowledge on the subject, providing a means to identify different species based partially on how they interact with their environment. 19th century In the 1820s, scientists were studying the properties of gases, particularly those in the Earth's atmosphere and their interactions with heat from the Sun. Later that century, studies suggested that the Earth had experienced an Ice Age and that warming of the Earth was partially due to what are now known as greenhouse gases (GHG). The greenhouse effect was introduced, although climate science was not yet recognized as an important topic in environmental science due to minimal industrialization and lower rates of greenhouse gas emissions at the time. 20th century In the 1900s, the discipline of environmental science as it is known today began to take shape. The century is marked by significant research, literature, and international cooperation in the field. In the early 20th century, criticism from dissenters downplayed the effects of global warming. At this time, few researchers were studying the dangers of fossil fuels. After a 1.3 degrees Celsius temperature anomaly was found in the Atlantic Ocean in the 1940s, however, scientists renewed their studies of gaseous heat trapping from the greenhouse effect (although only carbon dioxide and water vapor were known to be greenhouse gases then). Nuclear development following the Second World War allowed environmental scientists to intensively study the effects of carbon and make advancements in the field. Further knowledge from archaeological evidence brought to light the changes in climate over time, particularly ice core sampling. Environmental science was brought to the forefront of society in 1962 when Rachel Carson published an influential piece of environmental literature, Silent Spring. Carson's writing led the American public to pursue environmental safeguards, such as bans on harmful chemicals like the insecticide DDT. Another important work, The Tragedy of the Commons, was published by Garrett Hardin in 1968 in response to accelerating natural degradation. In 1969, environmental science once again became a household term after two striking disasters: Ohio's Cuyahoga River caught fire due to the amount of pollution in its waters and a Santa Barbara oil spill endangered thousands of marine animals, both receiving prolific media coverage. Consequently, the United States passed an abundance of legislation, including the Clean Water Act and the Great Lakes Water Quality Agreement. The following year, in 1970, the first ever Earth Day was celebrated worldwide and the United States Environmental Protection Agency (EPA) was formed, legitimizing the study of environmental science in government policy. In the next two years, the United Nations created the United Nations Environment Programme (UNEP) in Stockholm, Sweden to address global environmental degradation. Much of the interest in environmental science throughout the 1970s and the 1980s was characterized by major disasters and social movements. In 1978, hundreds of people were relocated from Love Canal, New York after carcinogenic pollutants were found to be buried underground near residential areas. The next year, in 1979, the nuclear power plant on Three Mile Island in Pennsylvania suffered a meltdown and raised concerns about the dangers of radioactive waste and the safety of nuclear energy. In response to landfills and toxic waste often disposed of near their homes, the official Environmental Justice Movement was started by a Black community in North Carolina in 1982. Two years later, the toxic methyl isocyanate gas was released to the public from a power plant disaster in Bhopal, India, harming hundreds of thousands of people living near the disaster site, the effects of which are still felt today. In a groundbreaking discovery in 1985, a British team of researchers studying Antarctica found evidence of a hole in the ozone layer, inspiring global agreements banning the use of chlorofluorocarbons (CFCs), which were previously used in nearly all aerosols and refrigerants. Notably, in 1986, the meltdown at the Chernobyl nuclear power plant in Ukraine released radioactive waste to the public, leading to international studies on the ramifications of environmental disasters. Over the next couple of years, the Brundtland Commission (previously known as the World Commission on Environment and Development) published a report titled Our Common Future and the Montreal Protocol formed the International Panel on Climate Change (IPCC) as international communication focused on finding solutions for climate change and degradation. In the late 1980s, the Exxon Valdez company was fined for spilling large quantities of crude oil off the coast of Alaska and the resulting cleanup, involving the work of environmental scientists. After hundreds of oil wells were burned in combat in 1991, warfare between Iraq and Kuwait polluted the surrounding atmosphere just below the air quality threshold environmental scientistenvironmental scientists believed was life-threatening. 21st century Many niche disciplines of environmental science have emerged over the years, although climatology is one of the most known topics. Since the 2000s, environmental scientists have focused on modeling the effects of climate change and encouraging global cooperation to minimize potential damages. In 2002, the Society for the Environment as well as the Institute of Air Quality Management were founded to share knowledge and develop solutions around the world. Later, in 2008, the United Kingdom became the first country to pass legislation (the Climate Change Act) that aims to reduce carbon dioxide output to a specified threshold. In 2016 the Kyoto Protocol became the Paris Agreement, which sets concrete goals to reduce greenhouse gas emissions and restricts Earth's rise in temperature to a 2 degrees Celsius maximum. The agreement is one of the most expansive international efforts to limit the effects of global warming to date. Most environmental disasters in this time period involve crude oil pollution or the effects of rising temperatures. In 2010, BP was responsible for the largest American oil spill in the Gulf of Mexico, known as the Deepwater Horizon spill, which killed a number of the company's workers and released large amounts of crude oil into the water. Furthermore, throughout this century, much of the world has been ravaged by widespread wildfires and water scarcity, prompting regulations on the sustainable use of natural resources as determined by environmental scientists. The 21st century is marked by significant technological advancements. New technology in environmental science has transformed how researchers gather information about various topics in the field. Research in engines, fuel efficiency, and decreasing emissions from vehicles since the times of the Industrial Revolution has reduced the amount of carbon and other pollutants into the atmosphere. Furthermore, investment in researching and developing clean energy (i.e. wind, solar, hydroelectric, and geothermal power) has significantly increased in recent years, indicating the beginnings of the divestment from fossil fuel use. Geographic information systems (GIS) are used to observe sources of air or water pollution through satellites and digital imagery analysis. This technology allows for advanced farming techniques like precision agriculture as well as monitoring water usage in order to set market prices. In the field of water quality, developed strains of natural and manmade bacteria contribute to bioremediation, the treatment of wastewaters for future use. This method is more eco-friendly and cheaper than manual cleanup or treatment of wastewaters. Most notably, the expansion of computer technology has allowed for large data collection, advanced analysis, historical archives, public awareness of environmental issues, and international scientific communication. The ability to crowdsource on the Internet, for example, represents the process of collectivizing knowledge from researchers around the world to create increased opportunity for scientific progress. With crowdsourcing, data is released to the public for personal analyses which can later be shared as new information is found. Another technological development, blockchain technology, monitors and regulates global fisheries. By tracking the path of fish through global markets, environmental scientists can observe whether certain species are being overharvested to the point of extinction. Additionally, remote sensing allows for the detection of features of the environment without physical intervention. The resulting digital imagery is used to create increasingly accurate models of environmental processes, climate change, and much more. Advancements to remote sensing technology are particularly useful in locating the nonpoint sources of pollution and analyzing ecosystem health through image analysis across the electromagnetic spectrum. Lastly, thermal imaging technology is used in wildlife management to catch and discourage poachers and other illegal wildlife traffickers from killing endangered animals, proving useful for conservation efforts. Artificial intelligence has also been used to predict the movement of animal populations and protect the habitats of wildlife. Components Atmospheric sciences Atmospheric sciences focus on the Earth's atmosphere, with an emphasis upon its interrelation to other systems. Atmospheric sciences can include studies of meteorology, greenhouse gas phenomena, atmospheric dispersion modeling of airborne contaminants, sound propagation phenomena related to noise pollution, and even light pollution. Taking the example of the global warming phenomena, physicists create computer models of atmospheric circulation and infrared radiation transmission, chemists examine the inventory of atmospheric chemicals and their reactions, biologists analyze the plant and animal contributions to carbon dioxide fluxes, and specialists such as meteorologists and oceanographers add additional breadth in understanding the atmospheric dynamics. Ecology As defined by the Ecological Society of America, "Ecology is the study of the relationships between living organisms, including humans, and their physical environment; it seeks to understand the vital connections between plants and animals and the world around them." Ecologists might investigate the relationship between a population of organisms and some physical characteristic of their environment, such as concentration of a chemical; or they might investigate the interaction between two populations of different organisms through some symbiotic or competitive relationship. For example, an interdisciplinary analysis of an ecological system which is being impacted by one or more stressors might include several related environmental science fields. In an estuarine setting where a proposed industrial development could impact certain species by water and air pollution, biologists would describe the flora and fauna, chemists would analyze the transport of water pollutants to the marsh, physicists would calculate air pollution emissions and geologists would assist in understanding the marsh soils and bay muds. Environmental chemistry Environmental chemistry is the study of chemical alterations in the environment. Principal areas of study include soil contamination and water pollution. The topics of analysis include chemical degradation in the environment, multi-phase transport of chemicals (for example, evaporation of a solvent containing lake to yield solvent as an air pollutant), and chemical effects upon biota. As an example study, consider the case of a leaking solvent tank which has entered the habitat soil of an endangered species of amphibian. As a method to resolve or understand the extent of soil contamination and subsurface transport of solvent, a computer model would be implemented. Chemists would then characterize the molecular bonding of the solvent to the specific soil type, and biologists would study the impacts upon soil arthropods, plants, and ultimately pond-dwelling organisms that are the food of the endangered amphibian. Geosciences Geosciences include environmental geology, environmental soil science, volcanic phenomena and evolution of the Earth's crust. In some classification systems this can also include hydrology, including oceanography. As an example study, of soils erosion, calculations would be made of surface runoff by soil scientists. Fluvial geomorphologists would assist in examining sediment transport in overland flow. Physicists would contribute by assessing the changes in light transmission in the receiving waters. Biologists would analyze subsequent impacts to aquatic flora and fauna from increases in water turbidity. Regulations driving the studies In the United States the National Environmental Policy Act (NEPA) of 1969 set forth requirements for analysis of federal government actions (such as highway construction projects and land management decisions) in terms of specific environmental criteria. Numerous state laws have echoed these mandates, applying the principles to local-scale actions. The upshot has been an explosion of documentation and study of environmental consequences before the fact of development actions. One can examine the specifics of environmental science by reading examples of Environmental Impact Statements prepared under NEPA such as: Wastewater treatment expansion options discharging into the San Diego/Tijuana Estuary, Expansion of the San Francisco International Airport, Development of the Houston, Metro Transportation system, Expansion of the metropolitan Boston MBTA transit system, and Construction of Interstate 66 through Arlington, Virginia. In England and Wales the Environment Agency (EA), formed in 1996, is a public body for protecting and improving the environment and enforces the regulations listed on the communities and local government site. (formerly the office of the deputy prime minister). The agency was set up under the Environment Act 1995 as an independent body and works closely with UK Government to enforce the regulations. See also Environmental engineering science Environmental informatics Environmental monitoring Environmental planning Environmental statistics Glossary of environmental science List of environmental studies topics References External links Glossary of environmental terms – Global Development Research Center

Education is the transmission of knowledge and skills and the development of character traits. Formal education occurs within a structured institutional framework, such as public schools, following a curriculum. Non-formal education also follows a structured approach but occurs outside the formal schooling system, while informal education involves unstructured learning through daily experiences. Formal and non-formal education are categorized into levels, including early childhood education, primary education, secondary education, and tertiary education. Other classifications focus on teaching methods, such as teacher-centered and student-centered education, and on subjects, such as science education, language education, and physical education. Additionally, the term "education" can denote the mental states and qualities of educated individuals and the academic field studying educational phenomena. The precise definition of education is disputed, and there are disagreements about the aims of education and the extent to which education differs from indoctrination by fostering critical thinking. These disagreements impact how to identify, measure, and enhance various forms of education. Essentially, education socializes children into society by instilling cultural values and norms, equipping them with the skills necessary to become productive members of society. In doing so, it stimulates economic growth and raises awareness of local and global problems. Organized institutions play a significant role in education. For instance, governments establish education policies to determine the timing of school classes, the curriculum, and attendance requirements. International organizations, such as UNESCO, have been influential in promoting primary education for all children. Many factors influence the success of education. Psychological factors include motivation, intelligence, and personality. Social factors, such as socioeconomic status, ethnicity, and gender, are often associated with discrimination. Other factors encompass access to educational technology, teacher quality, and parental involvement. The primary academic field examining education is known as education studies. It delves into the nature of education, its objectives, impacts, and methods for enhancement. Education studies encompasses various subfields, including philosophy, psychology, sociology, and economics of education. Additionally, it explores topics such as comparative education, pedagogy, and the history of education. In prehistory, education primarily occurred informally through oral communication and imitation. With the emergence of ancient civilizations, the invention of writing led to an expansion of knowledge, prompting a transition from informal to formal education. Initially, formal education was largely accessible to elites and religious groups. The advent of the printing press in the 15th century facilitated widespread access to books, thus increasing general literacy. In the 18th and 19th centuries, public education gained significance, paving the way for the global movement to provide primary education to all, free of charge, and compulsory up to a certain age. Presently, over 90% of primary-school-age children worldwide attend primary school. Definitions The term "education" originates from the Latin words educare, meaning "to bring up," and educere, meaning "to bring forth." The definition of education has been explored by theorists from various fields. Many agree that education is a purposeful activity aimed at achieving goals like the transmission of knowledge, skills, and character traits. However, extensive debate surrounds its precise nature beyond these general features. One approach views education as a process occurring during events such as schooling, teaching, and learning. Another perspective perceives education not as a process but as the mental states and dispositions of educated individuals resulting from this process. Furthermore, the term may also refer to the academic field that studies the methods, processes, and social institutions involved in teaching and learning. Having a clear understanding of the term is crucial when attempting to identify educational phenomena, measure educational success, and improve educational practices. Some theorists provide precise definitions by identifying specific features exclusive to all forms of education. Education theorist R. S. Peters, for instance, outlines three essential features of education, including imparting knowledge and understanding to the student, ensuring the process is beneficial, and conducting it in a morally appropriate manner. While such precise definitions often characterize the most typical forms of education effectively, they face criticism because less common types of education may occasionally fall outside their parameters. Dealing with counterexamples not covered by precise definitions can be challenging, which is why some theorists prefer offering less exact definitions based on family resemblance instead. This approach suggests that all forms of education are similar to each other but need not share a set of essential features common to all. Some education theorists, such as Keira Sewell and Stephen Newman, argue that the term "education" is context-dependent. Evaluative or thick conceptions of education assert that it is inherent in the nature of education to lead to some form of improvement. They contrast with thin conceptions, which offer a value-neutral explanation. Some theorists provide a descriptive conception of education by observing how the term is commonly used in ordinary language. Prescriptive conceptions, on the other hand, define what constitutes good education or how education should be practiced. Many thick and prescriptive conceptions view education as an endeavor that strives to achieve specific objectives, which may encompass acquiring knowledge, learning to think rationally, and cultivating character traits such as kindness and honesty. Various scholars emphasize the importance of critical thinking in distinguishing education from indoctrination. They argue that indoctrination focuses solely on instilling beliefs in students, regardless of their rationality; whereas education also encourages the rational ability to critically examine and question those beliefs. However, it is not universally accepted that these two phenomena can be clearly distinguished, as some forms of indoctrination may be necessary in the early stages of education when the child's mind is not yet fully developed. This is particularly relevant in cases where young children must learn certain things without comprehending the underlying reasons, such as specific safety rules and hygiene practices. Education can be characterized from both the teacher's and the student's perspectives. Teacher-centered definitions emphasize the perspective and role of the teacher in transmitting knowledge and skills in a morally appropriate manner. On the other hand, student-centered definitions analyze education based on the student's involvement in the learning process, suggesting that this process transforms and enriches their subsequent experiences. It is also possible to consider definitions that incorporate both perspectives. In this approach, education is seen as a process of shared experience, involving the discovery of a common world and the collaborative solving of problems. Types There are several classifications of education. One classification depends on the institutional framework, distinguishing between formal, non-formal, and informal education. Another classification involves different levels of education based on factors such as the student's age and the complexity of the content. Further categories focus on the topic, teaching method, medium used, and funding. Formal, non-formal, and informal The most common division is between formal, non-formal, and informal education. Formal education occurs within a structured institutional framework, typically with a chronological and hierarchical order. The modern schooling system organizes classes based on the student's age and progress, ranging from primary school to university. Formal education is usually overseen and regulated by the government and often mandated up to a certain age. Non-formal and informal education occur outside the formal schooling system, with non-formal education serving as a middle ground. Like formal education, non-formal education is organized, systematic, and pursued with a clear purpose, as seen in activities such as tutoring, fitness classes, and participation in the scouting movement. Informal education, on the other hand, occurs in an unsystematic manner through daily experiences and exposure to the environment. Unlike formal and non-formal education, there is typically no designated authority figure responsible for teaching. Informal education unfolds in various settings and situations throughout one's life, often spontaneously, such as children learning their first language from their parents or individuals mastering cooking skills by preparing a dish together. Some theorists differentiate between the three types based on the learning environment: formal education occurs within schools, non-formal education takes place in settings not regularly frequented, such as museums, and informal education unfolds in the context of everyday routines. Additionally, there are disparities in the source of motivation. Formal education tends to be propelled by extrinsic motivation, driven by external rewards. Conversely, in non-formal and informal education, intrinsic motivation, stemming from the enjoyment of the learning process, typically prevails. While the differentiation among the three types is generally clear, certain forms of education may not neatly fit into a single category. In primitive cultures, education predominantly occurred informally, with little distinction between educational activities and other daily endeavors. Instead, the entire environment served as a classroom, and adults commonly assumed the role of educators. However, informal education often proves insufficient for imparting large quantities of knowledge. To address this limitation, formal educational settings and trained instructors are typically necessary. This necessity contributed to the increasing significance of formal education throughout history. Over time, formal education led to a shift towards more abstract learning experiences and topics, distancing itself from daily life. There was a greater emphasis on understanding general principles and concepts rather than simply observing and imitating specific behaviors. Levels Types of education are often categorized into different levels or stages. One influential framework is the International Standard Classification of Education, maintained by the United Nations Educational, Scientific and Cultural Organization (UNESCO). This classification encompasses both formal and non-formal education and distinguishes levels based on factors such as the student's age, the duration of learning, and the complexity of the content covered. Additional criteria include entry requirements, teacher qualifications, and the intended outcome of successful completion. The levels are grouped into early childhood education (level 0), primary education (level 1), secondary education (levels 2–3), post-secondary non-tertiary education (level 4), and tertiary education (levels 5–8). Early childhood education, also referred to as preschool education or nursery education, encompasses the period from birth until the commencement of primary school. It is designed to facilitate holistic child development, addressing physical, mental, and social aspects. Early childhood education is pivotal in fostering socialization and personality development, while also imparting fundamental skills in communication, learning, and problem-solving. Its overarching goal is to prepare children for the transition to primary education. While preschool education is typically optional, in certain countries such as Brazil, it is mandatory starting from the age of four. Primary (or elementary) education usually begins between the ages of five and seven and spans four to seven years. It has no additional entry requirements and aims to impart fundamental skills in reading, writing, and mathematics. Additionally, it provides essential knowledge in subjects such as history, geography, the sciences, music, and art. Another objective is to facilitate personal development. Presently, primary education is compulsory in nearly all nations, with over 90% of primary-school-age children worldwide attending such schools. Secondary education succeeds primary education and typically spans the ages of 12 to 18 years. It is normally divided into lower secondary education (such as middle school or junior high school) and upper secondary education (like high school, senior high school, or college, depending on the country). Lower secondary education usually requires the completion of primary school as its entry prerequisite. It aims to expand and deepen learning outcomes, with a greater focus on subject-specific curricula, and teachers often specialize in one or a few specific subjects. One of its goals is to acquaint students with fundamental theoretical concepts across various subjects, laying a strong foundation for lifelong learning. In certain instances, it may also incorporate rudimentary forms of vocational training. Lower secondary education is compulsory in numerous countries across Central and East Asia, Europe, and the Americas. In some nations, it represents the final phase of compulsory education. However, mandatory lower secondary education is less common in Arab states, sub-Saharan Africa, and South and West Asia. Upper secondary education typically commences around the age of 15, aiming to equip students with the necessary skills and knowledge for employment or tertiary education. Completion of lower secondary education is normally a prerequisite. The curriculum encompasses a broader range of subjects, often affording students the opportunity to select from various options. Attainment of a formal qualification, such as a high school diploma, is frequently linked to successful completion of upper secondary education. Education beyond the secondary level may fall under the category of post-secondary non-tertiary education, which is akin to secondary education in complexity but places greater emphasis on vocational training to ready students for the workforce. In some countries, tertiary education is synonymous with higher education, while in others, tertiary education encompasses a broader spectrum. Tertiary education builds upon the foundation laid in secondary education but delves deeper into specific fields or subjects. Its culmination results in an academic degree. Tertiary education comprises four levels: short-cycle tertiary, bachelor's, master's, and doctoral education. These levels often form a hierarchical structure, with the attainment of earlier levels serving as a prerequisite for higher ones. Short-cycle tertiary education concentrates on practical aspects, providing advanced vocational and professional training tailored to specialized professions. Bachelor's level education, also known as undergraduate education, is typically longer than short-cycle tertiary education. It is commonly offered by universities and culminates in an intermediary academic credential known as a bachelor's degree. Master's level education is more specialized than undergraduate education and often involves independent research, normally in the form of a master's thesis. Doctoral level education leads to an advanced research qualification, usually a doctor's degree, such as a Doctor of Philosophy (PhD). It usually involves the submission of a substantial academic work, such as a dissertation. More advanced levels include post-doctoral studies and habilitation. Successful completion of formal education typically leads to certification, a prerequisite for advancing to higher levels of education and entering certain professions. Undetected cheating during exams, such as utilizing a cheat sheet, poses a threat to this system by potentially certifying unqualified students. In most countries, primary and secondary education is provided free of charge. However, there are significant global disparities in the cost of tertiary education. Some countries, such as Sweden, Finland, Poland, and Mexico, offer tertiary education for free or at a low cost. Conversely, in nations like the United States and Singapore, tertiary education often comes with high tuition fees, leading students to rely on substantial loans to finance their studies. High education costs can pose a significant barrier for students in developing countries, as their families may struggle to cover school fees, purchase uniforms, and buy textbooks. Others The academic literature explores various types of education, including traditional and alternative approaches. Traditional education encompasses long-standing and conventional schooling methods, characterized by teacher-centered instruction within a structured school environment. Regulations govern various aspects, such as the curriculum and class schedules. Alternative education serves as an umbrella term for schooling methods that diverge from the conventional traditional approach. These variances might encompass differences in the learning environment, curriculum content, or the dynamics of the teacher-student relationship. Characteristics of alternative schooling include voluntary enrollment, relatively modest class and school sizes, and customized instruction, fostering a more inclusive and emotionally supportive environment. This category encompasses various forms, such as charter schools and specialized programs catering to challenging or exceptionally talented students, alongside homeschooling and unschooling. Alternative education incorporates diverse educational philosophies, including Montessori schools, Waldorf education, Round Square schools, Escuela Nueva schools, free schools, and democratic schools. Alternative education encompasses indigenous education, which emphasizes the preservation and transmission of knowledge and skills rooted in indigenous heritage. This approach often employs traditional methods such as oral narration and storytelling. Other forms of alternative schooling include gurukul schools in India, madrasa schools in the Middle East, and yeshivas in Jewish tradition. Some distinctions revolve around the recipients of education. Categories based on the age of the learner are childhood education, adolescent education, adult education, and elderly education. Categories based on the biological sex of students include single-sex education and mixed-sex education. Special education is tailored to meet the unique needs of students with disabilities, addressing various impairments on intellectual, social, communicative, and physical levels. Its goal is to overcome the challenges posed by these impairments, providing affected students with access to an appropriate educational structure. In the broadest sense, special education also encompasses education for intellectually gifted children, who require adjusted curricula to reach their fullest potential. Classifications based on the teaching method include teacher-centered education, where the teacher plays a central role in imparting information to students, and student-centered education, where students take on a more active and responsible role in shaping classroom activities. In conscious education, learning and teaching occur with a clear purpose in mind. Unconscious education unfolds spontaneously without conscious planning or guidance. This may occur, in part, through the influence of teachers' and adults' personalities, which can indirectly impact the development of students' personalities. Evidence-based education employs scientific studies to determine the most effective educational methods. Its aim is to optimize the effectiveness of educational practices and policies by ensuring they are grounded in the best available empirical evidence. This encompasses evidence-based teaching, evidence-based learning, and school effectiveness research. Autodidacticism, or self-education, occurs independently of teachers and institutions. Primarily observed in adult education, it offers the freedom to choose what and when to study, making it a potentially more fulfilling learning experience. However, the lack of structure and guidance may lead to aimless learning, while the absence of external feedback could result in autodidacts developing misconceptions and inaccurately assessing their learning progress. Autodidacticism is closely associated with lifelong education, which entails continuous learning throughout one's life. Categories of education based on the subject encompass science education, language education, art education, religious education, physical education, and sex education. Special mediums such as radio or websites are utilized in distance education, including e-learning (use of computers), m-learning (use of mobile devices), and online education. Often, these take the form of open education, wherein courses and materials are accessible with minimal barriers, contrasting with traditional classroom or onsite education. However, not all forms of online education are open; for instance, some universities offer full online degree programs that are not part of open education initiatives. State education, also known as public education, is funded and controlled by the government and available to the general public. It typically does not require tuition fees and is therefore a form of free education. In contrast, private education is funded and managed by private institutions. Private schools often have a more selective admission process and offer paid education by charging tuition fees. A more detailed classification focuses on the social institutions responsible for education, such as family, school, civil society, state, and church. Compulsory education refers to education that individuals are legally mandated to receive, primarily affecting children who must attend school up to a certain age. This stands in contrast to voluntary education, which individuals pursue based on personal choice rather than legal obligation. Role in society Education serves various roles in society, spanning social, economic, and personal domains. Socially, education establishes and maintains a stable society by imparting fundamental skills necessary for interacting with the environment and fulfilling individual needs and aspirations. In contemporary society, these skills encompass speaking, reading, writing, arithmetic, and proficiency in information and communications technology. Additionally, education facilitates socialization by instilling awareness of dominant social and cultural norms, shaping appropriate behavior across diverse contexts. It fosters social cohesion, stability, and peace, fostering productive engagement in daily activities. While socialization occurs throughout life, early childhood education holds particular significance. Moreover, education plays a pivotal role in democracies by enhancing civic participation through voting and organizing, while also promoting equal opportunities for all. On an economic level, individuals become productive members of society through education, acquiring the technical and analytical skills necessary for their professions, as well as for producing goods and providing services to others. In early societies, there was minimal specialization, with children typically learning a broad range of skills essential for community functioning. However, modern societies are increasingly complex, with many professions requiring specialized training alongside general education. Consequently, only a relatively small number of individuals master certain professions. Additionally, skills and tendencies acquired for societal functioning may sometimes conflict, with their value dependent on context. For instance, fostering curiosity and questioning established teachings promotes critical thinking and innovation, while at times, obedience to authority is necessary to maintain social stability. By facilitating individuals' integration into society, education fosters economic growth and diminishes poverty. It enables workers to enhance their skills, thereby improving the quality of goods and services produced, which ultimately fosters prosperity and enhances competitiveness. Public education is widely regarded as a long-term investment that benefits society as a whole, with primary education showing particularly high rates of return. Additionally, besides bolstering economic prosperity, education contributes to technological and scientific advancements, reduces unemployment, and promotes social equity. Moreover, increased education is associated with lower birth rates, partly due to heightened awareness of family planning, expanded opportunities for women, and delayed marriage. Education plays a pivotal role in equipping a country to adapt to changes and effectively confront new challenges. It raises awareness and contributes to addressing contemporary global issues, including climate change, sustainability, and the widening disparities between the rich and the poor. By instilling in students an understanding of how their lives and actions impact others, education can inspire individuals to strive towards realizing a more sustainable and equitable world. Thus, education not only serves to maintain societal norms but also acts as a catalyst for social development. This extends to evolving economic circumstances, where technological advancements, notably increased automation, impose new demands on the workforce that education can help meet. As circumstances evolve, skills and knowledge taught may become outdated, necessitating curriculum adjustments to include subjects like digital literacy, and promote proficiency in handling new technologies. Moreover, education can embrace innovative forms such as massive open online courses to prepare individuals for emerging challenges and opportunities. On a more individual level, education fosters personal development, encompassing learning new skills, honing talents, nurturing creativity, enhancing self-knowledge, and refining problem-solving and decision-making abilities. Moreover, education contributes positively to health and well-being. Educated individuals are often better informed about health issues and adjust their behavior accordingly, benefit from stronger social support networks and coping strategies, and enjoy higher incomes, granting them access to superior healthcare services. The social significance of education is underscored by the annual International Day of Education on January 24, established by the United Nations, which designated 1970 as the International Education Year. Role of institutions Organized institutions play a pivotal role in multiple facets of education. Entities such as schools, universities, teacher training institutions, and ministries of education comprise the education sector. They interact not only with one another but also with various stakeholders, including parents, local communities, religious groups, non-governmental organizations, healthcare professionals, law enforcement agencies, media platforms, and political leaders. Numerous individuals are directly engaged in the education sector, such as students, teachers, school principals, as well as school nurses and curriculum developers. Various aspects of formal education are regulated by the policies of governmental institutions. These policies determine at what age children need to attend school and at what times classes are held, as well as issues pertaining to the school environment, such as infrastructure. Regulations also cover the exact qualifications and requirements that teachers need to fulfill. An important aspect of education policy concerns the curriculum used for teaching at schools, colleges, and universities. A curriculum is a plan of instruction or a program of learning that guides students to achieve their educational goals. The topics are usually selected based on their importance and depend on the type of school. The goals of public school curricula are usually to offer a comprehensive and well-rounded education, while vocational training focuses more on specific practical skills within a field. The curricula also cover various aspects besides the topic to be discussed, such as the teaching method, the objectives to be reached, and the standards for assessing progress. By determining the curricula, governmental institutions have a strong impact on what knowledge and skills are transmitted to the students. Examples of governmental institutions include the Ministry of Education in India, the Department of Basic Education in South Africa, and the Secretariat of Public Education in Mexico. International organizations also play a pivotal role in education. For example, UNESCO is an intergovernmental organization that promotes education through various means. One of its activities is advocating for education policies, such as the treaty Convention on the Rights of the Child, which declares education as a fundamental human right for all children and young people. The Education for All initiative aimed to provide basic education to all children, adolescents, and adults by 2015, later succeeded by the Sustainable Development Goals initiative, particularly goal 4. Related policies include the Convention against Discrimination in Education and the Futures of Education initiative. Some influential organizations are non-governmental rather than intergovernmental. For instance, the International Association of Universities promotes collaboration and knowledge exchange among colleges and universities worldwide, while the International Baccalaureate offers international diploma programs. Institutions like the Erasmus Programme facilitate student exchanges between countries, while initiatives such as the Fulbright Program provide similar services for teachers. Factors of educational success Educational success, also referred to as student and academic achievement, pertains to the extent to which educational objectives are met, such as the acquisition of knowledge and skills by students. For practical purposes, it is often primarily measured in terms of official exam scores, but numerous additional indicators exist, including attendance rates, graduation rates, dropout rates, student attitudes, and post-school indicators such as later income and incarceration rates. Several factors influence educational achievement, such as psychological factors related to the individual student, and sociological factors associated with the student's social environment. Additional factors encompass access to educational technology, teacher quality, and parental involvement. Many of these factors overlap and mutually influence each other. Psychological On a psychological level, relevant factors include motivation, intelligence, and personality. Motivation is the internal force propelling people to engage in learning. Motivated students are more likely to interact with the content to be learned by participating in classroom activities like discussions, resulting in a deeper understanding of the subject. Motivation can also help students overcome difficulties and setbacks. An important distinction lies between intrinsic and extrinsic motivation. Intrinsically motivated students are driven by an interest in the subject and the learning experience itself. Extrinsically motivated students seek external rewards such as good grades and recognition from peers. Intrinsic motivation tends to be more beneficial, leading to increased creativity, engagement, and long-term commitment. Educational psychologists aim to discover methods to increase motivation, such as encouraging healthy competition among students while maintaining a balance of positive and negative feedback through praise and constructive criticism. Intelligence significantly influences individuals' responses to education. It is a cognitive trait associated with the capacity to learn from experience, comprehend, and apply knowledge and skills to solve problems. Individuals with higher scores in intelligence metrics typically perform better academically and pursue higher levels of education. Intelligence is often closely associated with the concept of IQ, a standardized numerical measure assessing intelligence based on mathematical-logical and verbal abilities. However, it has been argued that intelligence encompasses various types beyond IQ. Psychologist Howard Gardner posited distinct forms of intelligence in domains such as mathematics, logic, spatial cognition, language, and music. Additional types of intelligence influence interpersonal and intrapersonal interactions. These intelligences are largely autonomous, meaning that an individual may excel in one type while performing less well in another. According to proponents of learning style theory, the preferred method of acquiring knowledge and skills is another factor. They hold that students with an auditory learning style find it easy to comprehend spoken lectures and discussions, whereas visual learners benefit from information presented visually, such as in diagrams and videos. To facilitate efficient learning, it may be advantageous to incorporate a wide variety of learning modalities. Learning styles have been criticized for ambiguous empirical evidence of student benefits and unreliability of student learning style assessment by teachers. The learner's personality may also influence educational achievement. For instance, characteristics such as conscientiousness and openness to experience, identified in the Big Five personality traits, are associated with academic success. Other mental factors include self-efficacy, self-esteem, and metacognitive abilities. Sociological Sociological factors center not on the psychological attributes of learners but on their environment and societal position. These factors encompass socioeconomic status, ethnicity, cultural background, and gender, drawing significant interest from researchers due to their association with inequality and discrimination. Consequently, they play a pivotal role in policy-making endeavors aimed at mitigating their impact. Socioeconomic status is influenced by factors beyond just income, including financial security, social status, social class, and various attributes related to quality of life. Low socioeconomic status impacts educational success in several ways. It correlates with slower cognitive development in language and memory, as well as higher dropout rates. Families with limited financial means may struggle to meet their children's basic nutritional needs, hindering their development. Additionally, they may lack resources to invest in educational materials such as stimulating toys, books, and computers. Financial constraints may also prevent attendance at prestigious schools, leading to enrollment in institutions located in economically disadvantaged areas. Such schools often face challenges such as teacher shortages and inadequate educational materials and facilities like libraries, resulting in lower teaching standards. Moreover, parents may be unable to afford private lessons for children falling behind academically. In some cases, students from economically disadvantaged backgrounds are compelled to drop out of school to contribute to family income. Limited access to information about higher education and challenges in securing and repaying student loans further exacerbate the situation. Low socioeconomic status is also associated with poorer physical and mental health, contributing to a cycle of social inequality that persists across generations. Ethnic background correlates with cultural distinctions and language barriers, which can pose challenges for students in adapting to the school environment and comprehending classes. Moreover, explicit and implicit biases and discrimination against ethnic minorities further compound these difficulties. Such biases can impact students' self-esteem, motivation, and access to educational opportunities. For instance, teachers may harbor stereotypical perceptions, albeit not overtly racist, leading to differential grading of comparable performances based on a child's ethnicity. Historically, gender has played a pivotal role in education as societal norms dictated distinct roles for men and women. Education traditionally favored men, who were tasked with providing for the family, while women were expected to manage households and care for children, often limiting their access to education. Although these disparities have improved in many modern societies, gender differences persist in education. This includes biases and stereotypes related to gender roles in various academic domains, notably in fields such as science, technology, engineering, and mathematics (STEM), which are often portrayed as male-dominated. Such perceptions can deter female students from pursuing these subjects. In various instances, discrimination based on gender and social factors occurs openly as part of official educational policies, such as the severe restrictions imposed on female education by the Taliban in Afghanistan, and the school segregation of migrants and locals in urban China under the hukou system. One facet of several social factors is characterized by the expectations linked to stereotypes. These expectations operate externally, influenced by how others respond to individuals belonging to specific groups, and internally, shaped by how individuals internalize and conform to them. In this regard, these expectations can manifest as self-fulfilling prophecies by affecting the educational outcomes they predict. Such outcomes may be influenced by both positive and negative stereotypes. Technology and others Technology plays a crucial role in educational success. While educational technology is often linked with modern digital devices such as computers, its scope extends far beyond that. It encompasses a diverse array of resources and tools for learning, including traditional aids like books and worksheets, in addition to digital devices. Educational technology can enhance learning in various ways. In the form of media, it often serves as the primary source of information in the classroom, allowing teachers to allocate their time and energy to other tasks such as lesson planning, student guidance, and performance assessment. By presenting information using graphics, audio, and video instead of mere text, educational technology can also enhance comprehension. Interactive elements, such as educational games, further engage learners in the learning process. Moreover, technology facilitates the accessibility of educational materials to a wide audience, particularly through online resources, while also promoting collaboration among students and communication with teachers. The integration of artificial intelligence in education holds promise for providing new learning experiences to students and supporting teachers in their work. However, it also introduces new risks related to data privacy, misinformation, and manipulation. Various organizations advocate for student access to educational technologies, including initiatives such as the One Laptop per Child initiative, the African Library Project, and Pratham. School infrastructure also plays a crucial role in educational success. It encompasses physical aspects such as the school's location, size, and available facilities and equipment. A healthy and safe environment, well-maintained classrooms, appropriate classroom furniture, as well as access to a library and a canteen, all contribute to fostering educational success. Additionally, the quality of teachers significantly impacts student achievement. Skilled teachers possess the ability to motivate and inspire students, and tailor instructions to individual abilities and needs. Their skills depend on their own education, training, and teaching experience. A meta-analysis by Engin Karadağ et al. concludes that, compared to other influences, factors related to the school and the teacher have the greatest impact on educational success. Parent involvement also enhances achievement and can increase children's motivation and commitment when they know their parents are invested in their educational endeavors. This often results in heightened self-esteem, improved attendance rates, and more positive behavior at school. Parent involvement covers communication with teachers and other school staff to raise awareness of current issues and explore potential resolutions. Other relevant factors, occasionally addressed in academic literature, encompass historical, political, demographic, religious, and legal aspects. Education studies The primary field exploring education is known as education studies, also termed education sciences. It seeks to understand how knowledge is transmitted and acquired by examining various methods and forms of education. This discipline delves into the goals, impacts, and significance of education, along with the cultural, societal, governmental, and historical contexts that influence it. Education theorists draw insights from various disciplines, including philosophy, psychology, sociology, economics, history, politics, and international relations. Consequently, some argue that education studies lacks the clear methodological and subject delineations found in disciplines like physics or history. Education studies focuses on academic analysis and critical reflection and differs in this respect from teacher training programs, which show participants how to become effective teachers. Furthermore, it encompasses not only formal education but also explores all forms and facets of educational processes. Various research methods are utilized to investigate educational phenomena, broadly categorized into quantitative, qualitative, and mixed-methods approaches. Quantitative research mirrors the methodologies of the natural sciences, employing precise numerical measurements to collect data from numerous observations and utilizing statistical tools for analysis. Its goal is to attain an objective and impartial understanding. Conversely, qualitative research typically involves a smaller sample size and seeks to gain a nuanced insight into subjective and personal factors, such as individuals' experiences within the educational process. Mixed-methods research aims to integrate data gathered from both approaches to achieve a balanced and comprehensive understanding. Data collection methods vary and may include direct observation, test scores, interviews, and questionnaires. Research projects may investigate fundamental factors influencing all forms of education or focus on specific applications, seek solutions to particular problems, or evaluate the effectiveness of educational initiatives and policies. Subfields Education studies encompasses various subfields such as pedagogy, educational research, comparative education, and the philosophy, psychology, sociology, economics, and history of education. The philosophy of education is the branch of applied philosophy that examines many of the fundamental assumptions underlying the theory and practice of education. It explores education both as a process and a discipline while seeking to provide precise definitions of its nature and distinctions from other phenomena. Additionally, it delves into the purpose of education, its various types, and the conceptualization of teachers, students, and their relationship. Furthermore, it encompasses educational ethics, which examines the moral implications of education, such as the ethical principles guiding it and how teachers should apply them to specific situations. The philosophy of education boasts a long history and was a subject of discourse in ancient Greek philosophy. The term "pedagogy" is sometimes used interchangeably with education studies, but in a more specific sense, it refers to the subfield focused on teaching methods. It investigates how educational objectives, such as knowledge transmission or the development of skills and character traits, can be achieved. Pedagogy is concerned with the methods and techniques employed in teaching within conventional educational settings. While some definitions confine it to this context, in a broader sense, it encompasses all forms of education, including teaching methods beyond traditional school environments. In this broader context, it explores how teachers can facilitate learning experiences for students to enhance their understanding of the subject matter and how learning itself occurs. The psychology of education delves into the mental processes underlying learning, focusing on how individuals acquire new knowledge and skills and experience personal development. It investigates the various factors influencing educational outcomes, how these factors vary among individuals, and the extent to which nature or nurture contribute to these outcomes. Key psychological theories shaping education encompass behaviorism, cognitivism, and constructivism. Related disciplines include educational neuroscience and the neurology of education, which explore the neuropsychological processes and changes associated with learning. The field of sociology of education delves into how education shapes socialization, examining how social factors and ideologies influence access to education and individual success within it. It explores the impact of education on different societal groups and its role in shaping personal identity. Specifically, the sociology of education focuses on understanding the root causes of inequalities, offering insights relevant to education policy aimed at identifying and addressing factors contributing to inequality. Two prominent perspectives within this field are consensus theory and conflict theory. Consensus theorists posit that education benefits society by preparing individuals for their societal roles, while conflict theorists view education as a tool employed by the ruling class to perpetuate inequalities. The field of economics of education investigates the production, distribution, and consumption of education. It seeks to optimize resource allocation to enhance education, such as assessing the impact of increased teacher salaries on teacher quality. Additionally, it explores the effects of smaller class sizes and investments in new educational technologies. By providing insights into resource allocation, the economics of education aids policymakers in making decisions that maximize societal benefits. Furthermore, it examines the long-term economic implications of education, including its role in fostering a highly skilled workforce and enhancing national competitiveness. A related area of interest involves analyzing the economic advantages and disadvantages of different educational systems. Comparative education is the discipline that examines and contrasts education systems. Comparisons can occur from a general perspective or focus on specific factors like social, political, or economic aspects. Often applied to different countries, comparative education assesses the similarities and differences of their educational institutions and practices, evaluating the consequences of distinct approaches. It can be used to glean insights from other countries on effective education policies and how one's own system may be improved. This practice, known as policy borrowing, presents challenges as policy success can hinge on the social and cultural context of students and teachers. A related and contentious topic concerns whether the educational systems of developed countries are superior and should be exported to less developed ones. Other key topics include the internationalization of education and the role of education in transitioning from authoritarian regimes to democracies. The history of education delves into the evolution of educational practices, systems, and institutions. It explores various key processes, their potential causes and effects, and their interrelations. Aims and ideologies A central topic in education studies revolves around how people should be educated and what goals should guide this process. Various aims have been proposed, including the acquisition of knowledge and skills, personal development, and the cultivation of character traits. Commonly suggested attributes encompass qualities like curiosity, creativity, rationality, and critical thinking, along with tendencies to think, feel, and act morally. Scholars diverge on whether to prioritize liberal values such as freedom, autonomy, and open-mindedness, or qualities like obedience to authority, ideological purity, piety, and religious faith. Some education theorists concentrate on a single overarching purpose of education, viewing more specific aims as means to this end. At a personal level, this purpose is often equated with assisting the student in leading a good life. Societally, education aims to cultivate individuals into productive members of society. There is debate regarding whether the primary aim of education is to benefit the educated individual or society as a whole. Educational ideologies encompass systems of fundamental philosophical assumptions and principles utilized to interpret, understand, and assess existing educational practices and policies. They address various aspects beyond the aims of education, including the subjects taught, the structure of learning activities, the role of teachers, methods for assessing educational progress, and the design of institutional frameworks and policies. These ideologies are diverse and often interrelated. Teacher-centered ideologies prioritize the role of teachers in imparting knowledge to students, while student-centered ideologies afford students a more active role in the learning process. Process-based ideologies focus on the methods of teaching and learning, contrasting with product-based ideologies, which consider education in terms of the desired outcomes. Conservative ideologies uphold traditional practices, whereas Progressive ideologies advocate for innovation and creativity. Additional categories are humanism, romanticism, essentialism, encyclopaedism, pragmatism, as well as authoritarian and democratic ideologies. Learning theories Learning theories attempt to elucidate the mechanisms underlying learning. Influential theories include behaviorism, cognitivism, and constructivism. Behaviorism posits that learning entails a modification in behavior in response to environmental stimuli. This occurs through the presentation of a stimulus, the association of this stimulus with the desired response, and the reinforcement of this stimulus-response connection. Cognitivism views learning as a transformation in cognitive structures and emphasizes the mental processes involved in encoding, retrieving, and processing information. Constructivism asserts that learning is grounded in the individual's personal experiences and places greater emphasis on social interactions and their interpretation by the learner. These theories carry significant implications for instructional practices. For instance, behaviorists often emphasize repetitive drills, cognitivists may advocate for mnemonic techniques, and constructivists typically employ collaborative learning strategies. Various theories suggest that learning is more effective when it is based on personal experience. Additionally, aiming for a deeper understanding by connecting new information to pre-existing knowledge is considered more beneficial than simply memorizing a list of unrelated facts. An influential developmental theory of learning is proposed by psychologist Jean Piaget, who outlines four stages of learning through which children progress on their way to adulthood: the sensorimotor, pre-operational, concrete operational, and formal operational stages. These stages correspond to different levels of abstraction, with early stages focusing more on simple sensory and motor activities, while later stages involve more complex internal representations and information processing, such as logical reasoning. Teaching methods The teaching method pertains to how the content is delivered by the teacher, such as whether group work is employed rather than focusing on individual learning. There is a wide array of teaching methods available, and the most effective one in a given scenario depends on factors like the subject matter and the learner's age and level of competence. This is reflected in modern school systems, which organize students into different classes based on age, competence, specialization, and native language to ensure an effective learning process. Different subjects often employ distinct approaches; for example, language education frequently emphasizes verbal learning, while mathematical education focuses on abstract and symbolic thinking alongside deductive reasoning. One crucial aspect of teaching methodologies is ensuring that learners remain motivated, either through intrinsic factors like interest and curiosity or through external rewards. The teaching method also includes the utilization of instructional media, such as books, worksheets, and audio-visual recordings, as well as implementing some form of test or evaluation to gauge learning progress. Educational assessment is the process of documenting the student's knowledge and skills, which can happen formally or informally and may take place before, during, or after the learning activity. Another significant pedagogical element in many modern educational approaches is that each lesson is part of a broader educational framework governed by a syllabus, which often spans several months or years. According to Herbartianism, teaching is broken down into phases. The initial phase involves preparing the student's mind for new information. Subsequently, new ideas are introduced to the learner and then linked to concepts already familiar to them. In later phases, understanding transitions to a more general level beyond specific instances, and the ideas are then applied in practical contexts. History The history of education delves into the processes, methods, and institutions entwined with teaching and learning, aiming to elucidate their interplay and influence on educational practices over time. Prehistory Education during prehistory primarily facilitated enculturation, emphasizing practical knowledge and skills essential for daily life, such as food production, clothing, shelter, and safety. Formal schools and specialized instructors were absent, with adults in the community assuming teaching roles, and learning transpiring informally through daily activities, including observation and imitation of elders. In oral societies, storytelling served as a pivotal means of transmitting cultural and religious beliefs across generations. With the advent of agriculture during the Neolithic Revolution around 9000 BCE, a gradual educational shift toward specialization ensued, driven by the formation of larger communities and the demand for increasingly intricate artisanal and technical skills. Ancient era Commencing in the 4th millennium BCE and spanning subsequent eras, a pivotal transformation in educational methodologies unfolded with the advent of writing in regions such as Mesopotamia, ancient Egypt, the Indus Valley, and ancient China. This breakthrough profoundly influenced the trajectory of education. Writing facilitated the storage, preservation, and dissemination of information, ushering in subsequent advancements such as the creation of educational aids like textbooks and the establishment of institutions such as schools. Another significant aspect of ancient education was the establishment of formal education. This became necessary as civilizations evolved and the volume of knowledge expanded, surpassing what informal education could effectively transmit across generations. Teachers assumed specialized roles to impart knowledge, leading to a more abstract educational approach less tied to daily life. Formal education remained relatively rare in ancient societies, primarily accessible to the intellectual elite. It covered fields like reading and writing, record keeping, leadership, civic and political life, religion, and technical skills associated with specific professions. Formal education introduced a new teaching paradigm that emphasized discipline and drills over the informal methods prevalent earlier. Two notable achievements of ancient education include the founding of Plato's Academy in Ancient Greece, often regarded as the earliest institution of higher learning, and the establishment of the Great Library of Alexandria in Ancient Egypt, renowned as one of the ancient world's premier libraries. Medieval era Many facets of education during the medieval period were profoundly influenced by religious traditions. In Europe, the Catholic Church wielded considerable authority over formal education. In the Arab world, the rapid spread of Islam led to various educational advancements during the Islamic Golden Age, integrating classical and religious knowledge and establishing madrasa schools. In Jewish communities, yeshivas emerged as institutions dedicated to the study of religious texts and Jewish law. In China, an expansive state educational and examination system, shaped by Confucian teachings, was instituted. As new complex societies emerged in regions like Africa, the Americas, Northern Europe, and Japan, some adopted existing educational practices, while others developed new traditions. Additionally, this era witnessed the establishment of various institutes of higher education and research. Prominent among these were the University of Bologna (the world's oldest university in continuous operation), the University of Paris, and Oxford University in Europe. Other influential centers included the Al-Qarawiyyin University in Morocco, Al-Azhar University in Egypt, and the House of Wisdom in Iraq. Another significant development was the formation of guilds, associations of skilled craftsmen and merchants who regulated their trades and provided vocational education. Prospective members underwent various stages of training on their journey to mastery. Modern era Starting in the early modern period, education in Europe during the Renaissance slowly began to shift from a religious approach towards one that was more secular. This development was tied to an increased appreciation of the importance of education and a broadened range of topics, including a revived interest in ancient literary texts and educational programs. The turn toward secularization was accelerated during the Age of Enlightenment starting in the 17th century, which emphasized the role of reason and the empirical sciences. European colonization affected education in the Americas through Christian missionary initiatives. In China, the state educational system was further expanded and focused more on the teachings of neo-Confucianism. In the Islamic world, the outreach of formal education increased and remained under the influence of religion. A key development in the early modern period was the invention and popularization of the printing press in the middle of the 15th century, which had a profound impact on general education. It significantly reduced the cost of producing books, which were hand-written before, and thereby augmented the dissemination of written documents, including new forms like newspapers and pamphlets. The increased availability of written media had a major influence on the general literacy of the population. These alterations paved the way for the advancement of public education during the 18th and 19th centuries. This era witnessed the establishment of publicly funded schools with the goal of providing education for all, in contrast to previous periods when formal education was primarily delivered by private schools, religious institutions, and individual tutors. An exception to this trend was the Aztec civilization, where formal education was compulsory for youth across social classes as early as the 14th century. Closely related changes were to make education compulsory and free of charge for all children up to a certain age. Contemporary era The promotion of public education and universal access to education gained momentum in the 20th and 21st centuries, endorsed by intergovernmental organizations such as the UN. Key initiatives included the Universal Declaration of Human Rights, the Convention on the Rights of the Child, the Education for All initiative, the Millennium Development Goals, and the Sustainable Development Goals. These endeavors led to a consistent increase in all forms of education, particularly impacting primary education. In 1970, 28% of all primary-school-age children worldwide were not enrolled in school; by 2015, this figure had decreased to 9%. The establishment of public education was accompanied by the introduction of standardized curricula for public schools as well as standardized tests to assess the progress of students. Contemporary examples are the Test of English as a Foreign Language, which is a globally used test to assess language proficiency in non-native English speakers, and the Programme for International Student Assessment, which evaluates education systems across the world based on the performance of 15-year-old students in reading, mathematics, and science. Similar shifts impacted teachers, with the establishment of institutions and norms to regulate and oversee teacher training, including certification mandates for teaching in public schools. Emerging educational technologies have significantly influenced modern education. The widespread availability of computers and the internet has notably expanded access to educational resources and facilitated new forms of learning, such as online education. This became particularly pertinent during the COVID-19 pandemic when schools worldwide closed for prolonged periods, prompting many to adopt remote learning methods through video conferencing or pre-recorded video lessons to sustain instruction. Additionally, contemporary education is impacted by the increasing globalization and internationalization of educational practices. See also References Notes Citations Sources External links Education – OECD Education – UNESCO Education – World Bank

Cultural studies is an academic field that explores the dynamics of contemporary culture (including the politics of popular culture) and its social and historical foundations. Cultural studies researchers investigate how cultural practices relate to wider systems of power associated with, or operating through, social phenomena. These include ideology, class structures, national formations, ethnicity, sexual orientation, gender, and generation. Employing cultural analysis, cultural studies views cultures not as fixed, bounded, stable, and discrete entities, but rather as constantly interacting and changing sets of practices and processes. Cultural studies was initially developed by British Marxist academics in the late 1950s, 1960s, and 1970s, and has been subsequently taken up and transformed by scholars from many different disciplines around the world. Cultural studies is avowedly and even radically interdisciplinary and can sometimes be seen as anti-disciplinary. A key concern for cultural studies practitioners is the examination of the forces within and through which socially organized people conduct and participate in the construction of their everyday lives. Cultural studies combines a variety of politically engaged critical approaches including semiotics, Marxism, feminist theory, ethnography, post-structuralism, postcolonialism, social theory, political theory, history, philosophy, literary theory, media theory, film/video studies, communication studies, political economy, translation studies, museum studies and art history/criticism to study cultural phenomena in various societies and historical periods. Cultural studies seeks to understand how meaning is generated, disseminated, contested, bound up with systems of power and control, and produced from the social, political and economic spheres within a particular social formation or conjuncture. The movement has generated important theories of cultural hegemony and agency. Its practitioners attempt to explain and analyze the cultural forces related and processes of globalization. During the rise of neoliberalism in Britain and the U.S., cultural studies both became a global phenomenon, and attracted the attention of many conservative opponents both within and beyond universities for a variety of reasons. A worldwide movement of students and practitioners with a raft of scholarly associations and programs, annual international conferences and publications carry on work in this field today. Distinct approaches to cultural studies have emerged in different national and regional contexts. Overview Sardar's characteristics In his 1994 book, Introducing Cultural Studies, orientalist scholar Ziauddin Sardar lists the following five main characteristics of cultural studies: The objective of cultural studies is to understand culture in all its complex forms, and analyzing the social and political context in which culture manifests itself. Cultural study is a site of both study/analysis and political criticism. For example, not only would a cultural studies scholar study an object, but they may also connect this study to a larger political project. Cultural studies attempts to expose and reconcile constructed divisions of knowledge that purport to be grounded in nature. Cultural studies has a commitment to an ethical evaluation of modern society. One aim of cultural studies could be to examine cultural practices and their relation to power, following critical theory. For example, a study of a subculture (such as white working-class youth in London) would consider their social practices against those of the dominant culture (in this example, the middle and upper classes in London who control the political and financial sectors that create policies affecting the well-being of white working-class youth in London). British cultural studies Dennis Dworkin writes that "a critical moment" in the beginning of cultural studies as a field was when Richard Hoggart used the term in 1964 in founding the Centre for Contemporary Cultural Studies (CCCS) at the University of Birmingham. The centre would become home to the development of the intellectual orientation that has become known internationally as the "Birmingham School" of cultural studies, thus becoming the world's first institutional home of cultural studies. Hoggart appointed as his assistant Stuart Hall, who would effectively be directing CCCS by 1968. Hall formally assumed the directorship of CCCS in 1971, when Hoggart left Birmingham to become Assistant Director-General of UNESCO. Thereafter, the field of cultural studies became closely associated with Hall's work. In 1979, Hall left Birmingham to accept a prestigious chair in sociology at the Open University, and Richard Johnson took over the directorship of the centre. In the late 1990s, "restructuring" at the University of Birmingham led to the elimination of CCCS and the creation of a new Department of Cultural Studies and Sociology (CSS) in 1999. Then, in 2002, the university's senior administration abruptly announced the disestablishment of CSS, provoking a substantial international outcry. The immediate reason for disestablishment of the new department was an unexpectedly low result in the UK's Research Assessment Exercise of 2001, though a dean from the university attributed the decision to "inexperienced 'macho management'." The RAE, a holdover initiative of the Margaret Thatcher-led British government of 1986, determines research funding for university programs. To trace the development of British Cultural Studies, see, for example, the work of Richard Hoggart, E. P. Thompson, Raymond Williams, Stuart Hall, Paul Willis, Angela McRobbie, Paul Gilroy, David Morley, Charlotte Brunsdon, Richard Dyer, and others. There are also many published overviews of the historical development of cultural studies, including Graeme Turner's British Cultural Studies: An Introduction, 3rd Ed. and John Hartley's A Short History of Cultural Studies. Stuart Hall's directorship of CCCS Beginning in 1964, after the initial appearance of the founding works of British Cultural Studies in the late 1950s, Stuart Hall's pioneering work at CCCS, along with that of his colleagues and postgraduate students, gave shape and substance to the field of cultural studies. This would include such people as Paul Willis, Dick Hebdige, David Morley, Charlotte Brunsdon, John Clarke, Richard Dyer, Judith Williamson, Richard Johnson, Iain Chambers, Dorothy Hobson, Chris Weedon, Tony Jefferson, Michael Green and Angela McRobbie. Many cultural studies scholars employed Marxist methods of analysis, exploring the relationships between cultural forms (i.e., the superstructure) and that of the political economy (i.e., the base). By the 1970s, the work of Louis Althusser radically rethought the Marxist account of base and superstructure in ways that had a significant influence on the "Birmingham School". Much of the work done at CCCS studied youth-subcultural expressions of antagonism toward "respectable" middle-class British culture in the post-WWII period. Also during the 1970s, the politically formidable British working classes were in decline. Britain's manufacturing industries while continuing to grow in output and value, were decreasing in share of GDP and numbers employed, and union rolls were shrinking. Millions of working-class Britons backed the rise of Margaret Thatcher, through the labour losses. For Stuart Hall and his colleagues, this shift in loyalty from the Labour Party to the Conservative Party had to be explained in terms of cultural politics, which they had been tracking even before Thatcher's first victory. Some of this work was presented in the cultural studies classic, Policing the Crisis, and in other later texts such as Hall's The Hard Road to Renewal: Thatcherism and the Crisis of the Left, and New Times: The Changing Face of Politics in the 1990s. In 2016, Duke University Press launched a new series of Stuart Hall's collected writings, many of which detail his major and decisive contributions toward the establishment of the field of cultural studies. In 2023, a new Stuart Hall Archive Project was launched at the University of Birmingham to commemorate Hall's contributions in pioneering the field of cultural studies at CCCS. Late-1970s and beyond By the late 1970s, scholars associated with The Birmingham School had firmly placed questions of gender and race on the cultural studies agenda, where they have remained ever since. Also by the late 1970s, cultural studies had begun to attract a great deal of international attention. It spread globally throughout the 1980s and 1990s. As it did so, it both encountered new conditions of knowledge production, and engaged with other major international intellectual currents such as poststructuralism, postmodernism, and postcolonialism. The wide range of cultural studies journals now located throughout the world, as shown below, is one indication of the globalization of the field. For overviews of and commentaries on developments in cultural studies during the twenty-first century, see Lawrence Grossberg's Cultural Studies in the Future Tense, Gilbert Rodman's Why Cultural Studies? and Graeme Turner's What's Become of Cultural Studies? Hall's cultural studies Hall's cultural studies explores culture as a system that affects individuals' identities through the meanings and practices that arise from the constant power dynamics that comprise culture. Hall viewed culture as a "critical site of social action and intervention, where power relations are both established and potentially unsettled." He perceived culture as a power dynamic, in which the media unintentionally possesses more control over ideology than the public. Hall viewed the media as a source of preserving the status quo of a reflection that already exists in society. The media hegemony in question, he emphasized, "is not a conscious plot or conspiracy, it's not overtly coercive, and its effects are not total." Compared to other thinkers on this subject, he studied and analyzed symbols, ideologies, signs, and other representations within cultural studies. Most of his contributions occurred in the 1980s, where he looked at how media cultivates cultural power, how it is consumed, mediated and negotiated, etc. Hall has also been accredited with the expansion of cultural studies through "the primacy of culture's role as an educational site where identities are being continually transformed, power is enacted, and learning assumes a political dynamic." He viewed politics as being used mainly for power instead of the betterment of society. This led to the belief that political dynamics could change with a reform in the education system (if one changes the education system, then one can change the culture). Hall viewed culture as something that is institutionalized, which could only be studied through the interactional patterns that people within a culture exhibit and experience. Culture is something that makes up society, is a learned trait, and is influenced by various forms of media that help to establish it. Power is the underlying tone of Hall's cultural studies. Hall believed that culture has some power, but the media's use of it is what sways and dictates culture itself. Developments outside the UK In the US, prior to the emergence of British cultural studies, several versions of cultural analysis had emerged largely from pragmatic and liberal-pluralist philosophical traditions. However, in the late 1970s and 1980s, when British Cultural Studies began to spread internationally, and to engage with feminism, poststructuralism, postmodernism, and race, critical cultural studies (i.e., Marxist, feminist, poststructuralist, etc.) expanded tremendously in American universities in fields such as communication studies, education, sociology, and literature. Cultural Studies, the flagship journal of the field, has been based in the US since its founding editor, John Fiske, brought it there from Australia in 1987. A thriving cultural studies scene has existed in Australia since the late 1970s, when several key CS practitioners emigrated there from the UK, bringing British cultural studies with them, after Margaret Thatcher became Prime Minister of the UK in 1979. A school of cultural studies known as cultural policy studies is one of the distinctive Australian contributions to the field, though it is not the only one. Australia also gave birth to the world's first professional cultural studies association (now known as the Cultural Studies Association of Australasia) in 1990. Cultural studies journals based in Australia include International Journal of Cultural Studies, Continuum: Journal of Media & Cultural Studies, and Cultural Studies Review. In Canada, cultural studies has sometimes focused on issues of technology and society, continuing the emphasis in the work of Marshall McLuhan, Harold Innis, and others. Cultural studies journals based in Canada include Topia: Canadian Journal of Cultural Studies. In Africa, human rights and third-world issues are among the central topics treated. There is a thriving cultural and media studies scholarship in Southern Africa, with its locus in South Africa and Zimbabwe. Cultural studies journals based in Africa include the Journal of African Cultural Studies. In Latin America, cultural studies have drawn on thinkers such as José Martí, Ángel Rama, and other Latin-American figures, in addition to the Western theoretical sources associated with cultural studies in other parts of the world. Leading Latin American cultural studies scholars include Néstor García Canclini, Jésus Martín-Barbero, and Beatriz Sarlo. Among the key issues addressed by Latin American cultural studies scholars are decoloniality, urban cultures, and postdevelopment theory. Latin American cultural studies journals include the Journal of Latin American Cultural Studies. Even though cultural studies developed much more rapidly in the UK than in continental Europe, there is significant cultural studies presence in countries such as France, Spain, and Portugal. The field is relatively undeveloped in Germany, probably due to the continued influence of the Frankfurt School, which is now often said to be in its third generation, which includes notable figures such as Axel Honneth. Cultural studies journals based in continental Europe include the European Journal of Cultural Studies, the Journal of Spanish Cultural Studies, French Cultural Studies, and Portuguese Cultural Studies. In Germany, the term cultural studies specifically refers to the field in the Anglosphere, especially British cultural studies, to differentiate it from the German Kulturwissenschaft which developed along different lines and is characterized by its distance from political science. However, Kulturwissenschaft and cultural studies are often used interchangeably, particularly by lay people. Throughout Asia, cultural studies have thrived since at least the beginning of the 1990s. Cultural studies journals based in Asia include Inter-Asia Cultural Studies. In India, the Centre for Study of Culture and Society, Bangalore and the Department of Cultural Studies at The English and Foreign Languages and the University of Hyderabad are two major institutional spaces for Cultural Studies. Issues, concepts, and approaches Marxism has been an important influence upon cultural studies. Those associated with CCCS initially engaged deeply with the structuralism of Louis Althusser, and later in the 1970s turned decisively toward Antonio Gramsci. Cultural studies has also embraced the examination of race, gender, and other aspects of identity, as is illustrated, for example, by a number of key books published collectively under the name of CCCS in the late 1970s and early 1980s, including Women Take Issue: Aspects of Women's Subordination (1978), and The Empire Strikes Back: Race and Racism in 70s Britain (1982). Gramsci and hegemony To understand the changing political circumstances of class, politics, and culture in the United Kingdom, scholars at The Birmingham School turned to the work of Antonio Gramsci, an Italian thinker, writer, and Communist Party leader. Gramsci had been concerned with similar issues: why would Italian laborers and peasants vote for fascists? What strategic approach is necessary to mobilize popular support in more progressive directions? Gramsci modified classical Marxism, and argued that culture must be understood as a key site of political and social struggle. In his view, capitalists used not only brute force (police, prisons, repression, military) to maintain control, but also penetrated the everyday culture of working people in a variety of ways in their efforts to win popular "consent". For Gramsci, historical leadership, or hegemony, involves the formation of alliances between class factions, and struggles within the cultural realm of everyday common sense. Hegemony was always, for Gramsci, an interminable, unstable and contested process. Scott Lash writes: In the work of Hall, Hebdige and McRobbie, popular culture came to the fore... What Gramsci gave to this was the importance of consent and culture. If the fundamental Marxists saw the power in terms of class-versus-class, then Gramsci gave to us a question of class alliance. The rise of cultural studies itself was based on the decline of the prominence of fundamental class-versus-class politics. Edgar and Sedgwick write: The theory of hegemony was of central importance to the development of British cultural studies [particularly The Birmingham School. It facilitated the analysis of the ways subordinate groups actively resist and respond to political and economic domination. The subordinate groups needed not to be seen merely as the passive dupes of the dominant class and its ideology. Structure and agency The development of hegemony theory in cultural studies was in some ways consonant with work in other fields exploring agency, a theoretical concept that insists on the active, critical capacities of subordinated people (e.g. the working classes, colonized peoples, women). As Stuart Hall famously argued in his 1981 essay, "Notes on Deconstructing 'the Popular'": "ordinary people are not cultural dopes." Insistence on accounting for the agency of subordinated people run counter to the work of traditional structuralists. Some analysts have however been critical of some work in cultural studies that they feel overstates the significance of or even romanticizes some forms of popular cultural agency. Cultural studies often concerns itself with the agency at the level of the practices of everyday life, and approaches such research from a standpoint of radical contextualism. In other words, cultural studies rejects universal accounts of cultural practices, meanings, and identities. Judith Butler, an American feminist theorist whose work is often associated with cultural studies, wrote that: the move from a structuralist account in which capital is understood to structure social relations in relatively homologous ways to a view of hegemony in which power relations are subject to repetition, convergence, and rearticulation brought the question of temporality into the thinking of structure. It has marked a shift from a form of Althusserian theory that takes structural totalities as theoretical objects to one in which the insights into the contingent possibility of structure inaugurate a renewed conception of hegemony as bound up with the contingent sites and strategies of the rearticulation of power. Globalization In recent decades, as capitalism has spread throughout the world via contemporary forms of globalization, cultural studies has generated important analyses of local sites and practices of negotiation with and resistance to Western hegemony. Cultural consumption Cultural studies criticizes the traditional view of the passive consumer, particularly by underlining the different ways people read, receive and interpret cultural texts, or appropriate other kinds of cultural products, or otherwise participate in the production and circulation of meanings. On this view, a consumer can appropriate, actively rework, or challenge the meanings circulated through cultural texts. In some of its variants, cultural studies has shifted the analytical focus from traditional understandings of production to consumption – viewed as a form of production (of meanings, of identities, etc.) in its own right. Stuart Hall, John Fiske, and others have been influential in these developments. A special 2008 issue of the field's flagship journal, Cultural Studies, examined "anti-consumerism" from a variety of cultural studies angles. Jeremy Gilbert noted in the issue, cultural studies must grapple with the fact that "we now live in an era when, throughout the capitalist world, the overriding aim of government economic policy is to maintain consumer spending levels. This is an era when 'consumer confidence' is treated as the key indicator and cause of economic effectiveness." Concept of "text" Cultural studies, drawing upon and developing semiotics, uses the concept of text to designate not only written language, but also television programs, films, photographs, fashion, hairstyles, and so forth; the texts of cultural studies comprise all the meaningful artifacts of culture. This conception of textuality derives especially from the work of the pioneering and influential semiotician, Roland Barthes, but also owes debts to other sources, such as Juri Lotman and his colleagues from Tartu–Moscow School. Similarly, the field widens the concept of culture. Cultural studies approach the sites and spaces of everyday life, such as pubs, living rooms, gardens, and beaches, as "texts". Culture, in this context, includes not only high culture, but also everyday meanings and practices, a central focus of cultural studies. Jeff Lewis summarized much of the work on textuality and textual analysis in his cultural studies textbook and a post-9/11 monograph on media and terrorism. According to Lewis, textual studies use complex and difficult heuristic methods and require both powerful interpretive skills and a subtle conception of politics and contexts. The task of the cultural analyst, for Lewis, is to engage with both knowledge systems and texts and observe and analyze the ways the two interact with one another. This engagement represents the critical dimensions of the analysis, its capacity to illuminate the hierarchies within and surrounding the given text and its discourse. Academic reception Cultural studies has evolved through its uptake across a variety of different disciplines—anthropology, media studies, communication studies, literary studies, education, geography, philosophy, sociology, politics, and others. While some have accused certain areas of cultural studies of meandering into political relativism and a kind of empty version of "postmodern" analysis, others hold that at its core, cultural studies provides a significant conceptual and methodological framework for cultural, social, and economic critique. This critique is designed to "deconstruct" the meanings and assumptions that are inscribed in the institutions, texts, and practices that work with and through, and produce and re-present, culture. Thus, while some scholars and disciplines have dismissed cultural studies for its methodological rejection of disciplinarity, its core strategies of critique and analysis have influenced areas of the social sciences and humanities; for example, cultural studies work on forms of social differentiation, control and inequality, identity, community-building, media, and knowledge production has had a substantial impact. Moreover, the influence of cultural studies has become increasingly evident in areas as diverse as translation studies, health studies, international relations, development studies, computer studies, economics, archaeology, and neurobiology. Cultural studies has also diversified its own interests and methodologies, incorporating a range of studies on media policy, democracy, design, leisure, tourism, warfare, and development. While certain key concepts such as ideology or discourse, class, hegemony, identity, and gender remain significant, cultural studies has long engaged with and integrated new concepts and approaches. The field thus continues to pursue political critique through its engagements with the forces of culture and politics. Integration of popular culture in CS and education The integration of popular culture in classrooms has influenced educational practices in cultural studies. Through the analysis of TV series, movies, memes, and other cultural materials, educators can encourage media literacy, critical thinking, and a deeper understanding of social issues. Incorporating popular culture into education through cultural studies helps students critically engage with the world around them, fostering media literacy and critical thinking. Educators can use cultural texts to discuss societal issues, challenge norms, and prepare students for active participation in a media-dominated world. Popular culture can be an effective tool for critical pedagogy. Evan Faidley explores how TV shows, movies, and memes can be used in the classroom to discuss topics like social justice and identity. Shows like South Park allow students to evaluate societal norms and political issues, using a pedagogy of resistance. Cultural studies encourage students to analyze intertextuality. Patricia Duff discusses how popular culture incorporates with academic discourse to build media literacy which helps students critically engage with the media they consume daily. Kathy Mills also highlights the importance of multiliteracies, which encourages students to utilize a variety of communication media outside of the standard text, including digital and visual media. Diane Penrod argues that incorporating popular culture in education makes learning more relevant and engaging. Teachers can aid students in comprehending difficult concepts like gender, ethnicity, and class by utilizing works from their own culture. Students are also encouraged to develop critical analytical abilities which they can use in both academic and everyday situations when popular culture is integrated into the classroom. Literary scholars Many cultural studies practitioners work in departments of English or comparative literature. Nevertheless, some traditional literary scholars such as Yale professor Harold Bloom have been outspoken critics of cultural studies. On the level of methodology, these scholars dispute the theoretical underpinning of the movement's critical framework. Bloom stated his position during the 3 September 2000 episode of C-SPAN's Booknotes, while discussing his book How to Read and Why: [T]here are two enemies of reading now in the world, not just in the English-speaking world. One [is] the lunatic destruction of literary studies...and its replacement by what is called cultural studies in all of the universities and colleges in the English-speaking world, and everyone knows what that phenomenon is. I mean, the...now-weary phrase 'political correctness' remains a perfectly good descriptive phrase for what has gone on and is, alas, still going on almost everywhere and which dominates, I would say, rather more than three-fifths of the tenured faculties in the English-speaking world, who really do represent treason of the intellectuals, I think, a 'betrayal of the clerks'." Marxist literary critic Terry Eagleton is not wholly opposed to cultural studies, but has criticised aspects of it and highlighted what he sees as its strengths and weaknesses in books such as After Theory (2003). For Eagleton, literary and cultural theory have the potential to say important things about the "fundamental questions" in life, but theorists have rarely realized this potential. English departments also host cultural rhetorics scholars. This academic field defines cultural rhetorics as "the study and practice of making meaning and knowledge with the belief that all cultures are rhetorical and all rhetorics are cultural." Cultural rhetorics scholars are interested in investigating topics like climate change, autism, Asian American rhetoric, and more. Sociology Cultural studies have also had a substantial impact on sociology. For example, when Stuart Hall left CCCS at Birmingham, it was to accept a prestigious professorship in Sociology at the Open University in Britain. The subfield of cultural sociology, in particular, is disciplinary home to many cultural studies practitioners. Nevertheless, there are some differences between sociology as a discipline and the field of cultural studies as a whole. While sociology was founded upon various historic works purposefully distinguishing the subject from philosophy or psychology, cultural studies have explicitly interrogated and criticized traditional understandings and practices of disciplinarity. Most CS practitioners think it is best that cultural studies neither emulate disciplines nor aspire to disciplinarity for cultural studies. Rather, they promote a kind of radical interdisciplinarity as the basis for cultural studies. One sociologist whose work has had a major influence on cultural studies is Pierre Bourdieu, whose work makes innovative use of statistics and in-depth interviews. However, although Bourdieu's work has been highly influential within cultural studies, and although Bourdieu regarded his work as a form of science, cultural studies has never embraced the idea that it should aspire toward "scientificity", and has marshalled a wide range of theoretical and methodological arguments against the fetishization of "scientificity" as a basis for cultural studies. Two sociologists who have been critical of cultural studies, Chris Rojek and Bryan S. Turner, argue in their article, "Decorative sociology: towards a critique of the cultural turn," that cultural studies, particularly the flavor championed by Stuart Hall, lacks a stable research agenda, and privileges the contemporary reading of texts, thus producing an ahistorical theoretical focus. Many, however, would argue, following Hall, that cultural studies have always sought to avoid the establishment of a fixed research agenda; this follows from its critique of disciplinarity. Moreover, Hall and many others have long argued against the misunderstanding that textual analysis is the sole methodology of cultural studies, and have practiced numerous other approaches, as noted above. Rojek and Turner also level the accusation that there is "a sense of moral superiority about the correctness of the political views articulated" in cultural studies. Science wars In 1996, physicist Alan Sokal expressed his opposition to cultural studies by submitting a hoax article to a cultural studies journal, Social Text. The article, which was crafted as a parody of what Sokal referred to as the "fashionable nonsense" of postmodernism, was accepted by the editors of the journal, which did not at the time practice peer review. When the paper appeared in print, Sokal published a second article in a self-described "academic gossip" magazine, Lingua Franca, revealing his hoax on Social Text. Sokal stated that his motivation stemmed from his rejection of contemporary critiques of scientific rationalism: Politically, I'm angered because most (though not all) of this silliness is emanating from the self-proclaimed Left. We're witnessing here a profound historical volte-face. For most of the past two centuries, the Left has been identified with science and against obscurantism; we have believed that rational thought and the fearless analysis of objective reality (both natural and social) are incisive tools for combating the mystifications promoted by the powerful – not to mention being desirable human ends in their own right. The recent turn of many "progressive" or "leftist" academic humanists and social scientists toward one or another form of epistemic relativism betrays this worthy heritage and undermines the already fragile prospects for progressive social critique. Theorizing about "the social construction of reality" won't help us find an effective treatment for AIDS or devise strategies for preventing global warming. Nor can we combat false ideas in history, sociology, economics and politics if we reject the notions of truth and falsity. In response to this critique, Jacques Derrida wrote: In whose interest was it to go for a quick practical joke rather than taking part in the work which, sadly, it replaced? Founding works Hall and others have identified some core originating texts, or the original "curricula", of the field of cultural studies: Richard Hoggart's The Uses of Literacy Raymond Williams' Culture and Society and The Long Revolution E. P. Thompson's The Making of the English Working Class See also Culturology Cultural Studies Association (US) European Communication Research and Education Association (Norway) International Association for Translation and Intercultural Studies (South Korea) Popular culture studies References Sources Bitar, Amer (2020). Bedouin Visual Leadership in the Middle East: The Power of Aesthetics and Practical Implications. Springer Nature. ISBN 9783030573973. Du Gay, Paul, et al. 1997. Doing Cultural Studies: The Story of the Sony Walkman. Culture, Media and Identities. London: SAGE, in association with Open University. During, Simon (2007). The cultural studies reader (3rd ed.). London: Routledge. ISBN 978-0-415-37412-5. Edgar, Andrew, and Peter Sedgwick. 2005. Cultural Theory: The Key Concepts (2nd ed.). New York: Routledge. Engel, Manfred. 2008. "Cultural and Literary Studies." Canadian Review of Comparative Literature 31:460–67. Grossberg, Lawrence (2010). Cultural Studies in the Future Tense. Durham, NC: Duke University Press. Grossberg, Lawrence; Nelson, Cary; Treichler, Paula, eds. (1992). Cultural Studies. New York: Routledge. ISBN 0-415-90351-3.. Hall, Gary & Birchall, Claire, eds. (2006). New Cultural Studies: Adventures in Theory. Edinburgh: Edinburgh University Press. Hall, Stuart, ed. (1980). Culture, Media, Language: Working Papers in Cultural Studies, 1972-1979. London: Routledge in association with the Centre for Contemporary Cultural Studies, University of Birmingham. ISBN 0-09-142070-9. —— (1980b). "Cultural studies: Two paradigms". Media, Culture & Society. 2: 57–72. doi:10.1177/016344378000200106. —— 1992. "Race, Culture, and Communications: Looking Backward and Forward at Cultural Studies." Rethinking Marxism 5(1):10–18. Hoggart, Richard. 1957. The Uses of Literacy: Aspects of Working Class Life. Chatto and Windus. ISBN 0-7011-0763-4 Hartley, John (2003). A Short History of Cultural Studies. London: Sage. Johnson, Richard. 1986–87. "What Is Cultural Studies Anyway?" Social Text 16:38–80. —— 2004. "Multiplying Methods: From Pluralism to Combination." pp. 26–43 in Practice of Cultural Studies. London: SAGE. —— "Post-Hegemony? I Don't Think So" Theory, Culture & Society 24(3):95–110. Lash, Scott (May 2007). "Power after Hegemony: Cultural Studies in Mutation?". Theory, Culture & Society. 24 (3): 55–78. doi:10.1177/0263276407075956. S2CID 145639801. Lewis, Jeff (2008). Cultural Studies: The Basics (2nd ed.). London: SAGE Publications. ISBN 978-1-4129-2229-6. Lindlof, T. R., and B. C. Taylor. 2002. Qualitative Communication Research Methods (2nd ed.). Thousand Oaks, CA: SAGE. Longhurst, Brian, Greg Smith, Gaynor Bagnall, Garry Crawford, and Michael Ogborn. 2008. Introducing Cultural Studies (2nd ed.). London: Pearson. ISBN 978-1-4058-5843-4. Miller, Toby, ed. (2006). A Companion to Cultural Studies. Malden, MA: Blackwell Publishers. ISBN 978-0-631-21788-6. Pollock, Griselda, ed. 1996. Generations and Geographies: Critical Theories and Critical Practices in Feminism and the Visual Arts. Routledge. —— 2006. Psychoanalysis and the Image. Boston: Blackwell. Sardar, Ziauddin, Van Loon, Borin (1997). Introducing Cultural Studies. New York: Totem Books. Smith, Paul. 1991. "A Course In 'Cultural Studies'." The Journal of the Midwest Modern Language Association 24(1):39–49. —— 2006. "Looking Backwards and Forwards at Cultural Studies." pp. 331–40 in A Companion to Cultural Studies, edited by T. Miller. Malden, MA: Blackwell Publishers. ISBN 978-0-631-21788-6. Rodman, Gil (2015). Why Cultural Studies? Maldon, MA: Wiley Blackwell. Turner, Graeme (2003). British Cultural Studies: An Introduction (Third ed.). London: Routledge. —— 2012. What's Become of Cultural Studies? Los Angeles: SAGE. Williams, Jeffrey, interviewer. 1994. "Questioning Cultural Studies: An Interview with Paul Smith." Hartford, CT: MLG Institute for Culture and Society, Trinity College. Retrieved 1 July 2020. Williams, Raymond. 1985. Keywords: A Vocabulary of Culture and Society (revised ed.). New York: Oxford University Press. —— 1966. Culture and Society, 1780-1950. New York: Harper & Row. External links CCCS publications (Annual Reports and Stencilled Occasional [sic] Papers) of the University of Birmingham Archived 10 May 2019 at the Wayback Machine CSAA: Cultural Studies Association of Australasia Cultural Studies International Journal of Cultural Studies Stuart Hall Archive Project, University of Birmingham, UK Stuart Hall: Selected Writings, Duke University Press Research institute for Interculturality

Sport is a physical activity or game, often competitive and organized, that maintains or improves physical ability and skills. Sport may provide enjoyment to participants and entertainment to spectators. The number of participants in a particular sport can vary from hundreds of people to a single individual. Sport competitions may use a team or single person format, and may be open, allowing a broad range of participants, or closed, restricting participation to specific groups or those invited. Competitions may allow a "tie" or "draw", in which there is no single winner; others provide tie-breaking methods to ensure there is only one winner. They also may be arranged in a tournament format, producing a champion. Many sports leagues make an annual champion by arranging games in a regular sports season, followed in some cases by playoffs. Sport is generally recognised as system of activities based in physical athleticism or physical dexterity, with major competitions admitting only sports meeting this definition. Some organisations, such as the Council of Europe, preclude activities without any physical element from classification as sports. However, a number of competitive, but non-physical, activities claim recognition as mind sports. The International Olympic Committee who oversee the Olympic Games recognises both chess and bridge as sports. SportAccord, the international sports federation association, recognises five non-physical sports: chess, bridge, draughts, Go and xiangqi. However, they limit the number of mind games which can be admitted as sports. Sport is usually governed by a set of rules or customs, which serve to ensure fair competition. Winning can be determined by physical events such as scoring goals or crossing a line first. It can also be determined by judges who are scoring elements of the sporting performance, including objective or subjective measures such as technical performance or artistic impression. Records of performance are often kept, and for popular sports, this information may be widely announced or reported in sport news. Sport is also a major source of entertainment for non-participants, with spectator sport drawing large crowds to sport venues, and reaching wider audiences through broadcasting. Sport betting is in some cases severely regulated, and in others integral to the sport. According to A.T. Kearney, a consultancy, the global sporting industry is worth up to $620 billion as of 2013. The world's most accessible and practised sport is running, while association football is the most popular spectator sport. Meaning and usage Etymology The word "sport" comes from the Old French desport meaning "leisure", with the oldest definition in English from around 1300 being "anything humans find amusing or entertaining". Other meanings include gambling and events staged for the purpose of gambling; hunting; and games and diversions, including ones that require exercise. Roget's defines the noun sport as an "activity engaged in for relaxation and amusement" with synonyms including diversion and recreation. Nomenclature The singular term "sport" is used in most English dialects to describe the overall concept, e.g. "children taking part in sport", with "sports" used to describe multiple activities, e.g. "football and rugby are the most popular sports in England". American English uses "sports" for both senses. Definition The precise definition of what differentiates a sport from other leisure activities varies between sources. The closest to an international agreement on a definition is provided by the Global Association of International Sports Federations (GAISF), which is the association for all the largest international sports federations (including association football, athletics, cycling, tennis, equestrian sports, and more), and is therefore the de facto representative of international sport. GAISF uses the following criteria, determining that a sport should: have an element of competition be in no way harmful to any living creature not rely on equipment provided by a single supplier (excluding proprietary games such as arena football) not rely on any "luck" element specifically designed into the sport. They also recognise that sport can be primarily physical (such as rugby or athletics), primarily mind (such as chess or Go), predominantly motorised (such as Formula 1 or powerboating), primarily co-ordination and dexterity (such as snooker and other cue sports), or primarily animal-supported (such as equestrian sport). The inclusion of mind sports within sport definitions has not been universally accepted, leading to legal challenges from governing bodies in regards to being denied funding available to sports. Whilst GAISF recognises a small number of mind sports, it is not open to admitting any further mind sports. There has been an increase in the application of the term "sport" to a wider set of non-physical challenges such as video games, also called esports (from "electronic sports"), especially due to the large scale of participation and organised competition, but these are not widely recognised by mainstream sports organisations. According to Council of Europe, European Sports Charter, article 2.i, "'Sport' means all forms of physical activity which, through casual or organised participation, aim at expressing or improving physical fitness and mental well-being, forming social relationships or obtaining results in competition at all levels." Competition There are opposing views on the necessity of competition as a defining element of a sport, with almost all professional sports involving competition, and governing bodies requiring competition as a prerequisite of recognition by the International Olympic Committee (IOC) or GAISF. Other bodies advocate widening the definition of sport to include all forms of physical activity, not only organised or competitive events. For instance, the Council of Europe’s Sports Charter defines “sport” as: “all forms of physical activity which, through casual or organised participation, are aimed at maintaining or improving physical fitness and mental well-being, forming social relationships or obtaining results in competition at all levels,” explicitly encompassing recreational exercise undertaken purely for fun. To widen participation, and reduce the impact of losing on less able participants, there has been an introduction of non-competitive physical activity to traditionally competitive events such as school sports days, although moves like this can be controversial. In competitive events, participants are graded or classified based on their "result" and often divided into groups of comparable performance, e.g. gender, weight and age. The measurement of the result may be objective or subjective, and corrected with "handicaps" or penalties. In a race, for example, the time to complete the course is an objective measurement. In gymnastics or diving the result is decided by judges, and therefore subjective. There are shades of judging between boxing and mixed martial arts, where if neither competitor secures a victory before the time limit, the outcome is determined by judges’ scorecards. In boxing, three judges independently score each round, based on criteria such as clean punching, effective aggression, ring generalship and defense. History Artifacts and structures suggest sport in China as early as 2000 BC. Gymnastics appears to have been popular in China's ancient past. Monuments to the Pharaohs indicate that a number of sports, including swimming and fishing, were well-developed and regulated several thousands of years ago in ancient Egypt. Other Egyptian sports included javelin throwing, high jump, and wrestling. Ancient Persian sports such as the traditional Iranian martial art of Zoorkhaneh had a close connection to warfare skills. Among other sports that originated in ancient Persia are polo and jousting. Various traditional games of India such as Kho kho and Kabbadi have been played for thousands of years. The kabaddi was played potentially as a preparation for hunting. A wide range of sports were already established by the time of Ancient Greece and the military culture and the development of sport in Greece influenced one another considerably. Sport became such a prominent part of their culture that the Greeks created the Olympic Games, which in ancient times were held every four years in a small village in the Peloponnesus called Olympia. Sports have been increasingly organised and regulated from the time of the ancient Olympics up to the present century. Industrialisation has brought motorised transportation and increased leisure time, letting people attend and follow spectator sports and participate in athletic activities. These trends continued with the advent of mass media and global communication. Professionalism became prevalent, further adding to the increase in sport's popularity, as sports fans followed the exploits of professional athletes – all while enjoying the exercise and competition associated with amateur participation in sports. Since the turn of the 21st century, there has been increasing debate about whether transgender sports people should be able to participate in sport events that conform with their post-transition gender identity. Fair play Sportsmanship Sportsmanship is an attitude that strives for fair play, courtesy toward teammates and opponents, ethical behaviour and integrity, and grace in victory or defeat. Sportsmanship expresses an aspiration or ethos that the activity will be enjoyed for its own sake. The well-known sentiment by sports journalist Grantland Rice, that it is “it’s not whether you win or lose, but how you play the game,” and the modern Olympic creed expressed by its founder Pierre de Coubertin: “The most important thing in the Olympic Games is not to win but to take part,” are typical expressions of this sentiment. Cheating Key principles of sport include that the result should not be predetermined, and all participants must have an equal opportunity to win. Rules and regulations are established by governing bodies to ensure fair play and integrity. However, participants sometimes breach these rules to gain an unfair advantage. Participants may cheat to increase their chances of winning, secure financial gain or other benefit. The prevalence of gambling on sporting outcomes creates incentives for match fixing, in which one or more participants collude to predetermine results rather than compete honestly. Doping and drugs The competitive nature of sport encourages some participants to attempt to enhance their performance through medicines, or other means such as increasing the volume of blood in their bodies through artificial means. All sports recognised by the IOC, or SportAccord, are required to implement a testing programme, looking for a list of banned drugs, with suspensions or bans being placed on participants who test positive for banned substances. Violence Violence in sports involves crossing the line between fair competition and intentional aggressive violence. Athletes, coaches, fans, and parents sometimes unleash violent behaviour on people or property, in misguided shows of loyalty, dominance, anger, or celebration. Rioting or hooliganism by fans in particular is a problem at some national and international sporting contests. Participation Gender participation Female participation in sports has risen alongside expanded opportunities and growing recognition of the benefits of athletic activity for child development and physical fitness. Despite these gains, a gender gap persists. At Olympic level, women accounted for 49% at Tokyo 2020, reaching full 50 % parity at Paris 2024. But global surveys report only 20% of women versus 31% of men participate in sporting activity monthly, and the World Health Organization notes women are 5 percentage points less likely than men to meet recommended activity guidelines. Certain sports are mixed-sex, allowing, or even requiring, men and women to play on the same team. One example of this is Baseball5, which is the first mixed-gender sport to be admitted to the Olympics. Youth participation Youth sport presents children with opportunities for fun, socialisation, forming peer relationships, physical fitness, and athletic scholarships. Activists for education and the war on drugs encourage youth sport as a means to increase educational participation and to fight the illegal drug trade. According to the Center for Injury Research and Policy at Nationwide Children's Hospital, the biggest risk for youth sport is death or serious injury including concussion. These risks come from running, basketball, association football, volleyball, gridiron, gymnastics, and ice hockey. Youth sport in the US is a $15 billion industry including equipment up to private coaching. Disabled participation Disabled or adaptive sports are played by people with a disability, including physical and intellectual disabilities. As many of these are based on existing sports modified to meet the needs of people with a disability, they are sometimes referred to as adapted sports. However, not all disabled sports are adapted; several that have been specifically created for people with a disability have no equivalent in able-bodied sports, such as goalball and boccia. Older participation Masters sport, senior sport, or veteran sport is an age category of sport, that usually contains age groups of those 35 and older. It may concern unaltered or adapted sport activities, with and without competitions. Competitions World Masters Games organized since 1985 every four years. European Masters Games organized for the first time in 2008 and then since 2011 every four years. Senior Olympics (Senior Games) USATF Masters Outdoor Championships began 1968 USATF Masters Indoor Championships began 1975 World Masters Athletics Championships began 1975 Spectator involvement The competition element of sport, along with the aesthetic appeal of some disciplines, has resulted in the phenomenon of spectator sport. Amateur and professional sports attract audiences in person at venues and via broadcast media—radio, television and internet streaming—each of which may levy fees such as entrance tickets or pay-per-view subscriptions. Sports leagues and tournaments provide the primary organisational frameworks for regular competition among teams or individual athletes. High-profile events command vast audiences, driving lucrative media-rights deals. The 2006 FIFA World Cup final drew over 700 million viewers worldwide, and the 2011 Cricket World Cup final was watched by approximately 135 million viewers in India alone. In the US, the Super Bowl ranks as the most-watched annual television broadcast, with Super Bowl XLIX in 2015 averaging 114 million viewers. Super Bowl Sunday is considered an unofficial national holiday, and in 2015 a 30-second advertising spot sold for approximately US$4.5 million. Amateur and professional Sport can be undertaken on an amateur, professional or semi-professional basis, depending on whether participants are incentivised for participation (usually through payment of a wage or salary). Amateur participation in sport at lower levels is often called "grassroots sport". The popularity of spectator sport as a recreation for non-participants has led to sport becoming a major business in its own right, and this has incentivised a high paying professional sport culture, where high performing participants are rewarded with pay far in excess of average wages, which can run into millions of dollars. Some sports, or individual competitions within a sport, retain a policy of allowing only amateur sport. The Olympic Games started with a principle of amateur competition with those who practised a sport professionally considered to have an unfair advantage over those who practised it merely as a hobby. From 1971, Olympic athletes were allowed to receive compensation and sponsorship, and from 1986, the IOC decided to make all professional athletes eligible for the Olympics, with the exceptions of boxing, and wrestling. Technology Technology plays an important part in modern sport. It is a necessary part of some sports (such as motorsport), and it is used in others to improve performance. Some sports also use it to allow off-field decision making. Sports science can be applied to areas including athlete performance, such as the use of video analysis to fine-tune technique, or to equipment, such as improved running shoes or competitive swimwear. Sports engineering emerged as a discipline in 1998 with an increasing focus not just on materials design but also the use of technology in sport, from analytics and big data to wearable technology. In order to control the impact of technology on fair play, governing bodies frequently have specific rules that are set to control the impact of technical advantage between participants. For example, in 2010, full-body, non-textile swimsuits were banned by FINA, as they were enhancing swimmers' performances. The increase in technology has also allowed many decisions in sports matches to be taken, or reviewed, off-field, with another official using instant replays to make decisions. In some sports, players can now challenge decisions made by officials. In Association football, goal-line technology makes decisions on whether a ball has crossed the goal line or not. The technology is not compulsory, but was used in the 2014 FIFA World Cup in Brazil, and the 2015 FIFA Women's World Cup in Canada, as well as in the Premier League from 2013–14, and the Bundesliga from 2015–16. In the NFL, a referee can ask for a review from the replay booth, or a head coach can issue a challenge to review the play using replays. The final decision rests with the referee. A video referee (commonly known as a Television Match Official or TMO) can also use replays to help decision-making in rugby (both league and union). In international cricket, an umpire can ask the Third umpire for a decision, and the third umpire makes the final decision. Since 2008, a decision review system for players to review decisions has been introduced and used in ICC-run tournaments, and optionally in other matches. Depending on the host broadcaster, a number of different technologies are used during an umpire or player review, including instant replays, Hawk-Eye, Hot Spot and Real Time Snickometer. Hawk-Eye is also used in tennis to challenge umpiring decisions. Sports and education Research suggests that sports have the capacity to connect youth to positive adult role models and provide positive development opportunities, as well as promote the learning and application of life skills. In recent years the use of sport to reduce crime, as well as to prevent violent extremism and radicalization, has become more widespread, especially as a tool to improve self-esteem, enhance social bonds and provide participants with a feeling of purpose. There is no high-quality evidence that shows the effectiveness of interventions to increase sports participation of the community in sports such as mass media campaigns, educational sessions, and policy changes. There is also no high-quality studies that investigate the effect of such interventions in promoting healthy behaviour change in the community. Educational qualifications are available in sport: for example, sports science is a well-established academic discipline. In the UK, a BTEC National Diploma in Sports Coaching and Development is offered by Pearson Education. There are four qualifications in this family: Extended Certificate in Sports Coaching Foundation Diploma in Sports Coaching and Development Diploma in Sports Coaching and Development Extended Diploma in Sports Coaching and Development. Politics Benito Mussolini used the 1934 FIFA World Cup, held in Italy, to showcase Fascist Italy. Adolf Hitler used the 1936 Summer Olympics held in Berlin, and the 1936 Winter Olympics held in Garmisch-Partenkirchen, to promote the Nazi ideology of the superiority of the Aryan race, and inferiority of the Jews and other "undesirables". Germany used the Olympics to give off a peaceful image while secretly preparing for war. When apartheid was official policy in South Africa, many sports people, particularly in rugby union, adopted the conscientious approach that they should not appear in competitive sports there. Some feel this was an effective contribution to the eventual end of apartheid, others feel it may have prolonged and reinforced its worst effects. In the history of Ireland, Gaelic sports were connected with cultural nationalism. Until the mid-20th century a person could have been banned from playing Gaelic football, hurling, or other sports administered by the Gaelic Athletic Association (GAA) if she/he played or supported Association football, or other games seen to be of British origin. The GAA banned the playing of football and rugby union at Gaelic venues. This ban, also known as Rule 42, is still enforced, but was modified to allow football and rugby to be played in Croke Park while Lansdowne Road was redeveloped into Aviva Stadium. Under Rule 21, the GAA banned members of the British security forces and members of the RUC from playing Gaelic games, but the advent of the Good Friday Agreement in 1998 led to removal of the ban. Nationalism is often evident in the pursuit of sport or in its reporting: athletes compete in national teams, and commentators or audiences frequently adopt partisan perspectives. On occasion, such tensions erupt into violence among players or spectators, as during the 1969 Football War between El Salvador and Honduras, a conflict sparked by rioting at World Cup qualifiers. Such episodes are viewed as contrary to the fundamental ethos of sport—namely, that it be contested for its own sake and for the enjoyment of participants. Politics and sport tragically intersected at the 1972 Munich Olympics, when Palestinian militants infiltrated the Olympic Village, took Israeli team members hostage, and ultimately killed 11 athletes in what became known as the Munich massacre. A study of US elections has shown that the result of sports events can affect the results. A study published in the Proceedings of the National Academy of Sciences showed that when the home team wins the game before the election, the incumbent candidates can increase their share of the vote by 1.5%. A loss had the opposite effect, and the effect is greater for higher-profile teams or unexpected wins and losses. When the Washington Commanders win their final game before an election, then the incumbent president is more likely to win, and if they lose, then the opposition candidate is more likely to win; this has become known as the Redskins Rule. As a means of controlling and subduing populations Étienne de La Boétie, in his essay Discourse on Voluntary Servitude describes athletic spectacles as means for tyrants to control their subjects by distracting them. Do not imagine that there is any bird more easily caught by decoy, nor any fish sooner fixed on the hook by wormy bait, than are all these poor fools neatly tricked into servitude...they let themselves be caught so quickly at the slightest tickling of their fancy. Plays, farces, spectacles, gladiators, strange beasts, medals, pictures, and other such opiates, these were for ancient peoples the bait toward slavery, the price of their liberty, the instruments of tyranny. By these...enticements the ancient dictators so successfully lulled their subjects under the yoke, that the stupefied peoples, fascinated by the pastimes and vain pleasures flashed before their eyes, learned subservience as naïvely...as little children learn to read by looking at bright picture books. During the British rule of Bengal, British and European sports began to supplant traditional Bengali sports, resulting in a loss of native culture. In communist controlled East Germany, from the 1970's to 1990, 'an estimated 3,000 unofficial collaborators were used each year in top-level sport, including many football players, fans and referees'. Among the most important reasons for the Stasi setting up this extensive network of collaborators was to prevent athletes escaping to the West, using both methods of surveillance and repression. Religious views Sport was an important form of worship in Ancient Greek religion. The ancient Olympic Games were held in honour of the head deity, Zeus, and featured various forms of religious dedication to him and other gods. Many Greeks travelled to see the games and the combination of religious pilgrimage and sport served as a way of uniting them as one people. The practice of athletic competitions has been criticised by some Christian thinkers as a form of idolatry, in which "human beings extol themselves, adore themselves, sacrifice themselves and reward themselves." Sports are seen by these critics as a manifestation of "collective pride" and "national self-deification" in which feats of human power are idolised at the expense of divine worship. Tertullian condemns the athletic performances of his day, insisting "the entire apparatus of the shows is based upon idolatry." The shows, says Tertullian, excite passions foreign to the calm temperament cultivated by the Christian: God has enjoined us to deal calmly, gently, quietly, and peacefully with the Holy Spirit, because these things are alone in keeping with the goodness of His nature, with His tenderness and sensitiveness. ... Well, how shall this be made to accord with the shows? For the show always leads to spiritual agitation, since where there is pleasure, there is keenness of feeling giving pleasure its zest; and where there is keenness of feeling, there is rivalry giving in turn its zest to that. Then, too, where you have rivalry, you have rage, bitterness, wrath and grief, with all bad things which flow from them – the whole entirely out of keeping with the religion of Christ. Christian clerics in the Wesleyan-Holiness movement oppose the viewing of or participation in professional sports, believing that professional sports leagues profane the Sabbath, compete with a Christian's primary commitment to God, exhibit a lack of modesty in the players' and cheerleaders' uniforms, are associated with violence and extensive use of profanity among many players, and encourage gambling, as well as alcohol and other drugs at sporting events, which go against a commitment to teetotalism. See also Related topics References Sources European Commission (2007), The White Paper on Sport Council of Europe (2001), The European sport charter This article incorporates text from a free content work. Licensed under CC BY-SA 3.0 IGO. Text taken from Strengthening the rule of law through education: a guide for policymakers​, UNESCO. Further reading The Meaning of Sports by Michael Mandel (PublicAffairs, ISBN 1-58648-252-1). Journal of the Philosophy of Sport Sullivan, George. The Complete Sports Dictionary. New York: Scholastic Book Services, 1979. p. 199. ISBN 0-590-05731-6

A game is a structured type of play usually undertaken for entertainment or fun, and sometimes used as an educational tool. Many games are also considered to be work (such as professional players of spectator sports or video games) or art (such as games involving an artistic layout such as mahjong, solitaire, or some video games). There are many types of games; popular formats include board games, video games, online games, and card games. Games can be played in a variety of circumstances, and some can be played even without any materials or company. Games can be played either for enjoyment or for competition; they can be played alone or in teams; they can be played offline or online. In a notable, competitive setting, players may have an audience to watch them play. Examples of games that generally draw audiences are chess championships, e-sports, and professional sports. All games must have a challenge and a structure; barring certain exceptions like sandbox games, all games also have an objective. Multiplayer games also include interaction between two or more players. Not all forms of play are considered games; toys and puzzles, for instance, are not games, as they do not have a structure. Games generally involve either mental stimulation, physical stimulation, or both. Many games help develop practical skills, serve as a form of exercise, or perform an educational, simulational, or psychological role. Attested as early as 2600 BC, games are a universal part of human experience and present in all cultures. The Royal Game of Ur, Senet, and Mancala are some of the oldest known games. Definitions Ludwig Wittgenstein Ludwig Wittgenstein is well known in the history of philosophy for having addressed the definition of the word game. In his Philosophical Investigations, Wittgenstein argued that the elements of games, such as play, rules, and competition, all fail to adequately define what games are. From this, Wittgenstein concluded that people apply the term game to a range of disparate human activities that bear to one another only what one might call family resemblances. As the following game definitions show, this conclusion was not a final one, and today many philosophers, like Thomas Hurka, think that Wittgenstein was wrong and that Bernard Suits' definition is a good answer to the problem. Roger Caillois French sociologist Roger Caillois, in his book Les jeux et les hommes (Games and Men)(1961), defined a game as an activity that must have the following characteristics: fun: the activity is chosen for its light-hearted character separate: it is circumscribed in time and place uncertain: the outcome of the activity is unforeseeable non-productive: participation does not accomplish anything useful governed by rules: the activity has rules that are different from everyday life fictitious: it is accompanied by the awareness of a different reality Chris Crawford Game designer Chris Crawford defined the term in the context of computers. Using a series of dichotomies: Creative expression is art if made for its own beauty and entertainment if made for money. A piece of entertainment is a plaything if it is interactive. Movies and books are cited as examples of non-interactive entertainment. If no goals are associated with a plaything, it is a toy. (Crawford notes that by his definition, (a) a toy can become a game element if the player makes up rules, and (b) The Sims and SimCity are toys, not games.) If it has goals, a plaything is a challenge. If a challenge has no "active agent against whom you compete," it is a puzzle; if there is one, it is a conflict. (Crawford admits that this is a subjective test. Video games with noticeably algorithmic artificial intelligence can be played as puzzles; these include the patterns used to evade ghosts in Pac-Man.) Finally, if the player can only outperform the opponent but not attack them to interfere with their performance, the conflict is a competition. (Competitions include racing and figure skating.) However, if attacks are allowed, then the conflict qualifies as a game. Crawford's definition may thus be rendered as an interactive, goal-oriented activity made for money, with active agents to play against, in which players (including active agents) can interfere with each other. Other definitions, however, as well as history, show that entertainment and games are not necessarily undertaken for monetary gain. Other definitions "My conclusion is that to play a game is to engage in activity directed towards bringing about a specific state of affairs, using only means permitted by rules, where the rules prohibit more efficient in favour of less efficient means, and where such rules are accepted just because they make possible such activity." Bernard Suits "A game is a form of art in which participants, termed players, make decisions in order to manage resources through game tokens in the pursuit of a goal." (Greg Costikyan) According to this definition, some "games" that do not involve choices, such as Chutes and Ladders, Candy Land, and War are not technically games any more than a slot machine is. "A game is a form of play with goals and structure." (Kevin J. Maroney) "A game is a system in which players engage in an artificial conflict, defined by rules, that results in a quantifiable outcome." (Katie Salen and Eric Zimmerman) "A game is an activity among two or more independent decision-makers seeking to achieve their objectives in some limiting context." (Clark C. Abt) "At its most elementary level then we can define game as an exercise of voluntary control systems in which there is an opposition between forces, confined by a procedure and rules in order to produce a disequilibrial outcome." (Elliot Avedon and Brian Sutton-Smith) "To play a game is to engage in activity directed toward bringing about a specific state of affairs, using only means permitted by specific rules, where the means permitted by the rules are more limited in scope than they would be in the absence of the rules, and where the sole reason for accepting such limitation is to make possible such activity." (Bernard Suits) "When you strip away the genre differences and the technological complexities, all games share four defining traits: a goal, rules, a feedback system, and voluntary participation." (Jane McGonigal) Gameplay elements and classification Games can be characterized by "what the player does". This is often referred to as gameplay. Major key elements identified in this context are tools and rules that define the overall context of game. Tools Games are often classified by the components required to play them (e.g., miniatures, a ball, cards, a board and pieces, or a computer). In places where the use of leather is well-established, the ball has been a popular game piece throughout recorded history, resulting in a worldwide popularity of ball games such as rugby, basketball, soccer (football), cricket, tennis, and volleyball. Other tools are more idiosyncratic to a certain region. Many countries in Europe, for instance, have unique standard decks of playing cards. Other games, such as chess, may be traced primarily through the development and evolution of their game pieces. Many game tools are tokens, meant to represent other things. A token may be a pawn on a board, play money, or an intangible item such as a point scored. Games such as hide-and-seek or tag do not use any obvious tool; rather, their interactivity is defined by the environment. Games with the same or similar rules may have different gameplay if the environment is altered. For example, hide-and-seek in a school building differs from the same game in a park; an auto race can be radically different depending on the track or street course, even with the same cars. Rules and aims Games are often characterized by their tools and rules. While rules are subject to variations and changes, enough change in the rules usually results in a "new" game. For instance, baseball can be played with "real" baseballs or with wiffleballs. However, if the players decide to play with only three bases, they are arguably playing a different game. There are exceptions to this in that some games deliberately involve the changing of their own rules, but even then there are often immutable meta-rules. Rules generally determine the time-keeping system, the rights and responsibilities of the players, scoring techniques, preset boundaries, and each player's goals. The rules of a game may be distinguished from its aims. For most competitive games, the ultimate aim is winning: in this sense, checkmate is the aim of chess. Common win conditions are being the first to amass a certain quota of points or tokens (as in Settlers of Catan), having the greatest number of tokens at the end of the game (as in Monopoly), or some relationship of one's game tokens to those of one's opponent (as in chess's checkmate). There may also be intermediate aims, which are tasks that move a player toward winning. For instance, an intermediate aim in football is to score goals, because scoring goals will increase one's likelihood of winning the game, but is not alone sufficient to win the game. An aim identifies a sufficient condition for successful action, whereas the rule identifies a necessary condition for permissible action. For example, the aim of chess is to checkmate, but although it is expected that players will try to checkmate each other, it is not a rule of chess that a player must checkmate the other player whenever possible. Similarly, it is not a rule of football that a player must score a goal on a penalty; while it is expected the player will try, it is not required. While meeting the aims often requires a certain degree of skill and (in some cases) luck, following the rules of a game merely requires knowledge of the rules and some careful attempt to follow them; it rarely (if ever) requires luck or demanding skills. Skill, strategy, and chance A game's tools and rules will result in its requiring skill, strategy, luck, or a combination thereof and are classified accordingly. Games of skill include games of physical skill, such as wrestling, tug of war, hopscotch, target shooting, and games of mental skill, such as checkers and chess. Games of strategy include checkers, chess, Go, arimaa, and tic-tac-toe, and often require special equipment to play them. Games of chance include gambling games (blackjack, Mahjong, roulette, etc.), as well as snakes and ladders and rock, paper, scissors; most require equipment such as cards or dice. However, most games contain two or all three of these elements. For example, American football and baseball involve both physical skill and strategy, while tiddlywinks, poker, and Monopoly combine strategy and chance. Many card and board games combine all three; most trick-taking games involve mental skill, strategy, and an element of chance, as do many strategic board games such as Risk, Settlers of Catan, and Carcassonne. Single-player games Most games require multiple players. However, single-player games are unique in respect to the type of challenges a player faces. Unlike a game with multiple players competing with or against each other to reach the game's goal, a one-player game is a battle solely against an element of the environment (an artificial opponent), against one's own skills, against time, or against chance. Playing with a yo-yo or playing tennis against a wall is not generally recognized as playing a game due to the lack of any formidable opposition. Many games described as "single-player" may be termed actually puzzles or recreations. Multiplayer games A multiplayer game is a game of several players who may be independent opponents or teams. Games with many independent players are difficult to analyze formally using game theory as the players may form and switch coalitions. The term "game" in this context may mean either a true game played for entertainment or a competitive activity describable in principle by mathematical game theory. Game theory John Nash proved that games with several players have a stable solution provided that coalitions between players are disallowed. Nash won the Nobel Prize for economics for this important result which extended von Neumann's theory of zero-sum games. Nash's stable solution is known as the Nash equilibrium. If cooperation between players is allowed, then the game becomes more complex; many concepts have been developed to analyze such games. While these have had some partial success in the fields of economics, politics and conflict, no good general theory has yet been developed. In quantum game theory, it has been found that the introduction of quantum information into multiplayer games allows a new type of equilibrium strategy not found in traditional games. The entanglement of player's choices can have the effect of a contract by preventing players from profiting from what is known as betrayal. Types Games can take a variety of forms, from competitive sports to board games and video games. Sports Many sports require special equipment and dedicated playing fields, leading to the involvement of a community much larger than the group of players. A city or town may set aside such resources for the organization of sports leagues. Popular sports may have spectators who are entertained just by watching games. A community will often align itself with a local sports team that supposedly represents it (even if the team or most of its players only recently moved in); they often align themselves against their opponents or have traditional rivalries. The concept of fandom began with sports fans. Lawn games Lawn games are outdoor games that can be played on a lawn, an area of mowed grass (or alternately, on graded soil) generally smaller than a sports field (pitch). Variations of many games that are traditionally played on a sports field are marketed as "lawn games" for home use in a front or back yard. Common lawn games include horseshoes, sholf, croquet, bocce, and lawn bowls. Tabletop games A tabletop game is a game where the elements of play are confined to a small area and require little physical exertion, usually simply placing, picking up, and moving game pieces. Most of these games are played at a table around which the players are seated and on which the game's elements are located. However, many games falling into this category, particularly party games, are more free-form in their play and can involve physical activity such as mime. Still, these games do not require a large area in which to play them, large amounts of strength or stamina, or specialized equipment other than what comes in a box. Dexterity and coordination games This class of games includes any game in which the skill element involved relates to manual dexterity or hand-eye coordination but excludes the class of video games (see below). Games such as jacks, paper football, and Jenga require only very portable or improvised equipment and can be played on any flat level surface, while other examples, such as pinball, billiards, air hockey, foosball, and table hockey, require specialized tables or other self-contained modules on which the game is played. The advent of home video game systems largely replaced some of these, such as table hockey; however, air hockey, billiards, pinball and foosball remain popular fixtures in private and public game rooms. These games and others, as they require reflexes and coordination, are generally performed more poorly by intoxicated persons but are unlikely to result in injury because of this; as such, the games are popular as drinking games. In addition, dedicated drinking games such as quarters and beer pong also involve physical coordination and are popular for similar reasons. Board games Board games use as a central tool a board on which the players' status, resources, and progress are tracked using physical tokens. Many also involve dice or cards. Most games that simulate war are board games (though a large number of video games have been created to simulate strategic combat), and the board may be a map on which the players' tokens move. Virtually all board games involve "turn-based" play; one player contemplates and then makes a move, then the next player does the same, and a player can only act on their turn. This is opposed to "real-time" play as is found in some card games, most sports and most video games. Some games, such as chess and Go, are entirely deterministic, relying only on the strategy element for their interest. Such games are usually described as having "perfect information"; the only unknown is the exact thought processes of one's opponent, not the outcome of any unknown event inherent in the game (such as a card draw or die roll). Children's games, on the other hand, tend to be very luck-based, with games such as Candy Land and Chutes and Ladders having virtually no decisions to be made. By some definitions, such as that by Greg Costikyan, they are not games since there are no decisions to make which affect the outcome. Many other games involving a high degree of luck do not allow direct attacks between opponents; the random event simply determines a gain or loss in the standing of the current player within the game, which is independent of any other player; the "game" then is actually a "race" by definitions such as Crawford's. Most other board games combine strategy and luck factors; the game of backgammon requires players to decide the best strategic move based on the roll of two dice. Trivia games have a great deal of randomness based on the questions a person gets. German-style board games are notable for often having rather less of a luck factor than many board games. Board game groups include race games, roll-and-move games, abstract strategy games, word games, and wargames, as well as trivia and other elements. Some board games fall into multiple groups or incorporate elements of other genres: Cranium is one popular example, where players must succeed in each of four skills: artistry, live performance, trivia, and language. Card games Card games use a deck of cards as their central tool. These cards may be a standard Anglo-American (52-card) deck of playing cards (such as for bridge, poker, Rummy, etc.), a regional deck using 32, 36 or 40 cards and different suit signs (such as for the popular German game skat), a tarot deck of 78 cards (used in Europe to play a variety of trick-taking games collectively known as Tarot, Tarock or Tarocchi games), or a deck specific to the individual game (such as Set or 1000 Blank White Cards). Uno and Rook are examples of games that were originally played with a standard deck and have since been commercialized with customized decks. Some collectible card games such as Magic: The Gathering are played with a small selection of cards that have been collected or purchased individually from large available sets. Some board games include a deck of cards as a gameplay element, normally for randomization or to keep track of game progress. Conversely, some card games such as Cribbage use a board with movers, normally to keep score. The differentiation between the two genres in such cases depends on which element of the game is foremost in its play; a board game using cards for random actions can usually use some other method of randomization, while Cribbage can just as easily be scored on paper. These elements as used are simply the traditional and easiest methods to achieve their purpose. Dice games Dice games use a number of dice as their central element. Board games often use dice for a randomization element, and thus each roll of the dice has a profound impact on the outcome of the game, however dice games are differentiated in that the dice do not determine the success or failure of some other element of the game; they instead are the central indicator of the person's standing in the game. Popular dice games include Yahtzee, Farkle, Bunco, liar's dice/Perudo, and poker dice. As dice are, by their very nature, designed to produce random numbers, these games usually involve a high degree of luck, which can be directed to some extent by the player through more strategic elements of play and through tenets of probability theory. Such games are thus popular as gambling games; the game of craps is perhaps the most famous example, though liar's dice and poker dice were originally conceived of as gambling games. Domino and tile games Domino games are similar in many respects to card games, but the generic device is instead a set of tiles called dominoes, which traditionally each have two ends, each with a given number of dots, or "pips", and each combination of two possible end values as it appears on a tile is unique in the set. The games played with dominoes largely center around playing a domino from the player's "hand" onto the matching end of another domino, and the overall object could be to always be able to make a play, to make all open endpoints sum to a given number or multiple, or simply to play all dominoes from one's hand onto the board. Sets vary in the number of possible dots on one end, and thus of the number of combinations and pieces; the most common set historically is double-six, though in more recent times "extended" sets such as double-nine have been introduced to increase the number of dominoes available, which allows larger hands and more players in a game. Muggins, Mexican Train, and Chicken Foot are very popular domino games. Texas 42 is a domino game more similar in its play to a "trick-taking" card game. Variations of traditional dominoes abound: Triominoes are similar in theory but are triangular and thus have three values per tile. Similarly, a game known as Quad-Ominos uses four-sided tiles. Some other games use tiles in place of cards; Rummikub is a variant of the Rummy card game family that uses tiles numbered in ascending rank among four colors, very similar in makeup to a 2-deck "pack" of Anglo-American playing cards. Mahjong is another game very similar to Rummy that uses a set of tiles with card-like values and art. Lastly, some games use graphical tiles to form a board layout, on which other elements of the game are played. Settlers of Catan and Carcassonne are examples. In each, the "board" is made up of a series of tiles; in Settlers of Catan the starting layout is random but static, while in Carcassonne the game is played by "building" the board tile-by-tile. Hive, an abstract strategy game using tiles as moving pieces, has mechanical and strategic elements similar to chess, although it has no board; the pieces themselves both form the layout and can move within it. Pencil and paper games Pencil and paper games require little or no specialized equipment other than writing materials, though some such games have been commercialized as board games (Scrabble, for instance, is based on the idea of a crossword puzzle, and tic-tac-toe sets with a boxed grid and pieces are available commercially). These games vary widely, from games centering on a design being drawn such as Pictionary and "connect-the-dots" games like sprouts, to letter and word games such as Boggle and Scattergories, to solitaire and logic puzzle games such as Sudoku and crossword puzzles. Guessing games A guessing game has as its core a piece of information that one player knows, and the object is to coerce others into guessing that piece of information without actually divulging it in text or spoken word. Charades is probably the most well-known game of this type, and has spawned numerous commercial variants that involve differing rules on the type of communication to be given, such as Catch Phrase, Taboo, Pictionary, and similar. The genre also includes many game shows such as Win, Lose or Draw, Password and $25,000 Pyramid. Video games Video games are computer- or microprocessor-controlled games. Computers can create virtual spaces for a wide variety of game types. Some video games simulate conventional game objects like cards or dice, while others can simulate environs either grounded in reality or fantastical in design, each with its own set of rules or goals. A computer or video game uses one or more input devices, typically a button/joystick combination (on arcade games); a keyboard, mouse or trackball (computer games); or a controller or a motion sensitive tool (console games). More esoteric devices such as paddle controllers have also been used for input. There are many genres of video game; the first commercial video game, Pong, was a simple simulation of table tennis. As processing power increased, new genres such as adventure and action games were developed that involved a player guiding a character from a third person perspective through a series of obstacles. This "real-time" element cannot be easily reproduced by a board game, which is generally limited to "turn-based" strategy; this advantage allows video games to simulate situations such as combat more realistically. Additionally, the playing of a video game does not require the same physical skill, strength or danger as a real-world representation of the game, and can provide either very realistic, exaggerated or impossible physics, allowing for elements of a fantastical nature, games involving physical violence, or simulations of sports. Lastly, a computer can, with varying degrees of success, simulate one or more human opponents in traditional table games such as chess, leading to simulations of such games that can be played by a single player. In more open-ended video games, such as sandbox games, a virtual environment is provided in which the player may be free to do whatever they like within the confines of a particular game's universe. Sometimes, there is a lack of goals or opposition, which has stirred some debate on whether these should be considered "games" or "toys". (Crawford specifically mentions Will Wright's SimCity as an example of a toy.) Online games Online games have been part of culture from the very earliest days of networked and time-shared computers. Early commercial systems such as Plato were at least as widely famous for their games as for their strictly educational value. In 1958, Tennis for Two dominated Visitor's Day and drew attention to the oscilloscope at the Brookhaven National Laboratory; during the 1980s, Xerox PARC was known mainly for Maze War, which was offered as a hands-on demo to visitors. Modern online games are played using an Internet connection; some have dedicated client programs, while others require only a web browser. Some simpler browser games appeal to more casual game-playing demographic groups (notably older audiences) that otherwise play very few video games. Role-playing games Role-playing games, often abbreviated as RPGs, are a type of game in which the participants (usually) assume the roles of characters acting in a fictional setting. The original role playing games – or at least those explicitly marketed as such – are played with a handful of participants, usually face-to-face, and keep track of the developing fiction with pen and paper. Together, the players may collaborate on a story involving those characters; create, develop, and "explore" the setting; or vicariously experience an adventure outside the bounds of everyday life. Pen-and-paper role-playing games include, for example, Dungeons & Dragons and GURPS. The term role-playing game has also been appropriated by the video game industry to describe a genre of video games. These may be single-player games where one player experiences a programmed environment and story, or they may allow players to interact through the internet. The experience is usually quite different from traditional role-playing games. Single-player games include Final Fantasy, Fable, The Elder Scrolls, and Mass Effect. Online multi-player games, often referred to as massively multiplayer online role playing games, or MMORPGs, include RuneScape, EverQuest 2, Guild Wars, MapleStory, Anarchy Online, and Dofus. As of 2009, the most successful MMORPG has been World of Warcraft, which controls the vast majority of the market. Business games Business games can take a variety of forms, from interactive board games to interactive games involving different props (balls, ropes, hoops, etc.) and different kinds of activities. The purpose of these games is to link to some aspect of organizational performance and to generate discussions about business improvement. Many business games focus on organizational behaviors. Some of these are computer simulations while others are simple designs for play and debriefing. Team building is a common focus of such activities. Simulation The term "game" can include simulation or re-enactment of various activities or use in "real life" for various purposes: e.g., training, analysis, prediction. Well-known examples are war games and role-playing. The root of this meaning may originate in the human prehistory of games deduced by anthropology from observing primitive cultures, in which children's games mimic the activities of adults to a significant degree: hunting, warring, nursing, etc. These kinds of games are preserved in modern times. See also Game club – Association of people united by a common interest or goalPages displaying short descriptions of redirect targets Game mechanics – Construct, rule, or method designed for interaction with a game's state Gamer – Someone who plays games Girls' games and toys – Subset of toy and games that appeal to female childrenPages displaying short descriptions of redirect targets History of games Learning through play – Concept in education and psychology List of games – Overview of and topical guide to gamesPages displaying short descriptions of redirect targets Ludeme – Basic unit of play Ludibrium – Latin word Ludology – Study of games and the act of playing them Ludomania – Repetitive gambling despite demonstrable harm and adverse consequencesPages displaying short descriptions of redirect targets Mobile game – Video game played on a mobile device N-player game – Game which can have any number of players Personal computer game – Electronic game played on a personal computerPages displaying short descriptions of redirect targets References Further reading Avedon, Elliot; Sutton-Smith, Brian. The Study of Games (Philadelphia: Wiley, 1971), reprinted Krieger, 1979. ISBN 0-89874-045-2.

Food is any substance consumed by an organism for nutritional support. Food is usually of plant, animal, or fungal origin and contains essential nutrients such as carbohydrates, fats, proteins, vitamins, or minerals. The substance is ingested by an organism and assimilated by the organism's cells to provide energy, maintain life, or support growth. Different species of animals have different feeding behaviours that satisfy the needs of their metabolisms and have evolved to fill specific ecological niches within specific geographical contexts. Omnivorous humans are highly adaptable and have adapted to obtaining food in many different ecosystems. Humans generally use cooking to prepare food for consumption. The majority of the food energy required is supplied by the industrial food industry, which produces food through intensive agriculture and distributes it through complex food processing and food distribution systems. This system of conventional agriculture relies heavily on fossil fuels, which means that the food and agricultural systems are one of the major contributors to climate change, accounting for as much as 37% of total greenhouse gas emissions. The food system has a significant impact on a wide range of other social and political issues, including sustainability, biological diversity, economics, population growth, water supply, and food security. Food safety and security are monitored by international agencies, like the International Association for Food Protection, the World Resources Institute, the World Food Programme, the Food and Agriculture Organization, and the International Food Information Council. Definition and classification Food is any substance consumed to provide nutritional support and energy to an organism. It can be raw, processed, or formulated and is consumed orally by animals for growth, health, or pleasure. Food is mainly composed of water, lipids, proteins, and carbohydrates. Other organic substances (e.g., vitamins) and minerals (e.g., salts) can also be found in food. Plants, algae, and some microorganisms use photosynthesis to make some of their own nutrients. Water is found in nearly all foods and has been defined as food by itself. Water has no food energy, and fibers have low energy densities, or food energy relative to volume, some providing none, while fat is the most energy-dense component. Some inorganic substances are also essential for plant and animal functioning. Human food can be classified in various ways, either by related content or by how it is processed. The number and composition of food groups can vary. Most systems include four basic groups described by their origins and relative nutritional functions: vegetables and fruit, cereals and bread, dairy, and meat. Studies that look into diet quality group food into whole grains, refined grains, vegetables, fruits, nuts, legumes, eggs, dairy products, fish, red meat, processed meat, and sugar-sweetened beverages. The Food and Agriculture Organization and World Health Organization use a system with eighteen or nineteen food classifications, including: cereals and their products; roots, tubers, plantains and their products; pulses, seeds and nuts and their products; milk and milk products; eggs and their products; fish, shellfish and their products; meat and meat products; insects, grubs and their products; vegetables and their products; fruits and their products; fats and oils; sweets and sugars; spices and condiments; beverages; foods for particular nutritional uses; food additives; composite dishes; and savory snacks. (The source claims nineteen but lists eighteen, numbered 1–15 and 17–19.) Food sources In a given ecosystem, food forms a web of interlocking chains with primary producers at the bottom and apex predators at the top. Other aspects of the web include detrovores (that eat detritis) and decomposers (that break down dead organisms). Primary producers include algae, plants, bacteria and protists that acquire their energy from sunlight. Primary consumers are the herbivores that consume the plants, and secondary consumers are the carnivores that consume those herbivores. Some organisms, including most mammals and birds, have diets consisting of both animals and plants, and are considered omnivores. The chain ends with the apex predators, the animals that have no known predators in its ecosystem. Humans are considered apex predators. Humans are omnivores, finding sustenance in vegetables, fruits, cooked meat, milk, eggs, mushrooms and seaweed. Cereal grain is a staple food that provides more food energy worldwide than any other type of crop. Corn (maize), wheat, and rice account for 87% of all grain production worldwide. Just over half of the world's crops are used to feed humans (55 percent), with 36 percent grown as animal feed and 9 percent for biofuels. Fungi and bacteria are also used in the preparation of fermented foods like bread, wine, cheese and yogurt. Photosynthesis During photosynthesis, energy from the sun is absorbed and used to transform water and carbon dioxide in the air or soil into oxygen and glucose. The oxygen is then released, and the glucose stored as an energy reserve. Photosynthetic plants, algae and certain bacteria often represent the lowest point of the food chains, making photosynthesis the primary source of energy and food for nearly all life on earth. Plants also absorb important nutrients and minerals from the air, natural waters, and soil. Carbon, oxygen and hydrogen are absorbed from the air or water and are the basic nutrients needed for plant survival. The three main nutrients absorbed from the soil for plant growth are nitrogen, phosphorus and potassium, with other important nutrients including calcium, sulfur, magnesium, iron boron, chlorine, manganese, zinc, copper molybdenum and nickel. Microorganisms Bacteria and other microorganisms also form the lower rungs of the food chain. They obtain their energy from photosynthesis or by breaking down dead organisms, waste or chemical compounds. Some form symbiotic relationships with other organisms to obtain their nutrients. Bacteria provide a source of food for protozoa, who in turn provide a source of food for other organisms such as small invertebrates. Other organisms that feed on bacteria include nematodes, fan worms, shellfish and a species of snail. In the marine environment, plankton (which includes bacteria, archaea, algae, protozoa and microscopic fungi) provide a crucial source of food to many small and large aquatic organisms. Without bacteria, life would scarcely exist because bacteria convert atmospheric nitrogen into nutritious ammonia. Ammonia is the precursor to proteins, nucleic acids, and most vitamins. Since the advent of the industrial process for nitrogen fixation, the Haber-Bosch Process, the majority of ammonia in the world is human-made. Plants Plants as a food source are divided into seeds, fruits, vegetables, legumes, grains and nuts. Where plants fall within these categories can vary, with botanically described fruits such as the tomato, squash, pepper and eggplant or seeds like peas commonly considered vegetables. Food is a fruit if the part eaten is derived from the reproductive tissue, so seeds, nuts and grains are technically fruit. From a culinary perspective, fruits are generally considered the remains of botanically described fruits after grains, nuts, seeds and fruits used as vegetables are removed. Grains can be defined as seeds that humans eat or harvest, with cereal grains (oats, wheat, rice, corn, barley, rye, sorghum and millet) belonging to the Poaceae (grass) family and pulses coming from the Fabaceae (legume) family. Whole grains are foods that contain all the elements of the original seed (bran, germ, and endosperm). Nuts are dry fruits, distinguishable by their woody shell. Fleshy fruits (distinguishable from dry fruits like grain, seeds and nuts) can be further classified as stone fruits (cherries and peaches), pome fruits (apples, pears), berries (blackberry, strawberry), citrus (oranges, lemon), melons (watermelon, cantaloupe), Mediterranean fruits (grapes, fig), tropical fruits (banana, pineapple). Vegetables refer to any other part of the plant that can be eaten, including roots, stems, leaves, flowers, bark or the entire plant itself. These include root vegetables (potatoes and carrots), bulbs (onion family), flowers (cauliflower and broccoli), leaf vegetables (spinach and lettuce) and stem vegetables (celery and asparagus). The carbohydrate, protein and lipid content of plants is highly variable. Carbohydrates are mainly in the form of starch, fructose, glucose and other sugars. Most vitamins are found from plant sources, with the exception of vitamin D and vitamin B12. Minerals can also be plentiful or not. Fruit can consist of up to 90% water, contain high levels of simple sugars that contribute to their sweet taste, and have a high vitamin C content. Compared to fleshy fruit (excepting Bananas) vegetables are high in starch, potassium, dietary fiber, folate and vitamins and low in fat and calories. Grains are more starch based and nuts have a high protein, fiber, vitamin E and B content. Seeds are a good source of food for animals because they are abundant and contain fiber and healthful fats, such as omega-3 fats. Complicated chemical interactions can enhance or depress bioavailability of certain nutrients. Phytates can prevent the release of some sugars and vitamins. Animals that only eat plants are called herbivores, with those that mostly just eat fruits known as frugivores, while leaf and shoot eaters are folivores (pandas) and wood eaters termed xylophages (termites). Frugivores include a diverse range of species from annelids to elephants, chimpanzees and many birds. About 182 fish consume seeds or fruit. Animals (domesticated and wild) use as many types of grasses that have adapted to different locations as their main source of nutrients. Humans eat thousands of plant species; there may be as many as 75,000 edible species of angiosperms, of which perhaps 7,000 are often eaten. Plants can be processed into breads, pasta, cereals, juices and jams or raw ingredients such as sugar, herbs, spices and oils can be extracted. Oilseeds are pressed to produce rich oils – ⁣sunflower, flaxseed, rapeseed (including canola oil) and sesame. Many plants and animals have coevolved in such a way that the fruit is a good source of nutrition for the animal, who then excretes the seeds some distance away, allowing greater dispersal. Even seed predation can be mutually beneficial, as some seeds can survive the digestion process. Insects are major eaters of seeds, with ants being the only real seed dispersers. Birds, although being major dispersers, only rarely eat seeds as a source of food and can be identified by their thick beak that is used to crack open the seed coat. Mammals eat a more diverse range of seeds, as they are able to crush harder and larger seeds with their teeth. Animals Animals are used as food either directly or indirectly. This includes meat, eggs, shellfish and dairy products like milk and cheese. They are an important source of protein and are considered complete proteins for human consumption as they contain all the essential amino acids that the human body needs. One 4-ounce (110 g) steak, chicken breast or pork chop contains about 30 grams of protein. One large egg has 7 grams of protein. A 4-ounce (110 g) serving of cheese has about 15 grams of protein. And 1 cup (~240 mL) of milk has about 8 grams of protein. Other nutrients found in animal products include calories, fat, essential vitamins (including B12) and minerals (including zinc, iron, calcium, magnesium). Food products produced by animals include milk produced by mammary glands, which in many cultures is drunk or processed into dairy products (cheese, butter, etc.). Eggs laid by birds and other animals are eaten and bees produce honey, a reduced nectar from flowers that is used as a popular sweetener in many cultures. Some cultures consume blood, such as in blood sausage, as a thickener for sauces, or in a cured, salted form for times of food scarcity, and others use blood in stews such as jugged hare. Taste Animals, specifically humans, typically have five different types of taste sense: sweet, sour, salty, bitter, and umami. The differing tastes are important for distinguishing between foods that are nutritionally beneficial and those which may contain toxins. As animals have evolved, the tastes that provide the most energy are the most pleasant to eat while others are not enjoyable, although humans in particular can acquire a preference for some substances which are initially unenjoyable. Water, while important for survival, has no taste. Sweetness is almost always caused by a type of simple sugar, such as glucose or fructose, or disaccharides, such as sucrose, a molecule combining glucose and fructose. Sourness is caused by acids, such as vinegar. Sour foods include citrus, especially lemons and limes. Sour is evolutionarily significant as it can signal a food that may have gone rancid due to bacteria. Saltiness is the taste of alkali metal ions such as sodium and potassium. It is found in almost every food in low to moderate proportions and enhances flavor. Bitter taste is a sensation considered unpleasant, caused by foods such as unsweetened dark chocolate, caffeine, lemon rind, and some types of fruit. Umami, commonly described as savory, is a marker of proteins and characteristic of broths and cooked meats. Foods that have a strong umami flavor include cheese, meat and mushrooms. While most animals’ taste buds are located in their mouths, some insects’ taste receptors are located on their legs, and some fish have taste buds along their entire bodies. Dogs, cats, and birds have relatively few taste buds (a chicken has about 30),, and an adult human has between 2000 and 4000, while a catfish can have more than a million. Herbivores generally have more than carnivores as they need to tell which plants may be poisonous. Not all mammals share the same tastes: some rodents can taste starch, cats cannot taste sweetness, and several carnivores (including hyenas, dolphins, and sea lions) have lost the ability to sense up to four of the five taste modalities found in humans. Digestion Food is broken into nutrient components through digestive processes. Proper digestion consists of mechanical processes (chewing, peristalsis) and chemical processes (the actions of digestive enzymes and microorganisms). The digestive systems of herbivores and carnivores are very different as plant matter is harder to digest. Carnivores’ mouths are designed for tearing and biting compared to the grinding action found in herbivores. Herbivores, however, have comparatively longer digestive tracts and larger stomachs to aid in digesting the cellulose in plants. Food safety According to the World Health Organization (WHO), about 600 million people worldwide get sick, and 420,000 die each year, from eating contaminated food. Diarrhea is the most common illness caused by consuming contaminated food, with about 550 million cases and 230,000 deaths from diarrhea each year. Children under five years of age account for 40% of the burden of foodborne illness, with 125,000 deaths each year. A 2003 World Health Organization (WHO) report concluded that about 30% of reported food poisoning outbreaks in the WHO European Region occur in private homes. According to the WHO and CDC, in the US alone, annually, there are 76 million cases of foodborne illness leading to 325,000 hospitalizations and 5,000 deaths. In Vietnam, from 2011 to 2016, on average, there were 668,673 cases of foodborne illness and 21 deaths each year. In addition, during this period, 1,007 food poisoning outbreaks with 30,395 cases of food poisoning were reported. See also Food pairing List of food and drink monuments References Further reading Collingham, E. M. (2011). The Taste of War: World War Two and the Battle for Food Katz, Solomon (2003). The Encyclopedia of Food and Culture, Scribner Mobbs, Michael (2012). Sustainable Food Sydney: NewSouth Publishing, ISBN 978-1-920705-54-1 Nestle, Marion (2007). Food Politics: How the Food Industry Influences Nutrition and Health, University Presses of California, revised and expanded edition, ISBN 0-520-25403-1 The Future of Food (2015). A panel discussion at the 2015 Digital Life Design (DLD) Annual Conference. "How can we grow and enjoy food, closer to home, further into the future? MIT Media Lab's Kevin Slavin hosts a conversation with food artist, educator, and entrepreneur Emilie Baltz, professor Caleb Harper from MIT Media Lab's CityFarm project, the Barbarian Group's Benjamin Palmer, and Andras Forgacs, the co-founder and CEO of Modern Meadow, who is growing 'victimless' meat in a lab. The discussion addresses issues of sustainable urban farming, ecosystems, technology, food supply chains and their broad environmental and humanitarian implications, and how these changes in food production may change what people may find delicious ... and the other way around." Posted on the official YouTube Channel of DLD External links Media related to food at Wikimedia Commons Food travel guide from Wikivoyage Works related to Food at Wikisource The dictionary definition of food at Wiktionary Official website of Food Timeline Food, BBC Radio 4 discussion with Rebecca Spang, Ivan Day and Felipe Fernandez-Armesto (In Our Time, 27 December 2001)

Travel is the movement of people between distant geographical locations. Travel can be done by foot, bicycle, automobile, train, boat, bus, airplane, ship or other means, with or without luggage, and can be one way or round trip. Travel can also include relatively short stays between successive movements, as in the case of tourism. Etymology The origin of the word "travel" is most likely lost to history. The term "travel" may originate from the Old French word travail, which means 'work'. According to the Merriam-Webster dictionary, the first known use of the word travel was in the 14th century. It also states that the word comes from Middle English travailen, travelen (which means to torment, labor, strive, journey) and earlier from Old French travailler (which means to work strenuously, toil). In English, people still occasionally use the words travail, which means struggle. According to Simon Winchester in his book The Best Travelers' Tales (2004), the words travel and travail both share an even more ancient root: a Roman instrument of torture called the tripalium (in Latin it means "three stakes", as in to impale). This link may reflect the extreme difficulty of travel in ancient times. Travel in modern times may or may not be much easier, depending upon the destination. Travel to Mount Everest, the Amazon rainforest, extreme tourism, and adventure travel are more difficult forms of travel. Travel can also be more difficult depending on the method of travel, such as by bus, cruise ship, or even by bullock cart. Purpose and motivation Reasons for traveling include recreation, holidays, rejuvenation, tourism or vacationing, research travel, the gathering of information, visiting people, volunteer travel for charity, migration to begin life somewhere else, religious pilgrimages and mission trips, business travel, trade, commuting, obtaining health care, waging or fleeing war, for the enjoyment of traveling, or other reasons. Travelers may use human-powered transport such as walking or bicycling; or vehicles, such as public transport, automobiles, trains, ferries, boats, cruise ships and airplanes. Motives for travel include: Pleasure Relaxation Discovery and exploration Adventure Intercultural communications Taking personal time for building interpersonal relationships. Avoiding stress Forming memories Cultural experiences Volunteering Festivals and events History Travel dates back to antiquity where wealthy Greeks and Romans would travel for leisure to their summer homes and villas in cities such as Pompeii and Baiae. While early travel tended to be slower, more dangerous, and more dominated by trade and migration, cultural and technological advances over many years have tended to mean that travel has become easier and more accessible. Humankind has come a long way in transportation since Christopher Columbus sailed to the New World from Spain in 1492, an expedition which took over 10 weeks to arrive at the final destination; to the 21st century when aircraft allows travel from Spain to the United States overnight. Travel in the Middle Ages offered hardships and challenges, though it was important to the economy and to society. The wholesale sector depended (for example) on merchants dealing with/through caravans or sea-voyagers, end-user retailing often demanded the services of many itinerant peddlers wandering from village to hamlet, gyrovagues (wandering monks) and wandering friars brought theology and pastoral support to neglected areas, traveling minstrels toured, and armies ranged far and wide in various crusades and in sundry other wars. Pilgrimages were common in both the European and Islamic world and involved streams of travelers both locally and internationally. In the late 16th century, it became fashionable for young European aristocrats and wealthy upper-class men to travel to significant European cities as part of their education in the arts and literature. This was known as the Grand Tour, and included cities such as London, Paris, Venice, Florence, and Rome. However, the French Revolution brought with it the end of the Grand Tour. Travel by water often provided more comfort and speed than land-travel, at least until the advent of a network of railways in the 19th century. Travel for the purpose of tourism is reported to have started around this time when people began to travel for fun as travel was no longer a hard and challenging task. This was capitalized on by people like Thomas Cook selling tourism packages where trains and hotels were booked together. Airships and airplanes took over much of the role of long-distance surface travel in the 20th century, notably after the Second World War where there was a surplus of both aircraft and pilots. Air travel has become so ubiquitous in the 21st century that one woman, Alexis Alford, visited all 196 countries before the age of 21. Geographic types Travel may be local, regional, national (domestic) or international. In some countries, non-local internal travel may require an internal passport, while international travel typically requires a passport and visa. Tours are a common type of travel. Examples of travel tours are expedition cruises, small group tours, and river cruises. Safety Authorities emphasize the importance of taking precautions to ensure travel safety. When traveling abroad, the odds favor a safe and incident-free trip, however, travelers can be subject to difficulties, crime and violence. Some safety considerations include being aware of one's surroundings, avoiding being the target of a crime, leaving copies of one's passport and itinerary information with trusted people, obtaining medical insurance valid in the country being visited and registering with one's national embassy when arriving in a foreign country. Many countries do not recognize drivers' licenses from other countries; however most countries accept international driving permits. Automobile insurance policies issued in one's own country are often invalid in foreign countries, and it is often a requirement to obtain temporary auto insurance valid in the country being visited. It is also advisable to become oriented with the driving rules and regulations of destination countries. Wearing a seat belt is highly advisable for safety reasons; many countries have penalties for violating seatbelt laws. There are three main statistics which may be used to compare the safety of various forms of travel (based on a Department of the Environment, Transport and the Regions survey in October 2000): See also References External links "Travel". Merriam-Webster.com Dictionary. Merriam-Webster.

Fashion is a term used interchangeably to describe the creation of clothing, footwear, accessories, cosmetics, and jewellery of different cultural aesthetics and their mix and match into outfits that depict distinctive ways of dressing (styles and trends) as signifiers of social status, self-expression, and group belonging. As a multifaceted term, fashion describes an industry, designs, aesthetics, and trends. The term 'fashion' originates from the Latin word 'Facere,' which means 'to make,' and describes the manufacturing, mixing, and wearing of outfits adorned with specific cultural aesthetics, patterns, motifs, shapes, and cuts, allowing people to showcase their group belongings, values, meanings, beliefs, and ways of life. Given the rise in mass production of commodities and clothing at lower prices and global reach, reducing fashion's environmental impact and improving sustainability has become an urgent issue among politicians, brands, and consumers. Definitions The French word mode, meaning "fashion", dates as far back as 1482, while the English word denoting something "in style" dates only to the 16th century. Other words exist related to concepts of style and appeal that precede mode. In the 12th and 13th century Old French the concept of elegance begins to appear in the context of aristocratic preferences to enhance beauty and display refinement, and cointerie, the idea of making oneself more attractive to others by style or artifice in grooming and dress, appears in a 13th-century poem by Guillaume de Lorris advising men that "handsome clothes and handsome accessories improve a man a great deal". Fashion scholar Susan B. Kaiser states that everyone is "forced to appear", unmediated before others. Everyone is evaluated by their attire, and evaluation includes the consideration of colors, materials, silhouette, and how garments appear on the body. Garments identical in style and material also appear different depending on the wearer's body shape, or whether the garment has been washed, folded, mended, or is new. Fashion is defined in a number of different ways, and its application can be sometimes unclear. Though the term fashion connotes difference, as in "the new fashions of the season", it can also connote sameness, for example in reference to "the fashions of the 1960s", implying a general uniformity. Fashion can signify the latest trends, but may often reference fashions of a previous era, leading to the reappearance of fashions from a different time period. While what is fashionable can be defined by a relatively insular, esteemed and often rich aesthetic elite who make a look exclusive, such as fashion houses and haute couturiers, this 'look' is often designed by pulling references from subcultures and social groups who are not considered elite, and are thus excluded from making the distinction of what is fashion themselves. Whereas a trend often connotes a peculiar aesthetic expression, often lasting shorter than a season and being identifiable by visual extremes, fashion is a distinctive and industry-supported expression traditionally tied to the fashion season and collections. Style is an expression that lasts over many seasons and is often connected to cultural movements and social markers, symbols, class, and culture (such as Baroque and Rococo). According to sociologist Pierre Bourdieu, fashion connotes "the latest difference." Even though the terms fashion, clothing and costume are often used together, fashion differs from both. Clothing describes the material and the technical garment, devoid of any social meaning or connections; costume has come to mean fancy dress or masquerade wear. Fashion, by contrast, describes the social and temporal system that influences and "activates" dress as a social signifier in a certain time and context. Philosopher Giorgio Agamben connects fashion to the qualitative Ancient Greek concept of kairos, meaning "the right, critical, or opportune moment", and clothing to the quantitative concept of chronos, the personification of chronological or sequential time. While some exclusive brands may claim the label haute couture, in France, the term is technically limited to members of the Chambre Syndicale de la Haute Couture in Paris. Haute couture is more aspirational; inspired by art and culture, and in most cases, reserved for the economic elite. However, New York's fashion calendar hosts Couture Fashion Week, which strives for a more equitable and inclusive mission. Fashion is also a source of art, allowing people to display their unique tastes, sensibilities, and styles. Different fashion designers are influenced by outside stimuli and reflect this inspiration in their work. For example, Gucci's 'stained green' jeans may look like a grass stain, but to others, they display purity, freshness, and summer. Fashion is unique, self-fulfilling and may be a key part of someone's identity. Similarly to art, the aims of a person's choices in fashion are not necessarily to be liked by everyone, but instead to be an expression of personal taste. A person's personal style functions as a "societal formation always combining two opposite principles. It is a socially acceptable and secure way to distinguish oneself from others and, at the same time, it satisfies the individual's need for social adaptation and imitation." While philosopher Immanuel Kant believed that fashion "has nothing to do with genuine judgements of taste", and was instead "a case of unreflected and 'blind' imitation", sociologist Georg Simmel thought of fashion as something that "helped overcome the distance between an individual and his society". American sociologist Diana Crane also mentioned in her book that fashion is closely intertwined with personal and group identity, serving as a means of expressing cultural, social, and political affiliations. History of fashion Changes in clothing often took place at times of economic or social change, as occurred in ancient Rome and the medieval Caliphate, followed by a long period without significant changes. In eighth-century Moorish Spain, the musician Ziryab introduced to Córdoba sophisticated clothing styles based on seasonal and daily fashions from his native Baghdad, modified by his inspiration. Similar changes in fashion occurred in the 11th century in the Middle East following the arrival of the Turks, who introduced clothing styles from Central Asia and the Far East. Alleged Western distinctiveness Early Western travellers who visited India, Persia, Turkey, or China, would frequently remark on the absence of change in fashion in those countries. In 1609, the secretary of the Japanese shōgun bragged inaccurately to a Spanish visitor that Japanese clothing had not changed in over a thousand years. However, these conceptions of non-Western clothing undergoing little, if any, evolution are generally held to be untrue; for instance, there is considerable evidence in Ming China of rapidly changing fashions in Chinese clothing. In imperial China, clothing were not only an embodiment of freedom and comfort or used to cover the body or protect against the cold or used for decorative purposes; it was also regulated by strong sumptuary laws which was based on strict social hierarchy system and the ritual system of the Chinese society. It was expected for people to be dressed accordingly to their gender, social status and occupation; the Chinese clothing system had cleared evolution and varied in appearance in each period of history. However, ancient Chinese fashion, like in other cultures, was an indicator of the socioeconomic conditions of its population; for Confucian scholars, however, changing fashion was often associated with social disorder which was brought by rapid commercialization. Clothing which experienced fast changing fashion in ancient China was recorded in ancient Chinese texts, where it was sometimes referred as shiyang, "contemporary-styles", and was associated with the concept of fuyao, "outrageous dress", which typically holds a negative connotation. Similar changes in clothing can be seen in Japanese clothing between the Genroku period and the later centuries of the Edo period (1603–1867), during which a time clothing trends switched from flashy and expensive displays of wealth to subdued and subverted ones. The myth on the lack of fashion in what was considered the Orient was related to Western Imperialism also often accompanied Orientalism, and European imperialism was especially at its highest in the 19th century. In the 19th century time, Europeans described China in binary opposition to Europe, describing China as "lacking in fashion" among many other things, while Europeans deliberately placed themselves in a superior position when they would compare themselves to the Chinese as well as to other countries in Asia:Latent orientalism is an unconscious, untouchable certainty about what the Orient is, static and unanimous, separate, eccentric, backward, silently different, sensual, and passive. It has a tendency towards despotism and away from progress. [...] Its progress and value are judged in comparison to the West, so it is the Other. Many rigorous scholars [...] saw the Orient as a locale requiring Western attention, reconstruction, even redemption.Similar ideas were also applied to other countries in the East Asia, in India, and Middle East, where the perceived lack of fashion were associated with offensive remarks on the Asian social and political systems:I confess that the unchanging fashions of the Turks and other Eastern peoples do not attract me. It seems that their fashions tend to preserve their stupid despotism. Africa Additionally, there is a long history of fashion in West Africa. Cloth was used as a form of currency in trade with the Portuguese and Dutch as early as the 16th century, and locally produced cloth and cheaper European imports were assembled into new styles to accommodate the growing elite class of West Africans and resident gold and slave traders. There was an exceptionally strong tradition of weaving in the Oyo Empire, and the areas inhabited by the Igbo people. Fashion in the Western world The beginning in Europe of continual and accelerating change in clothing styles can be fairly reliably dated to late medieval times. Historians, including James Laver and Fernand Braudel, date the start of Western fashion in clothing to the middle of the 14th century, though they tend to rely heavily on contemporary imagery, as illuminated manuscripts were not common before the 14th century. The most dramatic early change in fashion was a sudden drastic shortening and tightening of the male over-garment from calf-length to barely covering the buttocks, sometimes accompanied with stuffing in the chest to make it look bigger. This created the distinctive Western outline of a tailored top worn over leggings or trousers. The pace of change accelerated considerably in the following century, and women's and men's fashion, especially in the dressing and adorning of the hair, became equally complex. Art historians are, therefore, able to use fashion with confidence and precision to date images, often to within five years, particularly in the case of images from the 15th century. Initially, changes in fashion led to a fragmentation across the upper classes of Europe of what had previously been a very similar style of dressing and the subsequent development of distinctive national styles. These national styles remained very different until a counter-movement in the 17th to 18th centuries imposed similar styles once again, mostly originating from Ancien Régime France. Though the rich usually led fashion, the increasing affluence of early modern Europe led to the bourgeoisie and even peasants following trends at a distance, but still uncomfortably close for the elites – a factor that Fernand Braudel regards as one of the main motors of changing fashion. In the 16th century, national differences were at their most pronounced. Ten 16th century portraits of German or Italian gentlemen may show ten entirely different hats. Albrecht Dürer illustrated the differences in his actual (or composite) contrast of Nuremberg and Venetian fashions at the close of the 15th century (illustration, right). The "Spanish style" of the late 16th century began the move back to synchronicity among upper-class Europeans, and after a struggle in the mid-17th century, French styles decisively took over leadership, a process completed in the 18th century. Though different textile colors and patterns changed from year to year, the cut of a gentleman's coat and the length of his waistcoat, or the pattern to which a lady's dress was cut, changed more slowly. Men's fashions were primarily derived from military models, and changes in a European male silhouette were galvanized in theaters of European war where gentleman officers had opportunities to make notes of different styles such as the "Steinkirk" cravat or necktie. Both parties wore shirts under their clothing, the cut and style of which had little cause to change over a number of centuries. Though there had been distribution of dressed dolls from France since the 16th century and Abraham Bosse had produced engravings of fashion in the 1620s, the pace of change picked up in the 1780s with increased publication of French engravings illustrating the latest Paris styles. By 1800, all Western Europeans were dressing alike (or thought they were); local variation became first a sign of provincial culture and later a badge of the conservative peasant. Although tailors and dressmakers were no doubt responsible for many innovations, and the textile industry indeed led many trends, the history of fashion design is generally understood to date from 1858 when the English-born Charles Frederick Worth opened the first authentic haute couture house in Paris. The Haute house was the name established by the government for the fashion houses that met the standards of the industry. These fashion houses continue to adhere to standards such as keeping at least twenty employees engaged in making the clothes, showing two collections per year at fashion shows, and presenting a certain number of patterns to costumers. Since then, the idea of the fashion designer as a celebrity in their own right has become increasingly dominant. Although fashion can be feminine or masculine, additional trends are androgynous. The idea of unisex dressing originated in the 1960s, when designers such as Pierre Cardin and Rudi Gernreich created garments, such as stretch jersey tunics or leggings, meant to be worn by both males and females. The impact of unisex wearability expanded more broadly to encompass various themes in fashion, including androgyny, mass-market retail, and conceptual clothing. The fashion trends of the 1970s, such as sheepskin jackets, flight jackets, duffel coats, and unstructured clothing, influenced men to attend social gatherings without a dinner jacket and to accessorize in new ways. Some men's styles blended the sensuality and expressiveness, and the growing gay-rights movement and an emphasis on youth allowed for a new freedom to experiment with style and with fabrics such as wool crepe, which had previously been associated with women's attire. The four major current fashion capitals are acknowledged to be New York City (Manhattan), Paris, Milan, and London, which are all headquarters to the most significant fashion companies and are renowned for their major influence on global fashion. Fashion weeks are held in these cities, where designers exhibit their new clothing collections to audiences. A study demonstrated that general proximity to New York's Garment District was important to participate in the American fashion ecosystem. Haute couture has now largely been subsidized by the sale of ready-to-wear collections and perfume using the same branding. Modern Westerners have a vast number of choices in the selection of their clothes. What a person chooses to wear can reflect their personality or interests. When people who have high cultural status start to wear new or different styles, they may inspire a new fashion trend. People who like or respect these people are influenced by their style and begin wearing similarly styled clothes. Fashions may vary considerably within a society according to age, social class, generation, occupation, and geography, and may also vary over time. The terms fashionista and fashion victim refer to someone who slavishly follows current fashions. Asia In the early 2000s, Asian fashion influences became increasingly significant in local and global markets. Countries such as China, Japan, India, and Pakistan have traditionally had large textile industries with a number of rich traditions; though these were often drawn upon by Western designers, Asian clothing styles gained considerable influence in the early- to mid-2000s. China Chinese fashion remained constantly changing over the centuries. In China, throughout the Tang Dynasty (618–907), women wore extravagant attire to demonstrate prosperity. Mongol men of the Yuan Dynasty (1279–1368) wore loose robes; horsemen sported shorter robes, trousers, and boots to provide ease when horseback riding. The leaders of the Qing Dynasty (1644–1911) maintained Manchu dress, while establishing new garments for officials; while foot binding—originally introduced in the 10th century—was not preserved, women of this era were expected to wear particular heels that pushed them to take on a ladylike walk. Then, in the 1920s, qipao was in vogue and the style consisted of stand collars, trumpet sleeves, straight silhouettes and short side slits. Since then, designers started to move into Western fashion like fur coats and cloaks and body-hugging dresses with long side slits as qipao became more popular. In the 1950s and 60s, ‘Lenin coats’ with double lines of buttons, slanting pockets and a belt came into vogue among Chinese men. India In India, it has been common for followers of different religions to wear corresponding pieces of clothing. During the 15th century, Muslim and Hindu women wore notably different articles of clothing. This is also seen in many other Eastern world countries. In the Victorian era, most women did not wear blouses under their saris, which did not suit the Victorian society; however, British and Indian fashion would be influenced by each other in following decades. In the 1920s, the nationalists adopted Khadi cloth as a symbol of resistance; here, Gandhi became the face of the resistance which made people spin, weave, and wear their Khadi. Today, the salwaar-kameez is recognized as the national dress of India. Japan For Japan, the people during the Meiji period (1868–1912) widely incorporated Western styles into Japanese fashion, which is considered to be a remarkable transformation for the Japanese vogue. They extensively adopted the style and practices of Western cultures.The upper classes wore more extravagant pieces of clothing like luxurious patterned silks and adorned themselves with fancy sashes. Women also started wearing Western dresses in public instead of their traditional Kimono. Most of the officials were also required to wear Western suits. In this way, the Japanese slowly adopted into Western fashion. Moreover, like India, different Japanese religions wear different pieces of clothing. Fashion industry In its most common use, the term fashion refers to the current expressions on sale through the fashion industry. The global fashion industry is a product of the modern age. In the Western world, tailoring has since medieval times been controlled by guilds, but with the emergence of industrialism, the power of the guilds was undermined. Before the mid-19th century, most clothing was custom-made. It was handmade for individuals, either as home production or on order from dressmakers and tailors. By the beginning of the 20th century, with the rise of new technologies such as the sewing machine, the rise of global trade, the development of the factory system of production, and the proliferation of retail outlets such as department stores, clothing became increasingly mass-produced in standard sizes and sold at fixed prices. Although the fashion industry developed first in Europe and America, as of 2017, it is an international and highly globalized industry, with clothing often designed in one country, manufactured in another, and sold worldwide. For example, an American fashion company might source fabric in China and have the clothes manufactured in Vietnam, finished in Italy, and shipped to a warehouse in the United States for distribution to retail outlets internationally. The fashion industry has for a long time been one of the largest employers in the United States, and it remains so in the 21st century. However, U.S. employment in fashion began to decline considerably as production increasingly moved overseas, especially to China. Because data regarding the fashion industry typically are reported for national economies and expressed in terms of the industry's many separate sectors, aggregate figures for the world production of textiles and clothing are difficult to obtain. However, by any measure, the clothing industry accounts for a significant share of world economic output. The fashion industry consists of four levels: The production of raw materials, principally fiber, and textiles but also leather and fur. The production of fashion goods by designers, manufacturers, contractors, and others. Retail sales. Various forms of advertising and promotion. The levels of focus in the fashion industry consist of many separate but interdependent sectors. These sectors include textile design and production, fashion design and manufacturing, fashion retailing, marketing and merchandising, fashion shows, and media and marketing. Each sector is devoted to the goal of satisfying consumer demand for apparel under conditions that enable participants in the industry to operate at a profit. Fashion trends A fashion trend signifies a specific look or expression that is spread across a population at a specific time and place. A trend is considered a more ephemeral look, not defined by the seasons when collections are released by the fashion industry. A trend can thus emerge from street style, across cultures, and from influencers and other celebrities. Fashion trends are influenced by several factors, including cinema, celebrities, climate, creative explorations, innovations, designs, political, economic, social, and technological. Examining these factors is called a PEST analysis. Fashion forecasters can use this information to help determine the growth or decline of a particular trend. People's minds as well as their perceptions and consciousness are constantly changing. Fads are inherently social, are constantly evolving in a contradiction between the old and the new, and are in a sense easily influenced by those around them, and therefore also begin to imitate constantly. Continuing on from the maximalist and 1980s influences of the early 2020s, vibrant coloured clothing had made a comeback for women in America, France, China, Korea, and Ukraine by the spring of 2023. This style, sometimes referred to as "dopamine dressing", featured long skirts and belted maxi dresses with thigh splits, lots of gold and pearl jewelry, oversized striped cardigan sweaters, multicoloured silk skirts with seashell or floral print, strappy sandals, pants with a contrasting stripe down the leg, ugg boots, floral print maxi skirts, Y2K inspired platform shoes, chunky red rain boots, shimmery jumpsuits, knitted dresses, leather pilot jackets with faux fur collars, skirts with bold contrasting vertical stripes, trouser suits with bootcut legs, jeans with glittery heart or star-shaped details, chunky white or black sandals, and zebra print tote bags. Big, oversized garments were often made from translucent materials and featured cutouts intended to expose the wearer's bare shoulder, thigh, or midriff, such as low-cut waists on the pants or tops with strappy necklines intended to be worn braless. Desirable colours included neon green, watermelon green, coral pink, orange, salmon pink, magenta, gold, electric blue, aquamarine, cyan, turquoise, and royal blue. In 2023, the predominant colors in the United States, Britain, and France were red, white, and blue. As in the mid to late 1970s, Western shirts with pearl snaps in denim or bright madras plaid made a comeback, and sometimes featured contrasting yokes and cuffs with intricate embroidery. Moccasins, stonewash denim waistcoats with decorative fringes, preppy loafers, navy blue suits and sportcoats, straight leg jeans instead of the skinny jeans fashionable from the late 2000s until the early 2020s, stetsons, white baseball jerseys with bold red or blue pinstripes, striped blue neckties, baggy white pants, Union Jack motifs, flared jeans, and duster coats as worn in the Yellowstone TV series, as well as preppy style college sweaters, retro blue and white striped football shirts, chelsea boots with cowboy boot styling, two-button blazers with red and blue boating stripes, V-neck sweater vests, royal blue baseball jackets with white sleeves, Howler Brothers gilets, shirts and suits worn open to expose the chest, and boxy leather reefer jackets were popular on both sides of the Atlantic. Social influences Fashion is inherently a social phenomenon. A person cannot have a fashion by oneself, but for something to be defined as fashion, there needs to be dissemination and followers. This dissemination can take several forms; from the top-down ("trickle-down") to bottom-up ("bubble up/trickle-up"), or transversally across cultures and through viral memes and media ("trickle-across"). Fashion relates to the social and cultural context of an environment. According to Matika, "Elements of popular culture become fused when a person's trend is associated with a preference for a genre of music […] like music, news, or literature, fashion has been fused into everyday lives." Fashion is not only seen as purely aesthetic; fashion is also a medium for people to create an overall effect and express their opinions and overall art. This mirrors what performers frequently accomplish through music videos. In the music video 'Formation' by Beyoncé, according to Carlos, The annual or seasonal runway show is a reflection of fashion trends and a designer's inspirations. For designers like Vivienne Westwood, runway shows are a platform for her voice on politics and current events. For her AW15 menswear show, according to Water, "where models with severely bruised faces channeled eco-warriors on a mission to save the planet." Another recent example is a staged feminist protest march for Chanel's SS15 show, rioting models chanting words of empowerment using signs like "Feminist but feminine" and "Ladies first." According to Water, "The show tapped into Chanel's long history of championing female independence: founder Coco Chanel was a trailblazer for liberating the female body in the post-WWI era, introducing silhouettes that countered the restrictive corsets then in favour." The annual Met Gala ceremony in Manhattan is the premier venue where fashion designers and their creations are celebrated. Social media is also a place where fashion is presented most often. Some influencers are paid huge amounts of money to promote a product or clothing item, where the business hopes many viewers will buy the product off the back of the advertisement. Instagram is the most popular platform for advertising, but Facebook, Snapchat, Twitter and other platforms are also used. In New York, the LGBT fashion design community contributes very significantly to promulgating fashion trends, and drag celebrities have developed a profound influence upon New York Fashion Week. Marketing Market research Consumers of different groups have varying needs and demands. Factors taken into consideration when analyzing consumers' needs include key demographics. To understand consumers' needs and predict fashion trends, fashion companies have to do market research There are two research methods: primary and secondary. Secondary methods are taking other information that has already been collected, for example using a book or an article for research. Primary research is collecting data through surveys, interviews, observation, and/or focus groups. Primary research often focuses on large sample sizes to determine customer's motivations to shop. The benefits of primary research are specific information about a fashion brand's consumer is explored. Surveys are helpful tools; questions can be open-ended or closed-ended. Negative factor surveys and interviews present is that the answers can be biased, due to wording in the survey or on face-to-face interactions. Focus groups, about 8 to 12 people, can be beneficial because several points can be addressed in depth. However, there are drawbacks to this tactic, too. With such a small sample size, it is hard to know if the greater public would react the same way as the focus group. Observation can really help a company gain insight on what a consumer truly wants. There is less of a bias because consumers are just performing their daily tasks, not necessarily realizing they are being observed. For example, observing the public by taking street style photos of people, the consumer did not get dressed in the morning knowing that would have their photo taken necessarily. They just wear what they would normally wear. Through observation patterns can be seen, helping trend forecasters know what their target market needs and wants. Knowing the needs of consumers will increase fashion companies' sales and profits. Through research and studying the consumers' lives the needs of the customer can be obtained and help fashion brands know what trends the consumers are ready for. Symbolic consumption Consumption is driven not only by need, the symbolic meaning for consumers is also a factor. Consumers engaging in symbolic consumption may develop a sense of self over an extended period of time as various objects are collected as part of the process of establishing their identity and, when the symbolic meaning is shared in a social group, to communicate their identity to others. For teenagers, consumption plays a role in distinguishing the child self from the adult. Researchers have found that the fashion choices of teenagers are used for self-expression and also to recognize other teens who wear similar clothes. The symbolic association of clothing items can link individuals' personality and interests, with music as a prominent factor influencing fashion decisions. Political influences Political figures have played a central role in the development of fashion, at least since the time of French king Louis XIV. For example, First Lady Jacqueline Kennedy was a fashion icon of the early 1960s. Wearing Chanel suits, structural Givenchy shift dresses, and soft color Cassini coats with large buttons, she inspired trends of both elegant formal dressing and classic feminine style. Cultural upheavals have also had an impact on fashion trends. For example, during the 1960s, the U.S. economy was robust, the divorce rate was increasing, and the government approved the birth control pill. These factors inspired the younger generation to rebel against entrenched social norms. The civil rights movement, a struggle for social justice and equal opportunity for Blacks, and the women's liberation movement, seeking equal rights and opportunities and greater personal freedom for women, were in full bloom. In 1964, the leg-baring mini-skirt was introduced and became a white-hot trend. Fashion designers then began to experiment with the shapes of garments: loose sleeveless dresses, micro-minis, flared skirts, and trumpet sleeves. Fluorescent colors, print patterns, bell-bottom jeans, fringed vests, and skirts became de rigueur outfits of the 1960s. Concern and protest over U.S. involvement in the failing Vietnam War also influenced fashion. Camouflage patterns in military clothing, developed to help military personnel be less visible to enemy forces, seeped into streetwear designs in the 1960s. Camouflage trends have disappeared and resurfaced several times since then, appearing in high fashion iterations in the 1990s. Designers such as Valentino, Dior, and Dolce & Gabbana combined camouflage into their runway and ready-to-wear collections. Today, variations of camouflage, including pastel shades, in every article of clothing or accessory, continue to enjoy popularity. Technology influences Today, technology plays a sizable role in society, and technological influences are correspondingly increasing within the realm of fashion. Wearable technology has become incorporated; for example, clothing constructed with solar panels that charge devices and smart fabrics that enhance wearer comfort by changing color or texture based on environmental changes. 3D printing technology has influenced designers such as Iris van Herpen and Kimberly Ovitz. As the technology evolves, 3D printers will become more accessible to designers and eventually, consumers — these could potentially reshape design and production in the fashion industry entirely. Internet technology, enabling the far reaches of online retailers and social media platforms, has created previously unimaginable ways for trends to be identified, marketed, and sold immediately. Trend-setting styles are easily displayed and communicated online to attract customers. Posts on Instagram or Facebook can quickly increase awareness about new trends in fashion, which subsequently may create high demand for specific items or brands, new "buy now button" technology can link these styles with direct sales. Machine vision technology has been developed to track how fashions spread through society. The industry can now see the direct correlation on how fashion shows influence street-chic outfits. Effects such as these can now be quantified and provide valuable feedback to fashion houses, designers, and consumers regarding trends. Environmental impact Media Media, including social media platforms, play a crucial role in shaping fashion trends, creating a rapid cycle of trend adoption and obsolescence. For instance, an important part of fashion is fashion journalism. Editorial critique, guidelines, and commentary can be found on television and in magazines, newspapers, fashion websites, social networks, and fashion blogs. In recent years, fashion blogging and YouTube videos have become a major outlet for spreading trends and fashion tips, creating an online culture of sharing one's style on a website or social media accounts (i.e. Instagram, TikTok, or Twitter). Through these media outlets, readers and viewers all over the world can learn about fashion, making it very accessible. In addition to fashion journalism, another media platform that is important in fashion industry is advertisement. Advertisements provide information to audiences and promote the sales of products and services. The fashion industry uses advertisements to attract consumers and promote its products to generate sales. A few decades ago when technology was still underdeveloped, advertisements heavily relied on radio, magazines, billboards, and newspapers. These days, there are more various ways in advertisements such as television ads, online-based ads using internet websites, and posts, videos, and live streaming in social media platforms. Fashion in printed media There are two subsets of print styling: editorial and lifestyle. Editorial styling is the high-fashion styling seen in fashion magazines, and this tends to be more artistic and fashion-forward. Lifestyle styling focuses on a more overtly commercial goal, like a department store advertisement, a website, or an advertisement where fashion is not what's being sold but the models are hired to promote the product in the photo. The dressing practices of the powerful have traditionally been mediated through art and the practices of the courts. The looks of the French court were disseminated through prints from the 16th century on, but gained cohesive design with the development of a centralized court under King Louis XIV, which produced an identifiable style that took his name. At the beginning of the 20th century, fashion magazines began to include photographs of various fashion designs and became even more influential than in the past. In cities throughout the world these magazines were greatly sought after and had a profound effect on public taste in clothing. Talented illustrators drew exquisite fashion plates for the publications which covered the most recent developments in fashion and beauty. Perhaps the most famous of these magazines was La Gazette du Bon Ton, which was founded in 1912 by Lucien Vogel and regularly published until 1925 (with the exception of the war years). Vogue, founded in Manhattan in 1892, has been the longest-lasting and most successful of the hundreds of fashion magazines that have come and gone. Increasing affluence after World War II and, most importantly, the advent of cheap color printing in the 1960s, led to a huge boost in its sales and heavy coverage of fashion in mainstream women's magazines, followed by men's magazines in the 1990s. One such example of Vogue's popularity is the younger version, Teen Vogue, which covers clothing and trends that are targeted more toward the "fashionista on a budget". Haute couture designers followed the trend by starting ready-to-wear and perfume lines which are heavily advertised in the magazines and now dwarf their original couture businesses. A recent development within fashion print media is the rise of text-based and critical magazines which aim to prove that fashion is not superficial, by creating a dialogue between fashion academia and the industry. Examples of this development are: Fashion Theory (1997), Fashion Practice: The Journal of Design, Creative Process & the Fashion Industry (2008), and Vestoj (2009). Fashion in television Television coverage began in the 1950s with small fashion features. In the 1960s and 1970s, fashion segments on various entertainment shows became more frequent, and by the 1980s, dedicated fashion shows such as FashionTelevision started to appear. FashionTV was the pioneer in this undertaking and has since grown to become the leader in both Fashion Television and new media channels. The Fashion Industry is beginning to promote their styles through Bloggers on social media's. Vogue specified Chiara Ferragni as "blogger of the moment" due to the rises of followers through her Fashion Blog, that became popular. A few days after the 2010 Fall Fashion Week in New York City came to a close, The New Islander's Fashion Editor, Genevieve Tax, criticized the fashion industry for running on a seasonal schedule of its own, largely at the expense of real-world consumers. "Because designers release their fall collections in the spring and their spring collections in the fall, fashion magazines such as Vogue always and only look forward to the upcoming season, promoting parkas come September while issuing reviews on shorts in January", she writes. "Savvy shoppers, consequently, have been conditioned to be extremely, perhaps impractically, farsighted with their buying." The fashion industry has been the subject of numerous films and television shows, including the reality show Project Runway and the drama series Ugly Betty. Specific fashion brands have been featured in film, not only as product placement opportunities, but as bespoke items that have subsequently led to trends in fashion. Videos in general have been very useful in promoting the fashion industry. This is evident not only from television shows directly spotlighting the fashion industry, but also movies, events and music videos which showcase fashion statements as well as promote specific brands through product placements. Controversial advertisements in the fashion industry Racism in fashion advertisements Some fashion advertisements have been accused of racism and led to boycotts from customers. Globally known Swedish fashion brand H&M faced this issue with one of its children's wear advertisements in 2018. A Black child wearing a hoodie with the slogan "coolest monkey in the jungle" was featured in the ad. This immediately led to controversy, as "monkey" is commonly used as slur against Black people, and caused many customers to boycott the brand. Many people, including celebrities, posted on social media about their resentments towards H&M and refusal to work with and buy its products. H&M issued a statement saying "we apologise to anyone this may have offended", though this too received some criticism for appearing insincere. Another fashion advertisement seen as racist was from GAP, an American worldwide clothing brand. GAP collaborated with Ellen DeGeneres in 2016 for the advertisement. It features four playful young girls, with a tall White girl leaning with her arm on a shorter Black girl's head. Upon release, some viewers harshly criticized it, claiming it shows an underlying passive racism. A representative from The Root commented that the ad portrays the message that Black people are undervalued and seen as props for White people to look better. Others saw little issue with the ad, and that the controversy was the result of people being oversensitive. GAP replaced the image in the ad and apologized to critics. Sexism in fashion advertisements Many fashion brands have published ads that were provocative and sexy to attract customers' attention. British high fashion brand, Jimmy Choo, was blamed for having sexism in its ad which featured a female British model wearing the brand's boots. In this two-minute ad, men whistle at a model, walking on the street with red, sleeveless mini dress. This ad gained much backlash and criticism by the viewers, as it was seen as promoting sexual harassment and other misconduct. Many people showed their dismay through social media posts, leading Jimmy Choo to pull down the ad from social media platforms. French luxury fashion brand Yves Saint Laurent also faced this issue with its print ad shown in Paris in 2017. The ad depicted a female model wearing fishnet tights with roller-skate stilettos reclining with her legs opened in front of the camera. This advertisement brought harsh comments from both viewers and French advertising organization directors for going against the advertising codes related to "respect for decency, dignity and those prohibiting submission, violence or dependence, as well as the use of stereotypes." and additionally said that this ad was causing "mental harm to adolescents." Due to the negative public reaction, the poster was removed from the city. Public relations and social media Fashion public relations involves being in touch with a company's audiences and creating strong relationships with them, reaching out to media, and initiating messages that project positive images of the company. Social media plays an important role in modern-day fashion public relations; enabling practitioners to reach a wide range of consumers through various platforms. Building brand awareness and credibility is a key implication of good public relations. In some cases, the hype is built about new designers' collections before they are released into the market, due to the immense exposure generated by practitioners. Social media, such as blogs, microblogs, podcasts, photo and video sharing sites have all become increasingly important to fashion public relations. The interactive nature of these platforms allows practitioners to engage and communicate with the public in real-time, and tailor their clients' brand or campaign messages to the target audience. With blogging platforms such as Instagram, Tumblr, WordPress, Squarespace, and other sharing sites, bloggers have emerged as expert fashion commentators, shaping brands and having a great impact on what is 'on trend'. Women in the fashion public relations industry such as Sweaty Betty PR founder Roxy Jacenko and Oscar de la Renta's PR girl Erika Bearman, have acquired copious followers on their social media sites, by providing a brand identity and a behind the scenes look into the companies they work for. Social media is changing the way practitioners deliver messages, as they are concerned with the media, and also customer relationship building. PR practitioners must provide effective communication among all platforms, in order to engage the fashion public in an industry socially connected via online shopping. Consumers have the ability to share their purchases on their personal social media pages (such as Facebook, Twitter, Instagram, etc.), and if practitioners deliver the brand message effectively and meet the needs of its public, word-of-mouth publicity will be generated and potentially provide a wide reach for the designer and their products. Fashion and political activism As fashion concerns people, and signifies social hierarchies, fashion intersects with politics and the social organization of societies. Whereas haute couture and business suits are associated by people in power, also groups aiming to challenge the political order also use clothes to signal their position. The explicit use of fashion as a form of activism, is usually referred to as "fashion activism." There is a complex relationship between fashion and feminism. Some feminists have argued that by participating in feminine fashions women are contributing to maintaining the gender differences which are part of women's oppression. Brownmiller felt that women should reject traditionally feminine dress, focusing on comfort and practicality rather than fashion. Others believe that it is the fashion system itself that is repressive in requiring women to seasonally change their clothes to keep up with trends. Greer has advocated this argument that seasonal changes in dress should be ignored; she argues that women can be liberated by replacing the compulsiveness of fashion with enjoyment of rejecting the norm to create their own personal styling. This rejection of seasonal fashion led to many protests in the 1960s alongside rejection of fashion on socialist, racial and environmental grounds. However, Mosmann has pointed out that the relationship between protesting fashion and creating fashion is dynamic because the language and style used in these protests has then become part of fashion itself. Fashion designers and brands have traditionally kept themselves out of political conflicts, there has been a movement in the industry towards taking more explicit positions across the political spectrum. From maintaining a rather apolitical stance, designers and brands today engage more explicitly in current debates. For example, considering the U.S.'s political climate in the surrounding months of the 2016 presidential election, during 2017 fashion weeks in London, Milan, New York, Paris and São Paulo amongst others, many designers took the opportunity to take political stances leveraging their platforms and influence to reach their customers. This has also led to some controversy over democratic values, as fashion is not always the most inclusive platform for political debate, but a one-way broadcast of top-down messages. When taking an explicit political stance, designers generally favor issues that can be identified in clear language with virtuous undertones. For example, aiming to "amplify a greater message of unity, inclusion, diversity, and feminism in a fashion space", designer Mara Hoffman invited the founders of the Women's March on Washington to open her show which featured modern silhouettes of utilitarian wear, described by critics as "Made for a modern warrior" and "Clothing for those who still have work to do". Prabal Gurung debuted his collection of T-shirts featuring slogans such as "The Future is Female", "We Will Not Be Silenced", and "Nevertheless She Persisted", with proceeds going to the ACLU, Planned Parenthood, and Gurung's own charity, "Shikshya Foundation Nepal". Similarly, The Business of Fashion launched the #TiedTogether movement on Social Media, encouraging member of the industry from editors to models, to wear a white bandana advocating for "unity, solidarity, and inclusiveness during fashion week". Fashion may be used to promote a cause, such as to promote healthy behavior, to raise money for a cancer cure, or to raise money for local charities such as the Juvenile Protective Association or a children's hospice. One fashion cause is trashion, which is using trash to make clothes, jewelry, and other fashion items in order to promote awareness of pollution. There are a number of modern trashion artists such as Marina DeBris, Ann Wizer, and Nancy Judd. Other designers have used DIY fashions, in the tradition of the punk movement, to address elitism in the industry to promote more inclusion and diversity. Anthropological perspective From an academic lens, the sporting of various fashions has been seen as a form of fashion language, a mode of communication that produced various fashion statements, using a grammar of fashion. This is a perspective promoted in the work of influential French philosopher and semiotician Roland Barthes. Anthropology, the study of culture and of human societies, examines fashion by asking why certain styles are deemed socially appropriate and others are not. From the theory of interactionism, a certain practice or expression is chosen by those in power in a community, and that becomes "the fashion" as defined at a certain time by the people under influence of those in power. If a particular style has a meaning in an already occurring set of beliefs, then that style may have a greater chance of become fashion. According to cultural theorists Ted Polhemus and Lynn Procter, one can describe fashion as adornment, of which there are two types: fashion and anti-fashion. Through the capitalization and commoditization of clothing, accessories, and shoes, etc., what once constituted anti-fashion becomes part of fashion as the lines between fashion and anti-fashion are blurred, as expressions that were once outside the changes of fashion are swept along with trends to signify new meanings. Examples range from how elements from ethnic dress becomes part of a trend and appear on catwalks or street cultures, for example how tattoos travel from sailors, laborers and criminals to popular culture. To cultural theorist Malcolm Bernard, fashion and anti-fashion differ as polar opposites. Anti-fashion is fixed and changes little over time, varying depending on the cultural or social group one is associated with or where one lives, but within that group or locality the style changes little. Fashion, in contrast, can change (evolve) very quickly and is not affiliated with one group or area of the world but spreads throughout the world wherever people can communicate easily with each other. An example of anti-fashion would be ceremonial or otherwise traditional clothing where specific garments and their designs are both reproduced faithfully and with the intent of maintaining a status quo of tradition. This can be seen in the clothing of some kabuki plays, where some character outfits are kept intact from designs of several centuries ago, in some cases retaining the crests of the actors considered to have 'perfected' that role. Anti-fashion is concerned with maintaining the status quo, while fashion is concerned with social mobility. Time is expressed in terms of continuity in anti-fashion, and in terms of change in fashion; fashion has changing modes of adornment, while anti-fashion has fixed modes of adornment. From this theoretical lens, change in fashion is part of the larger industrial system and is structured by the powerful actors in this system to be a deliberate change in style, promoted through the channels influenced by the industry (such as paid advertisements). Intellectual property In the fashion industry, intellectual property is not enforced as it is within the film industry and music industry. Robert Glariston, an intellectual property expert, mentioned in a fashion seminar held in LA that "Copyright law regarding clothing is a current hot-button issue in the industry. We often have to draw the line between designers being inspired by a design and those outright stealing it in different places." To take inspiration from others' designs contributes to the fashion industry's ability to establish clothing trends. For the past few years, WGSN has been a dominant source of fashion news and forecasts in encouraging fashion brands worldwide to be inspired by one another. Enticing consumers to buy clothing by establishing new trends is, some have argued, a key component of the industry's success. Intellectual property rules that interfere with this process of trend-making would, in this view, be counter-productive. On the other hand, it is often argued that the blatant theft of new ideas, unique designs, and design details by larger companies is what often contributes to the failure of many smaller or independent design companies. Since fakes are distinguishable by their poorer quality, there is still a demand for luxury goods, and as only a trademark or logo can be copyrighted, many fashion brands make this one of the most visible aspects of the garment or accessory. In handbags, especially, the designer's brand may be woven into the fabric (or the lining fabric) from which the bag is made, making the brand an intrinsic element of the bag. In 2005, the World Intellectual Property Organization (WIPO) held a conference calling for stricter intellectual property enforcement within the fashion industry to better protect small and medium businesses and promote competitiveness within the textile and clothing industries. See also References Further reading Breward, Christopher, The culture of fashion: a new history of fashionable dress, Manchester: Manchester University Press, 2003, ISBN 978-0-7190-4125-9 Cabrera, Ana, and Lesley Miller. "Genio y Figura. La influencia de la cultura española en la moda." Fashion Theory: The Journal of Dress, Body & Culture 13.1 (2009): 103–110 Cumming, Valerie: Understanding Fashion History, Costume & Fashion Press, 2004, ISBN 0-89676-253-X Davis, F. (1989). Of maids' uniforms and blue jeans: The drama of status ambivalences in clothing and fashion. Qualitative Sociology, 12(4), 337–355. Hollander, Anne, Seeing through clothes, Berkeley: University of California Press, 1993, ISBN 978-0-520-08231-1 Hanifie, Sowaibah (5 August 2020). "Australia's first National Indigenous Fashion Awards winners revealed, signaling hope for a more diverse industry". ABC News. Australian Broadcasting Corporation. Hollander, Anne, Sex and suits: the evolution of modern dress, New York: Knopf, 1994, ISBN 978-0-679-43096-4 Hollander, Anne, Feeding the eye: essays, New York: Farrar, Straus, and Giroux, 1999, ISBN 978-0-374-28201-1 Hollander, Anne, Fabric of vision: dress and drapery in painting, London: National Gallery, 2002, ISBN 978-0-300-09419-0 Kawamura, Yuniya, Fashion-ology: an introduction to Fashion Studies, Oxford and New York: Berg, 2005, ISBN 1-85973-814-1 Lipovetsky, Gilles (translated by Catherine Porter), The empire of fashion: dressing modern democracy, Woodstock: Princeton University Press, 2002, ISBN 978-0-691-10262-7 McDermott, Kathleen, Style for all: why fashion, invented by kings, now belongs to all of us (An illustrated history), 2010, ISBN 978-0-557-51917-0 – Many hand-drawn color illustrations, extensive annotated bibliography and reading guide Perrot, Philippe (translated by Richard Bienvenu), Fashioning the bourgeoisie: a history of clothing in the nineteenth century, Princeton NJ: Princeton University Press, 1994, ISBN 978-0-691-00081-7 Steele, Valerie, Paris fashion: a cultural history, (2. ed., rev. and updated), Oxford: Berg, 1998, ISBN 978-1-85973-973-0 Steele, Valerie, Fifty years of fashion: new look to now, New Haven: Yale University Press, 2000, ISBN 978-0-300-08738-3 Steele, Valerie, Encyclopedia of clothing and fashion, Detroit: Thomson Gale, 2005 Turrell, Claire (2 Mar 2023). "The Asian blouse that tells a tale of many cultures". BBC. External links Media related to Fashion at Wikimedia Commons The dictionary definition of fashion at Wiktionary Quotations related to Fashion at Wikiquote

A film, also known as a movie or motion picture, is a work of visual art that simulates experiences and otherwise communicates ideas, stories, perceptions, emotions, or atmosphere through the use of moving images that are generally, since the 1930s, synchronized with sound and some times using other sensory stimulations. Films are produced by recording actual people and objects with cameras or by creating them using animation techniques and special effects. They comprise a series of individual frames, but when these images are shown rapidly in succession, the illusion of motion is given to the viewer. Flickering between frames is not seen due to an effect known as persistence of vision, whereby the eye retains a visual image for a fraction of a second after the source has been removed. Also of relevance is what causes the perception of motion; a psychological effect identified as beta movement. Films are considered by many to be an important art form; films entertain, educate, enlighten and inspire audiences. The visual elements of cinema need no translation, giving the motion picture a universal power of communication. Any film can become a worldwide attraction, especially with the addition of dubbing or subtitles that translate the dialogue. Films are also artifacts created by specific cultures, which reflect those cultures, and, in turn, affect them. History Precursors The art of film has drawn on several earlier traditions in fields such as oral storytelling, literature, theatre and visual arts. Forms of art and entertainment that had already featured moving or projected images such as shadowgraphy, camera obscura, shadow puppetry and magic lantern. 1830s–1880s: Before celluloid The stroboscopic animation principle was introduced in 1833 with the stroboscopic disc (better known as the phénakisticope) and later applied in the zoetrope (since 1866), the flip book (since 1868), and the praxinoscope (since 1877), before it became the basic principle for cinematography. Experiments with early phénakisticope-based animation projectors were made at least as early as 1843 and publicly screened in 1847. Jules Duboscq marketed phénakisticope projection systems in France from c. 1853 until the 1890s. Photography was introduced in 1839, but initially photographic emulsions needed such long exposures that the recording of moving subjects seemed impossible. At least as early as 1844, photographic series of subjects posed in different positions were created to either suggest a motion sequence or document a range of different viewing angles. The advent of stereoscopic photography, with early experiments in the 1840s and commercial success since the early 1850s, raised interest in completing the photographic medium with the addition of means to capture color and motion. In 1849, Joseph Plateau published about the idea to combine his invention of the phénakisticope with the stereoscope, as suggested to him by stereoscope inventor Charles Wheatstone, and to use photographs of plaster sculptures in different positions to be animated in the combined device. In 1852, Jules Duboscq patented such an instrument as the "Stéréoscope-fantascope, ou Bïoscope", but he only marketed it very briefly, without success. One Bïoscope disc with stereoscopic photographs of a machine is in the Plateau collection of Ghent University, but no instruments or other discs have yet been found. By the late 1850s, the first examples of instantaneous photography came about and provided hope that motion photography would soon be possible, but it took a few decades before it was successfully combined with a method to record series of sequential images in real-time. In 1878, Eadweard Muybridge eventually managed to take a series of photographs of a running horse with a battery of cameras in a line along the track and published the results as The Horse in Motion on cabinet cards. Muybridge, as well as Étienne-Jules Marey, Ottomar Anschütz and many others, would create many more chronophotography studies. Muybridge had the contours of dozens of his chronophotographic series traced onto glass discs and projected them with his zoopraxiscope in his lectures from 1880 to 1895. Anschütz made his first instantaneous photographs in 1881. He developed a portable camera that allowed shutter speeds as short as 1/1000 of a second in 1882. The quality of his pictures was generally regarded as much higher than that of the chronophotography works of Muybridge and Étienne-Jules Marey. In 1886, Anschütz developed the Electrotachyscope, an early device that displayed short motion picture loops with 24 glass plate photographs on a 1.5 meter wide rotating wheel that was hand-cranked to a speed of circa 30 frames per second. Different versions were shown at many international exhibitions, fairs, conventions, and arcades from 1887 until at least 1894. Starting in 1891, some 152 examples of a coin-operated peep-box Electrotachyscope model were manufactured by Siemens & Halske in Berlin and sold internationally. Nearly 34,000 people paid to see it at the Berlin Exhibition Park in the summer of 1892. Others saw it in London or at the 1893 Chicago World's Fair. On 25 November 1894, Anschütz introduced a Electrotachyscope projector with a 6x8 meter screening in Berlin. Between 22 February and 30 March 1895, a total of circa 7,000 paying customers came to view a 1.5-hour show of some 40 scenes at a 300-seat hall in the old Reichstag building in Berlin. Émile Reynaud already mentioned the possibility of projecting images of the Praxinoscope in his 1877 patent application. He presented a praxinoscope projection device at the Société française de photographie on 4 June 1880, but did not market his praxinoscope a projection before 1882. He then further developed the device into the Théâtre Optique which could project longer sequences with separate backgrounds, patented in 1888. He created several movies for the machine by painting images on hundreds of gelatin plates that were mounted into cardboard frames and attached to a cloth band. From 28 October 1892 to March 1900 Reynaud gave over 12,800 shows to a total of over 500,000 visitors at the Musée Grévin in Paris. 1880s–1890s: First motion pictures By the end of the 1880s, the introduction of lengths of celluloid photographic film and the invention of motion picture cameras, which could photograph a rapid sequence of images using only one lens, allowed action to be captured and stored on a single compact reel of film. Movies were initially shown publicly to one person at a time through "peep show" devices such as the Electrotachyscope, Kinetoscope and the Mutoscope. Not much later, exhibitors managed to project films on large screens for theatre audiences. The first public screenings of films at which admission was charged were made in 1895 by the American Woodville Latham and his sons (using films produced by their Eidoloscope company), by the Skladanowsky brothers, and by French brothers Auguste and Louis Lumière, best known for L'Arrivée d'un train en gare de La Ciotat (1896), with ten of their own productions. Private screenings had preceded these by several months, with Latham's slightly predating the others'. 1910s: Early evolution The earliest films were simply one static shot that showed an event or action with no editing or other cinematic techniques. Typical films showed employees leaving a factory gate, people walking in the street, and the view from the front of a trolley as it traveled a city's Main Street. According to legend, when a film showed a locomotive at high speed approaching the audience, the audience panicked and ran from the theater. Around the turn of the 20th century, films started stringing several scenes together to tell a story. (The filmmakers who first put several shots or scenes discovered that, when one shot follows another, that act establishes a relationship between the content in the separate shots in the minds of the viewer. It is this relationship that makes all film storytelling possible. In a simple example, if a person is shown looking out a window, whatever the next shot shows, it will be regarded as the view the person was seeing.) Each scene was a single stationary shot with the action occurring before it. The scenes were later broken up into multiple shots photographed from different distances and angles. Other techniques such as camera movement were developed as effective ways to tell a story with film. Until sound film became commercially practical in the late 1920s, motion pictures were a purely visual art, but these innovative silent films had gained a hold on the public imagination. Rather than leave audiences with only the noise of the projector as an accompaniment, theater owners hired a pianist or organist or, in large urban theaters, a full orchestra to play music that fit the mood of the film at any given moment. By the early 1920s, most films came with a prepared list of sheet music to be used for this purpose, and complete film scores were composed for major productions. The rise of European cinema was interrupted by the outbreak of World War I, while the film industry in the United States flourished with the rise of Hollywood, typified most prominently by the innovative work of D. W. Griffith in The Birth of a Nation (1915) and Intolerance (1916). However, in the 1920s, European filmmakers such as Eisenstein, F. W. Murnau and Fritz Lang, in many ways inspired by the meteoric wartime progress of film through Griffith, along with the contributions of Charles Chaplin, Buster Keaton and others, quickly caught up with American film-making and continued to further advance the medium. 1920s–1960s: Evolution in sound In the 1920s, the development of electronic sound recording technologies made it practical to incorporate a soundtrack of speech, music and sound effects synchronized with the action on the screen. The resulting sound films were initially distinguished from the usual silent "moving pictures" or "movies" by calling them "talking pictures" or "talkies." The revolution they wrought was swift. By 1930, silent film was practically extinct in the US and already being referred to as "the old medium." The evolution of sound in cinema began with the idea of combining moving images with existing phonograph sound technology. Early sound-film systems, such as Thomas Edison's Kinetoscope and the Vitaphone used by Warner Bros., laid the groundwork for synchronized sound in film. The Vitaphone system, produced alongside Bell Telephone Company and Western Electric, faced initial resistance due to expensive equipping costs, but sound in cinema gained acceptance with movies like Don Juan (1926) and The Jazz Singer (1927). American film studios, while Europe standardized on Tobis-Klangfilm and Tri-Ergon systems. This new technology allowed for greater fluidity in film, giving rise to more complex and epic movies like King Kong (1933). As the television threat emerged in the 1940s and 1950s, the film industry needed to innovate to attract audiences. In terms of sound technology, this meant the development of surround sound and more sophisticated audio systems, such as Cinerama's seven-channel system. However, these advances required a large number of personnel to operate the equipment and maintain the sound experience in theaters. In 1966, Dolby Laboratories introduced the Dolby A noise reduction system, which became a standard in the recording industry and eliminated the hissing sound associated with earlier standardization efforts. Dolby Stereo, a revolutionary surround sound system, followed and allowed cinema designers to take acoustics into consideration when designing theaters. This innovation enabled audiences in smaller venues to enjoy comparable audio experiences to those in larger city theaters. Today, the future of sound in film remains uncertain, with potential influences from artificial intelligence, remastered audio, and personal viewing experiences shaping its development. However, it is clear that the evolution of sound in cinema has been marked by continuous innovation and a desire to create more immersive and engaging experiences for audiences. 1930s: Evolution in color A significant technological advancement in film was the introduction of "natural color," where color was captured directly from nature through photography, as opposed to being manually added to black-and-white prints using techniques like hand-coloring or stencil-coloring. Early color processes often produced colors that appeared far from "natural". Unlike the rapid transition from silent films to sound films, color's replacement of black-and-white happened more gradually. The crucial innovation was the three-strip version of the Technicolor process, first used in animated cartoons in 1932. The process was later applied to live-action short films, specific sequences in feature films, and finally, for an entire feature film, Becky Sharp, in 1935. Although the process was expensive, the positive public response, as evidenced by increased box office revenue, generally justified the additional cost. Consequently, the number of films made in color gradually increased year after year. One of the first mainstream films to use color was The Wizard of Oz (1939). 1950s: growing influence of television In the early 1950s, black-and-white television started receiving criticism with many believing that television failed to reach the lofty intellectual and cultural expectations that accompanied its introduction. In an attempt to lure audiences back into theaters, bigger screens were installed, widescreen processes, polarized 3D projection, and stereophonic sound were introduced, and more films were made in color, which soon became the rule rather than the exception. Some important mainstream Hollywood films were still being made in black-and-white as late as the mid-1960s, but they marked the end of an era. Color television receivers had been available in the US since the mid-1950s, but at first, they were very expensive and few broadcasts were in color. During the 1960s, prices gradually came down, color broadcasts became common, and sales boomed. The overwhelming public verdict in favor of color was clear. After the final flurry of black-and-white films had been released in mid-decade, all Hollywood studio productions were filmed in color, with the usual exceptions made only at the insistence of "star" filmmakers such as Peter Bogdanovich, Martin Scorsese and Alfred Hitchcock with his film Psycho (1960). 1960s–present: Modern cinema The decades following the decline of the studio system in the 1960s saw changes in the production and style of film. Various New Wave movements (including the French New Wave, New German Cinema wave, Indian New Wave, Japanese New Wave, New Hollywood, and Egyptian New Wave) and the rise of film-school-educated independent filmmakers contributed to the changes the medium experienced in the latter half of the 20th century. Digital technology has been the driving force for change throughout the 1990s and into the 2000s. Digital 3D projection largely replaced earlier problem-prone 3D film systems and that became briefly popular in the early 2010s with films like Avatar (2009). Large-screen cinemas systems using 35mm and 70mm film were developed in the late 2010s, with companies like the IMAX corporation. Film theory "Film theory" seeks to develop concise and systematic concepts that apply to the study of film as art. The concept of film as an art-form began in 1911 with Ricciotto Canudo's manifest The Birth of the Sixth Art. The Moscow Film School, the oldest film school in the world, was founded in 1919, in order to teach about and research film theory. Formalist film theory, led by Rudolf Arnheim, Béla Balázs, and Siegfried Kracauer, emphasized how film differed from reality and thus could be considered a valid fine art. André Bazin reacted against this theory by arguing that film's artistic essence lay in its ability to mechanically reproduce reality, not in its differences from reality, and this gave rise to realist theory. More recent analysis spurred by Jacques Lacan's psychoanalysis and Ferdinand de Saussure's semiotics among other things has given rise to psychoanalytic film theory, structuralist film theory, feminist film theory, and others. On the other hand, critics from the analytical philosophy tradition, influenced by Wittgenstein, try to clarify misconceptions used in theoretical studies and produce analysis of a film's vocabulary and its link to a form of life. Language Film is considered to have its own language. James Monaco wrote a classic text on film theory, titled "How to Read a Film," that addresses this. Director Ingmar Bergman famously said, "Andrei Tarkovsky for me is the greatest director, the one who invented a new language, true to the nature of film, as it captures life as a reflection, life as a dream." An example of the language is a sequence of back and forth images of one speaking actor's left profile, followed by another speaking actor's right profile, then a repetition of this, which is a language understood by the audience to indicate a conversation. This describes another theory of film, the 180-degree rule, as a visual story-telling device with an ability to place a viewer in a context of being psychologically present through the use of visual composition and editing. The "Hollywood style" includes this narrative theory, due to the overwhelming practice of the rule by movie studios based in Hollywood, California, during film's classical era. Another example of cinematic language is having a shot that zooms in on the forehead of an actor with an expression of silent reflection that cuts to a shot of a younger actor who vaguely resembles the first actor, indicating that the first person is remembering a past self, an edit of compositions that causes a time transition. Montage Montage is a film editing technique in which separate pieces of film are selected, edited, and assembled to create a new section or sequence within a film. This technique can be used to convey a narrative or to create an emotional or intellectual effect by juxtaposing different shots, often for the purpose of condensing time, space, or information. Montage can involve flashbacks, parallel action, or the interplay of various visual elements to enhance the storytelling or create symbolic meaning. The concept of montage emerged in the 1920s, with pioneering Soviet filmmakers such as Sergei Eisenstein and Lev Kuleshov developing the theory of montage. Eisenstein's film Battleship Potemkin (1925) is a prime example of the innovative use of montage, where he employed complex juxtapositions of images to create a visceral impact on the audience. As the art of montage evolved, filmmakers began incorporating musical and visual counterpoint to create a more dynamic and engaging experience for the viewer. The development of scene construction through mise-en-scène, editing, and special effects led to more sophisticated techniques that can be compared to those utilized in opera and ballet. The French New Wave movement of the late 1950s and 1960s also embraced the montage technique, with filmmakers such as Jean-Luc Godard and François Truffaut using montage to create distinctive and innovative films. This approach continues to be influential in contemporary cinema, with directors employing montage to create memorable sequences in their films. In contemporary cinema, montage continues to play an essential role in shaping narratives and creating emotional resonance. Filmmakers have adapted the traditional montage technique to suit the evolving aesthetics and storytelling styles of modern cinema. Rapid editing and fast-paced montages: With the advent of digital editing tools, filmmakers can now create rapid and intricate montages to convey information or emotions quickly. Films like Darren Aronofsky's Requiem for a Dream (2000) and Edgar Wright's Shaun of the Dead (2004) employ fast-paced editing techniques to create immersive and intense experiences for the audience. Music video influence: The influence of music videos on film has led to the incorporation of stylized montage sequences, often accompanied by popular music. Films like Guardians of the Galaxy (2014) and Baby Driver (2017) use montage to create visually striking sequences that are both entertaining and narratively functional. Sports and training montages: The sports and training montage has become a staple in modern cinema, often used to condense time and show a character's growth or development. Examples of this can be found in films like Rocky (1976), The Karate Kid (1984), and Million Dollar Baby (2004). Cross-cutting and parallel action: Contemporary filmmakers often use montage to create tension and suspense by cross-cutting between parallel storylines. Christopher Nolan's Inception (2010) and Dunkirk (2017) employ complex cross-cutting techniques to build narrative momentum and heighten the audience's emotional engagement. Thematic montage: Montage can also be used to convey thematic elements or motifs in a film. Wes Anderson's The Royal Tenenbaums (2001) employs montage to create a visual language that reflects the film's themes of family, nostalgia, and loss. As the medium of film continues to evolve, montage remains an integral aspect of visual storytelling, with filmmakers finding new and innovative ways to employ this powerful technique. Film criticism Film criticism is the analysis and evaluation of films. In general, these works can be divided into two categories: academic criticism by film scholars and journalistic film criticism that appears regularly in newspapers and other media. Film critics working for newspapers, magazines, and broadcast media mainly review new releases. Normally they only see any given film once and have only a day or two to formulate their opinions. Despite this, critics have an important impact on the audience response and attendance at films, especially those of certain genres. Mass marketed action, horror, and comedy films tend not to be greatly affected by a critic's overall judgment of a film. The plot summary and description of a film and the assessment of the director's and screenwriters' work that makes up the majority of most film reviews can still have an important impact on whether people decide to see a film. For prestige films such as most dramas and art films, the influence of reviews is important. Poor reviews from leading critics at major papers and magazines will often reduce audience interest and attendance. The impact of a reviewer on a given film's box office performance is a matter of debate. Some observers claim that movie marketing in the 2000s is so intense, well-coordinated and well financed that reviewers cannot prevent a poorly written or filmed blockbuster from attaining market success. However, the cataclysmic failure of some heavily promoted films which were harshly reviewed, as well as the unexpected success of critically praised independent films indicates that extreme critical reactions can have considerable influence. Other observers note that positive film reviews have been shown to spark interest in little-known films. Conversely, there have been several films in which film companies have so little confidence that they refuse to give reviewers an advanced viewing to avoid widespread panning of the film. However, this usually backfires, as reviewers are wise to the tactic and warn the public that the film may not be worth seeing and the films often do poorly as a result. Journalist film critics are sometimes called film reviewers. Critics who take a more academic approach to films, through publishing in film journals and writing books about films using film theory or film studies approaches, study how film and filming techniques work, and what effect they have on people. Rather than having their reviews published in newspapers or appearing on television, their articles are published in scholarly journals or up-market magazines. They also tend to be affiliated with colleges or universities as professors or instructors. Industry The making and showing of motion pictures became a source of profit almost as soon as the process was invented. Upon seeing how successful their new invention, and its product, was in their native France, the Lumières quickly set about touring the Continent to exhibit the first films privately to royalty and publicly to the masses. In each country, they would normally add new, local scenes to their catalogue and, quickly enough, found local entrepreneurs in the various countries of Europe to buy their equipment and photograph, export, import, and screen additional product commercially. The Oberammergau Passion Play of 1898 was the first commercial motion picture ever produced. Other pictures soon followed, and motion pictures became a separate industry that overshadowed the vaudeville world. Dedicated theaters and companies formed specifically to produce and distribute films, while motion picture actors became major celebrities and commanded huge fees for their performances. By 1917 Charlie Chaplin had a contract that called for an annual salary of one million dollars. From 1931 to 1956, film was also the only image storage and playback system for television programming until the introduction of videotape recorders. In the United States, much of the film industry is centered around Hollywood, California. Other regional centers exist in many parts of the world, such as Mumbai-centered Bollywood, the Indian film industry's Hindi cinema which produces the largest number of films in the world. Though the expense involved in making films has led cinema production to concentrate under the auspices of movie studios, recent advances in affordable film making equipment have allowed independent film productions to flourish. Profit is a key force in the industry, due to the costly and risky nature of filmmaking; many films have large cost overruns, an example being Kevin Costner's Waterworld. Yet many filmmakers strive to create works of lasting social significance. The Academy Awards (also known as "the Oscars") are the most prominent film awards in the United States, providing recognition each year to films, based on their artistic merits. There is also a large industry for educational and instructional films made in lieu of or in addition to lectures and texts. Revenue in the industry is sometimes volatile due to the reliance on blockbuster films released in movie theaters. The rise of alternative home entertainment has raised questions about the future of the cinema industry, and Hollywood employment has become less reliable, particularly for medium and low-budget films. Associated fields Derivative academic fields of study may both interact with and develop independently of filmmaking, as in film theory and analysis. Fields of academic study have been created that are derivative or dependent on the existence of film, such as film criticism, film history, divisions of film propaganda in authoritarian governments, or psychological on subliminal effects (e.g., of a flashing soda can during a screening). These fields may further create derivative fields, such as a movie review section in a newspaper or a television guide. Sub-industries can spin off from film, such as popcorn makers, and film-related toys (e.g., Star Wars figures). Sub-industries of pre-existing industries may deal specifically with film, such as product placement and other advertising within films. Terminology The terminology used for describing motion pictures varies considerably between British and American English. In British usage, the name of the medium is film. The word movie is understood but seldom used. Additionally, the pictures (plural) is used somewhat frequently to refer to the place where movies are exhibited; in American English this may be called the movies, but that term is becoming outdated. In other countries, the place where movies are exhibited may be called a cinema or movie theatre. By contrast, in the United States, movie is the predominant term for the medium. Although the words film and movie are sometimes used interchangeably, film is more often used when considering a work's artistic, theoretical, or technical aspects. The term movie more often refers to a work's entertainment or commercial aspects. Further terminology is used to distinguish various forms and media used in the film industry. Motion pictures and moving pictures are frequently used terms for films and movie productions specifically intended for theatrical exhibitions, such as Star Wars. DVD, Blu-ray Disc, and videotape are video formats that can reproduce a photochemical film. A reproduction based on such is called a transfer. After the advent of theatrical film as an industry, the television industry began using videotape as a recording medium. For many decades, tape was solely an analog medium onto which moving images could be either recorded or transferred. Film and filming refer to the photochemical medium that chemically records a visual image and the act of recording respectively. However, the act of shooting images with other visual media, such as with a digital camera, is still called filming, and the resulting works often called films as interchangeable to movies, despite not being shot on film. Silent films need not be utterly silent, but are films and movies without an audible dialogue, including those that have a musical accompaniment. The word talkies refers to the earliest sound films created to have audible dialogue recorded for playback along with the film, regardless of a musical accompaniment. Cinema either broadly encompasses both films and movies, or it is roughly synonymous with film and theatrical exhibition, and both are capitalized when referring to a category of art. The silver screen refers to the projection screen used to exhibit films and, by extension, is also used as a metonym for the entire film industry. Widescreen refers to a larger width to height in the frame, compared to earlier historic aspect ratios. A feature-length film, or feature film, is of a conventional full length, usually 60 minutes or more, and can commercially stand by itself without other films in a ticketed screening. A short is a film that is not as long as a feature-length film, often screened with other shorts, or preceding a feature-length film. An independent is a film made outside the conventional film industry. In US usage, one talks of a screening or projection of a movie or video on a screen at a public or private theater. In British English, a film showing happens at a cinema (never a theatre, which is a different medium and place altogether). Cinema usually refers to an arena designed specifically to exhibit films, where the screen is affixed to a wall, while theatre usually refers to a place where live, non-recorded action or combination thereof occurs from a podium or other type of stage, including the amphitheatre. Theatres can still screen movies in them, though the theatre would be retrofitted to do so. One might propose going to the cinema when referring to the activity, or sometimes to the pictures in British English, whereas the US expression is usually going to the movies. A cinema usually shows a mass-marketed movie using a front-projection screen process with either a film projector or, more recently, with a digital projector. But, cinemas may also show theatrical movies from their home video transfers that include Blu-ray Disc, DVD, and videocassette when they possess sufficient projection quality or based upon need, such as movies that exist only in their transferred state, which may be due to the loss or deterioration of the film master and prints from which the movie originally existed. Due to the advent of digital film production and distribution, physical film might be absent entirely. A double feature is a screening of two independently marketed, stand-alone feature films. A viewing is a watching of a film. Sales and at the box office refer to tickets sold at a theater, or more currently, rights sold for individual showings. A release is the distribution and often simultaneous screening of a film. A preview is a screening in advance of the main release. Any film may also have a sequel, which portrays events following those in the film. Bride of Frankenstein is an early example. When there are more films than one with the same characters, story arcs, or subject themes, these movies become a series, such as the James Bond series. Existing outside a specific story timeline usually does not exclude a film from being part of a series. A film that portrays events occurring earlier in a timeline with those in another film, but is released after that film, is sometimes called a prequel, an example being Butch and Sundance: The Early Days. The credits, or end credits, are a list that gives credit to the people involved in the production of a film. Films from before the 1970s usually start a film with credits, often ending with only a title card, saying "The End" or some equivalent, often an equivalent that depends on the language of the production. From then onward, a film's credits usually appear at the end of most films. However, films with credits that end a film often repeat some credits at or near the start of a film and therefore appear twice, such as that film's acting leads, while less frequently some appearing near or at the beginning only appear there, not at the end, which often happens to the director's credit. The credits appearing at or near the beginning of a film are usually called titles or beginning titles. A post-credits scene is a scene shown after the end of the credits. Ferris Bueller's Day Off has a post-credits scene in which Ferris tells the audience that the film is over and they should go home. A film's cast refers to a collection of the actors and actresses who appear, or star, in a film. A star is an actor or actress, often a popular one, and in many cases, a celebrity who plays a central character in a film. Occasionally the word can also be used to refer to the fame of other members of the crew, such as a director or other personality, such as Martin Scorsese. A crew is usually interpreted as the people involved in a film's physical construction outside cast participation, and it could include directors, film editors, photographers, grips, gaffers, set decorators, prop masters, and costume designers. A person can both be part of a film's cast and crew, such as Woody Allen, who directed and starred in Take the Money and Run. A film goer, movie goer, or film buff is a person who likes or often attends films and movies, and any of these, though more often the latter, could also see oneself as a student to films and movies or the filmic process. Intense interest in films, film theory, and film criticism, is known as cinephilia. A film enthusiast is known as a cinephile or cineaste. Preview Preview performance refers to a showing of a film to a select audience, usually for the purposes of corporate promotions, before the public film premiere itself. Previews are sometimes used to judge audience reaction, which if unexpectedly negative, may result in recutting or even refilming certain sections based on the audience response. One example of a film that was changed after a negative response from the test screening is 1982's First Blood. After the test audience responded very negatively to the death of protagonist John Rambo, a Vietnam veteran, at the end of the film, the company wrote and re-shot a new ending in which the character survives. Trailer and teaser Trailers or previews are advertisements for films that will be shown in 1 to 3 months at a cinema. Back in the early days of cinema, with theaters that had only one or two screens, only certain trailers were shown for the films that were going to be shown there. Later, when theaters added more screens or new theaters were built with a lot of screens, all different trailers were shown even if they were not going to play that film in that theater. Film studios realized that the more trailers that were shown (even if it was not going to be shown in that particular theater) the more patrons would go to a different theater to see the film when it came out. The term trailer comes from their having originally been shown at the end of a film program. That practice did not last long because patrons tended to leave the theater after the films ended, but the name has stuck. Trailers are now shown before the film (or the "A film" in a double feature program) begins. Film trailers are also common on DVDs and Blu-ray Discs, as well as on the Internet and mobile devices. Trailers are created to be engaging and interesting for viewers. As a result, in the Internet era, viewers often seek out trailers to watch them. Of the ten billion videos watched online annually in 2008, film trailers ranked third, after news and user-created videos. A teaser is a much shorter preview or advertisement that lasts only 10 to 30 seconds. Teasers are used to get patrons excited about a film coming out in the next six to twelve months. Teasers may be produced even before the film production is completed. Culture Films are cultural artifacts created by specific cultures, facilitating intercultural dialogue. It is considered to be an important art form that provides entertainment and historical value, often visually documenting a period of time. The visual basis of the medium gives it a universal power of communication, often stretched further through the use of dubbing or subtitles to translate the dialog into other languages. Just seeing a location in a film is linked to higher tourism to that location, demonstrating how powerful the suggestive nature of the medium can be. Education and propaganda Film is used for a range of goals, including education and propaganda due its ability to effectively intercultural dialogue. When the purpose is primarily educational, a film is called an "educational film". Examples are recordings of academic lectures and experiments, or a film based on a classic novel. Film may be propaganda, in whole or in part, such as the films made by Leni Riefenstahl in Nazi Germany, US war film trailers during World War II, or artistic films made under Stalin by Sergei Eisenstein. They may also be works of political protest, as in the films of Andrzej Wajda, or more subtly, the films of Andrei Tarkovsky. The same film may be considered educational by some, and propaganda by others as the categorization of a film can be subjective. Production At its core, the means to produce a film depend on the content the filmmaker wishes to show, and the apparatus for displaying it: the zoetrope merely requires a series of images on a strip of paper. Film production can, therefore, take as little as one person with a camera (or even without a camera, as in Stan Brakhage's 1963 film Mothlight), or thousands of actors, extras, and crew members for a live-action, feature-length epic. The necessary steps for almost any film can be boiled down to conception, planning, execution, revision, and distribution. The more involved the production, the more significant each of the steps becomes. In a typical production cycle of a Hollywood-style film, these main stages are defined as development, pre-production, production, post-production and distribution. This production cycle usually takes three years. The first year is taken up with development. The second year comprises preproduction and production. The third year, post-production and distribution. The bigger the production, the more resources it takes, and the more important financing becomes; most feature films are artistic works from the creators' perspective (e.g., film director, cinematographer, screenwriter) and for-profit business entities for the production companies. Crew A film crew is a group of people hired by a film company, employed during the "production" or "photography" phase, for the purpose of producing a film or motion picture. Crew is distinguished from cast, who are the actors who appear in front of the camera or provide voices for characters in the film. The crew interacts with but is also distinct from the production staff, consisting of producers, managers, company representatives, their assistants, and those whose primary responsibility falls in pre-production or post-production phases, such as screenwriters and film editors. Communication between production and crew generally passes through the director and his/her staff of assistants. Medium-to-large crews are generally divided into departments with well-defined hierarchies and standards for interaction and cooperation between the departments. Other than acting, the crew handles everything in the photography phase: props and costumes, shooting, sound, electrics (i.e., lights), sets, and production special effects. Caterers (known in the film industry as "craft services") are usually not considered part of the crew. Technology Film stock consists of transparent celluloid, acetate, or polyester base coated with an emulsion containing light-sensitive chemicals. Cellulose nitrate was the first type of film base used to record motion pictures, but due to its flammability was eventually replaced by safer materials. Stock widths and the film format for images on the reel have had a rich history, though most large commercial films are still shot on (and distributed to theaters) as 35 mm prints. Originally moving picture film was shot and projected at various speeds using hand-cranked cameras and projectors; though 1000 frames per minute (⁠16+2/3⁠ frame/s) is generally cited as a standard silent speed, research indicates most films were shot between 16 frame/s and 23 frame/s and projected from 18 frame/s on up (often reels included instructions on how fast each scene should be shown). When synchronized sound film was introduced in the late 1920s, a constant speed was required for the sound head. 24 frames per second were chosen because it was the slowest (and thus cheapest) speed which allowed for sufficient sound quality. The standard was set with Warner Bros.'s The Jazz Singer and their Vitaphone system in 1927. Improvements since the late 19th century include the mechanization of cameras – allowing them to record at a consistent speed, quiet camera design – allowing sound recorded on-set to be usable without requiring large "blimps" to encase the camera, the invention of more sophisticated filmstocks and lenses, allowing directors to film in increasingly dim conditions, and the development of synchronized sound, allowing sound to be recorded at exactly the same speed as its corresponding action. The soundtrack can be recorded separately from shooting the film, but for live-action pictures, many parts of the soundtrack are usually recorded simultaneously. As a medium, film is not limited to motion pictures, since the technology developed as the basis for photography. It can be used to present a progressive sequence of still images in the form of a slideshow. Film has also been incorporated into multimedia presentations and often has importance as primary historical documentation. However, historic films have problems in terms of preservation and storage, and the motion picture industry is exploring many alternatives. Most films on cellulose nitrate base have been copied onto modern safety films. Some studios save color films through the use of separation masters: three B&W negatives each exposed through red, green, or blue filters (essentially a reverse of the Technicolor process). Digital methods have also been used to restore films, although their continued obsolescence cycle makes them (as of 2006) a poor choice for long-term preservation. Film preservation of decaying film stock is a matter of concern to both film historians and archivists and to companies interested in preserving their existing products in order to make them available to future generations (and thereby increase revenue). Preservation is generally a higher concern for nitrate and single-strip color films, due to their high decay rates; black-and-white films on safety bases and color films preserved on Technicolor imbibition prints tend to keep up much better, assuming proper handling and storage. Some films in recent decades have been recorded using analog video technology similar to that used in television production. Modern digital video cameras and digital projectors are gaining ground as well. These approaches are preferred by some film-makers, especially because footage shot with digital cinema can be evaluated and edited with non-linear editing systems (NLE) without waiting for the film stock to be processed. The migration was gradual, and as of 2005, most major motion pictures were still shot on film. Independent Independent filmmaking often takes place outside Hollywood, or other major studio systems. An independent film (or indie film) is a film initially produced without financing or distribution from a major film studio. Creative, business and technological reasons have all contributed to the growth of the indie film scene in the late 20th and early 21st century. On the business side, the costs of big-budget studio films also lead to conservative choices in cast and crew. There is a trend in Hollywood towards co-financing (over two-thirds of the films put out by Warner Bros. in 2000 were joint ventures, up from 10% in 1987). A hopeful director is almost never given the opportunity to get a job on a big-budget studio film without significant industry experience in film or television. Also, the studios rarely produce films with unknown actors, particularly in lead roles. Before the advent of digital alternatives, the cost of professional film equipment and stock was also a hurdle to being able to produce, direct, or star in a traditional studio film. But the advent of consumer camcorders in 1985, and more importantly, the arrival of high-resolution digital video in the early 1990s, have lowered the technology barrier to film production significantly. Both production and post-production costs have been significantly lowered; in the 2000s, the hardware and software for post-production can be installed in a commodity-based personal computer. Technologies such as DVDs, FireWire connections and a wide variety of professional and consumer-grade video editing software make film-making relatively affordable. Since the introduction of digital video DV technology, the means of production have become more democratized. Filmmakers can conceivably shoot a film with a digital video camera and edit the film, create and edit the sound and music, and mix the final cut on a high-end home computer. However, while the means of production may be democratized, financing, distribution, and marketing remain difficult to accomplish outside the traditional system. Most independent filmmakers rely on film festivals to get their films noticed and sold for distribution. The arrival of internet-based video websites such as YouTube and Veoh has further changed the filmmaking landscape, enabling indie filmmakers to make their films available to the public. Open content film An open content film is much like an independent film, but it is produced through open collaborations; its source material is available under a license which is permissive enough to allow other parties to create fan fiction or derivative works rather than a traditional copyright. Like independent filmmaking, open source filmmaking takes place outside Hollywood and other major studio systems. Fan film A fan film is a film or video inspired by a film, television program, comic book or a similar source, created by fans rather than by the source's copyright holders or creators. Fan filmmakers have traditionally been amateurs, but some of the most notable films have actually been produced by professional filmmakers as film school class projects or as demonstration reels. Fan films vary tremendously in length, from short faux-teaser trailers for non-existent motion pictures to rarer full-length motion pictures. Distribution Film distribution is the process through which a film is made available for viewing by an audience. This is normally the task of a professional film distributor, who would determine the marketing strategy of the film, the media by which a film is to be exhibited or made available for viewing, and may set the release date and other matters. The film may be exhibited directly to the public either through a movie theater (historically the main way films were distributed) or television for personal home viewing (including on DVD-Video or Blu-ray Disc, video-on-demand, online downloading, television programs through broadcast syndication etc.). Other ways of distributing a film include rental or personal purchase of the film in a variety of media and formats, such as VHS tape or DVD, or Internet downloading or streaming using a computer. Animation Animation is a technique in which each frame of a film is produced individually, whether generated as a computer graphic, or by photographing a drawn image, or by repeatedly making small changes to a model unit (see claymation and stop motion), and then photographing the result with a special animation camera. When the frames are strung together and the resulting film is viewed at a speed of 16 or more frames per second, there is an illusion of continuous movement (due to the phi phenomenon). Generating such a film is very labor-intensive and tedious, though the development of computer animation has greatly sped up the process. Because animation is very time-consuming and often very expensive to produce, the majority of animation for TV and films comes from professional animation studios. However, the field of independent animation has existed at least since the 1950s, with animation being produced by independent studios (and sometimes by a single person). Several independent animation producers have gone on to enter the professional animation industry. Limited animation is a way of increasing production and decreasing costs of animation by using "short cuts" in the animation process. This method was pioneered by UPA and popularized by Hanna-Barbera in the United States, and by Osamu Tezuka in Japan, and adapted by other studios as cartoons moved from movie theaters to television. Although most animation studios are now using digital technologies in their productions, there is a specific style of animation that depends on film. Camera-less animation, made famous by film-makers like Norman McLaren, Len Lye, and Stan Brakhage, is painted and drawn directly onto pieces of film, and then run through a projector. See also Docufiction (hybrid genre) Filmophile Lists Bibliography of film by genre Glossary of motion picture terms Index of video-related articles List of film awards List of film festivals List of film periodicals List of years in film Lists of films List of books on films Outline of film Lost film The Movies, a simulation game about the film industry, taking place at the dawn of cinema Platforms Television film Web film Notes Citations References Further reading External links Allmovie – Information on films: actors, directors, biographies, reviews, cast and production credits, box office sales, and other movie data. Film Site – Reviews of classic films Rottentomatoes.com – Movie reviews, previews, forums, photos, cast info, and more. IMDb: The Internet Movie Database – Information on current and historical films and cast listings (archived 22 January 1997)

Theatre or theater is a collaborative form of performing art that uses live performers, usually actors, to present experiences of a real or imagined event before a live audience in a specific place, often a stage. The performers may communicate this experience to the audience through combinations of gesture, speech, song, music, and dance. It is the oldest form of drama, though live theatre has now been joined by modern recorded forms. Elements of art, such as painted scenery and stagecraft such as lighting are used to enhance the physicality, presence and immediacy of the experience. Places, normally buildings, where performances regularly take place are also called "theatres" (or "theaters"), as derived from the Ancient Greek θέατρον (théatron, "a place for viewing"), itself from θεάομαι (theáomai, "to see", "to watch", "to observe"). Modern Western theatre comes, in large measure, from the theatre of ancient Greece, from which it borrows technical terminology, classification into genres, and many of its themes, stock characters, and plot elements. Theatre artist Patrice Pavis defines theatricality, theatrical language, stage writing and the specificity of theatre as synonymous expressions that differentiate theatre from the other performing arts, literature and the arts in general. A theatre company is an organisation that produces theatrical performances, as distinct from a theatre troupe (or acting company), which is a group of theatrical performers working together. Modern theatre includes performances of plays and musical theatre. The art forms of ballet and opera are also theatre and use many conventions such as acting, costumes and staging. They were influential in the development of musical theatre. History of theatre Classical, Hellenistic Greece and Magna Graecia The city-state of Athens is where Western theatre originated. It was part of a broader culture of theatricality and performance in classical Greece that included festivals, religious rituals, politics, law, athletics and gymnastics, music, poetry, weddings, funerals, and symposia. Participation in the city-state's many festivals—and mandatory attendance at the City Dionysia as an audience member (or even as a participant in the theatrical productions) in particular—was an important part of citizenship. Civic participation also involved the evaluation of the rhetoric of orators evidenced in performances in the law-court or political assembly, both of which were understood as analogous to the theatre and increasingly came to absorb its dramatic vocabulary. The Greeks also developed the concepts of dramatic criticism and theatre architecture. Actors were either amateur or at best semi-professional. The theatre of ancient Greece consisted of three types of drama: tragedy, comedy, and the satyr play. The origins of theatre in ancient Greece, according to Aristotle (384–322 BCE), the first theoretician of theatre, are to be found in the festivals that honoured Dionysus. The performances were given in semi-circular auditoria cut into hillsides, capable of seating 10,000–20,000 people. The stage consisted of a dancing floor (orchestra), dressing room and scene-building area (skene). Since the words were the most important part, good acoustics and clear delivery were paramount. The actors (always men) wore masks appropriate to the characters they represented, and each might play several parts. Athenian tragedy—the oldest surviving form of tragedy—is a type of dance-drama that formed an important part of the theatrical culture of the city-state. Having emerged sometime during the 6th century BCE, it flowered during the 5th century BCE (from the end of which it began to spread throughout the Greek world), and continued to be popular until the beginning of the Hellenistic period. No tragedies from the 6th century BCE and only 32 of the more than a thousand that were performed in during the 5th century BCE have survived. We have complete texts extant by Aeschylus, Sophocles, and Euripides. The origins of tragedy remain obscure, though by the 5th century BCE it was institutionalized in competitions (agon) held as part of festivities celebrating Dionysus (the god of wine and fertility). As contestants in the City Dionysia's competition (the most prestigious of the festivals to stage drama) playwrights were required to present a tetralogy of plays (though the individual works were not necessarily connected by story or theme), which usually consisted of three tragedies and one satyr play. The performance of tragedies at the City Dionysia may have begun as early as 534 BCE; official records (didaskaliai) begin from 501 BCE, when the satyr play was introduced. Most Athenian tragedies dramatize events from Greek mythology, though The Persians—which stages the Persian response to news of their military defeat at the Battle of Salamis in 480 BCE—is the notable exception in the surviving drama. When Aeschylus won first prize for it at the City Dionysia in 472 BCE, he had been writing tragedies for more than 25 years, yet its tragic treatment of recent history is the earliest example of drama to survive. More than 130 years later, the philosopher Aristotle analysed 5th-century Athenian tragedy in the oldest surviving work of dramatic theory—his Poetics (c. 335 BCE). Athenian comedy is conventionally divided into three periods, "Old Comedy", "Middle Comedy", and "New Comedy". Old Comedy survives today largely in the form of the eleven surviving plays of Aristophanes, while Middle Comedy is largely lost (preserved only in relatively short fragments in authors such as Athenaeus of Naucratis). New Comedy is known primarily from the substantial papyrus fragments of Menander. Aristotle defined comedy as a representation of laughable people that involves some kind of blunder or ugliness that does not cause pain or disaster. In addition to the categories of comedy and tragedy at the City Dionysia, the festival also included the Satyr Play. Finding its origins in rural, agricultural rituals dedicated to Dionysus, the satyr play eventually found its way to Athens in its most well-known form. Satyr's themselves were tied to the god Dionysus as his loyal woodland companions, often engaging in drunken revelry and mischief at his side. The satyr play itself was classified as tragicomedy, erring on the side of the more modern burlesque traditions of the early twentieth century. The plotlines of the plays were typically concerned with the dealings of the pantheon of Gods and their involvement in human affairs, backed by the chorus of Satyrs. However, according to Webster, satyr actors did not always perform typical satyr actions and would break from the acting traditions assigned to the character type of a mythical forest creature. The Greek colonists in Southern Italy, the so-called Magna Graecia, brought theatrical art from their motherland. The Greek Theatre of Syracuse, the Greek Theatre of Segesta, the Greek Theatre of Tindari, the Greek Theatre of Hippana, the Greek Theatre of Akrai, the Greek Theatre of Monte Jato, the Greek Theatre of Morgantina and the most famous Greek Theater of Taormina, amply demonstrate this. Only fragments of original dramaturgical works are left, but the tragedies of the three great giants Aeschylus, Sophocles and Euripides and the comedies of Aristophanes are known. Some famous playwrights in the Greek language came directly from Magna Graecia. Others, such as Aeschylus and Epicharmus, worked for a long time in Sicily. Epicharmus can be considered Syracusan in all respects, having worked all his life with the tyrants of Syracuse. His comedy preceded that of the more famous Aristophanes by staging the gods for the first time in comedy. While Aeschylus, after a long stay in the Sicilian colonies, died in Sicily in the colony of Gela in 456 BC. Epicarmus and Phormis, both of 6th century BC, are the basis, for Aristotle, of the invention of the Greek comedy, as he says in his book on Poetics: As for the composition of the stories (Epicharmus and Phormis) it came in the beginning from Sicily Other native dramatic authors of Magna Graecia, in addition to the Syracusan Formides mentioned, are Achaeus of Syracuse, Apollodorus of Gela, Philemon of Syracuse and his son Philemon the younger. From Calabria, precisely from the colony of Thurii, came the playwright Alexis. While Rhinthon, although Sicilian from Syracuse, worked almost exclusively for the colony of Taranto in Apulia. Roman theatre Western theatre developed and expanded considerably under the Romans. The Roman historian Livy wrote that the Romans first experienced theatre in the 4th century BC, with a performance by Etruscan actors. Beacham argues that Romans had been familiar with "pre-theatrical practices" for some time before that recorded contact. The theatre of ancient Rome was a thriving and diverse art form, ranging from festival performances of street theatre, nude dancing, and acrobatics, to the staging of Plautus's broadly appealing situation comedies, to the high-style, verbally elaborate tragedies of Seneca. Although Rome had a native tradition of performance, the Hellenization of Roman culture in the 3rd century BC had a profound and energizing effect on Roman theatre and encouraged the development of Latin literature of the highest quality for the stage. Following the expansion of the Roman Republic (509–27 BC) into several Greek territories between 270 and 240 BC, Rome encountered Greek drama. From the later years of the republic and by means of the Roman Empire (27 BC-476 AD), theatre spread west across Europe, around the Mediterranean and reached England; Roman theatre was more varied, extensive and sophisticated than that of any culture before it. While Greek drama continued to be performed throughout the Roman period, the year 240 BC marks the beginning of regular Roman drama. From the beginning of the empire, however, interest in full-length drama declined in favour of a broader variety of theatrical entertainments. The first important works of Roman literature were the tragedies and comedies that Livius Andronicus wrote from 240 BC. Five years later, Gnaeus Naevius also began to write drama. No plays from either writer have survived. While both dramatists composed in both genres, Andronicus was most appreciated for his tragedies and Naevius for his comedies; their successors tended to specialise in one or the other, which led to a separation of the subsequent development of each type of drama. By the beginning of the 2nd century BC, drama was firmly established in Rome and a guild of writers (collegium poetarum) had been formed. The Roman comedies that have survived are all fabula palliata (comedies based on Greek subjects) and come from two dramatists: Titus Maccius Plautus (Plautus) and Publius Terentius Afer (Terence). In re-working the Greek originals, the Roman comic dramatists abolished the role of the chorus in dividing the drama into episodes and introduced musical accompaniment to its dialogue (between one-third of the dialogue in the comedies of Plautus and two-thirds in those of Terence). The action of all scenes is set in the exterior location of a street and its complications often follow from eavesdropping. Plautus, the more popular of the two, wrote between 205 and 184 BC and twenty of his comedies survive, of which his farces are best known; he was admired for the wit of his dialogue and his use of a variety of poetic meters. All of the six comedies that Terence wrote between 166 and 160 BC have survived; the complexity of his plots, in which he often combined several Greek originals, was sometimes denounced, but his double-plots enabled a sophisticated presentation of contrasting human behaviour. No early Roman tragedy survives, though it was highly regarded in its day; historians know of three early tragedians—Quintus Ennius, Marcus Pacuvius and Lucius Accius. From the time of the empire, the work of two tragedians survives—one is an unknown author, while the other is the Stoic philosopher Seneca. Nine of Seneca's tragedies survive, all of which are fabula crepidata (tragedies adapted from Greek originals); his Phaedra, for example, was based on Euripides' Hippolytus. Historians do not know who wrote the only extant example of the fabula praetexta (tragedies based on Roman subjects), Octavia, but in former times it was mistakenly attributed to Seneca due to his appearance as a character in the tragedy. In contrast to Ancient Greek theatre, the theatre in Ancient Rome did allow female performers. While the majority were employed for dancing and singing, a minority of actresses are known to have performed speaking roles, and there were actresses who achieved wealth, fame and recognition for their art, such as Eucharis, Dionysia, Galeria Copiola and Fabia Arete: they also formed their own acting guild, the Sociae Mimae, which was evidently quite wealthy. Indian theatre The first form of Indian theatre was the Sanskrit theatre, earliest-surviving fragments of which date from the 1st century CE. It began after the development of Greek and Roman theatre and before the development of theatre in other parts of Asia. It emerged sometime between the 2nd century BCE and the 1st century CE and flourished between the 1st century CE and the 10th, which was a period of relative peace in the history of India during which hundreds of plays were written. The wealth of archeological evidence from earlier periods offers no indication of the existence of a tradition of theatre. The ancient Vedas (hymns from between 1500 and 1000 BCE that are among the earliest examples of literature in the world) contain no hint of it (although a small number are composed in a form of dialogue) and the rituals of the Vedic period do not appear to have developed into theatre. The Mahābhāṣya by Patañjali contains the earliest reference to what may have been the seeds of Sanskrit drama. This treatise on grammar from 140 BCE provides a feasible date for the beginnings of theatre in India. The major source of evidence for Sanskrit theatre is A Treatise on Theatre (Nātyaśāstra), a compendium whose date of composition is uncertain (estimates range from 200 BCE to 200 CE) and whose authorship is attributed to Bharata Muni. The Treatise is the most complete work of dramaturgy in the ancient world. It addresses acting, dance, music, dramatic construction, architecture, costuming, make-up, props, the organisation of companies, the audience, competitions, and offers a mythological account of the origin of theatre. In doing so, it provides indications about the nature of actual theatrical practices. Sanskrit theatre was performed on sacred ground by priests who had been trained in the necessary skills (dance, music, and recitation) in a [hereditary process]. Its aim was both to educate and to entertain. Under the patronage of royal courts, performers belonged to professional companies that were directed by a stage manager (sutradhara), who may also have acted. This task was thought of as being analogous to that of a puppeteer—the literal meaning of "sutradhara" is "holder of the strings or threads". The performers were trained rigorously in vocal and physical technique. There were no prohibitions against female performers; companies were all-male, all-female, and of mixed gender. Certain sentiments were considered inappropriate for men to enact, however, and were thought better suited to women. Some performers played characters their own age, while others played ages different from their own (whether younger or older). Of all the elements of theatre, the Treatise gives most attention to acting (abhinaya), which consists of two styles: realistic (lokadharmi) and conventional (natyadharmi), though the major focus is on the latter. Its drama is regarded as the highest achievement of Sanskrit literature. It utilised stock characters, such as the hero (nayaka), heroine (nayika), or clown (vidusaka). Actors may have specialized in a particular type. Kālidāsa in the 1st century BCE, is arguably considered to be ancient India's greatest Sanskrit dramatist. Three famous romantic plays written by Kālidāsa are the Mālavikāgnimitram (Mālavikā and Agnimitra), Vikramuurvashiiya (Pertaining to Vikrama and Urvashi), and Abhijñānaśākuntala (The Recognition of Shakuntala). The last was inspired by a story in the Mahabharata and is the most famous. It was the first to be translated into English and German. Śakuntalā (in English translation) influenced Goethe's Faust (1808–1832). The next great Indian dramatist was Bhavabhuti (c. 7th century CE). He is said to have written the following three plays: Malati-Madhava, Mahaviracharita and Uttar Ramacharita. Among these three, the last two cover between them the entire epic of Ramayana. The powerful Indian emperor Harsha (606–648) is credited with having written three plays: the comedy Ratnavali, Priyadarsika, and the Buddhist drama Nagananda. East Asian theatre The Tang dynasty is sometimes known as "The Age of 1000 Entertainments". During this era, Ming Huang formed an acting school known as The Pear Garden to produce a form of drama that was primarily musical. That is why actors are commonly called "Children of the Pear Garden". During the dynasty of Empress Ling, shadow puppetry first emerged as a recognized form of theatre in China. There were two distinct forms of shadow puppetry, Pekingese (northern) and Cantonese (southern). The two styles were differentiated by the method of making the puppets and the positioning of the rods on the puppets, as opposed to the type of play performed by the puppets. Both styles generally performed plays depicting great adventure and fantasy, rarely was this very stylized form of theatre used for political propaganda. Japanese forms of Kabuki, Nō, and Kyōgen developed in the 17th century CE. Cantonese shadow puppets were the larger of the two. They were built using thick leather which created more substantial shadows. Symbolic colour was also very prevalent; a black face represented honesty, a red one bravery. The rods used to control Cantonese puppets were attached perpendicular to the puppets' heads. Thus, they were not seen by the audience when the shadow was created. Pekingese puppets were more delicate and smaller. They were created out of thin, translucent leather (usually taken from the belly of a donkey). They were painted with vibrant paints, thus they cast a very colourful shadow. The thin rods which controlled their movements were attached to a leather collar at the neck of the puppet. The rods ran parallel to the bodies of the puppet and then turned at a ninety degree angle to connect to the neck. While these rods were visible when the shadow was cast, they laid outside the shadow of the puppet; thus they did not interfere with the appearance of the figure. The rods are attached at the necks to facilitate the use of multiple heads with one body. When the heads were not being used, they were stored in a muslin book or fabric-lined box. The heads were always removed at night. This was in keeping with the old superstition that if left intact, the puppets would come to life at night. Some puppeteers went so far as to store the heads in one book and the bodies in another, to further reduce the possibility of reanimating puppets. Shadow puppetry is said to have reached its highest point of artistic development in the eleventh century before becoming a tool of the government. In the Song dynasty, there were many popular plays involving acrobatics and music. These developed in the Yuan dynasty into a more sophisticated form known as zaju, with a four- or five-act structure. Yuan drama spread across China and diversified into numerous regional forms, one of the best known of which is Peking Opera which is still popular today. Xiangsheng is a certain traditional Chinese comedic performance in the forms of monologue or dialogue. Indonesian theatre In Indonesia, theatre performances have become an important part of local culture, theatre performances in Indonesia have been developed for thousands of years. Most of Indonesia's oldest theatre forms are linked directly to local literary traditions (oral and written). The prominent puppet theatres—wayang golek (wooden rod-puppet play) of the Sundanese and wayang kulit (leather shadow-puppet play) of the Javanese and Balinese—draw much of their repertoire from indigenized versions of the Ramayana and Mahabharata. These tales also provide source material for the wayang wong (human theatre) of Java and Bali, which uses actors. Some wayang golek performances, however, also present Muslim stories, called menak. Wayang is an ancient form of storytelling that renowned for its elaborate puppet/human and complex musical styles. The earliest evidence is from the late 1st millennium CE, in medieval-era texts and archeological sites. The oldest known record that concerns wayang is from the 9th century. Around 840 AD an Old Javanese (Kawi) inscriptions called Jaha Inscriptions issued by Maharaja Sri Lokapala from Mataram Kingdom in Central Java mentions three sorts of performers: atapukan, aringgit, and abanol. Aringgit means Wayang puppet show, Atapukan means Mask dance show, and abanwal means joke art. Ringgit is described in an 11th-century Javanese poem as a leather shadow figure. Medieval Islamic traditions Theatre in the medieval Islamic world included puppet theatre (which included hand puppets, shadow plays and marionette productions) and live passion plays known as ta'ziyeh, where actors re-enact episodes from Muslim history. In particular, Shia Islamic plays revolved around the istishhād (martyrdom) of Ali's sons Hasan ibn Ali and Husayn ibn Ali. Secular plays were known as akhraja, recorded in medieval adab literature, though they were less common than puppetry and ta'ziya theatre. Early modern and modern theatre in the West Theatre took on many alternative forms in the West between the 15th and 19th centuries, including commedia dell'arte from Italian theatre, and melodrama. The general trend was away from the poetic drama of the Greeks and the Renaissance and toward a more naturalistic prose style of dialogue, especially following the Industrial Revolution. Theatre took a big pause during 1642 and 1660 in England because of the Puritan Interregnum. The rising anti-theatrical sentiment among Puritans saw William Prynne write Histriomastix (1633), the most notorious attack on theatre prior to the ban. Viewing theatre as sinful, the Puritans ordered the closure of London theatres in 1642. On 24 January 1643, the actors protested against the ban by writing a pamphlet titled The Actors remonstrance or complaint for the silencing of their profession, and banishment from their severall play-houses. This stagnant period ended once Charles II came back to the throne in 1660 in the Restoration. Theatre (among other arts) exploded, with influence from French culture, since Charles had been exiled in France in the years previous to his reign. In 1660, two companies were licensed to perform, the Duke's Company and the King's Company. Performances were held in converted buildings, such as Lisle's Tennis Court. The first West End theatre, known as Theatre Royal in Covent Garden, London, was designed by Thomas Killigrew and built on the site of the present Theatre Royal, Drury Lane. One of the big changes was the new theatre house. Instead of the type of the Elizabethan era, such as the Globe Theatre, round with no place for the actors to prepare for the next act and with no "theatre manners", the theatre house became transformed into a place of refinement, with a stage in front and stadium seating facing it. Since seating was no longer all the way around the stage, it became prioritized—some seats were obviously better than others. The king would have the best seat in the house: the very middle of the theatre, which got the widest view of the stage as well as the best way to see the point of view and vanishing point that the stage was constructed around. Philippe Jacques de Loutherbourg was one of the most influential set designers of the time because of his use of floor space and scenery. Because of the turmoil before this time, there was still some controversy about what should and should not be put on the stage. Jeremy Collier, a preacher, was one of the heads in this movement through his piece A Short View of the Immorality and Profaneness of the English Stage. The beliefs in this paper were mainly held by non-theatre goers and the remainder of the Puritans and very religious of the time. The main question was if seeing something immoral on stage affects behaviour in the lives of those who watch it, a controversy that is still playing out today. The seventeenth century had also introduced women to the stage, which was considered inappropriate earlier. These women were regarded as celebrities (also a newer concept, thanks to ideas on individualism that arose in the wake of Renaissance Humanism), but on the other hand, it was still very new and revolutionary that they were on the stage, and some said they were unladylike, and looked down on them. Charles II did not like young men playing the parts of young women, so he asked that women play their own parts. Because women were allowed on the stage, playwrights had more leeway with plot twists, like women dressing as men, and having narrow escapes from morally sticky situations as forms of comedy. Comedies were full of the young and very much in vogue, with the storyline following their love lives: commonly a young roguish hero professing his love to the chaste and free minded heroine near the end of the play, much like Sheridan's The School for Scandal. Many of the comedies were fashioned after the French tradition, mainly Molière, again hailing back to the French influence brought back by the King and the Royals after their exile. Molière was one of the top comedic playwrights of the time, revolutionizing the way comedy was written and performed by combining Italian commedia dell'arte and neoclassical French comedy to create some of the longest lasting and most influential satiric comedies. Tragedies were similarly victorious in their sense of righting political power, especially poignant because of the recent Restoration of the Crown. They were also imitations of French tragedy, although the French had a larger distinction between comedy and tragedy, whereas the English fudged the lines occasionally and put some comedic parts in their tragedies. Common forms of non-comedic plays were sentimental comedies as well as something that would later be called tragédie bourgeoise, or domestic tragedy—that is, the tragedy of common life—were more popular in England because they appealed more to English sensibilities. While theatre troupes were formerly often travelling, the idea of the national theatre gained support in the 18th century, inspired by Ludvig Holberg. The major promoter of the idea of the national theatre in Germany, and also of the Sturm und Drang poets, was Abel Seyler, the owner of the Hamburgische Entreprise and the Seyler Theatre Company. Through the 19th century, the popular theatrical forms of Romanticism, melodrama, Victorian burlesque and the well-made plays of Scribe and Sardou gave way to the problem plays of Naturalism and Realism; the farces of Feydeau; Wagner's operatic Gesamtkunstwerk; musical theatre (including Gilbert and Sullivan's operas); F. C. Burnand's, W. S. Gilbert's and Oscar Wilde's drawing-room comedies; Symbolism; proto-Expressionism in the late works of August Strindberg and Henrik Ibsen; and Edwardian musical comedy. These trends continued through the 20th century in the realism of Stanislavski and Lee Strasberg, the political theatre of Erwin Piscator and Bertolt Brecht, the so-called Theatre of the Absurd of Samuel Beckett and Eugène Ionesco, American and British musicals, the collective creations of companies of actors and directors such as Joan Littlewood's Theatre Workshop, experimental and postmodern theatre of Robert Wilson and Robert Lepage, the postcolonial theatre of August Wilson or Tomson Highway, and Augusto Boal's Theatre of the Oppressed. Types Drama Drama is the specific mode of fiction represented in performance. The term comes from a Greek word meaning "action", which is derived from the verb δράω, dráō, "to do" or "to act". The enactment of drama in theatre, performed by actors on a stage before an audience, presupposes collaborative modes of production and a collective form of reception. The structure of dramatic texts, unlike other forms of literature, is directly influenced by this collaborative production and collective reception. The early modern tragedy Hamlet (1601) by Shakespeare and the classical Athenian tragedy Oedipus Rex (c. 429 BCE) by Sophocles are among the masterpieces of the art of drama. A modern example is Long Day's Journey into Night by Eugene O'Neill (1956). Considered as a genre of poetry in general, the dramatic mode has been contrasted with the epic and the lyrical modes ever since Aristotle's Poetics (c. 335 BCE); the earliest work of dramatic theory. The use of "drama" in the narrow sense to designate a specific type of play dates from the 19th century. Drama in this sense refers to a play that is neither a comedy nor a tragedy—for example, Zola's Thérèse Raquin (1873) or Chekhov's Ivanov (1887). In Ancient Greece however, the word drama encompassed all theatrical plays, tragic, comic, or anything in between. Drama is often combined with music and dance: the drama in opera is generally sung throughout; musicals generally include both spoken dialogue and songs; and some forms of drama have incidental music or musical accompaniment underscoring the dialogue (melodrama and Japanese Nō, for example). In certain periods of history (the ancient Roman and modern Romantic) some dramas have been written to be read rather than performed. In improvisation, the drama does not pre-exist the moment of performance; performers devise a dramatic script spontaneously before an audience. Musical theatre Music and theatre have had a close relationship since ancient times—Athenian tragedy, for example, was a form of dance-drama that employed a chorus whose parts were sung (to the accompaniment of an aulos—an instrument comparable to the modern oboe), as were some of the actors' responses and their 'solo songs' (monodies). Modern musical theatre is a form of theatre that also combines music, spoken dialogue, and dance. It emerged from comic opera (especially Gilbert and Sullivan), variety, vaudeville, and music hall genres of the late 19th and early 20th century. After the Edwardian musical comedy that began in the 1890s, the Princess Theatre musicals of the early 20th century, and comedies in the 1920s and 1930s (such as the works of Rodgers and Hammerstein), with Oklahoma! (1943), musicals moved in a more dramatic direction. Famous musicals over the subsequent decades included My Fair Lady (1956), West Side Story (1957), The Fantasticks (1960), Hair (1967), A Chorus Line (1975), Les Misérables (1980), Cats (1981), Into the Woods (1986), and The Phantom of the Opera (1986), as well as more contemporary hits including Rent (1994), The Lion King (1997), Wicked (2003), Hamilton (2015) and Frozen (2018). Musical theatre may be produced on an intimate scale Off-Broadway, in regional theatres, and elsewhere, but it often includes spectacle. For instance, Broadway and West End musicals often include lavish costumes and sets supported by multimillion-dollar budgets. Comedy Theatre productions that use humour as a vehicle to tell a story qualify as comedies. This may include a modern farce such as Boeing Boeing or a classical play such as As You Like It. Theatre expressing bleak, controversial or taboo subject matter in a deliberately humorous way is referred to as black comedy. Black Comedy can have several genres like slapstick humour, dark and sarcastic comedy. Tragedy Tragedy, then, is an imitation of an action that is serious, complete, and of a certain magnitude: in language embellished with each kind of artistic ornament, the several kinds being found in separate parts of the play; in the form of action, not of narrative; through pity and fear effecting the proper purgation of these emotions. Aristotle's phrase "several kinds being found in separate parts of the play" is a reference to the structural origins of drama. In it the spoken parts were written in the Attic dialect whereas the choral (recited or sung) ones in the Doric dialect, these discrepancies reflecting the differing religious origins and poetic metres of the parts that were fused into a new entity, the theatrical drama. Tragedy refers to a specific tradition of drama that has played a unique and important role historically in the self-definition of Western civilisation. That tradition has been multiple and discontinuous, yet the term has often been used to invoke a powerful effect of cultural identity and historical continuity—"the Greeks and the Elizabethans, in one cultural form; Hellenes and Christians, in a common activity", as Raymond Williams puts it. From its obscure origins in the theatres of Athens 2,500 years ago, from which there survives only a fraction of the work of Aeschylus, Sophocles and Euripides, through its singular articulations in the works of Shakespeare, Lope de Vega, Racine, and Schiller, to the more recent naturalistic tragedy of Strindberg, Beckett's modernist meditations on death, loss and suffering, and Müller's postmodernist reworkings of the tragic canon, tragedy has remained an important site of cultural experimentation, negotiation, struggle, and change. In the wake of Aristotle's Poetics (335 BCE), tragedy has been used to make genre distinctions, whether at the scale of poetry in general (where the tragic divides against epic and lyric) or at the scale of the drama (where tragedy is opposed to comedy). In the modern era, tragedy has also been defined against drama, melodrama, the tragicomic, and epic theatre. Improvisation Improvisation has been a consistent feature of theatre, with the Commedia dell'arte in the sixteenth century being recognized as the first improvisation form. Popularized by 1997 Nobel Prize in Literature winner Dario Fo and troupes such as the Upright Citizens Brigade improvisational theatre continues to evolve with many different streams and philosophies. Keith Johnstone and Viola Spolin are recognized as the first teachers of improvisation in modern times, with Johnstone exploring improvisation as an alternative to scripted theatre and Spolin and her successors exploring improvisation principally as a tool for developing dramatic work or skills or as a form for situational comedy. Spolin also became interested in how the process of learning improvisation was applicable to the development of human potential. Spolin's son, Paul Sills popularized improvisational theatre as a theatrical art form when he founded, as its first director, The Second City in Chicago. Theories Having been an important part of human culture for more than 2,500 years, theatre has evolved a wide range of different theories and practices. Some are related to political or spiritual ideologies, while others are based purely on "artistic" concerns. Some processes focus on a story, some on theatre as event, and some on theatre as catalyst for social change. The classical Greek philosopher Aristotle, in his seminal treatise, Poetics (c. 335 BCE) is the earliest-surviving example and its arguments have influenced theories of theatre ever since. In it, he offers an account of what he calls "poetry" (a term which in Greek literally means "making" and in this context includes drama—comedy, tragedy, and the satyr play—as well as lyric poetry, epic poetry, and the dithyramb). He examines its "first principles" and identifies its genres and basic elements; his analysis of tragedy constitutes the core of the discussion. Aristotle argues that tragedy consists of six qualitative parts, which are (in order of importance) mythos or "plot", ethos or "character", dianoia or "thought", lexis or "diction", melos or "song", and opsis or "spectacle". "Although Aristotle's Poetics is universally acknowledged in the Western critical tradition", Marvin Carlson explains, "almost every detail about his seminal work has aroused divergent opinions." Important theatre practitioners of the 20th century include Konstantin Stanislavski, Vsevolod Meyerhold, Jacques Copeau, Edward Gordon Craig, Bertolt Brecht, Antonin Artaud, Joan Littlewood, Peter Brook, Jerzy Grotowski, Augusto Boal, Eugenio Barba, Dario Fo, Viola Spolin, Keith Johnstone and Robert Wilson (director). Stanislavski treated the theatre as an art-form that is autonomous from literature and one in which the playwright's contribution should be respected as that of only one of an ensemble of creative artists. His innovative contribution to modern acting theory has remained at the core of mainstream western performance training for much of the last century. That many of the precepts of his system of actor training seem to be common sense and self-evident testifies to its hegemonic success. Actors frequently employ his basic concepts without knowing they do so. Thanks to its promotion and elaboration by acting teachers who were former students and the many translations of his theoretical writings, Stanislavski's 'system' acquired an unprecedented ability to cross cultural boundaries and developed an international reach, dominating debates about acting in Europe and the United States. Many actors routinely equate his 'system' with the North American Method, although the latter's exclusively psychological techniques contrast sharply with Stanislavski's multivariant, holistic and psychophysical approach, which explores character and action both from the 'inside out' and the 'outside in' and treats the actor's mind and body as parts of a continuum. Technical aspects Theatre presupposes collaborative modes of production and a collective form of reception. The structure of dramatic texts, unlike other forms of literature, is directly influenced by this collaborative production and collective reception. The production of plays usually involves contributions from a playwright, director, a cast of actors, and a technical production team that includes a scenic or set designer, lighting designer, costume designer, sound designer, stage manager, production manager and technical director. Depending on the production, this team may also include a composer, dramaturg, video designer or fight director. Stagecraft is a generic term referring to the technical aspects of theatrical, film, and video production. It includes, but is not limited to, constructing and rigging scenery, hanging and focusing of lighting, design and procurement of costumes, makeup, procurement of props, stage management, and recording and mixing of sound. Stagecraft is distinct from the wider umbrella term of scenography. Considered a technical rather than an artistic field, it relates primarily to the practical implementation of a designer's artistic vision. In its most basic form, stagecraft is managed by a single person (often the stage manager of a smaller production) who arranges all scenery, costumes, lighting, and sound, and organizes the cast. At a more professional level, for example in modern Broadway houses, stagecraft is managed by hundreds of skilled carpenters, painters, electricians, stagehands, stitchers, wigmakers, and the like. This modern form of stagecraft is highly technical and specialized: it comprises many subdisciplines and a vast trove of history and tradition. The majority of stagecraft lies between these two extremes. Regional theatres and larger community theatres will generally have a technical director and a complement of designers, each of whom has a direct hand in their respective designs. Subcategories and organisation There are many modern theatre movements which produce theatre in a variety of ways. Theatrical enterprises vary enormously in sophistication and purpose. People who are involved vary from novices and hobbyists (in community theatre) to professionals (in Broadway and similar productions). Theatre can be performed with a shoestring budget or on a grand scale with multimillion-dollar budgets. This diversity manifests in the abundance of theatre subcategories, which include: Broadway theatre and West End theatre Community theatre Dinner theater Fringe theatre Immersive theater Interactive theatre Off-Broadway and Off West End Off-off-Broadway Playback theatre Regional theatre in the United States Site-specific theatre Street theatre Summer stock theatre Theatre and disability Touring theatre Repertory companies While most modern theatre companies rehearse one piece of theatre at a time, perform that piece for a set "run", retire the piece, and begin rehearsing a new show, repertory companies rehearse multiple shows at one time. These companies are able to perform these various pieces upon request and often perform works for years before retiring them. Most dance companies operate on this repertory system. The Royal National Theatre in London performs on a repertory system. Repertory theatre generally involves a group of similarly accomplished actors, and relies more on the reputation of the group than on an individual star actor. It also typically relies less on strict control by a director and less on adherence to theatrical conventions, since actors who have worked together in multiple productions can respond to each other without relying as much on convention or external direction. Other terminology A theatre company is an organisation that produces theatrical performances, as distinct from a theatre troupe (or acting company), which is a group of theatrical performers working together. A touring company is an independent theatre or dance company that travels, often internationally, being presented at a different theatre venue in each city. In order to put on a piece of theatre, both a theatre company and a theatre venue are needed. When a theatre company is the sole company in residence at a theatre venue, this theatre (and its corresponding theatre company) are called a resident theatre or a producing theatre, because the venue produces its own work. Other theatre companies, as well as dance companies, who do not have their own theatre venue, perform at rental theatres or at presenting theatres. Both rental and presenting theatres have no full-time resident companies. They do, however, sometimes have one or more part-time resident companies, in addition to other independent partner companies who arrange to use the space when available. A rental theatre allows the independent companies to seek out the space, while a presenting theatre seeks out the independent companies to support their work by presenting them on their stage. Some performance groups perform in non-theatrical spaces. Such performances can take place outside or inside, in a non-traditional performance space, and include street theatre, and site-specific theatre. Non-traditional venues can be used to create more immersive or meaningful environments for audiences. They can sometimes be modified more heavily than traditional theatre venues, or can accommodate different kinds of equipment, lighting and sets. Unions There are many theatre unions, including: Actors' Equity Association (AEA), for actors and stage managers in the U.S.) Canadian Actors' Equity Association, for actors in Canada Equity, for many kind of performing artists as well as designers, directors, and stage managers in the UK International Alliance of Theatrical Stage Employees (IATSE), for designers and technicians). Media, Entertainment and Arts Alliance, an Australian union created in 1992 as a merger of the unions covering actors, journalists and entertainment industry employees Stage Directors and Choreographers Society (SDC) See also Explanatory notes Citations General sources Further reading External links Theatre Archive Project (UK) British Library & University of Sheffield. University of Bristol Theatre Collection Music Hall and Theatre History of Britain and Ireland Department of Theatre Arts Design Portfolios

Architecture is the art and technique of designing and building, as distinguished from the skills associated with construction. It is both the process and the product of sketching, conceiving, planning, designing, and constructing buildings or other structures. The term comes from Latin architectura; from Ancient Greek ἀρχιτέκτων (arkhitéktōn) 'architect'; from ἀρχι- (arkhi-) 'chief' and τέκτων (téktōn) 'creator'. Architectural works, in the material form of buildings, are often perceived as cultural symbols and as works of art. Historical civilizations are often identified with their surviving architectural achievements. The practice, which began in the prehistoric era, has been used as a way of expressing culture by civilizations on all seven continents. For this reason, architecture is considered to be a form of art. Texts on architecture have been written since ancient times. The earliest surviving text on architectural theories is the 1st century BC treatise De architectura by the Roman architect Vitruvius, according to whom a good building embodies firmitas, utilitas, and venustas (durability, utility, and beauty). Centuries later, Leon Battista Alberti developed his ideas further, seeing beauty as an objective quality of buildings to be found in their proportions. In the 19th century, Louis Sullivan declared that "form follows function". "Function" began to replace the classical "utility" and was understood to include not only practical but also aesthetic, psychological, and cultural dimensions. The idea of sustainable architecture was introduced in the late 20th century. Architecture began as rural, oral vernacular architecture that developed from trial and error to successful replication. Ancient urban architecture was preoccupied with building religious structures and buildings symbolizing the political power of rulers until Greek and Roman architecture shifted focus to civic virtues. Indian and Chinese architecture influenced forms all over Asia and Buddhist architecture in particular took diverse local flavors. During the Middle Ages, pan-European styles of Romanesque and Gothic cathedrals and abbeys emerged while the Renaissance favored Classical forms implemented by architects known by name. Later, the roles of architects and engineers became separated. Modern architecture began after World War I as an avant-garde movement that sought to develop a completely new style appropriate for a new post-war social and economic order focused on meeting the needs of the middle and working classes. Emphasis was put on modern techniques, materials, and simplified geometric forms, paving the way for high-rise superstructures. Many architects became disillusioned with modernism which they perceived as ahistorical and anti-aesthetic, and postmodern and contemporary architecture developed. Over the years, the field of architectural construction has branched out to include everything from ship design to interior decorating. Definitions Architecture can mean: A general term to describe buildings and other physical structures. The art and science of designing buildings and (some) nonbuilding structures; sometimes called "architectonics." The style of design and method of construction of buildings and other physical structures. A unifying or coherent form or structure. The knowledge of art, science, technology, and humanity. The design activity of the architect, from the macro-level (urban design, landscape architecture) to the micro-level (construction details and furniture). The practice of the architect where architecture means offering or rendering professional services in connection with the design and construction of buildings or built environments. Theory The philosophy of architecture is a branch of the philosophy of art, dealing with aesthetic value of architecture, its semantics and its relation to the development of culture. Many philosophers and theoreticians from Plato to Michel Foucault, Gilles Deleuze, Robert Venturi and Ludwig Wittgenstein have concerned themselves with the nature of architecture and whether or not architecture is distinguished from building. Historic treatises The earliest surviving written work on the subject of architecture is De architectura by the Roman architect Vitruvius in the early 1st century BC. According to Vitruvius, a good building should satisfy the three principles of firmitas, utilitas, venustas, commonly known by the original translation – firmness, commodity and delight. An equivalent in modern English would be: Durability – a building should stand up robustly and remain in good condition Utility – it should be suitable for the purposes for which it is used Beauty – it should be aesthetically pleasing According to Vitruvius, the architect should strive to fulfill each of these three attributes as well as possible. Leon Battista Alberti, who elaborates on the ideas of Vitruvius in his treatise, De re aedificatoria, saw beauty primarily as a matter of proportion, although ornament also played a part. For Alberti, the rules of proportion were those that governed the idealized human figure, the golden mean. The most important aspect of beauty was, therefore, an inherent part of an object, rather than something applied superficially, and was based on universal, recognizable truths. The notion of style in the arts was not developed until the 16th century, with the writing of Giorgio Vasari. By the 18th century, his Lives of the Most Excellent Painters, Sculptors, and Architects had been translated into Italian, French, Spanish, and English. In the 16th century, Italian Mannerist architect, painter and theorist Sebastiano Serlio wrote Tutte L'Opere D'Architettura et Prospetiva (Complete Works on Architecture and Perspective). This treatise exerted immense influence throughout Europe, being the first handbook that emphasized the practical rather than the theoretical aspects of architecture, and it was the first to catalog the five orders. In the early 19th century, Augustus Welby Northmore Pugin wrote Contrasts (1836) that, as the title suggested, contrasted the modern, industrial world, which he disparaged, with an idealized image of neo-medieval world. Gothic architecture, Pugin believed, was the only "true Christian form of architecture." The 19th-century English art critic, John Ruskin, in his Seven Lamps of Architecture, published 1849, was much narrower in his view of what constituted architecture. Architecture was the "art which so disposes and adorns the edifices raised by men ... that the sight of them" contributes "to his mental health, power, and pleasure". For Ruskin, the aesthetic was of overriding significance. His work goes on to state that a building is not truly a work of architecture unless it is in some way "adorned". For Ruskin, a well-constructed, well-proportioned, functional building needed string courses or rustication, at the very least. On the difference between the ideals of architecture and mere construction, the 20th-century architect Le Corbusier wrote: "You employ stone, wood, and concrete, and with these materials you build houses and palaces: that is construction. Ingenuity is at work. But suddenly you touch my heart, you do me good. I am happy and I say: This is beautiful. That is Architecture". Le Corbusier's contemporary Ludwig Mies van der Rohe is said to have stated in a 1959 interview that "architecture starts when you carefully put two bricks together. There it begins." Modern concepts The notable 19th-century architect of skyscrapers, Louis Sullivan, promoted an overriding precept to architectural design: "Form follows function". While the notion that structural and aesthetic considerations should be entirely subject to functionality was met with both popularity and skepticism, it had the effect of introducing the concept of "function" in place of Vitruvius' "utility". "Function" came to be seen as encompassing all criteria of the use, perception and enjoyment of a building, not only practical but also aesthetic, psychological and cultural. Nunzia Rondanini stated, "Through its aesthetic dimension architecture goes beyond the functional aspects that it has in common with other human sciences. Through its own particular way of expressing values, architecture can stimulate and influence social life without presuming that, in and of itself, it will promote social development.... To restrict the meaning of (architectural) formalism to art for art's sake is not only reactionary; it can also be a purposeless quest for perfection or originality which degrades form into a mere instrumentality". The aesthetics of architecture remain a contested topic, with critics highlighting the disconnect between professionals and the public. Studies generally find that there is a strong public preference for traditional and classical architectural styles over modernist designs. James Stevens Curl argues that modernist architects often favour designs that are alienating and environmentally damaging. Léon Krier frames the dominance of traditional styles in private architecture as an "overwhelming democratic reality," contrasting with the prevalence of modernist designs in public commissions. Among the philosophies that have influenced modern architects and their approach to building design are Rationalism, Empiricism, Structuralism, Poststructuralism, Deconstruction and Phenomenology. In the late 20th century a new concept was added to those included in the compass of both structure and function, the consideration of sustainability, hence sustainable architecture. To satisfy the contemporary ethos, a building should be constructed in a manner which is environmentally friendly in terms of the production of its materials, its impact upon the natural and built environment of its surrounding area and the demands that it makes upon the natural environment for heating, ventilation and cooling, water use, waste products and lighting. History Origins and vernacular architecture Building first evolved out of the dynamics between needs (e.g. shelter, security, and worship) and means (available building materials and attendant skills). As human cultures developed and knowledge began to be formalized through oral traditions and practices, building became a craft, and architecture became the term used to describe the highly formalized and respected aspects of the craft. It is widely assumed that architectural success was achieved through trial and error, with progressively less trial and more replication as results became satisfactory over time. Vernacular architecture continues to be produced in many parts of the world. Prehistoric architecture Early human settlements were mostly rural. Expanding economies resulted in the creation of proto-cities or urban areas, which in some cases grew and evolved very rapidly, such as Çatalhöyük in modern-day Turkey and Mohenjo-daro in modern-day Pakistan. Neolithic archaeological sites include Göbekli Tepe and Çatalhöyük in Turkey, Jericho in the Levant, Mehrgarh in Pakistan, Skara Brae in Orkney, and Cucuteni-Trypillian culture settlements in Romania, Moldova and Ukraine. Classical era In many ancient civilizations, such as those of Egypt and Mesopotamia, architecture and urbanism reflected the constant engagement with the divine and the supernatural, and many ancient cultures resorted to monumentality in their architecture to symbolically represent the political power of the ruler or the state itself. The architecture and urbanism of classical civilizations such as the Greek and Roman civilizations evolved from civic ideals rather than religious or empirical ones. New building types emerged and architectural style developed in the form of the classical orders. Roman architecture was influenced by Greek architecture as they incorporated many Greek elements into their building practices. Texts on architecture have been written since ancient times—these texts provided both general advice and specific formal prescriptions or canons. Some examples of canons are found in the writings of Vitruvius in the 1st century BC. Some of the most important early examples of canonic architecture are religious. Asian architecture Asian architecture developed differently from European architecture, and the Buddhist, Hindu and Sikh architectural styles have different characteristics. Unlike Indian and Chinese architecture, which had great influence on the surrounding regions, Japanese architecture did not. Some Asian architecture showed great regional diversity, in particular Buddhist architecture. Moreover, another architectural achievement in Asia is the Hindu temple architecture, which developed from around the 5th century CE. It is, in theory, governed by concepts laid down in the Shastras, and is concerned with expressing both the macrocosm and the microcosm. In many Asian countries, pantheistic religion led to architectural forms that were designed specifically to enhance the natural landscape. Also, the grandest houses were relatively lightweight structures mainly using wood until recent times, and there are few survivals of great age. Buddhism was associated with a move to stone and brick religious structures, probably beginning as rock-cut architecture, which has often survived very well. Early Asian writings on architecture include the Kao Gong Ji of China from the 7th–5th centuries BC, the Shilpa Shastras of ancient India, Manjusri Vasthu Vidya Sastra of Sri Lanka, and Araniko of Nepal . Islamic architecture Islamic architecture began in the 7th century, incorporating architectural forms from the ancient Middle East and Byzantium, but also developing features to suit the religious and social needs of the society. Examples can be found throughout the Middle East, Turkey, North Africa, the Indian Sub-continent and in parts of Europe, such as Spain, Albania, and the Balkan States, as the result of the expansion of the Ottoman Empire. European medieval architecture In Europe during the Medieval period, guilds were formed by craftsmen to organize their trades and written contracts have survived, particularly in relation to ecclesiastical buildings. The role of architect was usually one with that of master mason, or Magister lathomorum as they are sometimes described in contemporary documents. The major architectural undertakings were the buildings of abbeys and cathedrals. From about 900 onward, the movements of both clerics and tradesmen carried architectural knowledge across Europe, resulting in the pan-European styles Romanesque and Gothic. Also, a significant part of the Middle Ages architectural heritage is numerous fortifications across the continent. From the Balkans to Spain, and from Malta to Estonia, these buildings represent an important part of European heritage. Renaissance architecture In Renaissance Europe, from about 1400 onwards, there was a revival of Classical learning accompanied by the development of Renaissance humanism, which placed greater emphasis on the role of the individual in society than had been the case during the Medieval period. Buildings were ascribed to specific architects – Brunelleschi, Alberti, Michelangelo, Palladio – and the cult of the individual had begun. There was still no dividing line between artist, architect, engineer, or any of the related vocations, and the appellation was often one of regional preference. A revival of the Classical style in architecture was accompanied by a burgeoning of science and engineering, which affected the proportions and structure of buildings. At this stage, it was still possible for an artist to design a bridge as the level of structural calculations involved was within the scope of the generalist. Early modern and the industrial age With the emerging knowledge in scientific fields and the rise of new materials and technology, architecture and engineering began to separate, and the architect began to concentrate on aesthetics and the humanist aspects, often at the expense of technical aspects of building design. There was also the rise of the "gentleman architect" who usually dealt with wealthy clients and concentrated predominantly on visual qualities derived usually from historical prototypes, typified by the many country houses of Great Britain that were created in the Neo Gothic or Scottish baronial styles. Formal architectural training in the 19th century, for example, at École des Beaux-Arts in France, gave much emphasis to the production of beautiful drawings and little to context and feasibility. Meanwhile, the Industrial Revolution laid open the door for mass production and consumption. Aesthetics became a criterion for the middle class as ornamented products, once within the province of expensive craftsmanship, became cheaper under machine production. Vernacular architecture became increasingly ornamental. Housebuilders could use current architectural design in their work by combining features found in pattern books and architectural journals. Modernism Around the beginning of the 20th century, general dissatisfaction with the emphasis on revivalist architecture and elaborate decoration gave rise to many new lines of thought that served as precursors to Modern architecture. Notable among these is the Deutscher Werkbund, formed in 1907 to produce better quality machine-made objects. The rise of the profession of industrial design is usually placed here. Following this lead, the Bauhaus school, founded in Weimar, Germany in 1919, redefined the architectural bounds prior set throughout history, viewing the creation of a building as the ultimate synthesis – the apex – of art, craft, and technology. When modern architecture was first practiced, it was an avant-garde movement with moral, philosophical, and aesthetic underpinnings. Immediately after World War I, pioneering modernist architects sought to develop a completely new style appropriate for a new post-war social and economic order focused on meeting the needs of the middle and working classes. They rejected the architectural practice of the academic refinement of historical styles which served the rapidly declining aristocratic order. The approach of the Modernist architects was to reduce buildings to pure forms, removing historical references and ornament in favor of functional details. Buildings displayed their functional and structural elements, exposing steel beams and concrete surfaces instead of hiding them behind decorative forms. Architects such as Frank Lloyd Wright developed organic architecture, in which the form was defined by its environment and purpose, with an aim to promote harmony between human habitation and the natural world with prime examples being Robie House and Fallingwater. Architects such as Mies van der Rohe, Philip Johnson, and Marcel Breuer worked to create beauty based on the inherent qualities of building materials and modern construction techniques, trading traditional historic forms for simplified geometric forms, celebrating the new means and methods made possible by the Industrial Revolution, including steel-frame construction, which gave birth to high-rise superstructures. Fazlur Rahman Khan's development of the tube structure was a technological breakthrough in building ever higher. By mid-century, Modernism had morphed into the International Style, an aesthetic epitomized in many ways by the Twin Towers of New York's World Trade Center designed by Minoru Yamasaki. Postmodernism Many architects resisted modernism, finding it devoid of the decorative richness of historical styles. As the first generation of modernists began to die after World War II, the second generation of architects including Paul Rudolph, Marcel Breuer, and Eero Saarinen tried to expand the aesthetics of modernism with Brutalism, buildings with expressive sculpture façades made of unfinished concrete. But an even younger postwar generation critiqued modernism and Brutalism for being too austere, standardized, monotone, and not taking into account the richness of human experience offered in historical buildings across time and in different places and cultures. One such reaction to the cold aesthetic of modernism and Brutalism is the school of metaphoric architecture, which includes such things as bio morphism and zoomorphic architecture, both using nature as the primary source of inspiration and design. While it is considered by some to be merely an aspect of postmodernism, others consider it to be a school in its own right and a later development of expressionist architecture. Beginning in the late 1950s and 1960s, architectural phenomenology emerged as an important movement in the early reaction against modernism, with architects like Charles Moore in the United States, Christian Norberg-Schulz in Norway, and Ernesto Nathan Rogers, Vittorio Gregotti, Michele Valori, and Bruno Zevi in Italy, who collectively popularized an interest in a new contemporary architecture aimed at expanding human experience using historical buildings as models and precedents. Postmodernism produced a style that combined contemporary building technology and cheap materials with the aesthetics of older pre-modern and non-modern styles, from high classical architecture to popular or vernacular regional building styles. Robert Venturi famously defined postmodern architecture as a "decorated shed" (an ordinary building which is functionally designed inside and embellished on the outside) and upheld it against modernist and brutalist "ducks" (buildings with unnecessarily expressive tectonic forms). Architecture today Since the 1980s, as the complexity of buildings began to increase (in terms of structural systems, services, energy and technologies), the field of architecture became multi-disciplinary with specializations for each project type, technological expertise or project delivery methods. Moreover, there has been an increased separation of the 'design' architect from the 'project' architect who ensures that the project meets the required standards and deals with matters of liability. The preparatory processes for the design of any large building have become increasingly complicated, and require preliminary studies of such matters as durability, sustainability, quality, money, and compliance with local laws. A large structure can no longer be the design of one person but must be the work of many. Modernism and Postmodernism have been criticized by some members of the architectural profession who feel that successful architecture is not a personal, philosophical, or aesthetic pursuit by individualists; rather it has to consider everyday needs of people and use technology to create livable environments, with the design process being informed by studies of behavioral, environmental, and social sciences. Environmental sustainability has become a mainstream issue, with a profound effect on the architectural profession. Many developers, those who support the financing of buildings, have become educated to encourage the facilitation of environmentally sustainable design, rather than solutions based primarily on immediate cost. Major examples of this can be found in passive solar building design, greener roof designs, biodegradable materials, and more attention to a structure's energy usage. This major shift in architecture has also changed architecture schools to focus more on the environment. There has been an acceleration in the number of buildings that seek to meet green building sustainable design principles. Sustainable practices that were at the core of vernacular architecture increasingly provide inspiration for environmentally and socially sustainable contemporary techniques. The U.S. Green Building Council's LEED (Leadership in Energy and Environmental Design) rating system has been influential in this. Concurrently, the recent movements of New Urbanism, Metaphoric architecture, contemporary Traditional architecture and New Classical architecture promote a sustainable approach towards construction that appreciates and develops smart growth, architectural tradition, and classical design. This in contrast to modernist and globally uniform architecture, as well as leaning against solitary housing estates and suburban sprawl. Glass curtain walls, which were the hallmark of the ultra modern urban life in many countries, surfaced even in developing countries like Nigeria where international styles had been represented since the mid-20th Century, mostly because of the leanings of foreign-trained architects. Types Residential architecture Residential architecture is the design which functionally fits the user's lifestyle while adhering to the building codes and zoning laws. Commercial architecture Commercial architecture is the design of commercial buildings that serves the needs of businesses, the government, and religious institutions. Industrial architecture Industrial architecture is the design of specialized industrial buildings, whose primary focus is designing buildings that can fulfil their function while ensuring the safe movement of labor and goods in the facility. Landscape architecture Landscape architecture is the design of outdoor public areas, landmarks, and structures to achieve environmental, social-behavioral, or aesthetic outcomes. It involves the systematic investigation of existing social, ecological, and soil conditions and processes in the landscape and the design of interventions that will produce the desired outcome. The scope of the profession includes landscape design, site planning, stormwater management, environmental restoration, parks and recreation planning, visual resource management, green infrastructure planning and provision, and private estate and residence landscape master planning and design all at varying scales of design, planning, and management. A practitioner in the profession of landscape architecture is called a landscape architect. Interior architecture Interior architecture is the design of a space which has been created by structural boundaries and the human interaction within these boundaries. It can also be the initial design and plan for use, then later redesigned to accommodate a changed purpose or a significantly revised design for adaptive reuse of the building shell. The latter is often part of sustainable architecture practices, conserving resources through "recycling" a structure by adaptive redesign. Generally referred to as the spatial art of environmental design, form and practice, interior architecture is the process through which the interiors of buildings are designed, concerned with all aspects of the human uses of structural spaces. Urban design Urban design is the process of designing and shaping the physical features of cities, towns, and villages. In contrast to architecture, which focuses on the design of individual buildings, urban design deals with the larger scale of groups of buildings, streets and public spaces, whole neighborhoods and districts, and entire cities, with the goal of making urban areas functional, attractive, and sustainable. Urban design is an interdisciplinary field that uses elements of many built environment professions, including landscape architecture, urban planning, architecture, civil engineering and municipal engineering. It is common for professionals in all these disciplines to practice urban design. In more recent times different sub-subfields of urban design have emerged such as strategic urban design, landscape urbanism, water-sensitive urban design, and sustainable urbanism. Other types of architecture Naval architecture Naval architecture, also known as naval engineering, is an engineering discipline dealing with the engineering design process, shipbuilding, maintenance, and operation of marine vessels and structures. Naval architecture involves basic and applied research, design, development, design evaluation, and calculations during all stages of the life of a marine vehicle. Preliminary design of the vessel, its detailed design, construction, trials, operation and maintenance, launching, and dry-docking are the main activities involved. Ship design calculations are also required for ships being modified (by means of conversion, rebuilding, modernization, or repair). Naval architecture also involves the formulation of safety regulations and damage control rules and the approval and certification of ship designs to meet statutory and non-statutory requirements. Metaphorical "architectures" "Architecture" is used as a metaphor for many modern techniques or fields for structuring abstractions. These include: Computer architecture, a set of rules and methods that describe the functionality, organization, and implementation of computer systems with software architecture, hardware architecture and network architecture covering more specific aspects. Business architecture, defined as "a blueprint of the enterprise that provides a common understanding of the organization and is used to align strategic objectives and tactical demands.” Enterprise architecture is another term. Cognitive architecture theories about the structure of the human mind. System architecture, a conceptual model that defines the structure, behavior, and more views of any type of system. Seismic architecture The term 'seismic architecture' or 'earthquake architecture' was first introduced in 1985 by Robert Reitherman. The phrase "earthquake architecture" is used to describe a degree of architectural expression of earthquake resistance or implication of architectural configuration, form, or style in earthquake resistance. It is also used to describe buildings in which seismic design considerations impacted its architecture. It may be considered a new aesthetic approach in designing structures in seismic prone areas. The wide breadth of expressive possibilities ranges from metaphorical uses of seismic issues to the more straightforward exposure of seismic technology. While outcomes of an earthquake architecture can be very diverse in their physical manifestations, architectural expression of seismic principles can also take many forms and levels of sophistication. See also Architectural design competition Architectural engineering Architectural technology Ephemeral architecture Index of architecture articles List of BIM software Outline of architecture Reverse architecture Timeline of architecture Notes References External links World Architecture Community Architecture.com, published by Royal Institute of British Architects Architectural centers and museums in the world, list of links from the UIA American Institute of Architects Glossary of Architectural Terms Archived 28 August 2021 at the Wayback Machine Cities and Buildings Database – Collection of digitized images of buildings and cities drawn from across time and throughout the world from the University of Washington Library "Architecture and Power", BBC Radio 4 discussion with Adrian Tinniswood, Gillian Darley and Gavin Stamp (In Our Time, Oct. 31, 2002)



